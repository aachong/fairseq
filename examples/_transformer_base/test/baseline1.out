2020-11-13 12:54:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-11-13 12:54:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-11-13 12:54:11 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='./examples/_transformer_base/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/_transformer_base/checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='test', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-11-13 12:54:11 | INFO | fairseq.tasks.translation | [ch] dictionary: 41952 types
2020-11-13 12:54:11 | INFO | fairseq.tasks.translation | [en] dictionary: 31264 types
2020-11-13 12:54:11 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/data-bin/valid.ch-en.ch
2020-11-13 12:54:11 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/data-bin/valid.ch-en.en
2020-11-13 12:54:11 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/data-bin valid ch-en 1664 examples
2020-11-13 12:54:13 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(41952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(31264, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=31264, bias=False)
  )
)
2020-11-13 12:54:13 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-11-13 12:54:13 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-11-13 12:54:13 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2020-11-13 12:54:13 | INFO | fairseq_cli.train | num. model params: 97632256 (num. trained: 97632256)
2020-11-13 12:54:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-11-13 12:54:17 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-11-13 12:54:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-11-13 12:54:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-11-13 12:54:17 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-11-13 12:54:17 | INFO | fairseq.trainer | loaded checkpoint ./examples/_transformer_base/checkpoints/checkpoint_last.pt (epoch 80 @ 0 updates)
2020-11-13 12:54:17 | INFO | fairseq.optim.adam | using FusedAdam
2020-11-13 12:54:17 | INFO | fairseq.trainer | loading train data for epoch 80
2020-11-13 12:54:17 | INFO | fairseq.data.data_utils | loaded 878 examples from: ./examples/_transformer_base/data-bin/test.ch-en.ch
2020-11-13 12:54:17 | INFO | fairseq.data.data_utils | loaded 878 examples from: ./examples/_transformer_base/data-bin/test.ch-en.en
2020-11-13 12:54:17 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/data-bin test ch-en 878 examples
111
2020-11-13 12:54:17 | INFO | fairseq.trainer | begin training epoch 81
2020-11-13 12:54:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:55:00 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.509 | nll_loss 8.462 | ppl 352.55 | bleu 15.4 | wps 1578 | wpb 2046.5 | bsz 53.7 | num_updates 12
2020-11-13 12:55:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:55:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 81 @ 12 updates, score 9.509) (writing took 8.021878230385482 seconds)
2020-11-13 12:55:08 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2020-11-13 12:55:08 | INFO | train | epoch 081 | loss 4.454 | nll_loss 2.871 | ppl 7.32 | wps 575.8 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 12 | lr 1.5997e-06 | gnorm 2.813 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:55:08 | INFO | fairseq.trainer | begin training epoch 82
2020-11-13 12:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:55:51 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.481 | nll_loss 8.432 | ppl 345.36 | bleu 15.61 | wps 1592.9 | wpb 2046.5 | bsz 53.7 | num_updates 24 | best_loss 9.481
2020-11-13 12:55:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:55:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 82 @ 24 updates, score 9.481) (writing took 8.04360331594944 seconds)
2020-11-13 12:55:59 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2020-11-13 12:55:59 | INFO | train | epoch 082 | loss 4.428 | nll_loss 2.843 | ppl 7.17 | wps 604 | ups 0.24 | wpb 2536.6 | bsz 73.2 | num_updates 24 | lr 3.0994e-06 | gnorm 2.754 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:55:59 | INFO | fairseq.trainer | begin training epoch 83
2020-11-13 12:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:56:44 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.439 | nll_loss 8.387 | ppl 334.79 | bleu 15.92 | wps 1504.8 | wpb 2046.5 | bsz 53.7 | num_updates 36 | best_loss 9.439
2020-11-13 12:56:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:56:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 83 @ 36 updates, score 9.439) (writing took 8.441220487467945 seconds)
2020-11-13 12:56:52 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2020-11-13 12:56:52 | INFO | train | epoch 083 | loss 4.393 | nll_loss 2.805 | ppl 6.99 | wps 572.8 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 36 | lr 4.5991e-06 | gnorm 2.688 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:56:52 | INFO | fairseq.trainer | begin training epoch 84
2020-11-13 12:56:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:57:37 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.386 | nll_loss 8.332 | ppl 322.18 | bleu 16.09 | wps 1501.7 | wpb 2046.5 | bsz 53.7 | num_updates 48 | best_loss 9.386
2020-11-13 12:57:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:57:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 84 @ 48 updates, score 9.386) (writing took 8.117920287884772 seconds)
2020-11-13 12:57:45 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2020-11-13 12:57:45 | INFO | train | epoch 084 | loss 4.34 | nll_loss 2.748 | ppl 6.72 | wps 572.8 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 48 | lr 6.0988e-06 | gnorm 2.605 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:57:45 | INFO | fairseq.trainer | begin training epoch 85
2020-11-13 12:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:58:29 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.33 | nll_loss 8.273 | ppl 309.32 | bleu 16.36 | wps 1526.7 | wpb 2046.5 | bsz 53.7 | num_updates 60 | best_loss 9.33
2020-11-13 12:58:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:58:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 85 @ 60 updates, score 9.33) (writing took 8.266214528121054 seconds)
2020-11-13 12:58:37 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2020-11-13 12:58:37 | INFO | train | epoch 085 | loss 4.271 | nll_loss 2.675 | ppl 6.39 | wps 581.4 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 60 | lr 7.5985e-06 | gnorm 2.509 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:58:37 | INFO | fairseq.trainer | begin training epoch 86
2020-11-13 12:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 12:59:25 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.276 | nll_loss 8.217 | ppl 297.53 | bleu 16.41 | wps 1429.1 | wpb 2046.5 | bsz 53.7 | num_updates 72 | best_loss 9.276
2020-11-13 12:59:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 12:59:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 86 @ 72 updates, score 9.276) (writing took 8.13755190744996 seconds)
2020-11-13 12:59:33 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2020-11-13 12:59:33 | INFO | train | epoch 086 | loss 4.227 | nll_loss 2.628 | ppl 6.18 | wps 550.9 | ups 0.22 | wpb 2536.6 | bsz 73.2 | num_updates 72 | lr 9.0982e-06 | gnorm 2.406 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 12:59:33 | INFO | fairseq.trainer | begin training epoch 87
2020-11-13 12:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:00:17 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.229 | nll_loss 8.169 | ppl 287.72 | bleu 16.5 | wps 1517.7 | wpb 2046.5 | bsz 53.7 | num_updates 84 | best_loss 9.229
2020-11-13 13:00:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:00:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 87 @ 84 updates, score 9.229) (writing took 8.052613561041653 seconds)
2020-11-13 13:00:25 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2020-11-13 13:00:25 | INFO | train | epoch 087 | loss 4.163 | nll_loss 2.56 | ppl 5.9 | wps 578.6 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 84 | lr 1.05979e-05 | gnorm 2.347 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:00:25 | INFO | fairseq.trainer | begin training epoch 88
2020-11-13 13:00:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:01:10 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.185 | nll_loss 8.123 | ppl 278.69 | bleu 16.54 | wps 1523.2 | wpb 2046.5 | bsz 53.7 | num_updates 96 | best_loss 9.185
2020-11-13 13:01:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:01:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 88 @ 96 updates, score 9.185) (writing took 8.037904934957623 seconds)
2020-11-13 13:01:18 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2020-11-13 13:01:18 | INFO | train | epoch 088 | loss 4.09 | nll_loss 2.48 | ppl 5.58 | wps 582.3 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 96 | lr 1.20976e-05 | gnorm 2.256 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:01:18 | INFO | fairseq.trainer | begin training epoch 89
2020-11-13 13:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:02:02 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.152 | nll_loss 8.087 | ppl 271.85 | bleu 16.52 | wps 1519.1 | wpb 2046.5 | bsz 53.7 | num_updates 108 | best_loss 9.152
2020-11-13 13:02:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:02:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 89 @ 108 updates, score 9.152) (writing took 8.155323181301355 seconds)
2020-11-13 13:02:10 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2020-11-13 13:02:10 | INFO | train | epoch 089 | loss 4.013 | nll_loss 2.396 | ppl 5.26 | wps 579.7 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 108 | lr 1.35973e-05 | gnorm 2.2 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:02:10 | INFO | fairseq.trainer | begin training epoch 90
2020-11-13 13:02:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:02:54 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.126 | nll_loss 8.058 | ppl 266.44 | bleu 16.63 | wps 1537.5 | wpb 2046.5 | bsz 53.7 | num_updates 120 | best_loss 9.126
2020-11-13 13:02:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:03:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 90 @ 120 updates, score 9.126) (writing took 8.21083430852741 seconds)
2020-11-13 13:03:02 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2020-11-13 13:03:02 | INFO | train | epoch 090 | loss 3.945 | nll_loss 2.317 | ppl 4.98 | wps 584.6 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 120 | lr 1.5097e-05 | gnorm 2.148 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:03:02 | INFO | fairseq.trainer | begin training epoch 91
2020-11-13 13:03:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:03:47 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.104 | nll_loss 8.033 | ppl 261.91 | bleu 16.6 | wps 1507.9 | wpb 2046.5 | bsz 53.7 | num_updates 132 | best_loss 9.104
2020-11-13 13:03:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:03:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 91 @ 132 updates, score 9.104) (writing took 8.131218271329999 seconds)
2020-11-13 13:03:55 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2020-11-13 13:03:55 | INFO | train | epoch 091 | loss 3.863 | nll_loss 2.224 | ppl 4.67 | wps 577.1 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 132 | lr 1.65967e-05 | gnorm 2.117 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:03:55 | INFO | fairseq.trainer | begin training epoch 92
2020-11-13 13:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:04:39 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.087 | nll_loss 8.012 | ppl 258.17 | bleu 16.51 | wps 1549.3 | wpb 2046.5 | bsz 53.7 | num_updates 144 | best_loss 9.087
2020-11-13 13:04:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:04:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 92 @ 144 updates, score 9.087) (writing took 7.961994078010321 seconds)
2020-11-13 13:04:47 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2020-11-13 13:04:47 | INFO | train | epoch 092 | loss 3.778 | nll_loss 2.127 | ppl 4.37 | wps 589.1 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 144 | lr 1.80964e-05 | gnorm 2.072 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:04:47 | INFO | fairseq.trainer | begin training epoch 93
2020-11-13 13:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:05:32 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.074 | nll_loss 7.996 | ppl 255.35 | bleu 16.48 | wps 1481.9 | wpb 2046.5 | bsz 53.7 | num_updates 156 | best_loss 9.074
2020-11-13 13:05:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:05:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 93 @ 156 updates, score 9.074) (writing took 8.249810533598065 seconds)
2020-11-13 13:05:40 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2020-11-13 13:05:40 | INFO | train | epoch 093 | loss 3.7 | nll_loss 2.037 | ppl 4.1 | wps 567.5 | ups 0.22 | wpb 2536.6 | bsz 73.2 | num_updates 156 | lr 1.95961e-05 | gnorm 2.012 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:05:40 | INFO | fairseq.trainer | begin training epoch 94
2020-11-13 13:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:06:25 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.067 | nll_loss 7.986 | ppl 253.53 | bleu 16.51 | wps 1515.1 | wpb 2046.5 | bsz 53.7 | num_updates 168 | best_loss 9.067
2020-11-13 13:06:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:06:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 94 @ 168 updates, score 9.067) (writing took 8.250358021818101 seconds)
2020-11-13 13:06:33 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2020-11-13 13:06:33 | INFO | train | epoch 094 | loss 3.631 | nll_loss 1.956 | ppl 3.88 | wps 577.1 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 168 | lr 2.10958e-05 | gnorm 1.986 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:06:33 | INFO | fairseq.trainer | begin training epoch 95
2020-11-13 13:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:07:17 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.066 | nll_loss 7.983 | ppl 252.93 | bleu 16.43 | wps 1518.6 | wpb 2046.5 | bsz 53.7 | num_updates 180 | best_loss 9.066
2020-11-13 13:07:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:07:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 95 @ 180 updates, score 9.066) (writing took 8.27982584014535 seconds)
2020-11-13 13:07:26 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2020-11-13 13:07:26 | INFO | train | epoch 095 | loss 3.556 | nll_loss 1.869 | ppl 3.65 | wps 578 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 180 | lr 2.25955e-05 | gnorm 1.938 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:07:26 | INFO | fairseq.trainer | begin training epoch 96
2020-11-13 13:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:08:10 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.062 | nll_loss 7.978 | ppl 252.15 | bleu 16.33 | wps 1532.9 | wpb 2046.5 | bsz 53.7 | num_updates 192 | best_loss 9.062
2020-11-13 13:08:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:08:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 96 @ 192 updates, score 9.062) (writing took 8.12529995944351 seconds)
2020-11-13 13:08:18 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2020-11-13 13:08:18 | INFO | train | epoch 096 | loss 3.478 | nll_loss 1.78 | ppl 3.43 | wps 583.9 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 192 | lr 2.40952e-05 | gnorm 1.893 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:08:18 | INFO | fairseq.trainer | begin training epoch 97
2020-11-13 13:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:09:02 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.056 | nll_loss 7.971 | ppl 250.93 | bleu 16.35 | wps 1516.7 | wpb 2046.5 | bsz 53.7 | num_updates 204 | best_loss 9.056
2020-11-13 13:09:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:09:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 97 @ 204 updates, score 9.056) (writing took 8.075914168730378 seconds)
2020-11-13 13:09:10 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2020-11-13 13:09:10 | INFO | train | epoch 097 | loss 3.394 | nll_loss 1.682 | ppl 3.21 | wps 579.5 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 204 | lr 2.55949e-05 | gnorm 1.858 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:09:10 | INFO | fairseq.trainer | begin training epoch 98
2020-11-13 13:09:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:09:55 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.049 | nll_loss 7.962 | ppl 249.38 | bleu 16.13 | wps 1524.6 | wpb 2046.5 | bsz 53.7 | num_updates 216 | best_loss 9.049
2020-11-13 13:09:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:10:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_best.pt (epoch 98 @ 216 updates, score 9.049) (writing took 8.238426539115608 seconds)
2020-11-13 13:10:03 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2020-11-13 13:10:03 | INFO | train | epoch 098 | loss 3.319 | nll_loss 1.598 | ppl 3.03 | wps 580.3 | ups 0.23 | wpb 2536.6 | bsz 73.2 | num_updates 216 | lr 2.70946e-05 | gnorm 1.784 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:10:03 | INFO | fairseq.trainer | begin training epoch 99
2020-11-13 13:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-11-13 13:10:48 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.056 | nll_loss 7.969 | ppl 250.63 | bleu 16.16 | wps 1498.2 | wpb 2046.5 | bsz 53.7 | num_updates 228 | best_loss 9.049
2020-11-13 13:10:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-11-13 13:10:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/checkpoints/checkpoint_last.pt (epoch 99 @ 228 updates, score 9.056) (writing took 5.242297208867967 seconds)
2020-11-13 13:10:53 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2020-11-13 13:10:53 | INFO | train | epoch 099 | loss 3.254 | nll_loss 1.521 | ppl 2.87 | wps 607.2 | ups 0.24 | wpb 2536.6 | bsz 73.2 | num_updates 228 | lr 2.85943e-05 | gnorm 1.758 | loss_scale None | train_wall 3 | wall 0
111
2020-11-13 13:10:53 | INFO | fairseq.trainer | begin training epoch 100
2020-11-13 13:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
