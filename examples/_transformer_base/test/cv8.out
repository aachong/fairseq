nohup: ignoring input
save_dir=./examples/_transformer_base/bash/../checkpoints/kl
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=4000
max_epoch=100
r3f_lambda=1
extr='--noised-no-grad --cv --cv-lambda 0.15'
2020-12-22 12:47:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:47:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12770
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12770
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12770
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12770
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-22 12:48:02 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-22 12:48:05 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=True, cv_lambda=0.15, data='./examples/_transformer_base/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:12770', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/_transformer_base/bash/../checkpoints/kl', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-22 12:48:05 | INFO | fairseq.tasks.translation | [ch] dictionary: 41952 types
2020-12-22 12:48:05 | INFO | fairseq.tasks.translation | [en] dictionary: 31264 types
2020-12-22 12:48:05 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/bash/../data-bin/valid.ch-en.ch
2020-12-22 12:48:05 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/bash/../data-bin/valid.ch-en.en
2020-12-22 12:48:05 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/bash/../data-bin valid ch-en 1664 examples
2020-12-22 12:48:07 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(41952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(31264, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=31264, bias=False)
  )
)
2020-12-22 12:48:07 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-22 12:48:07 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-22 12:48:07 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-22 12:48:07 | INFO | fairseq_cli.train | num. model params: 97632256 (num. trained: 97632256)
2020-12-22 12:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-22 12:48:08 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 12:48:08 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 12:48:08 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 12:48:08 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 12:48:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-22 12:48:08 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2020-12-22 12:48:08 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None
2020-12-22 12:48:08 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-22 12:48:09 | INFO | fairseq.trainer | loaded checkpoint ./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 80 @ 0 updates)
2020-12-22 12:48:09 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-22 12:48:09 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-22 12:48:09 | INFO | fairseq.data.data_utils | loaded 1252977 examples from: ./examples/_transformer_base/bash/../data-bin/train.ch-en.ch
2020-12-22 12:48:09 | INFO | fairseq.data.data_utils | loaded 1252977 examples from: ./examples/_transformer_base/bash/../data-bin/train.ch-en.en
2020-12-22 12:48:09 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/bash/../data-bin train ch-en 1252977 examples
2020-12-22 12:48:15 | INFO | fairseq.trainer | begin training epoch 1
2020-12-22 12:48:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:48:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:48:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:48:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 12:48:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:48:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 12:49:14 | INFO | train_inner | epoch 001:    100 / 3059 symm_kl=0.78, self_kl=0, self_cv=9.93, loss=6.642, nll_loss=1.669, ppl=3.18, wps=22945.8, ups=1.94, wpb=11838.1, bsz=405.1, num_updates=100, lr=1.0975e-06, gnorm=6.516, train_wall=52, wall=66
2020-12-22 12:50:05 | INFO | train_inner | epoch 001:    200 / 3059 symm_kl=0.733, self_kl=0, self_cv=9.249, loss=6.423, nll_loss=1.708, ppl=3.27, wps=23102.3, ups=1.95, wpb=11822.8, bsz=408.9, num_updates=200, lr=2.095e-06, gnorm=6.723, train_wall=51, wall=117
2020-12-22 12:50:56 | INFO | train_inner | epoch 001:    300 / 3059 symm_kl=0.647, self_kl=0, self_cv=7.889, loss=6.087, nll_loss=1.876, ppl=3.67, wps=23117.3, ups=1.94, wpb=11888.2, bsz=402, num_updates=300, lr=3.0925e-06, gnorm=6.462, train_wall=51, wall=169
2020-12-22 12:51:48 | INFO | train_inner | epoch 001:    400 / 3059 symm_kl=0.566, self_kl=0, self_cv=6.094, loss=5.831, nll_loss=2.239, ppl=4.72, wps=23069.7, ups=1.94, wpb=11870.5, bsz=420.2, num_updates=400, lr=4.09e-06, gnorm=4.532, train_wall=51, wall=220
2020-12-22 12:52:40 | INFO | train_inner | epoch 001:    500 / 3059 symm_kl=0.505, self_kl=0, self_cv=4.965, loss=5.706, nll_loss=2.514, ppl=5.71, wps=22843.1, ups=1.93, wpb=11848.5, bsz=424.2, num_updates=500, lr=5.0875e-06, gnorm=1.82, train_wall=52, wall=272
2020-12-22 12:53:31 | INFO | train_inner | epoch 001:    600 / 3059 symm_kl=0.474, self_kl=0, self_cv=4.627, loss=5.65, nll_loss=2.596, ppl=6.05, wps=22830, ups=1.94, wpb=11785.2, bsz=407.8, num_updates=600, lr=6.085e-06, gnorm=1.077, train_wall=51, wall=324
2020-12-22 12:54:23 | INFO | train_inner | epoch 001:    700 / 3059 symm_kl=0.455, self_kl=0, self_cv=4.507, loss=5.627, nll_loss=2.633, ppl=6.2, wps=22910.5, ups=1.95, wpb=11778.3, bsz=399.9, num_updates=700, lr=7.0825e-06, gnorm=1.032, train_wall=51, wall=375
2020-12-22 12:55:15 | INFO | train_inner | epoch 001:    800 / 3059 symm_kl=0.433, self_kl=0, self_cv=4.459, loss=5.538, nll_loss=2.581, ppl=5.98, wps=22998.9, ups=1.92, wpb=11948.8, bsz=453, num_updates=800, lr=8.08e-06, gnorm=0.964, train_wall=52, wall=427
2020-12-22 12:56:06 | INFO | train_inner | epoch 001:    900 / 3059 symm_kl=0.43, self_kl=0, self_cv=4.407, loss=5.579, nll_loss=2.646, ppl=6.26, wps=22921.7, ups=1.94, wpb=11844.4, bsz=410.5, num_updates=900, lr=9.0775e-06, gnorm=0.942, train_wall=51, wall=479
2020-12-22 12:56:58 | INFO | train_inner | epoch 001:   1000 / 3059 symm_kl=0.421, self_kl=0, self_cv=4.383, loss=5.561, nll_loss=2.648, ppl=6.27, wps=22865.2, ups=1.93, wpb=11841.6, bsz=409.3, num_updates=1000, lr=1.0075e-05, gnorm=0.932, train_wall=52, wall=530
2020-12-22 12:57:50 | INFO | train_inner | epoch 001:   1100 / 3059 symm_kl=0.416, self_kl=0, self_cv=4.366, loss=5.543, nll_loss=2.64, ppl=6.23, wps=22793, ups=1.93, wpb=11816.5, bsz=409.6, num_updates=1100, lr=1.10725e-05, gnorm=0.936, train_wall=52, wall=582
2020-12-22 12:58:42 | INFO | train_inner | epoch 001:   1200 / 3059 symm_kl=0.407, self_kl=0, self_cv=4.35, loss=5.527, nll_loss=2.64, ppl=6.23, wps=22785.1, ups=1.93, wpb=11796.6, bsz=403.2, num_updates=1200, lr=1.207e-05, gnorm=0.913, train_wall=52, wall=634
2020-12-22 12:59:33 | INFO | train_inner | epoch 001:   1300 / 3059 symm_kl=0.404, self_kl=0, self_cv=4.343, loss=5.526, nll_loss=2.647, ppl=6.26, wps=22786.9, ups=1.93, wpb=11780.9, bsz=427.1, num_updates=1300, lr=1.30675e-05, gnorm=0.93, train_wall=52, wall=686
2020-12-22 13:00:25 | INFO | train_inner | epoch 001:   1400 / 3059 symm_kl=0.397, self_kl=0, self_cv=4.332, loss=5.504, nll_loss=2.637, ppl=6.22, wps=23024.4, ups=1.94, wpb=11844.1, bsz=410.4, num_updates=1400, lr=1.4065e-05, gnorm=0.903, train_wall=51, wall=737
2020-12-22 13:01:16 | INFO | train_inner | epoch 001:   1500 / 3059 symm_kl=0.396, self_kl=0, self_cv=4.316, loss=5.522, nll_loss=2.662, ppl=6.33, wps=22760.1, ups=1.94, wpb=11749.5, bsz=389, num_updates=1500, lr=1.50625e-05, gnorm=0.924, train_wall=51, wall=789
2020-12-22 13:02:08 | INFO | train_inner | epoch 001:   1600 / 3059 symm_kl=0.388, self_kl=0, self_cv=4.31, loss=5.487, nll_loss=2.638, ppl=6.22, wps=22908.3, ups=1.93, wpb=11869.9, bsz=401, num_updates=1600, lr=1.606e-05, gnorm=0.888, train_wall=52, wall=841
2020-12-22 13:03:00 | INFO | train_inner | epoch 001:   1700 / 3059 symm_kl=0.387, self_kl=0, self_cv=4.301, loss=5.482, nll_loss=2.637, ppl=6.22, wps=22862.6, ups=1.93, wpb=11840.3, bsz=436.6, num_updates=1700, lr=1.70575e-05, gnorm=0.921, train_wall=52, wall=892
2020-12-22 13:03:51 | INFO | train_inner | epoch 001:   1800 / 3059 symm_kl=0.381, self_kl=0, self_cv=4.297, loss=5.487, nll_loss=2.653, ppl=6.29, wps=23111.4, ups=1.94, wpb=11890.7, bsz=425.4, num_updates=1800, lr=1.8055e-05, gnorm=0.889, train_wall=51, wall=944
2020-12-22 13:04:43 | INFO | train_inner | epoch 001:   1900 / 3059 symm_kl=0.377, self_kl=0, self_cv=4.286, loss=5.475, nll_loss=2.648, ppl=6.27, wps=22967.3, ups=1.93, wpb=11879.9, bsz=409.3, num_updates=1900, lr=1.90525e-05, gnorm=0.881, train_wall=52, wall=996
2020-12-22 13:05:35 | INFO | train_inner | epoch 001:   2000 / 3059 symm_kl=0.379, self_kl=0, self_cv=4.283, loss=5.486, nll_loss=2.66, ppl=6.32, wps=22808.5, ups=1.93, wpb=11828.4, bsz=400.9, num_updates=2000, lr=2.005e-05, gnorm=0.894, train_wall=52, wall=1047
2020-12-22 13:06:27 | INFO | train_inner | epoch 001:   2100 / 3059 symm_kl=0.372, self_kl=0, self_cv=4.271, loss=5.47, nll_loss=2.655, ppl=6.3, wps=22858.5, ups=1.93, wpb=11846, bsz=405.2, num_updates=2100, lr=2.10475e-05, gnorm=0.915, train_wall=52, wall=1099
2020-12-22 13:07:18 | INFO | train_inner | epoch 001:   2200 / 3059 symm_kl=0.372, self_kl=0, self_cv=4.276, loss=5.462, nll_loss=2.645, ppl=6.26, wps=23061, ups=1.94, wpb=11871.9, bsz=407.8, num_updates=2200, lr=2.2045e-05, gnorm=0.884, train_wall=51, wall=1151
2020-12-22 13:08:10 | INFO | train_inner | epoch 001:   2300 / 3059 symm_kl=0.368, self_kl=0, self_cv=4.273, loss=5.458, nll_loss=2.649, ppl=6.27, wps=23013.5, ups=1.93, wpb=11900.6, bsz=407.9, num_updates=2300, lr=2.30425e-05, gnorm=0.889, train_wall=52, wall=1202
2020-12-22 13:09:02 | INFO | train_inner | epoch 001:   2400 / 3059 symm_kl=0.369, self_kl=0, self_cv=4.252, loss=5.481, nll_loss=2.679, ppl=6.4, wps=22904.7, ups=1.93, wpb=11871.5, bsz=389.7, num_updates=2400, lr=2.404e-05, gnorm=0.883, train_wall=52, wall=1254
2020-12-22 13:09:54 | INFO | train_inner | epoch 001:   2500 / 3059 symm_kl=0.364, self_kl=0, self_cv=4.253, loss=5.463, nll_loss=2.666, ppl=6.35, wps=23016.9, ups=1.93, wpb=11901.5, bsz=396.6, num_updates=2500, lr=2.50375e-05, gnorm=0.875, train_wall=52, wall=1306
2020-12-22 13:10:45 | INFO | train_inner | epoch 001:   2600 / 3059 symm_kl=0.364, self_kl=0, self_cv=4.259, loss=5.462, nll_loss=2.664, ppl=6.34, wps=22973.4, ups=1.94, wpb=11841.1, bsz=395.7, num_updates=2600, lr=2.6035e-05, gnorm=0.881, train_wall=51, wall=1357
2020-12-22 13:11:37 | INFO | train_inner | epoch 001:   2700 / 3059 symm_kl=0.356, self_kl=0, self_cv=4.244, loss=5.436, nll_loss=2.652, ppl=6.29, wps=23071, ups=1.94, wpb=11894.8, bsz=417, num_updates=2700, lr=2.70325e-05, gnorm=0.868, train_wall=51, wall=1409
2020-12-22 13:12:29 | INFO | train_inner | epoch 001:   2800 / 3059 symm_kl=0.359, self_kl=0, self_cv=4.241, loss=5.457, nll_loss=2.672, ppl=6.37, wps=22785.4, ups=1.92, wpb=11855.7, bsz=407.8, num_updates=2800, lr=2.803e-05, gnorm=0.884, train_wall=52, wall=1461
2020-12-22 13:13:21 | INFO | train_inner | epoch 001:   2900 / 3059 symm_kl=0.359, self_kl=0, self_cv=4.237, loss=5.461, nll_loss=2.678, ppl=6.4, wps=22909.2, ups=1.93, wpb=11863.6, bsz=398.6, num_updates=2900, lr=2.90275e-05, gnorm=0.891, train_wall=52, wall=1513
2020-12-22 13:14:12 | INFO | train_inner | epoch 001:   3000 / 3059 symm_kl=0.349, self_kl=0, self_cv=4.237, loss=5.413, nll_loss=2.64, ppl=6.23, wps=23053.6, ups=1.93, wpb=11939.7, bsz=412.6, num_updates=3000, lr=3.0025e-05, gnorm=0.862, train_wall=52, wall=1565
2020-12-22 13:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 13:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 13:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 13:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 13:14:59 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.87 | nll_loss 8.052 | ppl 265.35 | bleu 16.37 | wps 4897 | wpb 6344.2 | bsz 166.4 | num_updates 3059
2020-12-22 13:14:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 13:15:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 1 @ 3059 updates, score 16.37) (writing took 3.7586382310837507 seconds)
2020-12-22 13:15:02 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-22 13:15:02 | INFO | train | epoch 001 | symm_kl 0.432 | self_kl 0 | self_cv 4.862 | loss 5.604 | nll_loss 2.541 | ppl 5.82 | wps 22655.4 | ups 1.91 | wpb 11852.2 | bsz 409.6 | num_updates 3059 | lr 3.06135e-05 | gnorm 1.616 | train_wall 1576 | wall 1615
2020-12-22 13:15:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:15:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:15:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:15:07 | INFO | fairseq.trainer | begin training epoch 2
2020-12-22 13:15:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:15:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:15:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:15:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:15:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:15:34 | INFO | train_inner | epoch 002:     41 / 3059 symm_kl=0.348, self_kl=0, self_cv=4.238, loss=5.4, nll_loss=2.627, ppl=6.18, wps=14487.4, ups=1.22, wpb=11847.4, bsz=403.5, num_updates=3100, lr=3.10225e-05, gnorm=0.866, train_wall=51, wall=1646
2020-12-22 13:16:26 | INFO | train_inner | epoch 002:    141 / 3059 symm_kl=0.349, self_kl=0, self_cv=4.24, loss=5.418, nll_loss=2.645, ppl=6.26, wps=22952.6, ups=1.92, wpb=11923.9, bsz=408.2, num_updates=3200, lr=3.202e-05, gnorm=0.867, train_wall=52, wall=1698
2020-12-22 13:17:18 | INFO | train_inner | epoch 002:    241 / 3059 symm_kl=0.348, self_kl=0, self_cv=4.222, loss=5.433, nll_loss=2.667, ppl=6.35, wps=22986.3, ups=1.94, wpb=11869.6, bsz=423.1, num_updates=3300, lr=3.30175e-05, gnorm=0.882, train_wall=51, wall=1750
2020-12-22 13:18:09 | INFO | train_inner | epoch 002:    341 / 3059 symm_kl=0.348, self_kl=0, self_cv=4.22, loss=5.442, nll_loss=2.678, ppl=6.4, wps=22906.3, ups=1.93, wpb=11838.2, bsz=409.8, num_updates=3400, lr=3.4015e-05, gnorm=0.871, train_wall=52, wall=1802
2020-12-22 13:19:01 | INFO | train_inner | epoch 002:    441 / 3059 symm_kl=0.344, self_kl=0, self_cv=4.215, loss=5.421, nll_loss=2.663, ppl=6.33, wps=22981.4, ups=1.95, wpb=11805.7, bsz=417.5, num_updates=3500, lr=3.50125e-05, gnorm=0.88, train_wall=51, wall=1853
2020-12-22 13:19:52 | INFO | train_inner | epoch 002:    541 / 3059 symm_kl=0.343, self_kl=0, self_cv=4.224, loss=5.414, nll_loss=2.654, ppl=6.3, wps=22990, ups=1.93, wpb=11894.8, bsz=411, num_updates=3600, lr=3.601e-05, gnorm=0.87, train_wall=52, wall=1905
2020-12-22 13:20:44 | INFO | train_inner | epoch 002:    641 / 3059 symm_kl=0.341, self_kl=0, self_cv=4.201, loss=5.427, nll_loss=2.679, ppl=6.4, wps=23007.5, ups=1.94, wpb=11872.5, bsz=419.7, num_updates=3700, lr=3.70075e-05, gnorm=0.87, train_wall=51, wall=1956
2020-12-22 13:21:36 | INFO | train_inner | epoch 002:    741 / 3059 symm_kl=0.343, self_kl=0, self_cv=4.208, loss=5.433, nll_loss=2.681, ppl=6.41, wps=22827.6, ups=1.93, wpb=11831.9, bsz=396.5, num_updates=3800, lr=3.8005e-05, gnorm=0.883, train_wall=52, wall=2008
2020-12-22 13:22:28 | INFO | train_inner | epoch 002:    841 / 3059 symm_kl=0.341, self_kl=0, self_cv=4.207, loss=5.44, nll_loss=2.692, ppl=6.46, wps=22936.7, ups=1.93, wpb=11880.8, bsz=404.7, num_updates=3900, lr=3.90025e-05, gnorm=0.879, train_wall=52, wall=2060
2020-12-22 13:23:20 | INFO | train_inner | epoch 002:    941 / 3059 symm_kl=0.339, self_kl=0, self_cv=4.208, loss=5.429, nll_loss=2.682, ppl=6.42, wps=23068.9, ups=1.93, wpb=11959.3, bsz=380.9, num_updates=4000, lr=4e-05, gnorm=0.862, train_wall=52, wall=2112
2020-12-22 13:24:11 | INFO | train_inner | epoch 002:   1041 / 3059 symm_kl=0.34, self_kl=0, self_cv=4.191, loss=5.449, nll_loss=2.708, ppl=6.54, wps=22659.2, ups=1.93, wpb=11764.6, bsz=412.1, num_updates=4100, lr=3.95092e-05, gnorm=0.883, train_wall=52, wall=2164
2020-12-22 13:25:04 | INFO | train_inner | epoch 002:   1141 / 3059 symm_kl=0.337, self_kl=0, self_cv=4.192, loss=5.431, nll_loss=2.693, ppl=6.47, wps=22791.1, ups=1.92, wpb=11895.8, bsz=405, num_updates=4200, lr=3.9036e-05, gnorm=0.879, train_wall=52, wall=2216
2020-12-22 13:25:55 | INFO | train_inner | epoch 002:   1241 / 3059 symm_kl=0.334, self_kl=0, self_cv=4.181, loss=5.434, nll_loss=2.703, ppl=6.51, wps=22926.5, ups=1.94, wpb=11837.6, bsz=418.7, num_updates=4300, lr=3.85794e-05, gnorm=0.887, train_wall=51, wall=2268
2020-12-22 13:26:47 | INFO | train_inner | epoch 002:   1341 / 3059 symm_kl=0.331, self_kl=0, self_cv=4.195, loss=5.413, nll_loss=2.681, ppl=6.41, wps=23031.1, ups=1.93, wpb=11913.1, bsz=409.7, num_updates=4400, lr=3.81385e-05, gnorm=0.861, train_wall=52, wall=2319
2020-12-22 13:27:39 | INFO | train_inner | epoch 002:   1441 / 3059 symm_kl=0.337, self_kl=0, self_cv=4.177, loss=5.453, nll_loss=2.722, ppl=6.6, wps=22806.8, ups=1.94, wpb=11762.4, bsz=415.8, num_updates=4500, lr=3.77124e-05, gnorm=0.884, train_wall=51, wall=2371
2020-12-22 13:28:30 | INFO | train_inner | epoch 002:   1541 / 3059 symm_kl=0.332, self_kl=0, self_cv=4.185, loss=5.426, nll_loss=2.698, ppl=6.49, wps=22932.4, ups=1.93, wpb=11898.6, bsz=403.5, num_updates=4600, lr=3.73002e-05, gnorm=0.865, train_wall=52, wall=2423
2020-12-22 13:29:23 | INFO | train_inner | epoch 002:   1641 / 3059 symm_kl=0.327, self_kl=0, self_cv=4.181, loss=5.399, nll_loss=2.676, ppl=6.39, wps=22851.1, ups=1.92, wpb=11897, bsz=443.4, num_updates=4700, lr=3.69012e-05, gnorm=0.887, train_wall=52, wall=2475
2020-12-22 13:30:14 | INFO | train_inner | epoch 002:   1741 / 3059 symm_kl=0.33, self_kl=0, self_cv=4.183, loss=5.426, nll_loss=2.702, ppl=6.51, wps=22813.7, ups=1.93, wpb=11829.8, bsz=402.5, num_updates=4800, lr=3.65148e-05, gnorm=0.872, train_wall=52, wall=2527
2020-12-22 13:31:06 | INFO | train_inner | epoch 002:   1841 / 3059 symm_kl=0.331, self_kl=0, self_cv=4.177, loss=5.445, nll_loss=2.723, ppl=6.6, wps=22777.8, ups=1.92, wpb=11853.5, bsz=392.5, num_updates=4900, lr=3.61403e-05, gnorm=0.872, train_wall=52, wall=2579
2020-12-22 13:31:58 | INFO | train_inner | epoch 002:   1941 / 3059 symm_kl=0.328, self_kl=0, self_cv=4.178, loss=5.424, nll_loss=2.706, ppl=6.52, wps=22823.2, ups=1.93, wpb=11814, bsz=387.1, num_updates=5000, lr=3.57771e-05, gnorm=0.878, train_wall=52, wall=2631
2020-12-22 13:32:50 | INFO | train_inner | epoch 002:   2041 / 3059 symm_kl=0.325, self_kl=0, self_cv=4.168, loss=5.414, nll_loss=2.701, ppl=6.5, wps=22726.7, ups=1.93, wpb=11791.8, bsz=416.3, num_updates=5100, lr=3.54246e-05, gnorm=0.87, train_wall=52, wall=2682
2020-12-22 13:33:42 | INFO | train_inner | epoch 002:   2141 / 3059 symm_kl=0.326, self_kl=0, self_cv=4.178, loss=5.431, nll_loss=2.715, ppl=6.56, wps=22786, ups=1.92, wpb=11872.3, bsz=385.6, num_updates=5200, lr=3.50823e-05, gnorm=0.862, train_wall=52, wall=2735
2020-12-22 13:34:34 | INFO | train_inner | epoch 002:   2241 / 3059 symm_kl=0.325, self_kl=0, self_cv=4.168, loss=5.423, nll_loss=2.711, ppl=6.55, wps=22947.3, ups=1.93, wpb=11918.9, bsz=438.4, num_updates=5300, lr=3.47498e-05, gnorm=0.899, train_wall=52, wall=2786
2020-12-22 13:35:26 | INFO | train_inner | epoch 002:   2341 / 3059 symm_kl=0.325, self_kl=0, self_cv=4.166, loss=5.433, nll_loss=2.722, ppl=6.6, wps=22908.8, ups=1.93, wpb=11889.8, bsz=409.5, num_updates=5400, lr=3.44265e-05, gnorm=0.864, train_wall=52, wall=2838
2020-12-22 13:36:18 | INFO | train_inner | epoch 002:   2441 / 3059 symm_kl=0.323, self_kl=0, self_cv=4.165, loss=5.419, nll_loss=2.711, ppl=6.55, wps=22860.4, ups=1.92, wpb=11903.7, bsz=427.8, num_updates=5500, lr=3.41121e-05, gnorm=0.865, train_wall=52, wall=2890
2020-12-22 13:37:10 | INFO | train_inner | epoch 002:   2541 / 3059 symm_kl=0.324, self_kl=0, self_cv=4.156, loss=5.429, nll_loss=2.723, ppl=6.6, wps=22729.5, ups=1.94, wpb=11692.4, bsz=420.1, num_updates=5600, lr=3.38062e-05, gnorm=0.884, train_wall=51, wall=2942
2020-12-22 13:38:02 | INFO | train_inner | epoch 002:   2641 / 3059 symm_kl=0.32, self_kl=0, self_cv=4.164, loss=5.409, nll_loss=2.704, ppl=6.52, wps=22882.3, ups=1.92, wpb=11895.9, bsz=419.5, num_updates=5700, lr=3.35083e-05, gnorm=0.862, train_wall=52, wall=2994
2020-12-22 13:38:53 | INFO | train_inner | epoch 002:   2741 / 3059 symm_kl=0.321, self_kl=0, self_cv=4.161, loss=5.433, nll_loss=2.73, ppl=6.63, wps=22839.4, ups=1.94, wpb=11789.5, bsz=409.4, num_updates=5800, lr=3.32182e-05, gnorm=0.869, train_wall=51, wall=3046
2020-12-22 13:39:45 | INFO | train_inner | epoch 002:   2841 / 3059 symm_kl=0.322, self_kl=0, self_cv=4.166, loss=5.42, nll_loss=2.714, ppl=6.56, wps=22899.2, ups=1.94, wpb=11822.7, bsz=391.8, num_updates=5900, lr=3.29355e-05, gnorm=0.859, train_wall=51, wall=3097
2020-12-22 13:40:37 | INFO | train_inner | epoch 002:   2941 / 3059 symm_kl=0.321, self_kl=0, self_cv=4.161, loss=5.428, nll_loss=2.725, ppl=6.61, wps=22698.6, ups=1.93, wpb=11782.3, bsz=401.7, num_updates=6000, lr=3.26599e-05, gnorm=0.871, train_wall=52, wall=3149
2020-12-22 13:41:29 | INFO | train_inner | epoch 002:   3041 / 3059 symm_kl=0.32, self_kl=0, self_cv=4.161, loss=5.425, nll_loss=2.725, ppl=6.61, wps=22860.9, ups=1.93, wpb=11863.7, bsz=413, num_updates=6100, lr=3.23911e-05, gnorm=0.872, train_wall=52, wall=3201
2020-12-22 13:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 13:41:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:41:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:41:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:41:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:41:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:41:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:41:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:41:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 13:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 13:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 13:41:54 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.81 | nll_loss 7.993 | ppl 254.69 | bleu 16.37 | wps 4847.8 | wpb 6344.2 | bsz 166.4 | num_updates 6118 | best_bleu 16.37
2020-12-22 13:41:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 13:42:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:42:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:42:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:42:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 2 @ 6118 updates, score 16.37) (writing took 8.196709847077727 seconds)
2020-12-22 13:42:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-22 13:42:02 | INFO | train | epoch 002 | symm_kl 0.333 | self_kl 0 | self_cv 4.187 | loss 5.427 | nll_loss 2.697 | ppl 6.48 | wps 22385.1 | ups 1.89 | wpb 11852.2 | bsz 409.6 | num_updates 6118 | lr 3.23434e-05 | gnorm 0.873 | train_wall 1580 | wall 3234
2020-12-22 13:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:42:06 | INFO | fairseq.trainer | begin training epoch 3
2020-12-22 13:42:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 13:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 13:42:54 | INFO | train_inner | epoch 003:     82 / 3059 symm_kl=0.32, self_kl=0, self_cv=4.17, loss=5.41, nll_loss=2.704, ppl=6.52, wps=13879.5, ups=1.17, wpb=11870.1, bsz=394.2, num_updates=6200, lr=3.21288e-05, gnorm=0.854, train_wall=51, wall=3286
2020-12-22 13:43:46 | INFO | train_inner | epoch 003:    182 / 3059 symm_kl=0.32, self_kl=0, self_cv=4.169, loss=5.41, nll_loss=2.705, ppl=6.52, wps=22643.9, ups=1.92, wpb=11772.1, bsz=417.8, num_updates=6300, lr=3.18728e-05, gnorm=0.873, train_wall=52, wall=3338
2020-12-22 13:44:38 | INFO | train_inner | epoch 003:    282 / 3059 symm_kl=0.315, self_kl=0, self_cv=4.162, loss=5.385, nll_loss=2.687, ppl=6.44, wps=22922, ups=1.92, wpb=11911.2, bsz=421.2, num_updates=6400, lr=3.16228e-05, gnorm=0.85, train_wall=52, wall=3390
2020-12-22 13:45:30 | INFO | train_inner | epoch 003:    382 / 3059 symm_kl=0.317, self_kl=0, self_cv=4.175, loss=5.39, nll_loss=2.685, ppl=6.43, wps=22823.6, ups=1.92, wpb=11890.3, bsz=418, num_updates=6500, lr=3.13786e-05, gnorm=0.857, train_wall=52, wall=3443
2020-12-22 13:46:22 | INFO | train_inner | epoch 003:    482 / 3059 symm_kl=0.315, self_kl=0, self_cv=4.168, loss=5.381, nll_loss=2.681, ppl=6.42, wps=22862.1, ups=1.93, wpb=11834.9, bsz=413.1, num_updates=6600, lr=3.114e-05, gnorm=0.862, train_wall=52, wall=3494
2020-12-22 13:47:14 | INFO | train_inner | epoch 003:    582 / 3059 symm_kl=0.314, self_kl=0, self_cv=4.165, loss=5.374, nll_loss=2.675, ppl=6.39, wps=22732.9, ups=1.93, wpb=11772.9, bsz=423.1, num_updates=6700, lr=3.09067e-05, gnorm=0.862, train_wall=52, wall=3546
2020-12-22 13:48:06 | INFO | train_inner | epoch 003:    682 / 3059 symm_kl=0.312, self_kl=0, self_cv=4.164, loss=5.371, nll_loss=2.676, ppl=6.39, wps=22883.1, ups=1.93, wpb=11864.3, bsz=441.6, num_updates=6800, lr=3.06786e-05, gnorm=0.849, train_wall=52, wall=3598
2020-12-22 13:48:57 | INFO | train_inner | epoch 003:    782 / 3059 symm_kl=0.318, self_kl=0, self_cv=4.153, loss=5.42, nll_loss=2.724, ppl=6.61, wps=22779.9, ups=1.93, wpb=11817.6, bsz=400, num_updates=6900, lr=3.04555e-05, gnorm=0.853, train_wall=52, wall=3650
2020-12-22 13:49:49 | INFO | train_inner | epoch 003:    882 / 3059 symm_kl=0.314, self_kl=0, self_cv=4.164, loss=5.383, nll_loss=2.686, ppl=6.43, wps=22868.5, ups=1.93, wpb=11873.5, bsz=417.8, num_updates=7000, lr=3.02372e-05, gnorm=0.862, train_wall=52, wall=3702
2020-12-22 13:50:41 | INFO | train_inner | epoch 003:    982 / 3059 symm_kl=0.317, self_kl=0, self_cv=4.157, loss=5.404, nll_loss=2.707, ppl=6.53, wps=22696.8, ups=1.93, wpb=11744.2, bsz=389.3, num_updates=7100, lr=3.00235e-05, gnorm=0.879, train_wall=52, wall=3753
2020-12-22 13:51:33 | INFO | train_inner | epoch 003:   1082 / 3059 symm_kl=0.316, self_kl=0, self_cv=4.152, loss=5.421, nll_loss=2.729, ppl=6.63, wps=22806.9, ups=1.93, wpb=11810.4, bsz=418.6, num_updates=7200, lr=2.98142e-05, gnorm=0.865, train_wall=52, wall=3805
2020-12-22 13:52:25 | INFO | train_inner | epoch 003:   1182 / 3059 symm_kl=0.314, self_kl=0, self_cv=4.158, loss=5.391, nll_loss=2.696, ppl=6.48, wps=22571.9, ups=1.92, wpb=11727.7, bsz=383.4, num_updates=7300, lr=2.96093e-05, gnorm=0.894, train_wall=52, wall=3857
2020-12-22 13:53:17 | INFO | train_inner | epoch 003:   1282 / 3059 symm_kl=0.313, self_kl=0, self_cv=4.161, loss=5.397, nll_loss=2.704, ppl=6.52, wps=23060.6, ups=1.93, wpb=11979.3, bsz=404, num_updates=7400, lr=2.94086e-05, gnorm=0.852, train_wall=52, wall=3909
2020-12-22 13:54:09 | INFO | train_inner | epoch 003:   1382 / 3059 symm_kl=0.313, self_kl=0, self_cv=4.158, loss=5.387, nll_loss=2.695, ppl=6.48, wps=22522.9, ups=1.91, wpb=11792.9, bsz=425, num_updates=7500, lr=2.92119e-05, gnorm=0.87, train_wall=52, wall=3962
2020-12-22 13:55:01 | INFO | train_inner | epoch 003:   1482 / 3059 symm_kl=0.312, self_kl=0, self_cv=4.151, loss=5.392, nll_loss=2.704, ppl=6.51, wps=22647.8, ups=1.92, wpb=11805.8, bsz=423.3, num_updates=7600, lr=2.90191e-05, gnorm=0.869, train_wall=52, wall=4014
2020-12-22 13:55:53 | INFO | train_inner | epoch 003:   1582 / 3059 symm_kl=0.311, self_kl=0, self_cv=4.158, loss=5.387, nll_loss=2.698, ppl=6.49, wps=22946.2, ups=1.93, wpb=11917.6, bsz=403.5, num_updates=7700, lr=2.883e-05, gnorm=0.851, train_wall=52, wall=4066
2020-12-22 13:56:45 | INFO | train_inner | epoch 003:   1682 / 3059 symm_kl=0.308, self_kl=0, self_cv=4.158, loss=5.367, nll_loss=2.68, ppl=6.41, wps=22884.8, ups=1.92, wpb=11889.9, bsz=434.3, num_updates=7800, lr=2.86446e-05, gnorm=0.853, train_wall=52, wall=4118
2020-12-22 13:57:37 | INFO | train_inner | epoch 003:   1782 / 3059 symm_kl=0.314, self_kl=0, self_cv=4.138, loss=5.424, nll_loss=2.74, ppl=6.68, wps=22619.2, ups=1.92, wpb=11779, bsz=412.3, num_updates=7900, lr=2.84627e-05, gnorm=0.868, train_wall=52, wall=4170
2020-12-22 13:58:29 | INFO | train_inner | epoch 003:   1882 / 3059 symm_kl=0.312, self_kl=0, self_cv=4.143, loss=5.415, nll_loss=2.731, ppl=6.64, wps=22694.8, ups=1.93, wpb=11757.4, bsz=402.6, num_updates=8000, lr=2.82843e-05, gnorm=0.866, train_wall=52, wall=4221
2020-12-22 13:59:21 | INFO | train_inner | epoch 003:   1982 / 3059 symm_kl=0.308, self_kl=0, self_cv=4.153, loss=5.372, nll_loss=2.687, ppl=6.44, wps=23000.7, ups=1.93, wpb=11906.2, bsz=409.9, num_updates=8100, lr=2.81091e-05, gnorm=0.852, train_wall=52, wall=4273
2020-12-22 14:00:13 | INFO | train_inner | epoch 003:   2082 / 3059 symm_kl=0.308, self_kl=0, self_cv=4.153, loss=5.375, nll_loss=2.691, ppl=6.46, wps=22853.3, ups=1.92, wpb=11886.8, bsz=420.8, num_updates=8200, lr=2.79372e-05, gnorm=0.852, train_wall=52, wall=4325
2020-12-22 14:01:04 | INFO | train_inner | epoch 003:   2182 / 3059 symm_kl=0.307, self_kl=0, self_cv=4.154, loss=5.38, nll_loss=2.697, ppl=6.48, wps=23021.9, ups=1.94, wpb=11857, bsz=392.3, num_updates=8300, lr=2.77684e-05, gnorm=0.849, train_wall=51, wall=4377
2020-12-22 14:01:56 | INFO | train_inner | epoch 003:   2282 / 3059 symm_kl=0.31, self_kl=0, self_cv=4.152, loss=5.395, nll_loss=2.71, ppl=6.54, wps=22840.7, ups=1.92, wpb=11912.5, bsz=392.1, num_updates=8400, lr=2.76026e-05, gnorm=0.854, train_wall=52, wall=4429
2020-12-22 14:02:48 | INFO | train_inner | epoch 003:   2382 / 3059 symm_kl=0.307, self_kl=0, self_cv=4.153, loss=5.378, nll_loss=2.696, ppl=6.48, wps=22912.8, ups=1.93, wpb=11886.2, bsz=400.1, num_updates=8500, lr=2.74398e-05, gnorm=0.856, train_wall=52, wall=4481
2020-12-22 14:03:40 | INFO | train_inner | epoch 003:   2482 / 3059 symm_kl=0.307, self_kl=0, self_cv=4.138, loss=5.398, nll_loss=2.722, ppl=6.6, wps=22966.2, ups=1.93, wpb=11884.2, bsz=410.5, num_updates=8600, lr=2.72798e-05, gnorm=0.851, train_wall=52, wall=4532
2020-12-22 14:04:32 | INFO | train_inner | epoch 003:   2582 / 3059 symm_kl=0.306, self_kl=0, self_cv=4.152, loss=5.373, nll_loss=2.693, ppl=6.47, wps=22816.8, ups=1.92, wpb=11864.4, bsz=390.4, num_updates=8700, lr=2.71225e-05, gnorm=0.849, train_wall=52, wall=4584
2020-12-22 14:05:24 | INFO | train_inner | epoch 003:   2682 / 3059 symm_kl=0.309, self_kl=0, self_cv=4.142, loss=5.413, nll_loss=2.735, ppl=6.66, wps=22949, ups=1.93, wpb=11908.7, bsz=402.3, num_updates=8800, lr=2.6968e-05, gnorm=0.855, train_wall=52, wall=4636
2020-12-22 14:06:16 | INFO | train_inner | epoch 003:   2782 / 3059 symm_kl=0.306, self_kl=0, self_cv=4.141, loss=5.387, nll_loss=2.71, ppl=6.54, wps=22982.8, ups=1.94, wpb=11844, bsz=414.5, num_updates=8900, lr=2.68161e-05, gnorm=0.851, train_wall=51, wall=4688
2020-12-22 14:07:08 | INFO | train_inner | epoch 003:   2882 / 3059 symm_kl=0.306, self_kl=0, self_cv=4.145, loss=5.386, nll_loss=2.708, ppl=6.53, wps=22807.4, ups=1.92, wpb=11899.4, bsz=404.3, num_updates=9000, lr=2.66667e-05, gnorm=0.851, train_wall=52, wall=4740
2020-12-22 14:08:00 | INFO | train_inner | epoch 003:   2982 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.147, loss=5.38, nll_loss=2.703, ppl=6.51, wps=22900.4, ups=1.92, wpb=11898.8, bsz=421.7, num_updates=9100, lr=2.65197e-05, gnorm=0.852, train_wall=52, wall=4792
2020-12-22 14:08:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 14:08:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:08:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:08:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:08:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:08:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:08:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:08:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:08:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 14:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 14:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 14:08:55 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.824 | nll_loss 8.008 | ppl 257.46 | bleu 16.25 | wps 4863.2 | wpb 6344.2 | bsz 166.4 | num_updates 9177 | best_bleu 16.37
2020-12-22 14:08:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 14:09:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 3 @ 9177 updates, score 16.25) (writing took 5.042332258075476 seconds)
2020-12-22 14:09:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-22 14:09:00 | INFO | train | epoch 003 | symm_kl 0.312 | self_kl 0 | self_cv 4.155 | loss 5.392 | nll_loss 2.703 | ppl 6.51 | wps 22401.5 | ups 1.89 | wpb 11852.2 | bsz 409.6 | num_updates 9177 | lr 2.64082e-05 | gnorm 0.859 | train_wall 1582 | wall 4853
2020-12-22 14:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:09:04 | INFO | fairseq.trainer | begin training epoch 4
2020-12-22 14:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:09:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:09:22 | INFO | train_inner | epoch 004:     23 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.144, loss=5.381, nll_loss=2.705, ppl=6.52, wps=14443.2, ups=1.21, wpb=11936.8, bsz=398.2, num_updates=9200, lr=2.63752e-05, gnorm=0.85, train_wall=51, wall=4875
2020-12-22 14:10:14 | INFO | train_inner | epoch 004:    123 / 3059 symm_kl=0.304, self_kl=0, self_cv=4.15, loss=5.358, nll_loss=2.68, ppl=6.41, wps=23182.7, ups=1.95, wpb=11891.3, bsz=431.8, num_updates=9300, lr=2.6233e-05, gnorm=0.856, train_wall=51, wall=4926
2020-12-22 14:11:05 | INFO | train_inner | epoch 004:    223 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.157, loss=5.357, nll_loss=2.676, ppl=6.39, wps=22951.9, ups=1.93, wpb=11901.3, bsz=401.3, num_updates=9400, lr=2.60931e-05, gnorm=0.856, train_wall=52, wall=4978
2020-12-22 14:11:58 | INFO | train_inner | epoch 004:    323 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.153, loss=5.359, nll_loss=2.678, ppl=6.4, wps=22789.5, ups=1.92, wpb=11859.5, bsz=429.2, num_updates=9500, lr=2.59554e-05, gnorm=0.855, train_wall=52, wall=5030
2020-12-22 14:12:49 | INFO | train_inner | epoch 004:    423 / 3059 symm_kl=0.308, self_kl=0, self_cv=4.153, loss=5.396, nll_loss=2.715, ppl=6.57, wps=22843.2, ups=1.93, wpb=11859.3, bsz=404.3, num_updates=9600, lr=2.58199e-05, gnorm=0.859, train_wall=52, wall=5082
2020-12-22 14:13:41 | INFO | train_inner | epoch 004:    523 / 3059 symm_kl=0.307, self_kl=0, self_cv=4.146, loss=5.383, nll_loss=2.703, ppl=6.51, wps=22806, ups=1.93, wpb=11833.9, bsz=413.5, num_updates=9700, lr=2.56865e-05, gnorm=0.859, train_wall=52, wall=5134
2020-12-22 14:14:33 | INFO | train_inner | epoch 004:    623 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.152, loss=5.374, nll_loss=2.695, ppl=6.48, wps=22978.7, ups=1.93, wpb=11930.5, bsz=435, num_updates=9800, lr=2.55551e-05, gnorm=0.863, train_wall=52, wall=5186
2020-12-22 14:15:25 | INFO | train_inner | epoch 004:    723 / 3059 symm_kl=0.305, self_kl=0, self_cv=4.142, loss=5.384, nll_loss=2.708, ppl=6.53, wps=22743.1, ups=1.93, wpb=11757.9, bsz=392.5, num_updates=9900, lr=2.54257e-05, gnorm=0.863, train_wall=52, wall=5237
2020-12-22 14:16:17 | INFO | train_inner | epoch 004:    823 / 3059 symm_kl=0.304, self_kl=0, self_cv=4.147, loss=5.378, nll_loss=2.703, ppl=6.51, wps=22908.9, ups=1.93, wpb=11876.5, bsz=422.3, num_updates=10000, lr=2.52982e-05, gnorm=0.858, train_wall=52, wall=5289
2020-12-22 14:17:09 | INFO | train_inner | epoch 004:    923 / 3059 symm_kl=0.303, self_kl=0, self_cv=4.145, loss=5.363, nll_loss=2.687, ppl=6.44, wps=22853.3, ups=1.93, wpb=11865.8, bsz=399.1, num_updates=10100, lr=2.51727e-05, gnorm=0.859, train_wall=52, wall=5341
2020-12-22 14:18:01 | INFO | train_inner | epoch 004:   1023 / 3059 symm_kl=0.304, self_kl=0, self_cv=4.144, loss=5.381, nll_loss=2.707, ppl=6.53, wps=22719.2, ups=1.93, wpb=11770.5, bsz=395.4, num_updates=10200, lr=2.5049e-05, gnorm=0.867, train_wall=52, wall=5393
2020-12-22 14:18:52 | INFO | train_inner | epoch 004:   1123 / 3059 symm_kl=0.302, self_kl=0, self_cv=4.15, loss=5.357, nll_loss=2.683, ppl=6.42, wps=22962.2, ups=1.93, wpb=11916.4, bsz=434, num_updates=10300, lr=2.49271e-05, gnorm=0.849, train_wall=52, wall=5445
2020-12-22 14:19:45 | INFO | train_inner | epoch 004:   1223 / 3059 symm_kl=0.304, self_kl=0, self_cv=4.137, loss=5.382, nll_loss=2.71, ppl=6.54, wps=22630, ups=1.92, wpb=11812.9, bsz=428.6, num_updates=10400, lr=2.48069e-05, gnorm=0.863, train_wall=52, wall=5497
2020-12-22 14:20:37 | INFO | train_inner | epoch 004:   1323 / 3059 symm_kl=0.303, self_kl=0, self_cv=4.147, loss=5.371, nll_loss=2.697, ppl=6.49, wps=22717.1, ups=1.92, wpb=11858.1, bsz=399.5, num_updates=10500, lr=2.46885e-05, gnorm=0.859, train_wall=52, wall=5549
2020-12-22 14:21:29 | INFO | train_inner | epoch 004:   1423 / 3059 symm_kl=0.303, self_kl=0, self_cv=4.143, loss=5.376, nll_loss=2.703, ppl=6.51, wps=22639.3, ups=1.93, wpb=11757.7, bsz=381.8, num_updates=10600, lr=2.45718e-05, gnorm=0.853, train_wall=52, wall=5601
2020-12-22 14:22:21 | INFO | train_inner | epoch 004:   1523 / 3059 symm_kl=0.304, self_kl=0, self_cv=4.143, loss=5.376, nll_loss=2.702, ppl=6.51, wps=22775.4, ups=1.92, wpb=11858, bsz=409.7, num_updates=10700, lr=2.44567e-05, gnorm=0.863, train_wall=52, wall=5653
2020-12-22 14:23:13 | INFO | train_inner | epoch 004:   1623 / 3059 symm_kl=0.302, self_kl=0, self_cv=4.148, loss=5.376, nll_loss=2.704, ppl=6.52, wps=22872.7, ups=1.92, wpb=11906.2, bsz=386.4, num_updates=10800, lr=2.43432e-05, gnorm=0.846, train_wall=52, wall=5705
2020-12-22 14:24:05 | INFO | train_inner | epoch 004:   1723 / 3059 symm_kl=0.302, self_kl=0, self_cv=4.136, loss=5.379, nll_loss=2.71, ppl=6.54, wps=22786.5, ups=1.93, wpb=11783.9, bsz=398.6, num_updates=10900, lr=2.42313e-05, gnorm=0.86, train_wall=52, wall=5757
2020-12-22 14:24:57 | INFO | train_inner | epoch 004:   1823 / 3059 symm_kl=0.303, self_kl=0, self_cv=4.135, loss=5.387, nll_loss=2.718, ppl=6.58, wps=22689.5, ups=1.92, wpb=11819.5, bsz=398.2, num_updates=11000, lr=2.41209e-05, gnorm=0.864, train_wall=52, wall=5809
2020-12-22 14:25:49 | INFO | train_inner | epoch 004:   1923 / 3059 symm_kl=0.302, self_kl=0, self_cv=4.134, loss=5.387, nll_loss=2.72, ppl=6.59, wps=22565.9, ups=1.92, wpb=11745.6, bsz=414.7, num_updates=11100, lr=2.4012e-05, gnorm=0.884, train_wall=52, wall=5861
2020-12-22 14:26:41 | INFO | train_inner | epoch 004:   2023 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.138, loss=5.368, nll_loss=2.702, ppl=6.51, wps=22883.9, ups=1.91, wpb=11964.1, bsz=418.9, num_updates=11200, lr=2.39046e-05, gnorm=0.851, train_wall=52, wall=5913
2020-12-22 14:27:33 | INFO | train_inner | epoch 004:   2123 / 3059 symm_kl=0.301, self_kl=0, self_cv=4.14, loss=5.378, nll_loss=2.71, ppl=6.54, wps=22808.6, ups=1.92, wpb=11865.6, bsz=378.8, num_updates=11300, lr=2.37986e-05, gnorm=0.853, train_wall=52, wall=5965
2020-12-22 14:28:25 | INFO | train_inner | epoch 004:   2223 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.139, loss=5.369, nll_loss=2.703, ppl=6.51, wps=22858.4, ups=1.93, wpb=11847.4, bsz=406.9, num_updates=11400, lr=2.3694e-05, gnorm=0.863, train_wall=52, wall=6017
2020-12-22 14:29:17 | INFO | train_inner | epoch 004:   2323 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.141, loss=5.362, nll_loss=2.695, ppl=6.48, wps=22733.7, ups=1.93, wpb=11807.8, bsz=418, num_updates=11500, lr=2.35907e-05, gnorm=0.866, train_wall=52, wall=6069
2020-12-22 14:30:09 | INFO | train_inner | epoch 004:   2423 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.147, loss=5.362, nll_loss=2.694, ppl=6.47, wps=22731.1, ups=1.91, wpb=11871.1, bsz=425.9, num_updates=11600, lr=2.34888e-05, gnorm=0.849, train_wall=52, wall=6121
2020-12-22 14:31:01 | INFO | train_inner | epoch 004:   2523 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.138, loss=5.35, nll_loss=2.684, ppl=6.43, wps=22829.2, ups=1.91, wpb=11933.5, bsz=419, num_updates=11700, lr=2.33882e-05, gnorm=0.844, train_wall=52, wall=6174
2020-12-22 14:31:53 | INFO | train_inner | epoch 004:   2623 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.139, loss=5.366, nll_loss=2.702, ppl=6.51, wps=22878.3, ups=1.93, wpb=11855.8, bsz=420.9, num_updates=11800, lr=2.32889e-05, gnorm=0.853, train_wall=52, wall=6225
2020-12-22 14:32:45 | INFO | train_inner | epoch 004:   2723 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.136, loss=5.381, nll_loss=2.718, ppl=6.58, wps=23067.4, ups=1.93, wpb=11929.5, bsz=409.4, num_updates=11900, lr=2.31908e-05, gnorm=0.851, train_wall=52, wall=6277
2020-12-22 14:33:37 | INFO | train_inner | epoch 004:   2823 / 3059 symm_kl=0.301, self_kl=0, self_cv=4.129, loss=5.398, nll_loss=2.736, ppl=6.66, wps=22815.5, ups=1.92, wpb=11859.1, bsz=404.6, num_updates=12000, lr=2.3094e-05, gnorm=0.857, train_wall=52, wall=6329
2020-12-22 14:34:29 | INFO | train_inner | epoch 004:   2923 / 3059 symm_kl=0.3, self_kl=0, self_cv=4.133, loss=5.389, nll_loss=2.725, ppl=6.61, wps=22676.5, ups=1.92, wpb=11831.6, bsz=388.3, num_updates=12100, lr=2.29984e-05, gnorm=0.859, train_wall=52, wall=6381
2020-12-22 14:35:21 | INFO | train_inner | epoch 004:   3023 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.121, loss=5.396, nll_loss=2.737, ppl=6.67, wps=22651.7, ups=1.93, wpb=11763.9, bsz=408.5, num_updates=12200, lr=2.29039e-05, gnorm=0.86, train_wall=52, wall=6433
2020-12-22 14:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 14:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:35:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 14:35:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 14:35:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 14:35:57 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.796 | nll_loss 7.98 | ppl 252.46 | bleu 16.49 | wps 4233 | wpb 6344.2 | bsz 166.4 | num_updates 12236 | best_bleu 16.49
2020-12-22 14:35:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 14:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:36:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 4 @ 12236 updates, score 16.49) (writing took 8.290107294917107 seconds)
2020-12-22 14:36:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-22 14:36:06 | INFO | train | epoch 004 | symm_kl 0.302 | self_kl 0 | self_cv 4.142 | loss 5.374 | nll_loss 2.702 | ppl 6.51 | wps 22309.7 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 12236 | lr 2.28702e-05 | gnorm 0.858 | train_wall 1584 | wall 6478
2020-12-22 14:36:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:36:09 | INFO | fairseq.trainer | begin training epoch 5
2020-12-22 14:36:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 14:36:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 14:36:49 | INFO | train_inner | epoch 005:     64 / 3059 symm_kl=0.3, self_kl=0, self_cv=4.138, loss=5.383, nll_loss=2.718, ppl=6.58, wps=13388.3, ups=1.14, wpb=11743.5, bsz=405.9, num_updates=12300, lr=2.28106e-05, gnorm=0.863, train_wall=51, wall=6521
2020-12-22 14:37:41 | INFO | train_inner | epoch 005:    164 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.142, loss=5.354, nll_loss=2.688, ppl=6.45, wps=22716.8, ups=1.93, wpb=11789.6, bsz=451.6, num_updates=12400, lr=2.27185e-05, gnorm=0.865, train_wall=52, wall=6573
2020-12-22 14:38:33 | INFO | train_inner | epoch 005:    264 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.146, loss=5.362, nll_loss=2.694, ppl=6.47, wps=22720.7, ups=1.91, wpb=11885.8, bsz=382.1, num_updates=12500, lr=2.26274e-05, gnorm=0.855, train_wall=52, wall=6625
2020-12-22 14:39:25 | INFO | train_inner | epoch 005:    364 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.142, loss=5.352, nll_loss=2.685, ppl=6.43, wps=22833.1, ups=1.92, wpb=11915.5, bsz=408.1, num_updates=12600, lr=2.25374e-05, gnorm=0.858, train_wall=52, wall=6677
2020-12-22 14:40:17 | INFO | train_inner | epoch 005:    464 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.138, loss=5.361, nll_loss=2.698, ppl=6.49, wps=22849, ups=1.93, wpb=11865.2, bsz=418.8, num_updates=12700, lr=2.24485e-05, gnorm=0.856, train_wall=52, wall=6729
2020-12-22 14:41:09 | INFO | train_inner | epoch 005:    564 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.133, loss=5.356, nll_loss=2.693, ppl=6.47, wps=22714.8, ups=1.92, wpb=11815, bsz=425.4, num_updates=12800, lr=2.23607e-05, gnorm=0.859, train_wall=52, wall=6781
2020-12-22 14:42:01 | INFO | train_inner | epoch 005:    664 / 3059 symm_kl=0.299, self_kl=0, self_cv=4.141, loss=5.371, nll_loss=2.705, ppl=6.52, wps=22835.4, ups=1.93, wpb=11840.7, bsz=383.6, num_updates=12900, lr=2.22738e-05, gnorm=0.857, train_wall=52, wall=6833
2020-12-22 14:42:53 | INFO | train_inner | epoch 005:    764 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.14, loss=5.353, nll_loss=2.689, ppl=6.45, wps=22913.2, ups=1.93, wpb=11869.9, bsz=419.7, num_updates=13000, lr=2.2188e-05, gnorm=0.859, train_wall=52, wall=6885
2020-12-22 14:43:44 | INFO | train_inner | epoch 005:    864 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.134, loss=5.36, nll_loss=2.695, ppl=6.48, wps=22939.8, ups=1.93, wpb=11886.4, bsz=410.4, num_updates=13100, lr=2.21032e-05, gnorm=0.854, train_wall=52, wall=6937
2020-12-22 14:44:37 | INFO | train_inner | epoch 005:    964 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.127, loss=5.371, nll_loss=2.711, ppl=6.55, wps=22436.5, ups=1.92, wpb=11714.1, bsz=405.9, num_updates=13200, lr=2.20193e-05, gnorm=0.856, train_wall=52, wall=6989
2020-12-22 14:45:29 | INFO | train_inner | epoch 005:   1064 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.141, loss=5.36, nll_loss=2.696, ppl=6.48, wps=22874.3, ups=1.92, wpb=11905, bsz=394.8, num_updates=13300, lr=2.19363e-05, gnorm=0.855, train_wall=52, wall=7041
2020-12-22 14:46:21 | INFO | train_inner | epoch 005:   1164 / 3059 symm_kl=0.296, self_kl=0, self_cv=4.135, loss=5.354, nll_loss=2.694, ppl=6.47, wps=22852.3, ups=1.92, wpb=11887.6, bsz=413.5, num_updates=13400, lr=2.18543e-05, gnorm=0.853, train_wall=52, wall=7093
2020-12-22 14:47:13 | INFO | train_inner | epoch 005:   1264 / 3059 symm_kl=0.296, self_kl=0, self_cv=4.137, loss=5.348, nll_loss=2.686, ppl=6.44, wps=22789.2, ups=1.93, wpb=11831.6, bsz=413.4, num_updates=13500, lr=2.17732e-05, gnorm=0.854, train_wall=52, wall=7145
2020-12-22 14:48:05 | INFO | train_inner | epoch 005:   1364 / 3059 symm_kl=0.298, self_kl=0, self_cv=4.138, loss=5.37, nll_loss=2.707, ppl=6.53, wps=22668.3, ups=1.92, wpb=11828.5, bsz=389.9, num_updates=13600, lr=2.1693e-05, gnorm=0.874, train_wall=52, wall=7197
2020-12-22 14:48:57 | INFO | train_inner | epoch 005:   1464 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.145, loss=5.327, nll_loss=2.667, ppl=6.35, wps=22904.9, ups=1.92, wpb=11934.8, bsz=438.3, num_updates=13700, lr=2.16137e-05, gnorm=0.849, train_wall=52, wall=7249
2020-12-22 14:49:49 | INFO | train_inner | epoch 005:   1564 / 3059 symm_kl=0.295, self_kl=0, self_cv=4.135, loss=5.353, nll_loss=2.693, ppl=6.46, wps=22577.5, ups=1.91, wpb=11847.5, bsz=428.6, num_updates=13800, lr=2.15353e-05, gnorm=0.851, train_wall=52, wall=7302
2020-12-22 14:50:42 | INFO | train_inner | epoch 005:   1664 / 3059 symm_kl=0.296, self_kl=0, self_cv=4.133, loss=5.365, nll_loss=2.705, ppl=6.52, wps=22730.1, ups=1.91, wpb=11897, bsz=427.4, num_updates=13900, lr=2.14577e-05, gnorm=0.852, train_wall=52, wall=7354
2020-12-22 14:51:34 | INFO | train_inner | epoch 005:   1764 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.137, loss=5.368, nll_loss=2.706, ppl=6.52, wps=22615.8, ups=1.91, wpb=11837.1, bsz=401, num_updates=14000, lr=2.13809e-05, gnorm=0.856, train_wall=52, wall=7406
2020-12-22 14:52:27 | INFO | train_inner | epoch 005:   1864 / 3059 symm_kl=0.296, self_kl=0, self_cv=4.13, loss=5.37, nll_loss=2.713, ppl=6.56, wps=22579.8, ups=1.91, wpb=11841.5, bsz=410.4, num_updates=14100, lr=2.13049e-05, gnorm=0.856, train_wall=52, wall=7459
2020-12-22 14:53:18 | INFO | train_inner | epoch 005:   1964 / 3059 symm_kl=0.296, self_kl=0, self_cv=4.131, loss=5.37, nll_loss=2.712, ppl=6.55, wps=22891.8, ups=1.93, wpb=11867.2, bsz=392, num_updates=14200, lr=2.12298e-05, gnorm=0.853, train_wall=52, wall=7511
2020-12-22 14:54:10 | INFO | train_inner | epoch 005:   2064 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.127, loss=5.343, nll_loss=2.689, ppl=6.45, wps=22537.9, ups=1.92, wpb=11710.4, bsz=425.5, num_updates=14300, lr=2.11554e-05, gnorm=0.87, train_wall=52, wall=7563
2020-12-22 14:55:03 | INFO | train_inner | epoch 005:   2164 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.131, loss=5.376, nll_loss=2.718, ppl=6.58, wps=22760.5, ups=1.92, wpb=11883.9, bsz=396.3, num_updates=14400, lr=2.10819e-05, gnorm=0.855, train_wall=52, wall=7615
2020-12-22 14:55:55 | INFO | train_inner | epoch 005:   2264 / 3059 symm_kl=0.294, self_kl=0, self_cv=4.137, loss=5.355, nll_loss=2.698, ppl=6.49, wps=22753.2, ups=1.92, wpb=11848.6, bsz=408.6, num_updates=14500, lr=2.1009e-05, gnorm=0.86, train_wall=52, wall=7667
2020-12-22 14:56:47 | INFO | train_inner | epoch 005:   2364 / 3059 symm_kl=0.295, self_kl=0, self_cv=4.135, loss=5.366, nll_loss=2.709, ppl=6.54, wps=22730.1, ups=1.91, wpb=11880.1, bsz=418.2, num_updates=14600, lr=2.0937e-05, gnorm=0.853, train_wall=52, wall=7719
2020-12-22 14:57:39 | INFO | train_inner | epoch 005:   2464 / 3059 symm_kl=0.294, self_kl=0, self_cv=4.134, loss=5.358, nll_loss=2.702, ppl=6.51, wps=22913.9, ups=1.92, wpb=11933.6, bsz=394.5, num_updates=14700, lr=2.08656e-05, gnorm=0.855, train_wall=52, wall=7771
2020-12-22 14:58:31 | INFO | train_inner | epoch 005:   2564 / 3059 symm_kl=0.297, self_kl=0, self_cv=4.129, loss=5.391, nll_loss=2.734, ppl=6.65, wps=22798.7, ups=1.94, wpb=11762.6, bsz=394.5, num_updates=14800, lr=2.0795e-05, gnorm=0.862, train_wall=51, wall=7823
2020-12-22 14:59:23 | INFO | train_inner | epoch 005:   2664 / 3059 symm_kl=0.294, self_kl=0, self_cv=4.133, loss=5.366, nll_loss=2.71, ppl=6.54, wps=22771.1, ups=1.92, wpb=11848, bsz=411.6, num_updates=14900, lr=2.07251e-05, gnorm=0.853, train_wall=52, wall=7875
2020-12-22 15:00:15 | INFO | train_inner | epoch 005:   2764 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.137, loss=5.344, nll_loss=2.689, ppl=6.45, wps=22907, ups=1.93, wpb=11895.1, bsz=421.2, num_updates=15000, lr=2.06559e-05, gnorm=0.851, train_wall=52, wall=7927
2020-12-22 15:01:07 | INFO | train_inner | epoch 005:   2864 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.129, loss=5.367, nll_loss=2.714, ppl=6.56, wps=22841, ups=1.92, wpb=11872.4, bsz=396.2, num_updates=15100, lr=2.05874e-05, gnorm=0.862, train_wall=52, wall=7979
2020-12-22 15:01:58 | INFO | train_inner | epoch 005:   2964 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.132, loss=5.359, nll_loss=2.704, ppl=6.52, wps=22860.3, ups=1.93, wpb=11871.6, bsz=410.8, num_updates=15200, lr=2.05196e-05, gnorm=0.862, train_wall=52, wall=8031
2020-12-22 15:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 15:02:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:02:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:02:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:02:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:03:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 15:03:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 15:03:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 15:03:04 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.791 | nll_loss 7.977 | ppl 251.88 | bleu 16.27 | wps 4885.4 | wpb 6344.2 | bsz 166.4 | num_updates 15295 | best_bleu 16.49
2020-12-22 15:03:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 15:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 5 @ 15295 updates, score 16.27) (writing took 5.217457806691527 seconds)
2020-12-22 15:03:09 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-22 15:03:09 | INFO | train | epoch 005 | symm_kl 0.296 | self_kl 0 | self_cv 4.135 | loss 5.361 | nll_loss 2.701 | ppl 6.5 | wps 22333.6 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 15295 | lr 2.04557e-05 | gnorm 0.857 | train_wall 1587 | wall 8101
2020-12-22 15:03:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:03:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:03:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:03:13 | INFO | fairseq.trainer | begin training epoch 6
2020-12-22 15:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:03:22 | INFO | train_inner | epoch 006:      5 / 3059 symm_kl=0.294, self_kl=0, self_cv=4.128, loss=5.366, nll_loss=2.713, ppl=6.56, wps=14297.7, ups=1.2, wpb=11891.8, bsz=403.8, num_updates=15300, lr=2.04524e-05, gnorm=0.852, train_wall=52, wall=8114
2020-12-22 15:04:13 | INFO | train_inner | epoch 006:    105 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.14, loss=5.331, nll_loss=2.673, ppl=6.38, wps=22971.1, ups=1.94, wpb=11849.4, bsz=433, num_updates=15400, lr=2.03859e-05, gnorm=0.85, train_wall=51, wall=8166
2020-12-22 15:05:05 | INFO | train_inner | epoch 006:    205 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.143, loss=5.339, nll_loss=2.68, ppl=6.41, wps=22959.8, ups=1.93, wpb=11914.3, bsz=393.1, num_updates=15500, lr=2.032e-05, gnorm=0.844, train_wall=52, wall=8217
2020-12-22 15:05:57 | INFO | train_inner | epoch 006:    305 / 3059 symm_kl=0.294, self_kl=0, self_cv=4.132, loss=5.351, nll_loss=2.694, ppl=6.47, wps=22803.1, ups=1.92, wpb=11851.3, bsz=413.7, num_updates=15600, lr=2.02548e-05, gnorm=0.866, train_wall=52, wall=8269
2020-12-22 15:06:49 | INFO | train_inner | epoch 006:    405 / 3059 symm_kl=0.295, self_kl=0, self_cv=4.119, loss=5.377, nll_loss=2.724, ppl=6.61, wps=22492.7, ups=1.92, wpb=11706.5, bsz=408.6, num_updates=15700, lr=2.01902e-05, gnorm=0.88, train_wall=52, wall=8321
2020-12-22 15:07:41 | INFO | train_inner | epoch 006:    505 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.132, loss=5.347, nll_loss=2.691, ppl=6.46, wps=22843.7, ups=1.92, wpb=11875.3, bsz=411.5, num_updates=15800, lr=2.01262e-05, gnorm=0.857, train_wall=52, wall=8373
2020-12-22 15:08:33 | INFO | train_inner | epoch 006:    605 / 3059 symm_kl=0.295, self_kl=0, self_cv=4.134, loss=5.371, nll_loss=2.713, ppl=6.56, wps=22713.2, ups=1.92, wpb=11857.2, bsz=390.2, num_updates=15900, lr=2.00628e-05, gnorm=0.86, train_wall=52, wall=8426
2020-12-22 15:09:25 | INFO | train_inner | epoch 006:    705 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.126, loss=5.347, nll_loss=2.693, ppl=6.47, wps=22720.7, ups=1.93, wpb=11798.9, bsz=427.2, num_updates=16000, lr=2e-05, gnorm=0.868, train_wall=52, wall=8478
2020-12-22 15:10:18 | INFO | train_inner | epoch 006:    805 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.136, loss=5.346, nll_loss=2.691, ppl=6.46, wps=22597.1, ups=1.91, wpb=11839.5, bsz=422.7, num_updates=16100, lr=1.99378e-05, gnorm=0.857, train_wall=52, wall=8530
2020-12-22 15:11:10 | INFO | train_inner | epoch 006:    905 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.137, loss=5.338, nll_loss=2.683, ppl=6.42, wps=22857.2, ups=1.92, wpb=11913.5, bsz=411.6, num_updates=16200, lr=1.98762e-05, gnorm=0.848, train_wall=52, wall=8582
2020-12-22 15:12:02 | INFO | train_inner | epoch 006:   1005 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.132, loss=5.355, nll_loss=2.701, ppl=6.5, wps=22695.5, ups=1.91, wpb=11865.6, bsz=406.7, num_updates=16300, lr=1.98151e-05, gnorm=0.851, train_wall=52, wall=8634
2020-12-22 15:12:54 | INFO | train_inner | epoch 006:   1105 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.133, loss=5.347, nll_loss=2.694, ppl=6.47, wps=22719.3, ups=1.92, wpb=11857, bsz=417.9, num_updates=16400, lr=1.97546e-05, gnorm=0.852, train_wall=52, wall=8687
2020-12-22 15:13:46 | INFO | train_inner | epoch 006:   1205 / 3059 symm_kl=0.295, self_kl=0, self_cv=4.109, loss=5.395, nll_loss=2.747, ppl=6.72, wps=22668.7, ups=1.92, wpb=11791.7, bsz=413.4, num_updates=16500, lr=1.96946e-05, gnorm=0.881, train_wall=52, wall=8739
2020-12-22 15:14:38 | INFO | train_inner | epoch 006:   1305 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.133, loss=5.347, nll_loss=2.694, ppl=6.47, wps=22899.3, ups=1.92, wpb=11904.1, bsz=424.1, num_updates=16600, lr=1.96352e-05, gnorm=0.849, train_wall=52, wall=8791
2020-12-22 15:15:30 | INFO | train_inner | epoch 006:   1405 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.132, loss=5.362, nll_loss=2.71, ppl=6.54, wps=22941.6, ups=1.92, wpb=11918.7, bsz=419.3, num_updates=16700, lr=1.95764e-05, gnorm=0.851, train_wall=52, wall=8843
2020-12-22 15:16:22 | INFO | train_inner | epoch 006:   1505 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.131, loss=5.357, nll_loss=2.704, ppl=6.52, wps=22940.9, ups=1.93, wpb=11887.8, bsz=395.5, num_updates=16800, lr=1.9518e-05, gnorm=0.853, train_wall=52, wall=8894
2020-12-22 15:17:14 | INFO | train_inner | epoch 006:   1605 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.123, loss=5.366, nll_loss=2.715, ppl=6.57, wps=22611.5, ups=1.92, wpb=11763.8, bsz=414.3, num_updates=16900, lr=1.94602e-05, gnorm=0.862, train_wall=52, wall=8946
2020-12-22 15:18:06 | INFO | train_inner | epoch 006:   1705 / 3059 symm_kl=0.293, self_kl=0, self_cv=4.128, loss=5.37, nll_loss=2.719, ppl=6.58, wps=22633.9, ups=1.91, wpb=11832.1, bsz=407.7, num_updates=17000, lr=1.94029e-05, gnorm=0.861, train_wall=52, wall=8999
2020-12-22 15:18:58 | INFO | train_inner | epoch 006:   1805 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.133, loss=5.343, nll_loss=2.69, ppl=6.45, wps=22735.4, ups=1.92, wpb=11861.3, bsz=399.3, num_updates=17100, lr=1.9346e-05, gnorm=0.86, train_wall=52, wall=9051
2020-12-22 15:19:51 | INFO | train_inner | epoch 006:   1905 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.134, loss=5.352, nll_loss=2.7, ppl=6.5, wps=22809.1, ups=1.92, wpb=11888.5, bsz=384.8, num_updates=17200, lr=1.92897e-05, gnorm=0.851, train_wall=52, wall=9103
2020-12-22 15:20:43 | INFO | train_inner | epoch 006:   2005 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.131, loss=5.342, nll_loss=2.692, ppl=6.46, wps=22689.3, ups=1.92, wpb=11819.5, bsz=414.5, num_updates=17300, lr=1.92339e-05, gnorm=0.864, train_wall=52, wall=9155
2020-12-22 15:21:35 | INFO | train_inner | epoch 006:   2105 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.121, loss=5.344, nll_loss=2.697, ppl=6.48, wps=22705.1, ups=1.93, wpb=11766, bsz=418.2, num_updates=17400, lr=1.91785e-05, gnorm=0.856, train_wall=52, wall=9207
2020-12-22 15:22:27 | INFO | train_inner | epoch 006:   2205 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.134, loss=5.346, nll_loss=2.695, ppl=6.48, wps=22791.9, ups=1.92, wpb=11895.2, bsz=388.8, num_updates=17500, lr=1.91237e-05, gnorm=0.854, train_wall=52, wall=9259
2020-12-22 15:23:19 | INFO | train_inner | epoch 006:   2305 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.126, loss=5.351, nll_loss=2.701, ppl=6.5, wps=22725.1, ups=1.93, wpb=11804.2, bsz=402.7, num_updates=17600, lr=1.90693e-05, gnorm=0.864, train_wall=52, wall=9311
2020-12-22 15:24:11 | INFO | train_inner | epoch 006:   2405 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.127, loss=5.358, nll_loss=2.708, ppl=6.53, wps=22771.9, ups=1.92, wpb=11833.7, bsz=396.2, num_updates=17700, lr=1.90153e-05, gnorm=0.861, train_wall=52, wall=9363
2020-12-22 15:25:03 | INFO | train_inner | epoch 006:   2505 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.136, loss=5.348, nll_loss=2.697, ppl=6.48, wps=22797, ups=1.92, wpb=11870.4, bsz=392.6, num_updates=17800, lr=1.89618e-05, gnorm=0.868, train_wall=52, wall=9415
2020-12-22 15:25:55 | INFO | train_inner | epoch 006:   2605 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.127, loss=5.354, nll_loss=2.704, ppl=6.52, wps=22922.1, ups=1.93, wpb=11892.6, bsz=413.8, num_updates=17900, lr=1.89088e-05, gnorm=0.874, train_wall=52, wall=9467
2020-12-22 15:26:47 | INFO | train_inner | epoch 006:   2705 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.128, loss=5.361, nll_loss=2.714, ppl=6.56, wps=22864.8, ups=1.92, wpb=11893, bsz=412.1, num_updates=18000, lr=1.88562e-05, gnorm=0.857, train_wall=52, wall=9519
2020-12-22 15:27:39 | INFO | train_inner | epoch 006:   2805 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.122, loss=5.358, nll_loss=2.712, ppl=6.55, wps=22597, ups=1.9, wpb=11879.6, bsz=420.5, num_updates=18100, lr=1.8804e-05, gnorm=0.861, train_wall=52, wall=9572
2020-12-22 15:28:31 | INFO | train_inner | epoch 006:   2905 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.123, loss=5.344, nll_loss=2.699, ppl=6.49, wps=22848.7, ups=1.93, wpb=11868.2, bsz=418.8, num_updates=18200, lr=1.87523e-05, gnorm=0.86, train_wall=52, wall=9623
2020-12-22 15:29:23 | INFO | train_inner | epoch 006:   3005 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.128, loss=5.335, nll_loss=2.687, ppl=6.44, wps=22678.2, ups=1.92, wpb=11828.5, bsz=407.8, num_updates=18300, lr=1.8701e-05, gnorm=0.856, train_wall=52, wall=9676
2020-12-22 15:29:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 15:29:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:29:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:29:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:29:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:29:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:29:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:29:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:29:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:30:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 15:30:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 15:30:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 15:30:08 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.777 | nll_loss 7.961 | ppl 249.19 | bleu 16.35 | wps 4655.9 | wpb 6344.2 | bsz 166.4 | num_updates 18354 | best_bleu 16.49
2020-12-22 15:30:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 15:30:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 6 @ 18354 updates, score 16.35) (writing took 4.860144555568695 seconds)
2020-12-22 15:30:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-22 15:30:13 | INFO | train | epoch 006 | symm_kl 0.291 | self_kl 0 | self_cv 4.129 | loss 5.352 | nll_loss 2.7 | ppl 6.5 | wps 22327.7 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 18354 | lr 1.86735e-05 | gnorm 0.859 | train_wall 1587 | wall 9725
2020-12-22 15:30:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:30:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:30:17 | INFO | fairseq.trainer | begin training epoch 7
2020-12-22 15:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:30:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:30:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:30:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:30:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:30:47 | INFO | train_inner | epoch 007:     46 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.129, loss=5.318, nll_loss=2.671, ppl=6.37, wps=14138, ups=1.19, wpb=11900, bsz=424.6, num_updates=18400, lr=1.86501e-05, gnorm=0.851, train_wall=52, wall=9760
2020-12-22 15:31:40 | INFO | train_inner | epoch 007:    146 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.132, loss=5.353, nll_loss=2.702, ppl=6.51, wps=22820.8, ups=1.92, wpb=11894.2, bsz=386.7, num_updates=18500, lr=1.85996e-05, gnorm=0.852, train_wall=52, wall=9812
2020-12-22 15:32:32 | INFO | train_inner | epoch 007:    246 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.131, loss=5.318, nll_loss=2.669, ppl=6.36, wps=22791.5, ups=1.92, wpb=11854.9, bsz=412.4, num_updates=18600, lr=1.85496e-05, gnorm=0.852, train_wall=52, wall=9864
2020-12-22 15:33:24 | INFO | train_inner | epoch 007:    346 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.128, loss=5.319, nll_loss=2.671, ppl=6.37, wps=22681.3, ups=1.9, wpb=11914.7, bsz=439.7, num_updates=18700, lr=1.84999e-05, gnorm=0.858, train_wall=52, wall=9916
2020-12-22 15:34:17 | INFO | train_inner | epoch 007:    446 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.135, loss=5.35, nll_loss=2.699, ppl=6.49, wps=22666.8, ups=1.91, wpb=11898.4, bsz=388.5, num_updates=18800, lr=1.84506e-05, gnorm=0.86, train_wall=52, wall=9969
2020-12-22 15:35:09 | INFO | train_inner | epoch 007:    546 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.124, loss=5.356, nll_loss=2.708, ppl=6.53, wps=22558.5, ups=1.91, wpb=11788.8, bsz=414.4, num_updates=18900, lr=1.84017e-05, gnorm=0.872, train_wall=52, wall=10021
2020-12-22 15:36:01 | INFO | train_inner | epoch 007:    646 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.125, loss=5.326, nll_loss=2.679, ppl=6.41, wps=22814.4, ups=1.91, wpb=11950.8, bsz=443.5, num_updates=19000, lr=1.83533e-05, gnorm=0.855, train_wall=52, wall=10074
2020-12-22 15:36:53 | INFO | train_inner | epoch 007:    746 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.128, loss=5.348, nll_loss=2.702, ppl=6.51, wps=22644.6, ups=1.92, wpb=11783.7, bsz=406.3, num_updates=19100, lr=1.83052e-05, gnorm=0.861, train_wall=52, wall=10126
2020-12-22 15:37:46 | INFO | train_inner | epoch 007:    846 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.132, loss=5.332, nll_loss=2.685, ppl=6.43, wps=22752.2, ups=1.91, wpb=11904.7, bsz=402.3, num_updates=19200, lr=1.82574e-05, gnorm=0.856, train_wall=52, wall=10178
2020-12-22 15:38:38 | INFO | train_inner | epoch 007:    946 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.119, loss=5.354, nll_loss=2.708, ppl=6.53, wps=22622, ups=1.92, wpb=11760.4, bsz=399.4, num_updates=19300, lr=1.82101e-05, gnorm=0.875, train_wall=52, wall=10230
2020-12-22 15:39:30 | INFO | train_inner | epoch 007:   1046 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.123, loss=5.348, nll_loss=2.702, ppl=6.51, wps=22634.6, ups=1.91, wpb=11823.5, bsz=408.7, num_updates=19400, lr=1.81631e-05, gnorm=0.861, train_wall=52, wall=10282
2020-12-22 15:40:22 | INFO | train_inner | epoch 007:   1146 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.132, loss=5.337, nll_loss=2.689, ppl=6.45, wps=22822.2, ups=1.91, wpb=11929, bsz=407.4, num_updates=19500, lr=1.81164e-05, gnorm=0.856, train_wall=52, wall=10334
2020-12-22 15:41:15 | INFO | train_inner | epoch 007:   1246 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.121, loss=5.366, nll_loss=2.719, ppl=6.58, wps=22525.5, ups=1.9, wpb=11830.4, bsz=408, num_updates=19600, lr=1.80702e-05, gnorm=0.869, train_wall=52, wall=10387
2020-12-22 15:42:07 | INFO | train_inner | epoch 007:   1346 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.122, loss=5.351, nll_loss=2.707, ppl=6.53, wps=22639, ups=1.91, wpb=11869.9, bsz=420.6, num_updates=19700, lr=1.80242e-05, gnorm=0.86, train_wall=52, wall=10439
2020-12-22 15:42:59 | INFO | train_inner | epoch 007:   1446 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.128, loss=5.331, nll_loss=2.684, ppl=6.42, wps=22887.8, ups=1.91, wpb=11999.1, bsz=411.4, num_updates=19800, lr=1.79787e-05, gnorm=0.851, train_wall=52, wall=10492
2020-12-22 15:43:52 | INFO | train_inner | epoch 007:   1546 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.121, loss=5.359, nll_loss=2.713, ppl=6.56, wps=22754.2, ups=1.92, wpb=11868.4, bsz=402.9, num_updates=19900, lr=1.79334e-05, gnorm=0.862, train_wall=52, wall=10544
2020-12-22 15:44:44 | INFO | train_inner | epoch 007:   1646 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.127, loss=5.361, nll_loss=2.712, ppl=6.55, wps=22407.4, ups=1.9, wpb=11764.3, bsz=398, num_updates=20000, lr=1.78885e-05, gnorm=0.87, train_wall=52, wall=10596
2020-12-22 15:45:36 | INFO | train_inner | epoch 007:   1746 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.127, loss=5.344, nll_loss=2.7, ppl=6.5, wps=22712.3, ups=1.91, wpb=11884.3, bsz=403.8, num_updates=20100, lr=1.7844e-05, gnorm=0.857, train_wall=52, wall=10649
2020-12-22 15:46:29 | INFO | train_inner | epoch 007:   1846 / 3059 symm_kl=0.291, self_kl=0, self_cv=4.12, loss=5.37, nll_loss=2.724, ppl=6.61, wps=22571.5, ups=1.92, wpb=11759.4, bsz=404.3, num_updates=20200, lr=1.77998e-05, gnorm=0.872, train_wall=52, wall=10701
2020-12-22 15:47:21 | INFO | train_inner | epoch 007:   1946 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.123, loss=5.348, nll_loss=2.705, ppl=6.52, wps=22648.8, ups=1.91, wpb=11860.2, bsz=417.7, num_updates=20300, lr=1.77559e-05, gnorm=0.86, train_wall=52, wall=10753
2020-12-22 15:48:13 | INFO | train_inner | epoch 007:   2046 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.132, loss=5.33, nll_loss=2.684, ppl=6.43, wps=22543.2, ups=1.91, wpb=11812.5, bsz=410.6, num_updates=20400, lr=1.77123e-05, gnorm=0.863, train_wall=52, wall=10806
2020-12-22 15:49:05 | INFO | train_inner | epoch 007:   2146 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.125, loss=5.355, nll_loss=2.711, ppl=6.55, wps=22748.2, ups=1.92, wpb=11818, bsz=397, num_updates=20500, lr=1.7669e-05, gnorm=0.861, train_wall=52, wall=10858
2020-12-22 15:49:58 | INFO | train_inner | epoch 007:   2246 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.127, loss=5.334, nll_loss=2.69, ppl=6.45, wps=22678.9, ups=1.91, wpb=11861.5, bsz=403.9, num_updates=20600, lr=1.76261e-05, gnorm=0.857, train_wall=52, wall=10910
2020-12-22 15:50:50 | INFO | train_inner | epoch 007:   2346 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.127, loss=5.334, nll_loss=2.69, ppl=6.45, wps=22690.2, ups=1.91, wpb=11866.6, bsz=406.5, num_updates=20700, lr=1.75835e-05, gnorm=0.865, train_wall=52, wall=10962
2020-12-22 15:51:42 | INFO | train_inner | epoch 007:   2446 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.12, loss=5.346, nll_loss=2.705, ppl=6.52, wps=22622.3, ups=1.91, wpb=11849.9, bsz=417.7, num_updates=20800, lr=1.75412e-05, gnorm=0.86, train_wall=52, wall=11015
2020-12-22 15:52:35 | INFO | train_inner | epoch 007:   2546 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.121, loss=5.365, nll_loss=2.721, ppl=6.59, wps=22580.8, ups=1.91, wpb=11814.5, bsz=391.7, num_updates=20900, lr=1.74991e-05, gnorm=0.885, train_wall=52, wall=11067
2020-12-22 15:53:27 | INFO | train_inner | epoch 007:   2646 / 3059 symm_kl=0.285, self_kl=0, self_cv=4.124, loss=5.338, nll_loss=2.697, ppl=6.48, wps=22641.7, ups=1.9, wpb=11888.2, bsz=427, num_updates=21000, lr=1.74574e-05, gnorm=0.853, train_wall=52, wall=11119
2020-12-22 15:54:19 | INFO | train_inner | epoch 007:   2746 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.127, loss=5.339, nll_loss=2.696, ppl=6.48, wps=22783.3, ups=1.91, wpb=11904.9, bsz=433.8, num_updates=21100, lr=1.7416e-05, gnorm=0.856, train_wall=52, wall=11172
2020-12-22 15:55:12 | INFO | train_inner | epoch 007:   2846 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.118, loss=5.344, nll_loss=2.703, ppl=6.51, wps=22581.9, ups=1.9, wpb=11863.4, bsz=413.4, num_updates=21200, lr=1.73749e-05, gnorm=0.858, train_wall=52, wall=11224
2020-12-22 15:56:04 | INFO | train_inner | epoch 007:   2946 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.114, loss=5.34, nll_loss=2.7, ppl=6.5, wps=22470.3, ups=1.9, wpb=11797.2, bsz=424.3, num_updates=21300, lr=1.73341e-05, gnorm=0.866, train_wall=52, wall=11277
2020-12-22 15:56:56 | INFO | train_inner | epoch 007:   3046 / 3059 symm_kl=0.29, self_kl=0, self_cv=4.12, loss=5.373, nll_loss=2.73, ppl=6.63, wps=22553.8, ups=1.92, wpb=11738.3, bsz=385, num_updates=21400, lr=1.72935e-05, gnorm=0.873, train_wall=52, wall=11329
2020-12-22 15:57:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 15:57:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 15:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 15:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 15:57:21 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.786 | nll_loss 7.972 | ppl 251.16 | bleu 16.29 | wps 4349.4 | wpb 6344.2 | bsz 166.4 | num_updates 21413 | best_bleu 16.49
2020-12-22 15:57:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 15:57:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 7 @ 21413 updates, score 16.29) (writing took 5.024371026083827 seconds)
2020-12-22 15:57:26 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-22 15:57:26 | INFO | train | epoch 007 | symm_kl 0.288 | self_kl 0 | self_cv 4.125 | loss 5.345 | nll_loss 2.699 | ppl 6.5 | wps 22205.2 | ups 1.87 | wpb 11852.2 | bsz 409.6 | num_updates 21413 | lr 1.72883e-05 | gnorm 0.862 | train_wall 1594 | wall 11358
2020-12-22 15:57:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:29 | INFO | fairseq.trainer | begin training epoch 8
2020-12-22 15:57:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:57:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 15:57:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 15:58:20 | INFO | train_inner | epoch 008:     87 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.122, loss=5.352, nll_loss=2.707, ppl=6.53, wps=14083.4, ups=1.19, wpb=11822.6, bsz=370.4, num_updates=21500, lr=1.72532e-05, gnorm=0.866, train_wall=51, wall=11413
2020-12-22 15:59:13 | INFO | train_inner | epoch 008:    187 / 3059 symm_kl=0.288, self_kl=0, self_cv=4.12, loss=5.34, nll_loss=2.696, ppl=6.48, wps=22599.4, ups=1.91, wpb=11832.8, bsz=403.8, num_updates=21600, lr=1.72133e-05, gnorm=0.873, train_wall=52, wall=11465
2020-12-22 16:00:05 | INFO | train_inner | epoch 008:    287 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.13, loss=5.33, nll_loss=2.685, ppl=6.43, wps=22516, ups=1.9, wpb=11827.7, bsz=410.2, num_updates=21700, lr=1.71736e-05, gnorm=0.86, train_wall=52, wall=11518
2020-12-22 16:00:57 | INFO | train_inner | epoch 008:    387 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.124, loss=5.338, nll_loss=2.695, ppl=6.48, wps=22833.7, ups=1.92, wpb=11920.6, bsz=418.9, num_updates=21800, lr=1.71341e-05, gnorm=0.852, train_wall=52, wall=11570
2020-12-22 16:01:49 | INFO | train_inner | epoch 008:    487 / 3059 symm_kl=0.292, self_kl=0, self_cv=4.119, loss=5.379, nll_loss=2.733, ppl=6.65, wps=22410.2, ups=1.93, wpb=11634.7, bsz=375.8, num_updates=21900, lr=1.7095e-05, gnorm=0.888, train_wall=52, wall=11622
2020-12-22 16:02:42 | INFO | train_inner | epoch 008:    587 / 3059 symm_kl=0.289, self_kl=0, self_cv=4.127, loss=5.343, nll_loss=2.696, ppl=6.48, wps=22670.8, ups=1.91, wpb=11870.7, bsz=408.6, num_updates=22000, lr=1.70561e-05, gnorm=0.87, train_wall=52, wall=11674
2020-12-22 16:03:34 | INFO | train_inner | epoch 008:    687 / 3059 symm_kl=0.284, self_kl=0, self_cv=4.131, loss=5.322, nll_loss=2.679, ppl=6.4, wps=22704.7, ups=1.92, wpb=11854.7, bsz=408.2, num_updates=22100, lr=1.70174e-05, gnorm=0.859, train_wall=52, wall=11726
2020-12-22 16:04:26 | INFO | train_inner | epoch 008:    787 / 3059 symm_kl=0.286, self_kl=0, self_cv=4.123, loss=5.347, nll_loss=2.704, ppl=6.52, wps=22604.7, ups=1.91, wpb=11850.7, bsz=428.7, num_updates=22200, lr=1.69791e-05, gnorm=0.861, train_wall=52, wall=11779
2020-12-22 16:05:19 | INFO | train_inner | epoch 008:    887 / 3059 symm_kl=0.287, self_kl=0, self_cv=4.122, loss=5.343, nll_loss=2.7, ppl=6.5, wps=22683.2, ups=1.91, wpb=11848.1, bsz=392.9, num_updates=22300, lr=1.69409e-05, gnorm=0.871, train_wall=52, wall=11831
2020-12-22 16:06:11 | INFO | train_inner | epoch 008:    987 / 3059 symm_kl=0.283, self_kl=0, self_cv=4.127, loss=5.32, nll_loss=2.68, ppl=6.41, wps=22810.1, ups=1.91, wpb=11932.9, bsz=426.2, num_updates=22400, lr=1.69031e-05, gnorm=0.855, train_wall=52, wall=11883
2020-12-22 16:07:03 | INFO | train_inner | epoch 008:   1087 / 3059 symm_kl=0.285, self_kl=0, self_cv=4.128, loss=5.329, nll_loss=2.686, ppl=6.43, wps=22641.5, ups=1.91, wpb=11876.6, bsz=426.2, num_updates=22500, lr=1.68655e-05, gnorm=0.855, train_wall=52, wall=11936
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 104 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
