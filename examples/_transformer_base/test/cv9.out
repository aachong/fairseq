nohup: ignoring input
save_dir=./examples/_transformer_base/bash/../checkpoints/kl
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=4000
max_epoch=100
r3f_lambda=0
extr='--noised-no-grad --cv --cv-lambda 0.1'
2020-12-22 16:08:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:08:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:08:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:08:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:08:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:08:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:08:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:08:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:08:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10449
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:10449
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10449
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:10449
2020-12-22 16:08:59 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2020-12-22 16:09:00 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-22 16:09:00 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-22 16:09:03 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=True, cv_lambda=0.1, data='./examples/_transformer_base/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10449', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3200, max_tokens_valid=3200, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/_transformer_base/bash/../checkpoints/kl', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-22 16:09:04 | INFO | fairseq.tasks.translation | [ch] dictionary: 41952 types
2020-12-22 16:09:04 | INFO | fairseq.tasks.translation | [en] dictionary: 31264 types
2020-12-22 16:09:04 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/bash/../data-bin/valid.ch-en.ch
2020-12-22 16:09:04 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/_transformer_base/bash/../data-bin/valid.ch-en.en
2020-12-22 16:09:04 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/bash/../data-bin valid ch-en 1664 examples
2020-12-22 16:09:06 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(41952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(31264, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=31264, bias=False)
  )
)
2020-12-22 16:09:06 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-22 16:09:06 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-22 16:09:06 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-22 16:09:06 | INFO | fairseq_cli.train | num. model params: 97632256 (num. trained: 97632256)
2020-12-22 16:09:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-22 16:09:06 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 16:09:06 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 16:09:06 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 16:09:06 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-22 16:09:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-22 16:09:06 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2020-12-22 16:09:06 | INFO | fairseq_cli.train | max tokens per GPU = 3200 and max sentences per GPU = None
2020-12-22 16:09:06 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-22 16:09:07 | INFO | fairseq.trainer | loaded checkpoint ./examples/_transformer_base/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 80 @ 0 updates)
2020-12-22 16:09:08 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-22 16:09:08 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-22 16:09:08 | INFO | fairseq.data.data_utils | loaded 1252977 examples from: ./examples/_transformer_base/bash/../data-bin/train.ch-en.ch
2020-12-22 16:09:08 | INFO | fairseq.data.data_utils | loaded 1252977 examples from: ./examples/_transformer_base/bash/../data-bin/train.ch-en.en
2020-12-22 16:09:08 | INFO | fairseq.tasks.translation | ./examples/_transformer_base/bash/../data-bin train ch-en 1252977 examples
2020-12-22 16:09:13 | INFO | fairseq.trainer | begin training epoch 1
2020-12-22 16:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:10:12 | INFO | train_inner | epoch 001:    100 / 3059 symm_kl=0.784, self_kl=0, self_cv=9.93, loss=4.801, nll_loss=1.669, ppl=3.18, wps=22894.8, ups=1.93, wpb=11838.1, bsz=405.1, num_updates=100, lr=1.0975e-06, gnorm=4.086, train_wall=52, wall=66
2020-12-22 16:11:04 | INFO | train_inner | epoch 001:    200 / 3059 symm_kl=0.758, self_kl=0, self_cv=9.281, loss=4.698, nll_loss=1.703, ppl=3.26, wps=22928.8, ups=1.94, wpb=11822.8, bsz=408.9, num_updates=200, lr=2.095e-06, gnorm=3.839, train_wall=51, wall=118
2020-12-22 16:11:55 | INFO | train_inner | epoch 001:    300 / 3059 symm_kl=0.704, self_kl=0, self_cv=8.146, loss=4.587, nll_loss=1.829, ppl=3.55, wps=23095.5, ups=1.94, wpb=11888.2, bsz=402, num_updates=300, lr=3.0925e-06, gnorm=2.969, train_wall=51, wall=169
2020-12-22 16:12:47 | INFO | train_inner | epoch 001:    400 / 3059 symm_kl=0.673, self_kl=0, self_cv=7.062, loss=4.548, nll_loss=2.015, ppl=4.04, wps=22874.8, ups=1.93, wpb=11870.5, bsz=420.2, num_updates=400, lr=4.09e-06, gnorm=1.486, train_wall=52, wall=221
2020-12-22 16:13:39 | INFO | train_inner | epoch 001:    500 / 3059 symm_kl=0.658, self_kl=0, self_cv=6.6, loss=4.536, nll_loss=2.097, ppl=4.28, wps=22901.4, ups=1.93, wpb=11848.5, bsz=424.2, num_updates=500, lr=5.0875e-06, gnorm=0.816, train_wall=52, wall=273
2020-12-22 16:14:31 | INFO | train_inner | epoch 001:    600 / 3059 symm_kl=0.651, self_kl=0, self_cv=6.477, loss=4.531, nll_loss=2.116, ppl=4.33, wps=22676.4, ups=1.92, wpb=11785.2, bsz=407.8, num_updates=600, lr=6.085e-06, gnorm=0.773, train_wall=52, wall=325
2020-12-22 16:15:23 | INFO | train_inner | epoch 001:    700 / 3059 symm_kl=0.647, self_kl=0, self_cv=6.421, loss=4.539, nll_loss=2.136, ppl=4.39, wps=22663.9, ups=1.92, wpb=11778.3, bsz=399.9, num_updates=700, lr=7.0825e-06, gnorm=0.787, train_wall=52, wall=377
2020-12-22 16:16:15 | INFO | train_inner | epoch 001:    800 / 3059 symm_kl=0.628, self_kl=0, self_cv=6.387, loss=4.482, nll_loss=2.079, ppl=4.23, wps=22918.6, ups=1.92, wpb=11948.8, bsz=453, num_updates=800, lr=8.08e-06, gnorm=0.756, train_wall=52, wall=429
2020-12-22 16:17:07 | INFO | train_inner | epoch 001:    900 / 3059 symm_kl=0.64, self_kl=0, self_cv=6.366, loss=4.529, nll_loss=2.135, ppl=4.39, wps=22899.7, ups=1.93, wpb=11844.4, bsz=410.5, num_updates=900, lr=9.0775e-06, gnorm=0.764, train_wall=52, wall=481
2020-12-22 16:17:59 | INFO | train_inner | epoch 001:   1000 / 3059 symm_kl=0.632, self_kl=0, self_cv=6.345, loss=4.525, nll_loss=2.135, ppl=4.39, wps=22751.5, ups=1.92, wpb=11841.6, bsz=409.3, num_updates=1000, lr=1.0075e-05, gnorm=0.758, train_wall=52, wall=533
2020-12-22 16:18:51 | INFO | train_inner | epoch 001:   1100 / 3059 symm_kl=0.635, self_kl=0, self_cv=6.341, loss=4.514, nll_loss=2.124, ppl=4.36, wps=22684.9, ups=1.92, wpb=11816.5, bsz=409.6, num_updates=1100, lr=1.10725e-05, gnorm=0.761, train_wall=52, wall=585
2020-12-22 16:19:43 | INFO | train_inner | epoch 001:   1200 / 3059 symm_kl=0.626, self_kl=0, self_cv=6.312, loss=4.512, nll_loss=2.128, ppl=4.37, wps=22750.7, ups=1.93, wpb=11796.6, bsz=403.2, num_updates=1200, lr=1.207e-05, gnorm=0.759, train_wall=52, wall=637
2020-12-22 16:20:35 | INFO | train_inner | epoch 001:   1300 / 3059 symm_kl=0.624, self_kl=0, self_cv=6.303, loss=4.517, nll_loss=2.135, ppl=4.39, wps=22708, ups=1.93, wpb=11780.9, bsz=427.1, num_updates=1300, lr=1.30675e-05, gnorm=0.765, train_wall=52, wall=689
2020-12-22 16:21:27 | INFO | train_inner | epoch 001:   1400 / 3059 symm_kl=0.618, self_kl=0, self_cv=6.297, loss=4.506, nll_loss=2.124, ppl=4.36, wps=22801.6, ups=1.93, wpb=11844.1, bsz=410.4, num_updates=1400, lr=1.4065e-05, gnorm=0.758, train_wall=52, wall=741
2020-12-22 16:22:18 | INFO | train_inner | epoch 001:   1500 / 3059 symm_kl=0.627, self_kl=0, self_cv=6.292, loss=4.523, nll_loss=2.145, ppl=4.42, wps=22764.1, ups=1.94, wpb=11749.5, bsz=389, num_updates=1500, lr=1.50625e-05, gnorm=0.773, train_wall=51, wall=792
2020-12-22 16:23:10 | INFO | train_inner | epoch 001:   1600 / 3059 symm_kl=0.615, self_kl=0, self_cv=6.28, loss=4.501, nll_loss=2.123, ppl=4.35, wps=22827.3, ups=1.92, wpb=11869.9, bsz=401, num_updates=1600, lr=1.606e-05, gnorm=0.753, train_wall=52, wall=844
2020-12-22 16:24:03 | INFO | train_inner | epoch 001:   1700 / 3059 symm_kl=0.617, self_kl=0, self_cv=6.271, loss=4.498, nll_loss=2.12, ppl=4.35, wps=22621.1, ups=1.91, wpb=11840.3, bsz=436.6, num_updates=1700, lr=1.70575e-05, gnorm=0.764, train_wall=52, wall=897
2020-12-22 16:24:55 | INFO | train_inner | epoch 001:   1800 / 3059 symm_kl=0.611, self_kl=0, self_cv=6.261, loss=4.512, nll_loss=2.139, ppl=4.4, wps=22845.6, ups=1.92, wpb=11890.7, bsz=425.4, num_updates=1800, lr=1.8055e-05, gnorm=0.763, train_wall=52, wall=949
2020-12-22 16:25:47 | INFO | train_inner | epoch 001:   1900 / 3059 symm_kl=0.609, self_kl=0, self_cv=6.251, loss=4.506, nll_loss=2.134, ppl=4.39, wps=22857.9, ups=1.92, wpb=11879.9, bsz=409.3, num_updates=1900, lr=1.90525e-05, gnorm=0.755, train_wall=52, wall=1001
2020-12-22 16:26:39 | INFO | train_inner | epoch 001:   2000 / 3059 symm_kl=0.616, self_kl=0, self_cv=6.254, loss=4.515, nll_loss=2.143, ppl=4.42, wps=22704.3, ups=1.92, wpb=11828.4, bsz=400.9, num_updates=2000, lr=2.005e-05, gnorm=0.763, train_wall=52, wall=1053
2020-12-22 16:27:31 | INFO | train_inner | epoch 001:   2100 / 3059 symm_kl=0.61, self_kl=0, self_cv=6.242, loss=4.508, nll_loss=2.138, ppl=4.4, wps=22698.6, ups=1.92, wpb=11846, bsz=405.2, num_updates=2100, lr=2.10475e-05, gnorm=0.761, train_wall=52, wall=1105
2020-12-22 16:28:23 | INFO | train_inner | epoch 001:   2200 / 3059 symm_kl=0.61, self_kl=0, self_cv=6.245, loss=4.499, nll_loss=2.127, ppl=4.37, wps=22884, ups=1.93, wpb=11871.9, bsz=407.8, num_updates=2200, lr=2.2045e-05, gnorm=0.757, train_wall=52, wall=1157
2020-12-22 16:29:15 | INFO | train_inner | epoch 001:   2300 / 3059 symm_kl=0.608, self_kl=0, self_cv=6.241, loss=4.503, nll_loss=2.133, ppl=4.39, wps=22851.3, ups=1.92, wpb=11900.6, bsz=407.9, num_updates=2300, lr=2.30425e-05, gnorm=0.76, train_wall=52, wall=1209
2020-12-22 16:30:07 | INFO | train_inner | epoch 001:   2400 / 3059 symm_kl=0.614, self_kl=0, self_cv=6.231, loss=4.524, nll_loss=2.158, ppl=4.46, wps=22756.1, ups=1.92, wpb=11871.5, bsz=389.7, num_updates=2400, lr=2.404e-05, gnorm=0.761, train_wall=52, wall=1261
2020-12-22 16:30:59 | INFO | train_inner | epoch 001:   2500 / 3059 symm_kl=0.607, self_kl=0, self_cv=6.222, loss=4.513, nll_loss=2.148, ppl=4.43, wps=22794.5, ups=1.92, wpb=11901.5, bsz=396.6, num_updates=2500, lr=2.50375e-05, gnorm=0.761, train_wall=52, wall=1313
2020-12-22 16:31:51 | INFO | train_inner | epoch 001:   2600 / 3059 symm_kl=0.609, self_kl=0, self_cv=6.226, loss=4.512, nll_loss=2.147, ppl=4.43, wps=22860.6, ups=1.93, wpb=11841.1, bsz=395.7, num_updates=2600, lr=2.6035e-05, gnorm=0.763, train_wall=52, wall=1365
2020-12-22 16:32:43 | INFO | train_inner | epoch 001:   2700 / 3059 symm_kl=0.599, self_kl=0, self_cv=6.202, loss=4.498, nll_loss=2.136, ppl=4.4, wps=22817, ups=1.92, wpb=11894.8, bsz=417, num_updates=2700, lr=2.70325e-05, gnorm=0.756, train_wall=52, wall=1417
2020-12-22 16:33:35 | INFO | train_inner | epoch 001:   2800 / 3059 symm_kl=0.608, self_kl=0, self_cv=6.217, loss=4.514, nll_loss=2.151, ppl=4.44, wps=22642.6, ups=1.91, wpb=11855.7, bsz=407.8, num_updates=2800, lr=2.803e-05, gnorm=0.765, train_wall=52, wall=1470
2020-12-22 16:34:27 | INFO | train_inner | epoch 001:   2900 / 3059 symm_kl=0.611, self_kl=0, self_cv=6.209, loss=4.519, nll_loss=2.158, ppl=4.46, wps=22798, ups=1.92, wpb=11863.6, bsz=398.6, num_updates=2900, lr=2.90275e-05, gnorm=0.773, train_wall=52, wall=1522
2020-12-22 16:35:20 | INFO | train_inner | epoch 001:   3000 / 3059 symm_kl=0.595, self_kl=0, self_cv=6.194, loss=4.486, nll_loss=2.125, ppl=4.36, wps=22896.8, ups=1.92, wpb=11939.7, bsz=412.6, num_updates=3000, lr=3.0025e-05, gnorm=0.752, train_wall=52, wall=1574
2020-12-22 16:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 16:35:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:35:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:35:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:35:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:35:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:35:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:35:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:35:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 16:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 16:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 16:36:08 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 9.056 | nll_loss 8.167 | ppl 287.47 | bleu 15.88 | wps 4345.3 | wpb 6344.2 | bsz 166.4 | num_updates 3059
2020-12-22 16:36:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 16:36:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 1 @ 3059 updates, score 15.88) (writing took 3.9813605342060328 seconds)
2020-12-22 16:36:12 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-22 16:36:12 | INFO | train | epoch 001 | symm_kl 0.634 | self_kl 0 | self_cv 6.595 | loss 4.531 | nll_loss 2.089 | ppl 4.25 | wps 22503.4 | ups 1.9 | wpb 11852.2 | bsz 409.6 | num_updates 3059 | lr 3.06135e-05 | gnorm 1.069 | train_wall 1585 | wall 1626
2020-12-22 16:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:36:17 | INFO | fairseq.trainer | begin training epoch 2
2020-12-22 16:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 16:36:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 16:36:44 | INFO | train_inner | epoch 002:     41 / 3059 symm_kl=0.595, self_kl=0, self_cv=6.193, loss=4.474, nll_loss=2.111, ppl=4.32, wps=14089.5, ups=1.19, wpb=11847.4, bsz=403.5, num_updates=3100, lr=3.10225e-05, gnorm=0.754, train_wall=51, wall=1658
2020-12-22 16:37:36 | INFO | train_inner | epoch 002:    141 / 3059 symm_kl=0.6, self_kl=0, self_cv=6.205, loss=4.487, nll_loss=2.124, ppl=4.36, wps=22871, ups=1.92, wpb=11923.9, bsz=408.2, num_updates=3200, lr=3.202e-05, gnorm=0.759, train_wall=52, wall=1710
2020-12-22 16:38:28 | INFO | train_inner | epoch 002:    241 / 3059 symm_kl=0.602, self_kl=0, self_cv=6.184, loss=4.504, nll_loss=2.146, ppl=4.43, wps=22801.7, ups=1.92, wpb=11869.6, bsz=423.1, num_updates=3300, lr=3.30175e-05, gnorm=0.773, train_wall=52, wall=1762
2020-12-22 16:39:20 | INFO | train_inner | epoch 002:    341 / 3059 symm_kl=0.604, self_kl=0, self_cv=6.185, loss=4.513, nll_loss=2.156, ppl=4.46, wps=22675.2, ups=1.92, wpb=11838.2, bsz=409.8, num_updates=3400, lr=3.4015e-05, gnorm=0.768, train_wall=52, wall=1814
2020-12-22 16:40:12 | INFO | train_inner | epoch 002:    441 / 3059 symm_kl=0.6, self_kl=0, self_cv=6.177, loss=4.497, nll_loss=2.141, ppl=4.41, wps=22704.5, ups=1.92, wpb=11805.7, bsz=417.5, num_updates=3500, lr=3.50125e-05, gnorm=0.772, train_wall=52, wall=1866
2020-12-22 16:41:04 | INFO | train_inner | epoch 002:    541 / 3059 symm_kl=0.599, self_kl=0, self_cv=6.188, loss=4.492, nll_loss=2.133, ppl=4.39, wps=22805.3, ups=1.92, wpb=11894.8, bsz=411, num_updates=3600, lr=3.601e-05, gnorm=0.764, train_wall=52, wall=1918
2020-12-22 16:41:57 | INFO | train_inner | epoch 002:    641 / 3059 symm_kl=0.598, self_kl=0, self_cv=6.154, loss=4.509, nll_loss=2.158, ppl=4.46, wps=22706.3, ups=1.91, wpb=11872.5, bsz=419.7, num_updates=3700, lr=3.70075e-05, gnorm=0.77, train_wall=52, wall=1971
2020-12-22 16:42:49 | INFO | train_inner | epoch 002:    741 / 3059 symm_kl=0.606, self_kl=0, self_cv=6.178, loss=4.511, nll_loss=2.156, ppl=4.46, wps=22586.2, ups=1.91, wpb=11831.9, bsz=396.5, num_updates=3800, lr=3.8005e-05, gnorm=0.775, train_wall=52, wall=2023
2020-12-22 16:43:41 | INFO | train_inner | epoch 002:    841 / 3059 symm_kl=0.603, self_kl=0, self_cv=6.171, loss=4.522, nll_loss=2.169, ppl=4.5, wps=22682.1, ups=1.91, wpb=11880.8, bsz=404.7, num_updates=3900, lr=3.90025e-05, gnorm=0.777, train_wall=52, wall=2076
2020-12-22 16:44:33 | INFO | train_inner | epoch 002:    941 / 3059 symm_kl=0.602, self_kl=0, self_cv=6.177, loss=4.513, nll_loss=2.159, ppl=4.47, wps=22945.9, ups=1.92, wpb=11959.3, bsz=380.9, num_updates=4000, lr=4e-05, gnorm=0.764, train_wall=52, wall=2128
2020-12-22 16:45:26 | INFO | train_inner | epoch 002:   1041 / 3059 symm_kl=0.605, self_kl=0, self_cv=6.151, loss=4.532, nll_loss=2.185, ppl=4.55, wps=22534.3, ups=1.92, wpb=11764.6, bsz=412.1, num_updates=4100, lr=3.95092e-05, gnorm=0.781, train_wall=52, wall=2180
2020-12-22 16:46:18 | INFO | train_inner | epoch 002:   1141 / 3059 symm_kl=0.603, self_kl=0, self_cv=6.159, loss=4.518, nll_loss=2.168, ppl=4.49, wps=22818.3, ups=1.92, wpb=11895.8, bsz=405, num_updates=4200, lr=3.9036e-05, gnorm=0.77, train_wall=52, wall=2232
2020-12-22 16:47:10 | INFO | train_inner | epoch 002:   1241 / 3059 symm_kl=0.6, self_kl=0, self_cv=6.135, loss=4.526, nll_loss=2.181, ppl=4.53, wps=22712.5, ups=1.92, wpb=11837.6, bsz=418.7, num_updates=4300, lr=3.85794e-05, gnorm=0.784, train_wall=52, wall=2284
2020-12-22 16:48:02 | INFO | train_inner | epoch 002:   1341 / 3059 symm_kl=0.592, self_kl=0, self_cv=6.148, loss=4.509, nll_loss=2.16, ppl=4.47, wps=22868.4, ups=1.92, wpb=11913.1, bsz=409.7, num_updates=4400, lr=3.81385e-05, gnorm=0.77, train_wall=52, wall=2336
2020-12-22 16:48:54 | INFO | train_inner | epoch 002:   1441 / 3059 symm_kl=0.606, self_kl=0, self_cv=6.139, loss=4.54, nll_loss=2.196, ppl=4.58, wps=22692.9, ups=1.93, wpb=11762.4, bsz=415.8, num_updates=4500, lr=3.77124e-05, gnorm=0.791, train_wall=52, wall=2388
2020-12-22 16:49:46 | INFO | train_inner | epoch 002:   1541 / 3059 symm_kl=0.598, self_kl=0, self_cv=6.147, loss=4.522, nll_loss=2.175, ppl=4.51, wps=22843.1, ups=1.92, wpb=11898.6, bsz=403.5, num_updates=4600, lr=3.73002e-05, gnorm=0.772, train_wall=52, wall=2440
2020-12-22 16:50:38 | INFO | train_inner | epoch 002:   1641 / 3059 symm_kl=0.593, self_kl=0, self_cv=6.128, loss=4.503, nll_loss=2.157, ppl=4.46, wps=22823.4, ups=1.92, wpb=11897, bsz=443.4, num_updates=4700, lr=3.69012e-05, gnorm=0.776, train_wall=52, wall=2492
2020-12-22 16:51:30 | INFO | train_inner | epoch 002:   1741 / 3059 symm_kl=0.597, self_kl=0, self_cv=6.14, loss=4.525, nll_loss=2.18, ppl=4.53, wps=22814.1, ups=1.93, wpb=11829.8, bsz=402.5, num_updates=4800, lr=3.65148e-05, gnorm=0.78, train_wall=52, wall=2544
2020-12-22 16:52:22 | INFO | train_inner | epoch 002:   1841 / 3059 symm_kl=0.602, self_kl=0, self_cv=6.138, loss=4.542, nll_loss=2.2, ppl=4.59, wps=22743.8, ups=1.92, wpb=11853.5, bsz=392.5, num_updates=4900, lr=3.61403e-05, gnorm=0.784, train_wall=52, wall=2596
2020-12-22 16:53:14 | INFO | train_inner | epoch 002:   1941 / 3059 symm_kl=0.597, self_kl=0, self_cv=6.14, loss=4.526, nll_loss=2.181, ppl=4.54, wps=22802.1, ups=1.93, wpb=11814, bsz=387.1, num_updates=5000, lr=3.57771e-05, gnorm=0.781, train_wall=52, wall=2648
2020-12-22 16:54:06 | INFO | train_inner | epoch 002:   2041 / 3059 symm_kl=0.592, self_kl=0, self_cv=6.117, loss=4.522, nll_loss=2.181, ppl=4.53, wps=22741.7, ups=1.93, wpb=11791.8, bsz=416.3, num_updates=5100, lr=3.54246e-05, gnorm=0.782, train_wall=52, wall=2700
2020-12-22 16:54:58 | INFO | train_inner | epoch 002:   2141 / 3059 symm_kl=0.596, self_kl=0, self_cv=6.132, loss=4.535, nll_loss=2.193, ppl=4.57, wps=22618, ups=1.91, wpb=11872.3, bsz=385.6, num_updates=5200, lr=3.50823e-05, gnorm=0.777, train_wall=52, wall=2752
2020-12-22 16:55:51 | INFO | train_inner | epoch 002:   2241 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.115, loss=4.53, nll_loss=2.191, ppl=4.56, wps=22722.4, ups=1.91, wpb=11918.9, bsz=438.4, num_updates=5300, lr=3.47498e-05, gnorm=0.785, train_wall=52, wall=2805
2020-12-22 16:56:43 | INFO | train_inner | epoch 002:   2341 / 3059 symm_kl=0.596, self_kl=0, self_cv=6.119, loss=4.539, nll_loss=2.199, ppl=4.59, wps=22801.6, ups=1.92, wpb=11889.8, bsz=409.5, num_updates=5400, lr=3.44265e-05, gnorm=0.782, train_wall=52, wall=2857
2020-12-22 16:57:35 | INFO | train_inner | epoch 002:   2441 / 3059 symm_kl=0.593, self_kl=0, self_cv=6.118, loss=4.529, nll_loss=2.188, ppl=4.56, wps=22698.4, ups=1.91, wpb=11903.7, bsz=427.8, num_updates=5500, lr=3.41121e-05, gnorm=0.78, train_wall=52, wall=2909
2020-12-22 16:58:27 | INFO | train_inner | epoch 002:   2541 / 3059 symm_kl=0.596, self_kl=0, self_cv=6.103, loss=4.539, nll_loss=2.202, ppl=4.6, wps=22485.3, ups=1.92, wpb=11692.4, bsz=420.1, num_updates=5600, lr=3.38062e-05, gnorm=0.803, train_wall=52, wall=2961
2020-12-22 16:59:20 | INFO | train_inner | epoch 002:   2641 / 3059 symm_kl=0.589, self_kl=0, self_cv=6.105, loss=4.523, nll_loss=2.185, ppl=4.55, wps=22725.8, ups=1.91, wpb=11895.9, bsz=419.5, num_updates=5700, lr=3.35083e-05, gnorm=0.777, train_wall=52, wall=3014
2020-12-22 17:00:11 | INFO | train_inner | epoch 002:   2741 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.116, loss=4.544, nll_loss=2.206, ppl=4.61, wps=22712.4, ups=1.93, wpb=11789.5, bsz=409.4, num_updates=5800, lr=3.32182e-05, gnorm=0.782, train_wall=52, wall=3066
2020-12-22 17:01:03 | INFO | train_inner | epoch 002:   2841 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.116, loss=4.532, nll_loss=2.193, ppl=4.57, wps=22740.2, ups=1.92, wpb=11822.7, bsz=391.8, num_updates=5900, lr=3.29355e-05, gnorm=0.778, train_wall=52, wall=3118
2020-12-22 17:01:55 | INFO | train_inner | epoch 002:   2941 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.108, loss=4.54, nll_loss=2.203, ppl=4.6, wps=22693, ups=1.93, wpb=11782.3, bsz=401.7, num_updates=6000, lr=3.26599e-05, gnorm=0.787, train_wall=52, wall=3170
2020-12-22 17:02:48 | INFO | train_inner | epoch 002:   3041 / 3059 symm_kl=0.591, self_kl=0, self_cv=6.108, loss=4.539, nll_loss=2.203, ppl=4.6, wps=22754.1, ups=1.92, wpb=11863.7, bsz=413, num_updates=6100, lr=3.23911e-05, gnorm=0.786, train_wall=52, wall=3222
2020-12-22 17:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 17:02:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:02:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:02:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:02:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 17:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 17:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 17:03:13 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.985 | nll_loss 8.097 | ppl 273.82 | bleu 15.98 | wps 4968.3 | wpb 6344.2 | bsz 166.4 | num_updates 6118 | best_bleu 15.98
2020-12-22 17:03:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 17:03:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:03:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 2 @ 6118 updates, score 15.98) (writing took 8.250907361507416 seconds)
2020-12-22 17:03:21 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-22 17:03:21 | INFO | train | epoch 002 | symm_kl 0.598 | self_kl 0 | self_cv 6.144 | loss 4.521 | nll_loss 2.174 | ppl 4.51 | wps 22254.6 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 6118 | lr 3.23434e-05 | gnorm 0.777 | train_wall 1589 | wall 3255
2020-12-22 17:03:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:03:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:03:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:03:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:03:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:03:25 | INFO | fairseq.trainer | begin training epoch 3
2020-12-22 17:03:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:03:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:04:14 | INFO | train_inner | epoch 003:     82 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.132, loss=4.518, nll_loss=2.175, ppl=4.52, wps=13751.3, ups=1.16, wpb=11870.1, bsz=394.2, num_updates=6200, lr=3.21288e-05, gnorm=0.774, train_wall=51, wall=3308
2020-12-22 17:05:06 | INFO | train_inner | epoch 003:    182 / 3059 symm_kl=0.595, self_kl=0, self_cv=6.129, loss=4.518, nll_loss=2.174, ppl=4.51, wps=22459.8, ups=1.91, wpb=11772.1, bsz=417.8, num_updates=6300, lr=3.18728e-05, gnorm=0.786, train_wall=52, wall=3360
2020-12-22 17:05:59 | INFO | train_inner | epoch 003:    282 / 3059 symm_kl=0.586, self_kl=0, self_cv=6.108, loss=4.503, nll_loss=2.162, ppl=4.47, wps=22703.5, ups=1.91, wpb=11911.2, bsz=421.2, num_updates=6400, lr=3.16228e-05, gnorm=0.768, train_wall=52, wall=3413
2020-12-22 17:06:51 | INFO | train_inner | epoch 003:    382 / 3059 symm_kl=0.59, self_kl=0, self_cv=6.131, loss=4.503, nll_loss=2.158, ppl=4.46, wps=22779.8, ups=1.92, wpb=11890.3, bsz=418, num_updates=6500, lr=3.13786e-05, gnorm=0.774, train_wall=52, wall=3465
2020-12-22 17:07:43 | INFO | train_inner | epoch 003:    482 / 3059 symm_kl=0.587, self_kl=0, self_cv=6.116, loss=4.498, nll_loss=2.156, ppl=4.46, wps=22695.5, ups=1.92, wpb=11834.9, bsz=413.1, num_updates=6600, lr=3.114e-05, gnorm=0.777, train_wall=52, wall=3517
2020-12-22 17:08:35 | INFO | train_inner | epoch 003:    582 / 3059 symm_kl=0.586, self_kl=0, self_cv=6.113, loss=4.492, nll_loss=2.15, ppl=4.44, wps=22540.4, ups=1.91, wpb=11772.9, bsz=423.1, num_updates=6700, lr=3.09067e-05, gnorm=0.778, train_wall=52, wall=3569
2020-12-22 17:09:27 | INFO | train_inner | epoch 003:    682 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.105, loss=4.493, nll_loss=2.152, ppl=4.44, wps=22773.3, ups=1.92, wpb=11864.3, bsz=441.6, num_updates=6800, lr=3.06786e-05, gnorm=0.769, train_wall=52, wall=3622
2020-12-22 17:10:19 | INFO | train_inner | epoch 003:    782 / 3059 symm_kl=0.596, self_kl=0, self_cv=6.113, loss=4.533, nll_loss=2.194, ppl=4.58, wps=22729.5, ups=1.92, wpb=11817.6, bsz=400, num_updates=6900, lr=3.04555e-05, gnorm=0.778, train_wall=52, wall=3674
2020-12-22 17:11:12 | INFO | train_inner | epoch 003:    882 / 3059 symm_kl=0.589, self_kl=0, self_cv=6.111, loss=4.501, nll_loss=2.16, ppl=4.47, wps=22780.5, ups=1.92, wpb=11873.5, bsz=417.8, num_updates=7000, lr=3.02372e-05, gnorm=0.777, train_wall=52, wall=3726
2020-12-22 17:12:03 | INFO | train_inner | epoch 003:    982 / 3059 symm_kl=0.596, self_kl=0, self_cv=6.118, loss=4.518, nll_loss=2.178, ppl=4.53, wps=22628.6, ups=1.93, wpb=11744.2, bsz=389.3, num_updates=7100, lr=3.00235e-05, gnorm=0.789, train_wall=52, wall=3778
2020-12-22 17:12:56 | INFO | train_inner | epoch 003:   1082 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.101, loss=4.538, nll_loss=2.203, ppl=4.6, wps=22635.1, ups=1.92, wpb=11810.4, bsz=418.6, num_updates=7200, lr=2.98142e-05, gnorm=0.788, train_wall=52, wall=3830
2020-12-22 17:13:48 | INFO | train_inner | epoch 003:   1182 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.113, loss=4.509, nll_loss=2.169, ppl=4.5, wps=22549.6, ups=1.92, wpb=11727.7, bsz=383.4, num_updates=7300, lr=2.96093e-05, gnorm=0.801, train_wall=52, wall=3882
2020-12-22 17:14:40 | INFO | train_inner | epoch 003:   1282 / 3059 symm_kl=0.589, self_kl=0, self_cv=6.115, loss=4.517, nll_loss=2.177, ppl=4.52, wps=22866.2, ups=1.91, wpb=11979.3, bsz=404, num_updates=7400, lr=2.94086e-05, gnorm=0.771, train_wall=52, wall=3934
2020-12-22 17:15:32 | INFO | train_inner | epoch 003:   1382 / 3059 symm_kl=0.588, self_kl=0, self_cv=6.106, loss=4.509, nll_loss=2.17, ppl=4.5, wps=22677.3, ups=1.92, wpb=11792.9, bsz=425, num_updates=7500, lr=2.92119e-05, gnorm=0.791, train_wall=52, wall=3986
2020-12-22 17:16:24 | INFO | train_inner | epoch 003:   1482 / 3059 symm_kl=0.588, self_kl=0, self_cv=6.096, loss=4.515, nll_loss=2.178, ppl=4.53, wps=22611.7, ups=1.92, wpb=11805.8, bsz=423.3, num_updates=7600, lr=2.90191e-05, gnorm=0.785, train_wall=52, wall=4038
2020-12-22 17:17:17 | INFO | train_inner | epoch 003:   1582 / 3059 symm_kl=0.585, self_kl=0, self_cv=6.104, loss=4.511, nll_loss=2.173, ppl=4.51, wps=22736.2, ups=1.91, wpb=11917.6, bsz=403.5, num_updates=7700, lr=2.883e-05, gnorm=0.775, train_wall=52, wall=4091
2020-12-22 17:18:09 | INFO | train_inner | epoch 003:   1682 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.093, loss=4.496, nll_loss=2.158, ppl=4.46, wps=22815.6, ups=1.92, wpb=11889.9, bsz=434.3, num_updates=7800, lr=2.86446e-05, gnorm=0.772, train_wall=52, wall=4143
2020-12-22 17:19:01 | INFO | train_inner | epoch 003:   1782 / 3059 symm_kl=0.594, self_kl=0, self_cv=6.084, loss=4.545, nll_loss=2.214, ppl=4.64, wps=22594.5, ups=1.92, wpb=11779, bsz=412.3, num_updates=7900, lr=2.84627e-05, gnorm=0.79, train_wall=52, wall=4195
2020-12-22 17:19:53 | INFO | train_inner | epoch 003:   1882 / 3059 symm_kl=0.592, self_kl=0, self_cv=6.093, loss=4.538, nll_loss=2.204, ppl=4.61, wps=22633.8, ups=1.93, wpb=11757.4, bsz=402.6, num_updates=8000, lr=2.82843e-05, gnorm=0.791, train_wall=52, wall=4247
2020-12-22 17:20:45 | INFO | train_inner | epoch 003:   1982 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.09, loss=4.502, nll_loss=2.166, ppl=4.49, wps=22837.8, ups=1.92, wpb=11906.2, bsz=409.9, num_updates=8100, lr=2.81091e-05, gnorm=0.778, train_wall=52, wall=4299
2020-12-22 17:21:37 | INFO | train_inner | epoch 003:   2082 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.094, loss=4.505, nll_loss=2.168, ppl=4.49, wps=22773.4, ups=1.92, wpb=11886.8, bsz=420.8, num_updates=8200, lr=2.79372e-05, gnorm=0.776, train_wall=52, wall=4351
2020-12-22 17:22:29 | INFO | train_inner | epoch 003:   2182 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.097, loss=4.51, nll_loss=2.173, ppl=4.51, wps=22893.7, ups=1.93, wpb=11857, bsz=392.3, num_updates=8300, lr=2.77684e-05, gnorm=0.775, train_wall=52, wall=4403
2020-12-22 17:23:21 | INFO | train_inner | epoch 003:   2282 / 3059 symm_kl=0.589, self_kl=0, self_cv=6.101, loss=4.521, nll_loss=2.185, ppl=4.55, wps=22791.4, ups=1.91, wpb=11912.5, bsz=392.1, num_updates=8400, lr=2.76026e-05, gnorm=0.779, train_wall=52, wall=4455
2020-12-22 17:24:14 | INFO | train_inner | epoch 003:   2382 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.099, loss=4.508, nll_loss=2.171, ppl=4.5, wps=22719.7, ups=1.91, wpb=11886.2, bsz=400.1, num_updates=8500, lr=2.74398e-05, gnorm=0.781, train_wall=52, wall=4508
2020-12-22 17:25:06 | INFO | train_inner | epoch 003:   2482 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.078, loss=4.528, nll_loss=2.197, ppl=4.58, wps=22695, ups=1.91, wpb=11884.2, bsz=410.5, num_updates=8600, lr=2.72798e-05, gnorm=0.779, train_wall=52, wall=4560
2020-12-22 17:25:59 | INFO | train_inner | epoch 003:   2582 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.094, loss=4.506, nll_loss=2.17, ppl=4.5, wps=22336, ups=1.88, wpb=11864.4, bsz=390.4, num_updates=8700, lr=2.71225e-05, gnorm=0.775, train_wall=53, wall=4613
2020-12-22 17:26:52 | INFO | train_inner | epoch 003:   2682 / 3059 symm_kl=0.588, self_kl=0, self_cv=6.087, loss=4.541, nll_loss=2.21, ppl=4.63, wps=22496.4, ups=1.89, wpb=11908.7, bsz=402.3, num_updates=8800, lr=2.6968e-05, gnorm=0.783, train_wall=53, wall=4666
2020-12-22 17:27:44 | INFO | train_inner | epoch 003:   2782 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.08, loss=4.519, nll_loss=2.187, ppl=4.55, wps=22639.6, ups=1.91, wpb=11844, bsz=414.5, num_updates=8900, lr=2.68161e-05, gnorm=0.779, train_wall=52, wall=4718
2020-12-22 17:28:36 | INFO | train_inner | epoch 003:   2882 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.088, loss=4.518, nll_loss=2.185, ppl=4.55, wps=22802.9, ups=1.92, wpb=11899.4, bsz=404.3, num_updates=9000, lr=2.66667e-05, gnorm=0.779, train_wall=52, wall=4771
2020-12-22 17:29:29 | INFO | train_inner | epoch 003:   2982 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.086, loss=4.515, nll_loss=2.181, ppl=4.53, wps=22691.4, ups=1.91, wpb=11898.8, bsz=421.7, num_updates=9100, lr=2.65197e-05, gnorm=0.779, train_wall=52, wall=4823
2020-12-22 17:30:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 17:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 17:30:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 17:30:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 17:30:25 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 9.016 | nll_loss 8.132 | ppl 280.53 | bleu 16.02 | wps 4952.8 | wpb 6344.2 | bsz 166.4 | num_updates 9177 | best_bleu 16.02
2020-12-22 17:30:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 17:30:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 3 @ 9177 updates, score 16.02) (writing took 8.28659002482891 seconds)
2020-12-22 17:30:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-22 17:30:33 | INFO | train | epoch 003 | symm_kl 0.588 | self_kl 0 | self_cv 6.102 | loss 4.515 | nll_loss 2.178 | ppl 4.52 | wps 22211.5 | ups 1.87 | wpb 11852.2 | bsz 409.6 | num_updates 9177 | lr 2.64082e-05 | gnorm 0.78 | train_wall 1592 | wall 4887
2020-12-22 17:30:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:37 | INFO | fairseq.trainer | begin training epoch 4
2020-12-22 17:30:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:30:55 | INFO | train_inner | epoch 004:     23 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.082, loss=4.513, nll_loss=2.18, ppl=4.53, wps=13846, ups=1.16, wpb=11936.8, bsz=398.2, num_updates=9200, lr=2.63752e-05, gnorm=0.776, train_wall=52, wall=4909
2020-12-22 17:31:47 | INFO | train_inner | epoch 004:    123 / 3059 symm_kl=0.58, self_kl=0, self_cv=6.09, loss=4.49, nll_loss=2.153, ppl=4.45, wps=23020.1, ups=1.94, wpb=11891.3, bsz=431.8, num_updates=9300, lr=2.6233e-05, gnorm=0.777, train_wall=51, wall=4961
2020-12-22 17:32:39 | INFO | train_inner | epoch 004:    223 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.106, loss=4.487, nll_loss=2.147, ppl=4.43, wps=22793.9, ups=1.92, wpb=11901.3, bsz=401.3, num_updates=9400, lr=2.60931e-05, gnorm=0.773, train_wall=52, wall=5013
2020-12-22 17:33:31 | INFO | train_inner | epoch 004:    323 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.091, loss=4.489, nll_loss=2.151, ppl=4.44, wps=22652.4, ups=1.91, wpb=11859.5, bsz=429.2, num_updates=9500, lr=2.59554e-05, gnorm=0.778, train_wall=52, wall=5065
2020-12-22 17:34:23 | INFO | train_inner | epoch 004:    423 / 3059 symm_kl=0.589, self_kl=0, self_cv=6.106, loss=4.521, nll_loss=2.184, ppl=4.55, wps=22880.2, ups=1.93, wpb=11859.3, bsz=404.3, num_updates=9600, lr=2.58199e-05, gnorm=0.784, train_wall=52, wall=5117
2020-12-22 17:35:16 | INFO | train_inner | epoch 004:    523 / 3059 symm_kl=0.588, self_kl=0, self_cv=6.098, loss=4.508, nll_loss=2.171, ppl=4.5, wps=22575.5, ups=1.91, wpb=11833.9, bsz=413.5, num_updates=9700, lr=2.56865e-05, gnorm=0.781, train_wall=52, wall=5170
2020-12-22 17:36:08 | INFO | train_inner | epoch 004:    623 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.097, loss=4.505, nll_loss=2.168, ppl=4.49, wps=22793.8, ups=1.91, wpb=11930.5, bsz=435, num_updates=9800, lr=2.55551e-05, gnorm=0.785, train_wall=52, wall=5222
2020-12-22 17:37:00 | INFO | train_inner | epoch 004:    723 / 3059 symm_kl=0.587, self_kl=0, self_cv=6.089, loss=4.513, nll_loss=2.178, ppl=4.53, wps=22548.2, ups=1.92, wpb=11757.9, bsz=392.5, num_updates=9900, lr=2.54257e-05, gnorm=0.79, train_wall=52, wall=5274
2020-12-22 17:37:52 | INFO | train_inner | epoch 004:    823 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.09, loss=4.51, nll_loss=2.175, ppl=4.52, wps=22698.1, ups=1.91, wpb=11876.5, bsz=422.3, num_updates=10000, lr=2.52982e-05, gnorm=0.781, train_wall=52, wall=5327
2020-12-22 17:38:45 | INFO | train_inner | epoch 004:    923 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.09, loss=4.496, nll_loss=2.16, ppl=4.47, wps=22619.7, ups=1.91, wpb=11865.8, bsz=399.1, num_updates=10100, lr=2.51727e-05, gnorm=0.784, train_wall=52, wall=5379
2020-12-22 17:39:37 | INFO | train_inner | epoch 004:   1023 / 3059 symm_kl=0.586, self_kl=0, self_cv=6.089, loss=4.513, nll_loss=2.179, ppl=4.53, wps=22572.2, ups=1.92, wpb=11770.5, bsz=395.4, num_updates=10200, lr=2.5049e-05, gnorm=0.789, train_wall=52, wall=5431
2020-12-22 17:40:29 | INFO | train_inner | epoch 004:   1123 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.089, loss=4.493, nll_loss=2.157, ppl=4.46, wps=22849.7, ups=1.92, wpb=11916.4, bsz=434, num_updates=10300, lr=2.49271e-05, gnorm=0.775, train_wall=52, wall=5483
2020-12-22 17:41:21 | INFO | train_inner | epoch 004:   1223 / 3059 symm_kl=0.585, self_kl=0, self_cv=6.075, loss=4.514, nll_loss=2.183, ppl=4.54, wps=22665.8, ups=1.92, wpb=11812.9, bsz=428.6, num_updates=10400, lr=2.48069e-05, gnorm=0.789, train_wall=52, wall=5535
2020-12-22 17:42:13 | INFO | train_inner | epoch 004:   1323 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.094, loss=4.505, nll_loss=2.169, ppl=4.5, wps=22710.5, ups=1.92, wpb=11858.1, bsz=399.5, num_updates=10500, lr=2.46885e-05, gnorm=0.786, train_wall=52, wall=5588
2020-12-22 17:43:05 | INFO | train_inner | epoch 004:   1423 / 3059 symm_kl=0.585, self_kl=0, self_cv=6.09, loss=4.51, nll_loss=2.176, ppl=4.52, wps=22668.2, ups=1.93, wpb=11757.7, bsz=381.8, num_updates=10600, lr=2.45718e-05, gnorm=0.785, train_wall=52, wall=5640
2020-12-22 17:43:58 | INFO | train_inner | epoch 004:   1523 / 3059 symm_kl=0.587, self_kl=0, self_cv=6.084, loss=4.508, nll_loss=2.174, ppl=4.51, wps=22714.7, ups=1.92, wpb=11858, bsz=409.7, num_updates=10700, lr=2.44567e-05, gnorm=0.789, train_wall=52, wall=5692
2020-12-22 17:44:50 | INFO | train_inner | epoch 004:   1623 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.093, loss=4.511, nll_loss=2.177, ppl=4.52, wps=22827.3, ups=1.92, wpb=11906.2, bsz=386.4, num_updates=10800, lr=2.43432e-05, gnorm=0.777, train_wall=52, wall=5744
2020-12-22 17:45:42 | INFO | train_inner | epoch 004:   1723 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.079, loss=4.514, nll_loss=2.182, ppl=4.54, wps=22665.3, ups=1.92, wpb=11783.9, bsz=398.6, num_updates=10900, lr=2.42313e-05, gnorm=0.789, train_wall=52, wall=5796
2020-12-22 17:46:34 | INFO | train_inner | epoch 004:   1823 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.071, loss=4.521, nll_loss=2.191, ppl=4.57, wps=22767.3, ups=1.93, wpb=11819.5, bsz=398.2, num_updates=11000, lr=2.41209e-05, gnorm=0.79, train_wall=52, wall=5848
2020-12-22 17:47:25 | INFO | train_inner | epoch 004:   1923 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.071, loss=4.523, nll_loss=2.194, ppl=4.57, wps=22667.2, ups=1.93, wpb=11745.6, bsz=414.7, num_updates=11100, lr=2.4012e-05, gnorm=0.811, train_wall=52, wall=5900
2020-12-22 17:48:18 | INFO | train_inner | epoch 004:   2023 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.072, loss=4.509, nll_loss=2.178, ppl=4.52, wps=22919.2, ups=1.92, wpb=11964.1, bsz=418.9, num_updates=11200, lr=2.39046e-05, gnorm=0.781, train_wall=52, wall=5952
2020-12-22 17:49:09 | INFO | train_inner | epoch 004:   2123 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.078, loss=4.516, nll_loss=2.184, ppl=4.55, wps=22899.9, ups=1.93, wpb=11865.6, bsz=378.8, num_updates=11300, lr=2.37986e-05, gnorm=0.783, train_wall=52, wall=6004
2020-12-22 17:50:02 | INFO | train_inner | epoch 004:   2223 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.076, loss=4.509, nll_loss=2.177, ppl=4.52, wps=22750.1, ups=1.92, wpb=11847.4, bsz=406.9, num_updates=11400, lr=2.3694e-05, gnorm=0.791, train_wall=52, wall=6056
2020-12-22 17:50:54 | INFO | train_inner | epoch 004:   2323 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.076, loss=4.504, nll_loss=2.172, ppl=4.51, wps=22711.3, ups=1.92, wpb=11807.8, bsz=418, num_updates=11500, lr=2.35907e-05, gnorm=0.79, train_wall=52, wall=6108
2020-12-22 17:51:46 | INFO | train_inner | epoch 004:   2423 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.079, loss=4.504, nll_loss=2.171, ppl=4.5, wps=22776.3, ups=1.92, wpb=11871.1, bsz=425.9, num_updates=11600, lr=2.34888e-05, gnorm=0.78, train_wall=52, wall=6160
2020-12-22 17:52:38 | INFO | train_inner | epoch 004:   2523 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.065, loss=4.494, nll_loss=2.162, ppl=4.47, wps=22885, ups=1.92, wpb=11933.5, bsz=419, num_updates=11700, lr=2.33882e-05, gnorm=0.775, train_wall=52, wall=6212
2020-12-22 17:53:30 | INFO | train_inner | epoch 004:   2623 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.07, loss=4.509, nll_loss=2.178, ppl=4.53, wps=22774, ups=1.92, wpb=11855.8, bsz=420.9, num_updates=11800, lr=2.32889e-05, gnorm=0.782, train_wall=52, wall=6264
2020-12-22 17:54:22 | INFO | train_inner | epoch 004:   2723 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.069, loss=4.522, nll_loss=2.193, ppl=4.57, wps=22850.2, ups=1.92, wpb=11929.5, bsz=409.4, num_updates=11900, lr=2.31908e-05, gnorm=0.781, train_wall=52, wall=6316
2020-12-22 17:55:14 | INFO | train_inner | epoch 004:   2823 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.067, loss=4.535, nll_loss=2.208, ppl=4.62, wps=22792.7, ups=1.92, wpb=11859.1, bsz=404.6, num_updates=12000, lr=2.3094e-05, gnorm=0.788, train_wall=52, wall=6368
2020-12-22 17:56:06 | INFO | train_inner | epoch 004:   2923 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.076, loss=4.527, nll_loss=2.198, ppl=4.59, wps=22590.2, ups=1.91, wpb=11831.6, bsz=388.3, num_updates=12100, lr=2.29984e-05, gnorm=0.789, train_wall=52, wall=6421
2020-12-22 17:56:58 | INFO | train_inner | epoch 004:   3023 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.048, loss=4.537, nll_loss=2.213, ppl=4.64, wps=22704.1, ups=1.93, wpb=11763.9, bsz=408.5, num_updates=12200, lr=2.29039e-05, gnorm=0.794, train_wall=52, wall=6472
2020-12-22 17:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 17:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 17:57:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 17:57:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 17:57:33 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 9 | nll_loss 8.117 | ppl 277.58 | bleu 16.19 | wps 4895.5 | wpb 6344.2 | bsz 166.4 | num_updates 12236 | best_bleu 16.19
2020-12-22 17:57:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 17:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_best.pt (epoch 4 @ 12236 updates, score 16.19) (writing took 8.1911335978657 seconds)
2020-12-22 17:57:41 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-22 17:57:41 | INFO | train | epoch 004 | symm_kl 0.582 | self_kl 0 | self_cv 6.082 | loss 4.509 | nll_loss 2.176 | ppl 4.52 | wps 22268.9 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 12236 | lr 2.28702e-05 | gnorm 0.785 | train_wall 1589 | wall 6515
2020-12-22 17:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:57:45 | INFO | fairseq.trainer | begin training epoch 5
2020-12-22 17:57:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 17:57:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 17:58:24 | INFO | train_inner | epoch 005:     64 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.083, loss=4.519, nll_loss=2.187, ppl=4.55, wps=13726.5, ups=1.17, wpb=11743.5, bsz=405.9, num_updates=12300, lr=2.28106e-05, gnorm=0.79, train_wall=51, wall=6558
2020-12-22 17:59:16 | INFO | train_inner | epoch 005:    164 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.072, loss=4.494, nll_loss=2.161, ppl=4.47, wps=22649.2, ups=1.92, wpb=11789.6, bsz=451.6, num_updates=12400, lr=2.27185e-05, gnorm=0.792, train_wall=52, wall=6610
2020-12-22 18:00:08 | INFO | train_inner | epoch 005:    264 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.091, loss=4.5, nll_loss=2.165, ppl=4.48, wps=22851.9, ups=1.92, wpb=11885.8, bsz=382.1, num_updates=12500, lr=2.26274e-05, gnorm=0.783, train_wall=52, wall=6662
2020-12-22 18:01:00 | INFO | train_inner | epoch 005:    364 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.08, loss=4.491, nll_loss=2.157, ppl=4.46, wps=22864.5, ups=1.92, wpb=11915.5, bsz=408.1, num_updates=12600, lr=2.25374e-05, gnorm=0.783, train_wall=52, wall=6714
2020-12-22 18:01:52 | INFO | train_inner | epoch 005:    464 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.073, loss=4.501, nll_loss=2.169, ppl=4.5, wps=22675.4, ups=1.91, wpb=11865.2, bsz=418.8, num_updates=12700, lr=2.24485e-05, gnorm=0.785, train_wall=52, wall=6767
2020-12-22 18:02:44 | INFO | train_inner | epoch 005:    564 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.059, loss=4.498, nll_loss=2.168, ppl=4.49, wps=22730.4, ups=1.92, wpb=11815, bsz=425.4, num_updates=12800, lr=2.23607e-05, gnorm=0.787, train_wall=52, wall=6818
2020-12-22 18:03:36 | INFO | train_inner | epoch 005:    664 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.087, loss=4.508, nll_loss=2.174, ppl=4.51, wps=22779.7, ups=1.92, wpb=11840.7, bsz=383.6, num_updates=12900, lr=2.22738e-05, gnorm=0.786, train_wall=52, wall=6870
2020-12-22 18:04:28 | INFO | train_inner | epoch 005:    764 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.074, loss=4.496, nll_loss=2.163, ppl=4.48, wps=22756.4, ups=1.92, wpb=11869.9, bsz=419.7, num_updates=13000, lr=2.2188e-05, gnorm=0.784, train_wall=52, wall=6923
2020-12-22 18:05:21 | INFO | train_inner | epoch 005:    864 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.07, loss=4.498, nll_loss=2.166, ppl=4.49, wps=22771.8, ups=1.92, wpb=11886.4, bsz=410.4, num_updates=13100, lr=2.21032e-05, gnorm=0.784, train_wall=52, wall=6975
2020-12-22 18:06:13 | INFO | train_inner | epoch 005:    964 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.063, loss=4.512, nll_loss=2.183, ppl=4.54, wps=22480.1, ups=1.92, wpb=11714.1, bsz=405.9, num_updates=13200, lr=2.20193e-05, gnorm=0.786, train_wall=52, wall=7027
2020-12-22 18:07:05 | INFO | train_inner | epoch 005:   1064 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.079, loss=4.502, nll_loss=2.169, ppl=4.5, wps=22828.9, ups=1.92, wpb=11905, bsz=394.8, num_updates=13300, lr=2.19363e-05, gnorm=0.782, train_wall=52, wall=7079
2020-12-22 18:07:57 | INFO | train_inner | epoch 005:   1164 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.068, loss=4.497, nll_loss=2.166, ppl=4.49, wps=22765.6, ups=1.92, wpb=11887.6, bsz=413.5, num_updates=13400, lr=2.18543e-05, gnorm=0.785, train_wall=52, wall=7131
2020-12-22 18:08:49 | INFO | train_inner | epoch 005:   1264 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.071, loss=4.491, nll_loss=2.159, ppl=4.47, wps=22665, ups=1.92, wpb=11831.6, bsz=413.4, num_updates=13500, lr=2.17732e-05, gnorm=0.783, train_wall=52, wall=7184
2020-12-22 18:09:41 | INFO | train_inner | epoch 005:   1364 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.077, loss=4.509, nll_loss=2.178, ppl=4.52, wps=22863.1, ups=1.93, wpb=11828.5, bsz=389.9, num_updates=13600, lr=2.1693e-05, gnorm=0.801, train_wall=52, wall=7235
2020-12-22 18:10:33 | INFO | train_inner | epoch 005:   1464 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.071, loss=4.477, nll_loss=2.143, ppl=4.42, wps=22934.1, ups=1.92, wpb=11934.8, bsz=438.3, num_updates=13700, lr=2.16137e-05, gnorm=0.776, train_wall=52, wall=7287
2020-12-22 18:11:25 | INFO | train_inner | epoch 005:   1564 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.067, loss=4.497, nll_loss=2.166, ppl=4.49, wps=22748.5, ups=1.92, wpb=11847.5, bsz=428.6, num_updates=13800, lr=2.15353e-05, gnorm=0.782, train_wall=52, wall=7339
2020-12-22 18:12:17 | INFO | train_inner | epoch 005:   1664 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.067, loss=4.508, nll_loss=2.179, ppl=4.53, wps=22853.3, ups=1.92, wpb=11897, bsz=427.4, num_updates=13900, lr=2.14577e-05, gnorm=0.784, train_wall=52, wall=7391
2020-12-22 18:13:09 | INFO | train_inner | epoch 005:   1764 / 3059 symm_kl=0.58, self_kl=0, self_cv=6.079, loss=4.508, nll_loss=2.176, ppl=4.52, wps=22741.3, ups=1.92, wpb=11837.1, bsz=401, num_updates=14000, lr=2.13809e-05, gnorm=0.787, train_wall=52, wall=7443
2020-12-22 18:14:01 | INFO | train_inner | epoch 005:   1864 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.067, loss=4.514, nll_loss=2.185, ppl=4.55, wps=22732.4, ups=1.92, wpb=11841.5, bsz=410.4, num_updates=14100, lr=2.13049e-05, gnorm=0.787, train_wall=52, wall=7496
2020-12-22 18:14:53 | INFO | train_inner | epoch 005:   1964 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.069, loss=4.513, nll_loss=2.184, ppl=4.54, wps=22935.9, ups=1.93, wpb=11867.2, bsz=392, num_updates=14200, lr=2.12298e-05, gnorm=0.785, train_wall=52, wall=7547
2020-12-22 18:15:45 | INFO | train_inner | epoch 005:   2064 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.053, loss=4.494, nll_loss=2.165, ppl=4.48, wps=22572.2, ups=1.93, wpb=11710.4, bsz=425.5, num_updates=14300, lr=2.11554e-05, gnorm=0.796, train_wall=52, wall=7599
2020-12-22 18:16:37 | INFO | train_inner | epoch 005:   2164 / 3059 symm_kl=0.58, self_kl=0, self_cv=6.072, loss=4.519, nll_loss=2.19, ppl=4.56, wps=22868.6, ups=1.92, wpb=11883.9, bsz=396.3, num_updates=14400, lr=2.10819e-05, gnorm=0.787, train_wall=52, wall=7651
2020-12-22 18:17:29 | INFO | train_inner | epoch 005:   2264 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.068, loss=4.504, nll_loss=2.173, ppl=4.51, wps=22772.3, ups=1.92, wpb=11848.6, bsz=408.6, num_updates=14500, lr=2.1009e-05, gnorm=0.79, train_wall=52, wall=7703
2020-12-22 18:18:21 | INFO | train_inner | epoch 005:   2364 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.07, loss=4.512, nll_loss=2.182, ppl=4.54, wps=22871, ups=1.93, wpb=11880.1, bsz=418.2, num_updates=14600, lr=2.0937e-05, gnorm=0.785, train_wall=52, wall=7755
2020-12-22 18:19:13 | INFO | train_inner | epoch 005:   2464 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.065, loss=4.506, nll_loss=2.176, ppl=4.52, wps=22882.1, ups=1.92, wpb=11933.6, bsz=394.5, num_updates=14700, lr=2.08656e-05, gnorm=0.787, train_wall=52, wall=7807
2020-12-22 18:20:05 | INFO | train_inner | epoch 005:   2564 / 3059 symm_kl=0.583, self_kl=0, self_cv=6.067, loss=4.532, nll_loss=2.205, ppl=4.61, wps=22698.5, ups=1.93, wpb=11762.6, bsz=394.5, num_updates=14800, lr=2.0795e-05, gnorm=0.797, train_wall=52, wall=7859
2020-12-22 18:20:57 | INFO | train_inner | epoch 005:   2664 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.063, loss=4.512, nll_loss=2.184, ppl=4.54, wps=22847.9, ups=1.93, wpb=11848, bsz=411.6, num_updates=14900, lr=2.07251e-05, gnorm=0.787, train_wall=52, wall=7911
2020-12-22 18:21:49 | INFO | train_inner | epoch 005:   2764 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.061, loss=4.496, nll_loss=2.166, ppl=4.49, wps=22890.1, ups=1.92, wpb=11895.1, bsz=421.2, num_updates=15000, lr=2.06559e-05, gnorm=0.784, train_wall=52, wall=7963
2020-12-22 18:22:41 | INFO | train_inner | epoch 005:   2864 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.06, loss=4.516, nll_loss=2.189, ppl=4.56, wps=22740.7, ups=1.92, wpb=11872.4, bsz=396.2, num_updates=15100, lr=2.05874e-05, gnorm=0.793, train_wall=52, wall=8015
2020-12-22 18:23:33 | INFO | train_inner | epoch 005:   2964 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.064, loss=4.508, nll_loss=2.179, ppl=4.53, wps=22861.4, ups=1.93, wpb=11871.6, bsz=410.8, num_updates=15200, lr=2.05196e-05, gnorm=0.793, train_wall=52, wall=8067
2020-12-22 18:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 18:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 18:24:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 18:24:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 18:24:38 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.998 | nll_loss 8.117 | ppl 277.57 | bleu 15.78 | wps 4770 | wpb 6344.2 | bsz 166.4 | num_updates 15295 | best_bleu 16.19
2020-12-22 18:24:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 18:24:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 5 @ 15295 updates, score 15.78) (writing took 4.814037589356303 seconds)
2020-12-22 18:24:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-22 18:24:43 | INFO | train | epoch 005 | symm_kl 0.578 | self_kl 0 | self_cv 6.07 | loss 4.505 | nll_loss 2.174 | ppl 4.51 | wps 22356.3 | ups 1.89 | wpb 11852.2 | bsz 409.6 | num_updates 15295 | lr 2.04557e-05 | gnorm 0.787 | train_wall 1586 | wall 8137
2020-12-22 18:24:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:47 | INFO | fairseq.trainer | begin training epoch 6
2020-12-22 18:24:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:24:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:24:56 | INFO | train_inner | epoch 006:      5 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.057, loss=4.514, nll_loss=2.187, ppl=4.55, wps=14352.2, ups=1.21, wpb=11891.8, bsz=403.8, num_updates=15300, lr=2.04524e-05, gnorm=0.788, train_wall=52, wall=8150
2020-12-22 18:25:48 | INFO | train_inner | epoch 006:    105 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.068, loss=4.478, nll_loss=2.144, ppl=4.42, wps=22875.9, ups=1.93, wpb=11849.4, bsz=433, num_updates=15400, lr=2.03859e-05, gnorm=0.778, train_wall=52, wall=8202
2020-12-22 18:26:40 | INFO | train_inner | epoch 006:    205 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.081, loss=4.484, nll_loss=2.149, ppl=4.44, wps=22851.2, ups=1.92, wpb=11914.3, bsz=393.1, num_updates=15500, lr=2.032e-05, gnorm=0.777, train_wall=52, wall=8254
2020-12-22 18:27:32 | INFO | train_inner | epoch 006:    305 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.069, loss=4.496, nll_loss=2.165, ppl=4.48, wps=22754.9, ups=1.92, wpb=11851.3, bsz=413.7, num_updates=15600, lr=2.02548e-05, gnorm=0.794, train_wall=52, wall=8306
2020-12-22 18:28:24 | INFO | train_inner | epoch 006:    405 / 3059 symm_kl=0.581, self_kl=0, self_cv=6.057, loss=4.518, nll_loss=2.191, ppl=4.57, wps=22383.1, ups=1.91, wpb=11706.5, bsz=408.6, num_updates=15700, lr=2.01902e-05, gnorm=0.808, train_wall=52, wall=8358
2020-12-22 18:29:16 | INFO | train_inner | epoch 006:    505 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.062, loss=4.494, nll_loss=2.163, ppl=4.48, wps=22799.1, ups=1.92, wpb=11875.3, bsz=411.5, num_updates=15800, lr=2.01262e-05, gnorm=0.788, train_wall=52, wall=8410
2020-12-22 18:30:09 | INFO | train_inner | epoch 006:    605 / 3059 symm_kl=0.58, self_kl=0, self_cv=6.068, loss=4.513, nll_loss=2.184, ppl=4.54, wps=22549.4, ups=1.9, wpb=11857.2, bsz=390.2, num_updates=15900, lr=2.00628e-05, gnorm=0.789, train_wall=52, wall=8463
2020-12-22 18:31:01 | INFO | train_inner | epoch 006:    705 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.059, loss=4.494, nll_loss=2.165, ppl=4.48, wps=22607.9, ups=1.92, wpb=11798.9, bsz=427.2, num_updates=16000, lr=2e-05, gnorm=0.796, train_wall=52, wall=8515
2020-12-22 18:31:53 | INFO | train_inner | epoch 006:    805 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.066, loss=4.495, nll_loss=2.164, ppl=4.48, wps=22598.1, ups=1.91, wpb=11839.5, bsz=422.7, num_updates=16100, lr=1.99378e-05, gnorm=0.787, train_wall=52, wall=8567
2020-12-22 18:32:46 | INFO | train_inner | epoch 006:    905 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.069, loss=4.488, nll_loss=2.156, ppl=4.46, wps=22822.5, ups=1.92, wpb=11913.5, bsz=411.6, num_updates=16200, lr=1.98762e-05, gnorm=0.781, train_wall=52, wall=8620
2020-12-22 18:33:38 | INFO | train_inner | epoch 006:   1005 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.07, loss=4.501, nll_loss=2.17, ppl=4.5, wps=22775.9, ups=1.92, wpb=11865.6, bsz=406.7, num_updates=16300, lr=1.98151e-05, gnorm=0.785, train_wall=52, wall=8672
2020-12-22 18:34:30 | INFO | train_inner | epoch 006:   1105 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.064, loss=4.495, nll_loss=2.165, ppl=4.49, wps=22713.7, ups=1.92, wpb=11857, bsz=417.9, num_updates=16400, lr=1.97546e-05, gnorm=0.785, train_wall=52, wall=8724
2020-12-22 18:35:22 | INFO | train_inner | epoch 006:   1205 / 3059 symm_kl=0.582, self_kl=0, self_cv=6.046, loss=4.538, nll_loss=2.216, ppl=4.65, wps=22730.7, ups=1.93, wpb=11791.7, bsz=413.4, num_updates=16500, lr=1.96946e-05, gnorm=0.806, train_wall=52, wall=8776
2020-12-22 18:36:14 | INFO | train_inner | epoch 006:   1305 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.061, loss=4.497, nll_loss=2.167, ppl=4.49, wps=22766.3, ups=1.91, wpb=11904.1, bsz=424.1, num_updates=16600, lr=1.96352e-05, gnorm=0.783, train_wall=52, wall=8828
2020-12-22 18:37:06 | INFO | train_inner | epoch 006:   1405 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.06, loss=4.51, nll_loss=2.182, ppl=4.54, wps=22810, ups=1.91, wpb=11918.7, bsz=419.3, num_updates=16700, lr=1.95764e-05, gnorm=0.783, train_wall=52, wall=8880
2020-12-22 18:37:58 | INFO | train_inner | epoch 006:   1505 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.066, loss=4.505, nll_loss=2.175, ppl=4.52, wps=22896.3, ups=1.93, wpb=11887.8, bsz=395.5, num_updates=16800, lr=1.9518e-05, gnorm=0.786, train_wall=52, wall=8932
2020-12-22 18:38:50 | INFO | train_inner | epoch 006:   1605 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.058, loss=4.512, nll_loss=2.185, ppl=4.55, wps=22556.6, ups=1.92, wpb=11763.8, bsz=414.3, num_updates=16900, lr=1.94602e-05, gnorm=0.797, train_wall=52, wall=8984
2020-12-22 18:39:42 | INFO | train_inner | epoch 006:   1705 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.062, loss=4.517, nll_loss=2.19, ppl=4.56, wps=22681.8, ups=1.92, wpb=11832.1, bsz=407.7, num_updates=17000, lr=1.94029e-05, gnorm=0.795, train_wall=52, wall=9037
2020-12-22 18:40:35 | INFO | train_inner | epoch 006:   1805 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.065, loss=4.492, nll_loss=2.162, ppl=4.47, wps=22777.9, ups=1.92, wpb=11861.3, bsz=399.3, num_updates=17100, lr=1.9346e-05, gnorm=0.787, train_wall=52, wall=9089
2020-12-22 18:41:27 | INFO | train_inner | epoch 006:   1905 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.069, loss=4.502, nll_loss=2.172, ppl=4.51, wps=22763.6, ups=1.91, wpb=11888.5, bsz=384.8, num_updates=17200, lr=1.92897e-05, gnorm=0.784, train_wall=52, wall=9141
2020-12-22 18:42:19 | INFO | train_inner | epoch 006:   2005 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.057, loss=4.496, nll_loss=2.167, ppl=4.49, wps=22737.7, ups=1.92, wpb=11819.5, bsz=414.5, num_updates=17300, lr=1.92339e-05, gnorm=0.795, train_wall=52, wall=9193
2020-12-22 18:43:10 | INFO | train_inner | epoch 006:   2105 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.044, loss=4.499, nll_loss=2.172, ppl=4.51, wps=22791.3, ups=1.94, wpb=11766, bsz=418.2, num_updates=17400, lr=1.91785e-05, gnorm=0.788, train_wall=51, wall=9245
2020-12-22 18:44:03 | INFO | train_inner | epoch 006:   2205 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.062, loss=4.499, nll_loss=2.17, ppl=4.5, wps=22751, ups=1.91, wpb=11895.2, bsz=388.8, num_updates=17500, lr=1.91237e-05, gnorm=0.787, train_wall=52, wall=9297
2020-12-22 18:44:55 | INFO | train_inner | epoch 006:   2305 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.055, loss=4.501, nll_loss=2.174, ppl=4.51, wps=22693.5, ups=1.92, wpb=11804.2, bsz=402.7, num_updates=17600, lr=1.90693e-05, gnorm=0.797, train_wall=52, wall=9349
2020-12-22 18:45:47 | INFO | train_inner | epoch 006:   2405 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.059, loss=4.506, nll_loss=2.178, ppl=4.53, wps=22703.3, ups=1.92, wpb=11833.7, bsz=396.2, num_updates=17700, lr=1.90153e-05, gnorm=0.793, train_wall=52, wall=9401
2020-12-22 18:46:39 | INFO | train_inner | epoch 006:   2505 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.067, loss=4.5, nll_loss=2.171, ppl=4.5, wps=22740.1, ups=1.92, wpb=11870.4, bsz=392.6, num_updates=17800, lr=1.89618e-05, gnorm=0.793, train_wall=52, wall=9453
2020-12-22 18:47:31 | INFO | train_inner | epoch 006:   2605 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.06, loss=4.505, nll_loss=2.177, ppl=4.52, wps=22816.6, ups=1.92, wpb=11892.6, bsz=413.8, num_updates=17900, lr=1.89088e-05, gnorm=0.805, train_wall=52, wall=9505
2020-12-22 18:48:24 | INFO | train_inner | epoch 006:   2705 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.056, loss=4.513, nll_loss=2.187, ppl=4.55, wps=22688.2, ups=1.91, wpb=11893, bsz=412.1, num_updates=18000, lr=1.88562e-05, gnorm=0.789, train_wall=52, wall=9558
2020-12-22 18:49:16 | INFO | train_inner | epoch 006:   2805 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.044, loss=4.511, nll_loss=2.187, ppl=4.55, wps=22668.5, ups=1.91, wpb=11879.6, bsz=420.5, num_updates=18100, lr=1.8804e-05, gnorm=0.791, train_wall=52, wall=9610
2020-12-22 18:50:08 | INFO | train_inner | epoch 006:   2905 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.045, loss=4.499, nll_loss=2.173, ppl=4.51, wps=22704.9, ups=1.91, wpb=11868.2, bsz=418.8, num_updates=18200, lr=1.87523e-05, gnorm=0.792, train_wall=52, wall=9662
2020-12-22 18:51:01 | INFO | train_inner | epoch 006:   3005 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.049, loss=4.49, nll_loss=2.163, ppl=4.48, wps=22633.2, ups=1.91, wpb=11828.5, bsz=407.8, num_updates=18300, lr=1.8701e-05, gnorm=0.789, train_wall=52, wall=9715
2020-12-22 18:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 18:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 18:51:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 18:51:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 18:51:44 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.975 | nll_loss 8.091 | ppl 272.74 | bleu 15.82 | wps 4928.2 | wpb 6344.2 | bsz 166.4 | num_updates 18354 | best_bleu 16.19
2020-12-22 18:51:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 18:51:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 6 @ 18354 updates, score 15.82) (writing took 4.9937017653137445 seconds)
2020-12-22 18:51:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-22 18:51:49 | INFO | train | epoch 006 | symm_kl 0.575 | self_kl 0 | self_cv 6.06 | loss 4.501 | nll_loss 2.173 | ppl 4.51 | wps 22290.7 | ups 1.88 | wpb 11852.2 | bsz 409.6 | num_updates 18354 | lr 1.86735e-05 | gnorm 0.79 | train_wall 1590 | wall 9764
2020-12-22 18:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:53 | INFO | fairseq.trainer | begin training epoch 7
2020-12-22 18:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:51:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 18:51:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 18:52:23 | INFO | train_inner | epoch 007:     46 / 3059 symm_kl=0.564, self_kl=0, self_cv=6.04, loss=4.476, nll_loss=2.149, ppl=4.43, wps=14382.8, ups=1.21, wpb=11900, bsz=424.6, num_updates=18400, lr=1.86501e-05, gnorm=0.782, train_wall=51, wall=9797
2020-12-22 18:53:15 | INFO | train_inner | epoch 007:    146 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.066, loss=4.501, nll_loss=2.171, ppl=4.5, wps=22807.8, ups=1.92, wpb=11894.2, bsz=386.7, num_updates=18500, lr=1.85996e-05, gnorm=0.786, train_wall=52, wall=9850
2020-12-22 18:54:07 | INFO | train_inner | epoch 007:    246 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.062, loss=4.472, nll_loss=2.14, ppl=4.41, wps=22757.1, ups=1.92, wpb=11854.9, bsz=412.4, num_updates=18600, lr=1.85496e-05, gnorm=0.783, train_wall=52, wall=9902
2020-12-22 18:55:00 | INFO | train_inner | epoch 007:    346 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.049, loss=4.473, nll_loss=2.143, ppl=4.42, wps=22603.9, ups=1.9, wpb=11914.7, bsz=439.7, num_updates=18700, lr=1.84999e-05, gnorm=0.79, train_wall=53, wall=9954
2020-12-22 18:55:52 | INFO | train_inner | epoch 007:    446 / 3059 symm_kl=0.576, self_kl=0, self_cv=6.074, loss=4.498, nll_loss=2.167, ppl=4.49, wps=22756.1, ups=1.91, wpb=11898.4, bsz=388.5, num_updates=18800, lr=1.84506e-05, gnorm=0.791, train_wall=52, wall=10007
2020-12-22 18:56:45 | INFO | train_inner | epoch 007:    546 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.057, loss=4.504, nll_loss=2.176, ppl=4.52, wps=22492.8, ups=1.91, wpb=11788.8, bsz=414.4, num_updates=18900, lr=1.84017e-05, gnorm=0.8, train_wall=52, wall=10059
2020-12-22 18:57:37 | INFO | train_inner | epoch 007:    646 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.048, loss=4.481, nll_loss=2.152, ppl=4.44, wps=22892.7, ups=1.92, wpb=11950.8, bsz=443.5, num_updates=19000, lr=1.83533e-05, gnorm=0.788, train_wall=52, wall=10111
2020-12-22 18:58:29 | INFO | train_inner | epoch 007:    746 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.059, loss=4.5, nll_loss=2.172, ppl=4.51, wps=22625.6, ups=1.92, wpb=11783.7, bsz=406.3, num_updates=19100, lr=1.83052e-05, gnorm=0.793, train_wall=52, wall=10163
2020-12-22 18:59:21 | INFO | train_inner | epoch 007:    846 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.057, loss=4.487, nll_loss=2.158, ppl=4.46, wps=22759.4, ups=1.91, wpb=11904.7, bsz=402.3, num_updates=19200, lr=1.82574e-05, gnorm=0.787, train_wall=52, wall=10216
2020-12-22 19:00:14 | INFO | train_inner | epoch 007:    946 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.052, loss=4.503, nll_loss=2.177, ppl=4.52, wps=22562.3, ups=1.92, wpb=11760.4, bsz=399.4, num_updates=19300, lr=1.82101e-05, gnorm=0.805, train_wall=52, wall=10268
2020-12-22 19:01:06 | INFO | train_inner | epoch 007:   1046 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.052, loss=4.5, nll_loss=2.173, ppl=4.51, wps=22619.7, ups=1.91, wpb=11823.5, bsz=408.7, num_updates=19400, lr=1.81631e-05, gnorm=0.794, train_wall=52, wall=10320
2020-12-22 19:01:58 | INFO | train_inner | epoch 007:   1146 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.062, loss=4.491, nll_loss=2.161, ppl=4.47, wps=22809.7, ups=1.91, wpb=11929, bsz=407.4, num_updates=19500, lr=1.81164e-05, gnorm=0.79, train_wall=52, wall=10372
2020-12-22 19:02:51 | INFO | train_inner | epoch 007:   1246 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.052, loss=4.514, nll_loss=2.188, ppl=4.56, wps=22581.2, ups=1.91, wpb=11830.4, bsz=408, num_updates=19600, lr=1.80702e-05, gnorm=0.801, train_wall=52, wall=10425
2020-12-22 19:03:43 | INFO | train_inner | epoch 007:   1346 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.044, loss=4.504, nll_loss=2.179, ppl=4.53, wps=22754.6, ups=1.92, wpb=11869.9, bsz=420.6, num_updates=19700, lr=1.80242e-05, gnorm=0.794, train_wall=52, wall=10477
2020-12-22 19:04:35 | INFO | train_inner | epoch 007:   1446 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.052, loss=4.485, nll_loss=2.157, ppl=4.46, wps=22899.6, ups=1.91, wpb=11999.1, bsz=411.4, num_updates=19800, lr=1.79787e-05, gnorm=0.78, train_wall=52, wall=10529
2020-12-22 19:05:28 | INFO | train_inner | epoch 007:   1546 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.055, loss=4.509, nll_loss=2.182, ppl=4.54, wps=22651.1, ups=1.91, wpb=11868.4, bsz=402.9, num_updates=19900, lr=1.79334e-05, gnorm=0.795, train_wall=52, wall=10582
2020-12-22 19:06:20 | INFO | train_inner | epoch 007:   1646 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.062, loss=4.51, nll_loss=2.182, ppl=4.54, wps=22544.9, ups=1.92, wpb=11764.3, bsz=398, num_updates=20000, lr=1.78885e-05, gnorm=0.8, train_wall=52, wall=10634
2020-12-22 19:07:12 | INFO | train_inner | epoch 007:   1746 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.055, loss=4.499, nll_loss=2.171, ppl=4.5, wps=22804.3, ups=1.92, wpb=11884.3, bsz=403.8, num_updates=20100, lr=1.7844e-05, gnorm=0.791, train_wall=52, wall=10686
2020-12-22 19:08:04 | INFO | train_inner | epoch 007:   1846 / 3059 symm_kl=0.579, self_kl=0, self_cv=6.051, loss=4.518, nll_loss=2.194, ppl=4.58, wps=22543.4, ups=1.92, wpb=11759.4, bsz=404.3, num_updates=20200, lr=1.77998e-05, gnorm=0.803, train_wall=52, wall=10738
2020-12-22 19:08:56 | INFO | train_inner | epoch 007:   1946 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.049, loss=4.502, nll_loss=2.176, ppl=4.52, wps=22659, ups=1.91, wpb=11860.2, bsz=417.7, num_updates=20300, lr=1.77559e-05, gnorm=0.795, train_wall=52, wall=10791
2020-12-22 19:09:49 | INFO | train_inner | epoch 007:   2046 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.057, loss=4.486, nll_loss=2.157, ppl=4.46, wps=22606, ups=1.91, wpb=11812.5, bsz=410.6, num_updates=20400, lr=1.77123e-05, gnorm=0.794, train_wall=52, wall=10843
2020-12-22 19:10:41 | INFO | train_inner | epoch 007:   2146 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.056, loss=4.509, nll_loss=2.183, ppl=4.54, wps=22654.5, ups=1.92, wpb=11818, bsz=397, num_updates=20500, lr=1.7669e-05, gnorm=0.796, train_wall=52, wall=10895
2020-12-22 19:11:33 | INFO | train_inner | epoch 007:   2246 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.053, loss=4.49, nll_loss=2.162, ppl=4.48, wps=22599.5, ups=1.91, wpb=11861.5, bsz=403.9, num_updates=20600, lr=1.76261e-05, gnorm=0.79, train_wall=52, wall=10947
2020-12-22 19:12:26 | INFO | train_inner | epoch 007:   2346 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.051, loss=4.491, nll_loss=2.163, ppl=4.48, wps=22487.9, ups=1.9, wpb=11866.6, bsz=406.5, num_updates=20700, lr=1.75835e-05, gnorm=0.792, train_wall=53, wall=11000
2020-12-22 19:13:19 | INFO | train_inner | epoch 007:   2446 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.045, loss=4.503, nll_loss=2.178, ppl=4.53, wps=22216, ups=1.87, wpb=11849.9, bsz=417.7, num_updates=20800, lr=1.75412e-05, gnorm=0.794, train_wall=53, wall=11054
2020-12-22 19:14:12 | INFO | train_inner | epoch 007:   2546 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.05, loss=4.517, nll_loss=2.192, ppl=4.57, wps=22266.3, ups=1.88, wpb=11814.5, bsz=391.7, num_updates=20900, lr=1.74991e-05, gnorm=0.813, train_wall=53, wall=11107
2020-12-22 19:15:05 | INFO | train_inner | epoch 007:   2646 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.04, loss=4.497, nll_loss=2.172, ppl=4.51, wps=22629.8, ups=1.9, wpb=11888.2, bsz=427, num_updates=21000, lr=1.74574e-05, gnorm=0.787, train_wall=52, wall=11159
2020-12-22 19:15:58 | INFO | train_inner | epoch 007:   2746 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.045, loss=4.497, nll_loss=2.171, ppl=4.5, wps=22567.5, ups=1.9, wpb=11904.9, bsz=433.8, num_updates=21100, lr=1.7416e-05, gnorm=0.787, train_wall=53, wall=11212
2020-12-22 19:16:51 | INFO | train_inner | epoch 007:   2846 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.033, loss=4.503, nll_loss=2.179, ppl=4.53, wps=22417.3, ups=1.89, wpb=11863.4, bsz=413.4, num_updates=21200, lr=1.73749e-05, gnorm=0.792, train_wall=53, wall=11265
2020-12-22 19:17:43 | INFO | train_inner | epoch 007:   2946 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.042, loss=4.497, nll_loss=2.172, ppl=4.51, wps=22420.3, ups=1.9, wpb=11797.2, bsz=424.3, num_updates=21300, lr=1.73341e-05, gnorm=0.801, train_wall=52, wall=11317
2020-12-22 19:18:35 | INFO | train_inner | epoch 007:   3046 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.054, loss=4.524, nll_loss=2.199, ppl=4.59, wps=22513.4, ups=1.92, wpb=11738.3, bsz=385, num_updates=21400, lr=1.72935e-05, gnorm=0.806, train_wall=52, wall=11370
2020-12-22 19:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 19:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:18:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 19:18:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 19:18:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 19:18:58 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.993 | nll_loss 8.112 | ppl 276.76 | bleu 15.94 | wps 4982.6 | wpb 6344.2 | bsz 166.4 | num_updates 21413 | best_bleu 16.19
2020-12-22 19:18:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 19:19:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 7 @ 21413 updates, score 15.94) (writing took 4.934674054384232 seconds)
2020-12-22 19:19:03 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-22 19:19:03 | INFO | train | epoch 007 | symm_kl 0.573 | self_kl 0 | self_cv 6.053 | loss 4.498 | nll_loss 2.171 | ppl 4.5 | wps 22199.3 | ups 1.87 | wpb 11852.2 | bsz 409.6 | num_updates 21413 | lr 1.72883e-05 | gnorm 0.794 | train_wall 1597 | wall 11397
2020-12-22 19:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:19:07 | INFO | fairseq.trainer | begin training epoch 8
2020-12-22 19:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:19:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:19:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:19:58 | INFO | train_inner | epoch 008:     87 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.063, loss=4.501, nll_loss=2.173, ppl=4.51, wps=14375.8, ups=1.22, wpb=11822.6, bsz=370.4, num_updates=21500, lr=1.72532e-05, gnorm=0.798, train_wall=51, wall=11452
2020-12-22 19:20:50 | INFO | train_inner | epoch 008:    187 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.054, loss=4.492, nll_loss=2.164, ppl=4.48, wps=22663, ups=1.92, wpb=11832.8, bsz=403.8, num_updates=21600, lr=1.72133e-05, gnorm=0.803, train_wall=52, wall=11504
2020-12-22 19:21:42 | INFO | train_inner | epoch 008:    287 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.055, loss=4.485, nll_loss=2.156, ppl=4.46, wps=22513.4, ups=1.9, wpb=11827.7, bsz=410.2, num_updates=21700, lr=1.71736e-05, gnorm=0.791, train_wall=52, wall=11557
2020-12-22 19:22:35 | INFO | train_inner | epoch 008:    387 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.05, loss=4.493, nll_loss=2.166, ppl=4.49, wps=22815.9, ups=1.91, wpb=11920.6, bsz=418.9, num_updates=21800, lr=1.71341e-05, gnorm=0.787, train_wall=52, wall=11609
2020-12-22 19:23:27 | INFO | train_inner | epoch 008:    487 / 3059 symm_kl=0.584, self_kl=0, self_cv=6.061, loss=4.522, nll_loss=2.197, ppl=4.58, wps=22330.4, ups=1.92, wpb=11634.7, bsz=375.8, num_updates=21900, lr=1.7095e-05, gnorm=0.818, train_wall=52, wall=11661
2020-12-22 19:24:19 | INFO | train_inner | epoch 008:    587 / 3059 symm_kl=0.577, self_kl=0, self_cv=6.059, loss=4.494, nll_loss=2.165, ppl=4.48, wps=22522.3, ups=1.9, wpb=11870.7, bsz=408.6, num_updates=22000, lr=1.70561e-05, gnorm=0.801, train_wall=53, wall=11714
2020-12-22 19:25:12 | INFO | train_inner | epoch 008:    687 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.054, loss=4.481, nll_loss=2.152, ppl=4.44, wps=22636.5, ups=1.91, wpb=11854.7, bsz=408.2, num_updates=22100, lr=1.70174e-05, gnorm=0.791, train_wall=52, wall=11766
2020-12-22 19:26:04 | INFO | train_inner | epoch 008:    787 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.049, loss=4.501, nll_loss=2.175, ppl=4.51, wps=22505.9, ups=1.9, wpb=11850.7, bsz=428.7, num_updates=22200, lr=1.69791e-05, gnorm=0.794, train_wall=52, wall=11819
2020-12-22 19:26:57 | INFO | train_inner | epoch 008:    887 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.052, loss=4.497, nll_loss=2.17, ppl=4.5, wps=22483.4, ups=1.9, wpb=11848.1, bsz=392.9, num_updates=22300, lr=1.69409e-05, gnorm=0.801, train_wall=53, wall=11871
2020-12-22 19:27:49 | INFO | train_inner | epoch 008:    987 / 3059 symm_kl=0.565, self_kl=0, self_cv=6.045, loss=4.481, nll_loss=2.153, ppl=4.45, wps=22801.9, ups=1.91, wpb=11932.9, bsz=426.2, num_updates=22400, lr=1.69031e-05, gnorm=0.788, train_wall=52, wall=11924
2020-12-22 19:28:42 | INFO | train_inner | epoch 008:   1087 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.052, loss=4.486, nll_loss=2.158, ppl=4.46, wps=22588.5, ups=1.9, wpb=11876.6, bsz=426.2, num_updates=22500, lr=1.68655e-05, gnorm=0.789, train_wall=52, wall=11976
2020-12-22 19:29:35 | INFO | train_inner | epoch 008:   1187 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.061, loss=4.501, nll_loss=2.172, ppl=4.51, wps=22521.1, ups=1.9, wpb=11867.4, bsz=406.2, num_updates=22600, lr=1.68281e-05, gnorm=0.792, train_wall=53, wall=12029
2020-12-22 19:30:27 | INFO | train_inner | epoch 008:   1287 / 3059 symm_kl=0.578, self_kl=0, self_cv=6.053, loss=4.516, nll_loss=2.191, ppl=4.56, wps=22454.7, ups=1.91, wpb=11750.6, bsz=400.2, num_updates=22700, lr=1.6791e-05, gnorm=0.807, train_wall=52, wall=12081
2020-12-22 19:31:19 | INFO | train_inner | epoch 008:   1387 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.045, loss=4.502, nll_loss=2.177, ppl=4.52, wps=22775, ups=1.92, wpb=11876.1, bsz=404.2, num_updates=22800, lr=1.67542e-05, gnorm=0.792, train_wall=52, wall=12133
2020-12-22 19:32:12 | INFO | train_inner | epoch 008:   1487 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.052, loss=4.493, nll_loss=2.166, ppl=4.49, wps=22264.2, ups=1.89, wpb=11797, bsz=401.4, num_updates=22900, lr=1.67175e-05, gnorm=0.798, train_wall=53, wall=12186
2020-12-22 19:33:05 | INFO | train_inner | epoch 008:   1587 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.049, loss=4.494, nll_loss=2.168, ppl=4.49, wps=22599.5, ups=1.91, wpb=11857.4, bsz=404.6, num_updates=23000, lr=1.66812e-05, gnorm=0.797, train_wall=52, wall=12239
2020-12-22 19:33:57 | INFO | train_inner | epoch 008:   1687 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.038, loss=4.488, nll_loss=2.162, ppl=4.48, wps=22589, ups=1.9, wpb=11903, bsz=423.4, num_updates=23100, lr=1.6645e-05, gnorm=0.791, train_wall=53, wall=12292
2020-12-22 19:34:49 | INFO | train_inner | epoch 008:   1787 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.054, loss=4.499, nll_loss=2.172, ppl=4.51, wps=22705.6, ups=1.92, wpb=11827.9, bsz=405.4, num_updates=23200, lr=1.66091e-05, gnorm=0.799, train_wall=52, wall=12344
2020-12-22 19:35:42 | INFO | train_inner | epoch 008:   1887 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.042, loss=4.491, nll_loss=2.166, ppl=4.49, wps=22625.4, ups=1.9, wpb=11893.1, bsz=432.5, num_updates=23300, lr=1.65734e-05, gnorm=0.789, train_wall=52, wall=12396
2020-12-22 19:36:34 | INFO | train_inner | epoch 008:   1987 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.04, loss=4.501, nll_loss=2.176, ppl=4.52, wps=22717.6, ups=1.91, wpb=11869.8, bsz=425.7, num_updates=23400, lr=1.6538e-05, gnorm=0.794, train_wall=52, wall=12448
2020-12-22 19:37:27 | INFO | train_inner | epoch 008:   2087 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.039, loss=4.496, nll_loss=2.172, ppl=4.51, wps=22716.3, ups=1.9, wpb=11931.3, bsz=405, num_updates=23500, lr=1.65027e-05, gnorm=0.792, train_wall=52, wall=12501
2020-12-22 19:38:19 | INFO | train_inner | epoch 008:   2187 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.046, loss=4.499, nll_loss=2.173, ppl=4.51, wps=22544.3, ups=1.9, wpb=11862.1, bsz=397.8, num_updates=23600, lr=1.64677e-05, gnorm=0.797, train_wall=52, wall=12554
2020-12-22 19:39:12 | INFO | train_inner | epoch 008:   2287 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.048, loss=4.5, nll_loss=2.174, ppl=4.51, wps=22583.3, ups=1.92, wpb=11773.4, bsz=385.8, num_updates=23700, lr=1.6433e-05, gnorm=0.814, train_wall=52, wall=12606
2020-12-22 19:40:04 | INFO | train_inner | epoch 008:   2387 / 3059 symm_kl=0.562, self_kl=0, self_cv=6.031, loss=4.477, nll_loss=2.151, ppl=4.44, wps=22724.9, ups=1.91, wpb=11903.2, bsz=430.2, num_updates=23800, lr=1.63984e-05, gnorm=0.782, train_wall=52, wall=12658
2020-12-22 19:40:56 | INFO | train_inner | epoch 008:   2487 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.039, loss=4.509, nll_loss=2.185, ppl=4.55, wps=22566.1, ups=1.92, wpb=11762.4, bsz=421, num_updates=23900, lr=1.63641e-05, gnorm=0.803, train_wall=52, wall=12710
2020-12-22 19:41:48 | INFO | train_inner | epoch 008:   2587 / 3059 symm_kl=0.565, self_kl=0, self_cv=6.045, loss=4.488, nll_loss=2.162, ppl=4.47, wps=22823.9, ups=1.92, wpb=11887.8, bsz=415.4, num_updates=24000, lr=1.63299e-05, gnorm=0.797, train_wall=52, wall=12762
2020-12-22 19:42:40 | INFO | train_inner | epoch 008:   2687 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.027, loss=4.507, nll_loss=2.186, ppl=4.55, wps=22837.1, ups=1.92, wpb=11866.9, bsz=397.2, num_updates=24100, lr=1.6296e-05, gnorm=0.793, train_wall=52, wall=12814
2020-12-22 19:43:33 | INFO | train_inner | epoch 008:   2787 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.04, loss=4.509, nll_loss=2.186, ppl=4.55, wps=22597.5, ups=1.9, wpb=11890.5, bsz=411.7, num_updates=24200, lr=1.62623e-05, gnorm=0.81, train_wall=52, wall=12867
2020-12-22 19:44:25 | INFO | train_inner | epoch 008:   2887 / 3059 symm_kl=0.562, self_kl=0, self_cv=6.032, loss=4.483, nll_loss=2.159, ppl=4.47, wps=22676.9, ups=1.91, wpb=11874, bsz=421.4, num_updates=24300, lr=1.62288e-05, gnorm=0.791, train_wall=52, wall=12919
2020-12-22 19:45:18 | INFO | train_inner | epoch 008:   2987 / 3059 symm_kl=0.566, self_kl=0, self_cv=6.035, loss=4.496, nll_loss=2.172, ppl=4.51, wps=22568.9, ups=1.9, wpb=11867.9, bsz=430.4, num_updates=24400, lr=1.61955e-05, gnorm=0.794, train_wall=52, wall=12972
2020-12-22 19:45:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 19:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 19:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 19:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 19:46:11 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.986 | nll_loss 8.106 | ppl 275.43 | bleu 15.79 | wps 4960.4 | wpb 6344.2 | bsz 166.4 | num_updates 24472 | best_bleu 16.19
2020-12-22 19:46:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 19:46:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 8 @ 24472 updates, score 15.79) (writing took 4.913164788857102 seconds)
2020-12-22 19:46:16 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-22 19:46:16 | INFO | train | epoch 008 | symm_kl 0.571 | self_kl 0 | self_cv 6.047 | loss 4.496 | nll_loss 2.17 | ppl 4.5 | wps 22200.1 | ups 1.87 | wpb 11852.2 | bsz 409.6 | num_updates 24472 | lr 1.61717e-05 | gnorm 0.796 | train_wall 1597 | wall 13030
2020-12-22 19:46:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:46:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:46:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:46:20 | INFO | fairseq.trainer | begin training epoch 9
2020-12-22 19:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 19:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 19:46:40 | INFO | train_inner | epoch 009:     28 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.044, loss=4.504, nll_loss=2.179, ppl=4.53, wps=14387.2, ups=1.21, wpb=11904.6, bsz=411.5, num_updates=24500, lr=1.61624e-05, gnorm=0.798, train_wall=52, wall=13055
2020-12-22 19:47:32 | INFO | train_inner | epoch 009:    128 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.051, loss=4.477, nll_loss=2.148, ppl=4.43, wps=22791, ups=1.93, wpb=11796.6, bsz=426.3, num_updates=24600, lr=1.61296e-05, gnorm=0.795, train_wall=52, wall=13106
2020-12-22 19:48:25 | INFO | train_inner | epoch 009:    228 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.048, loss=4.497, nll_loss=2.171, ppl=4.5, wps=22635.2, ups=1.91, wpb=11861.1, bsz=423.1, num_updates=24700, lr=1.60969e-05, gnorm=0.796, train_wall=52, wall=13159
2020-12-22 19:49:17 | INFO | train_inner | epoch 009:    328 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.039, loss=4.495, nll_loss=2.17, ppl=4.5, wps=22475.1, ups=1.91, wpb=11769.1, bsz=412.5, num_updates=24800, lr=1.60644e-05, gnorm=0.806, train_wall=52, wall=13211
2020-12-22 19:50:09 | INFO | train_inner | epoch 009:    428 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.049, loss=4.487, nll_loss=2.16, ppl=4.47, wps=22562.1, ups=1.91, wpb=11803.6, bsz=398.7, num_updates=24900, lr=1.60321e-05, gnorm=0.793, train_wall=52, wall=13263
2020-12-22 19:51:01 | INFO | train_inner | epoch 009:    528 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.06, loss=4.492, nll_loss=2.163, ppl=4.48, wps=22609.1, ups=1.92, wpb=11789.9, bsz=374.7, num_updates=25000, lr=1.6e-05, gnorm=0.8, train_wall=52, wall=13316
2020-12-22 19:51:54 | INFO | train_inner | epoch 009:    628 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.045, loss=4.478, nll_loss=2.151, ppl=4.44, wps=22754.1, ups=1.92, wpb=11878.4, bsz=413.8, num_updates=25100, lr=1.59681e-05, gnorm=0.795, train_wall=52, wall=13368
2020-12-22 19:52:46 | INFO | train_inner | epoch 009:    728 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.039, loss=4.501, nll_loss=2.177, ppl=4.52, wps=22616.8, ups=1.91, wpb=11851.8, bsz=425.8, num_updates=25200, lr=1.59364e-05, gnorm=0.801, train_wall=52, wall=13420
2020-12-22 19:53:39 | INFO | train_inner | epoch 009:    828 / 3059 symm_kl=0.563, self_kl=0, self_cv=6.047, loss=4.464, nll_loss=2.134, ppl=4.39, wps=22636.2, ups=1.9, wpb=11910.6, bsz=421.9, num_updates=25300, lr=1.59049e-05, gnorm=0.79, train_wall=52, wall=13473
2020-12-22 19:54:31 | INFO | train_inner | epoch 009:    928 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.051, loss=4.496, nll_loss=2.17, ppl=4.5, wps=22541.7, ups=1.89, wpb=11897.7, bsz=404.2, num_updates=25400, lr=1.58735e-05, gnorm=0.794, train_wall=53, wall=13526
2020-12-22 19:55:24 | INFO | train_inner | epoch 009:   1028 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.047, loss=4.497, nll_loss=2.172, ppl=4.51, wps=22521.6, ups=1.9, wpb=11863.8, bsz=403.8, num_updates=25500, lr=1.58424e-05, gnorm=0.813, train_wall=53, wall=13578
2020-12-22 19:56:17 | INFO | train_inner | epoch 009:   1128 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.046, loss=4.498, nll_loss=2.173, ppl=4.51, wps=22498.6, ups=1.89, wpb=11878.6, bsz=411.8, num_updates=25600, lr=1.58114e-05, gnorm=0.792, train_wall=53, wall=13631
2020-12-22 19:57:09 | INFO | train_inner | epoch 009:   1228 / 3059 symm_kl=0.574, self_kl=0, self_cv=6.051, loss=4.504, nll_loss=2.178, ppl=4.53, wps=22480.3, ups=1.91, wpb=11764.4, bsz=394.6, num_updates=25700, lr=1.57806e-05, gnorm=0.806, train_wall=52, wall=13683
2020-12-22 19:58:02 | INFO | train_inner | epoch 009:   1328 / 3059 symm_kl=0.564, self_kl=0, self_cv=6.044, loss=4.48, nll_loss=2.153, ppl=4.45, wps=22692.1, ups=1.91, wpb=11902.6, bsz=418.7, num_updates=25800, lr=1.575e-05, gnorm=0.79, train_wall=52, wall=13736
2020-12-22 19:58:54 | INFO | train_inner | epoch 009:   1428 / 3059 symm_kl=0.573, self_kl=0, self_cv=6.046, loss=4.503, nll_loss=2.178, ppl=4.52, wps=22741.4, ups=1.92, wpb=11867.5, bsz=411.7, num_updates=25900, lr=1.57195e-05, gnorm=0.799, train_wall=52, wall=13788
2020-12-22 19:59:46 | INFO | train_inner | epoch 009:   1528 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.043, loss=4.501, nll_loss=2.176, ppl=4.52, wps=22640.3, ups=1.92, wpb=11805.5, bsz=390.2, num_updates=26000, lr=1.56893e-05, gnorm=0.803, train_wall=52, wall=13840
2020-12-22 20:00:38 | INFO | train_inner | epoch 009:   1628 / 3059 symm_kl=0.563, self_kl=0, self_cv=6.032, loss=4.475, nll_loss=2.149, ppl=4.44, wps=22898.7, ups=1.93, wpb=11884.6, bsz=435.1, num_updates=26100, lr=1.56592e-05, gnorm=0.792, train_wall=52, wall=13892
2020-12-22 20:01:30 | INFO | train_inner | epoch 009:   1728 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.03, loss=4.5, nll_loss=2.177, ppl=4.52, wps=22622, ups=1.91, wpb=11863.3, bsz=417.3, num_updates=26200, lr=1.56293e-05, gnorm=0.797, train_wall=52, wall=13945
2020-12-22 20:02:23 | INFO | train_inner | epoch 009:   1828 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.039, loss=4.488, nll_loss=2.163, ppl=4.48, wps=22788.1, ups=1.91, wpb=11913.3, bsz=391.6, num_updates=26300, lr=1.55996e-05, gnorm=0.792, train_wall=52, wall=13997
2020-12-22 20:03:15 | INFO | train_inner | epoch 009:   1928 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.043, loss=4.511, nll_loss=2.188, ppl=4.56, wps=22579.8, ups=1.9, wpb=11862, bsz=407, num_updates=26400, lr=1.557e-05, gnorm=0.801, train_wall=52, wall=14049
2020-12-22 20:04:08 | INFO | train_inner | epoch 009:   2028 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.043, loss=4.497, nll_loss=2.172, ppl=4.51, wps=22556.9, ups=1.91, wpb=11820.9, bsz=407.8, num_updates=26500, lr=1.55406e-05, gnorm=0.8, train_wall=52, wall=14102
2020-12-22 20:05:00 | INFO | train_inner | epoch 009:   2128 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.043, loss=4.497, nll_loss=2.172, ppl=4.51, wps=22716, ups=1.91, wpb=11879.3, bsz=394.3, num_updates=26600, lr=1.55113e-05, gnorm=0.795, train_wall=52, wall=14154
2020-12-22 20:05:52 | INFO | train_inner | epoch 009:   2228 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.031, loss=4.497, nll_loss=2.174, ppl=4.51, wps=22730.9, ups=1.92, wpb=11852.3, bsz=417.8, num_updates=26700, lr=1.54823e-05, gnorm=0.801, train_wall=52, wall=14206
2020-12-22 20:06:44 | INFO | train_inner | epoch 009:   2328 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.041, loss=4.501, nll_loss=2.177, ppl=4.52, wps=22703.1, ups=1.91, wpb=11860.9, bsz=408.7, num_updates=26800, lr=1.54533e-05, gnorm=0.797, train_wall=52, wall=14258
2020-12-22 20:07:37 | INFO | train_inner | epoch 009:   2428 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.033, loss=4.514, nll_loss=2.193, ppl=4.57, wps=22226.2, ups=1.91, wpb=11648.5, bsz=394.2, num_updates=26900, lr=1.54246e-05, gnorm=0.813, train_wall=52, wall=14311
2020-12-22 20:08:29 | INFO | train_inner | epoch 009:   2528 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.033, loss=4.507, nll_loss=2.185, ppl=4.55, wps=22750.5, ups=1.91, wpb=11911.3, bsz=421.2, num_updates=27000, lr=1.5396e-05, gnorm=0.8, train_wall=52, wall=14363
2020-12-22 20:09:22 | INFO | train_inner | epoch 009:   2628 / 3059 symm_kl=0.559, self_kl=0, self_cv=6.025, loss=4.476, nll_loss=2.152, ppl=4.44, wps=22866.6, ups=1.91, wpb=12002.5, bsz=444.6, num_updates=27100, lr=1.53676e-05, gnorm=0.786, train_wall=52, wall=14416
2020-12-22 20:10:14 | INFO | train_inner | epoch 009:   2728 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.042, loss=4.498, nll_loss=2.173, ppl=4.51, wps=22727.6, ups=1.92, wpb=11833.5, bsz=405.9, num_updates=27200, lr=1.53393e-05, gnorm=0.809, train_wall=52, wall=14468
2020-12-22 20:11:06 | INFO | train_inner | epoch 009:   2828 / 3059 symm_kl=0.566, self_kl=0, self_cv=6.033, loss=4.495, nll_loss=2.172, ppl=4.51, wps=22674.5, ups=1.91, wpb=11854.7, bsz=405.5, num_updates=27300, lr=1.53112e-05, gnorm=0.798, train_wall=52, wall=14520
2020-12-22 20:11:58 | INFO | train_inner | epoch 009:   2928 / 3059 symm_kl=0.575, self_kl=0, self_cv=6.046, loss=4.516, nll_loss=2.193, ppl=4.57, wps=22622.7, ups=1.92, wpb=11788.6, bsz=375.2, num_updates=27400, lr=1.52832e-05, gnorm=0.808, train_wall=52, wall=14572
2020-12-22 20:12:51 | INFO | train_inner | epoch 009:   3028 / 3059 symm_kl=0.561, self_kl=0, self_cv=6.042, loss=4.476, nll_loss=2.149, ppl=4.44, wps=22749, ups=1.9, wpb=11951.5, bsz=410.8, num_updates=27500, lr=1.52554e-05, gnorm=0.79, train_wall=52, wall=14625
2020-12-22 20:13:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-22 20:13:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-22 20:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-22 20:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-22 20:13:24 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 8.994 | nll_loss 8.113 | ppl 276.82 | bleu 15.91 | wps 4413.7 | wpb 6344.2 | bsz 166.4 | num_updates 27531 | best_bleu 16.19
2020-12-22 20:13:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-22 20:13:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/_transformer_base/bash/../checkpoints/kl/checkpoint_last.pt (epoch 9 @ 27531 updates, score 15.91) (writing took 4.96827388741076 seconds)
2020-12-22 20:13:29 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-22 20:13:29 | INFO | train | epoch 009 | symm_kl 0.569 | self_kl 0 | self_cv 6.042 | loss 4.494 | nll_loss 2.169 | ppl 4.5 | wps 22203.4 | ups 1.87 | wpb 11852.2 | bsz 409.6 | num_updates 27531 | lr 1.52468e-05 | gnorm 0.798 | train_wall 1595 | wall 14663
2020-12-22 20:13:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:33 | INFO | fairseq.trainer | begin training epoch 10
2020-12-22 20:13:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:13:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-22 20:13:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-22 20:14:14 | INFO | train_inner | epoch 010:     69 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.027, loss=4.493, nll_loss=2.17, ppl=4.5, wps=14137.1, ups=1.2, wpb=11815.9, bsz=434.6, num_updates=27600, lr=1.52277e-05, gnorm=0.8, train_wall=51, wall=14708
2020-12-22 20:15:06 | INFO | train_inner | epoch 010:    169 / 3059 symm_kl=0.562, self_kl=0, self_cv=6.036, loss=4.466, nll_loss=2.138, ppl=4.4, wps=22923.3, ups=1.91, wpb=11980, bsz=421.7, num_updates=27700, lr=1.52002e-05, gnorm=0.783, train_wall=52, wall=14761
2020-12-22 20:15:59 | INFO | train_inner | epoch 010:    269 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.037, loss=4.499, nll_loss=2.175, ppl=4.52, wps=22663.8, ups=1.92, wpb=11826.6, bsz=408.6, num_updates=27800, lr=1.51729e-05, gnorm=0.805, train_wall=52, wall=14813
2020-12-22 20:16:51 | INFO | train_inner | epoch 010:    369 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.038, loss=4.489, nll_loss=2.165, ppl=4.48, wps=22561.4, ups=1.91, wpb=11800.1, bsz=417, num_updates=27900, lr=1.51456e-05, gnorm=0.8, train_wall=52, wall=14865
2020-12-22 20:17:43 | INFO | train_inner | epoch 010:    469 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.041, loss=4.481, nll_loss=2.155, ppl=4.45, wps=22482.1, ups=1.9, wpb=11804, bsz=419.8, num_updates=28000, lr=1.51186e-05, gnorm=0.817, train_wall=52, wall=14918
2020-12-22 20:18:36 | INFO | train_inner | epoch 010:    569 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.044, loss=4.492, nll_loss=2.167, ppl=4.49, wps=22496.1, ups=1.9, wpb=11816.3, bsz=386.2, num_updates=28100, lr=1.50917e-05, gnorm=0.799, train_wall=52, wall=14970
2020-12-22 20:19:28 | INFO | train_inner | epoch 010:    669 / 3059 symm_kl=0.561, self_kl=0, self_cv=6.044, loss=4.46, nll_loss=2.13, ppl=4.38, wps=22648, ups=1.91, wpb=11881.4, bsz=428.7, num_updates=28200, lr=1.50649e-05, gnorm=0.786, train_wall=52, wall=15023
2020-12-22 20:20:21 | INFO | train_inner | epoch 010:    769 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.045, loss=4.483, nll_loss=2.157, ppl=4.46, wps=22685.4, ups=1.91, wpb=11863.1, bsz=392.9, num_updates=28300, lr=1.50382e-05, gnorm=0.801, train_wall=52, wall=15075
2020-12-22 20:21:13 | INFO | train_inner | epoch 010:    869 / 3059 symm_kl=0.568, self_kl=0, self_cv=6.037, loss=4.484, nll_loss=2.159, ppl=4.47, wps=22459.7, ups=1.9, wpb=11792.9, bsz=406.2, num_updates=28400, lr=1.50117e-05, gnorm=0.804, train_wall=52, wall=15127
2020-12-22 20:22:05 | INFO | train_inner | epoch 010:    969 / 3059 symm_kl=0.565, self_kl=0, self_cv=6.039, loss=4.488, nll_loss=2.164, ppl=4.48, wps=22869.9, ups=1.91, wpb=11949.3, bsz=415, num_updates=28500, lr=1.49854e-05, gnorm=0.798, train_wall=52, wall=15180
2020-12-22 20:22:58 | INFO | train_inner | epoch 010:   1069 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.042, loss=4.493, nll_loss=2.168, ppl=4.49, wps=22683.3, ups=1.91, wpb=11857.6, bsz=402.2, num_updates=28600, lr=1.49592e-05, gnorm=0.797, train_wall=52, wall=15232
2020-12-22 20:23:50 | INFO | train_inner | epoch 010:   1169 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.04, loss=4.484, nll_loss=2.158, ppl=4.46, wps=22707.6, ups=1.91, wpb=11878.1, bsz=413.5, num_updates=28700, lr=1.49331e-05, gnorm=0.798, train_wall=52, wall=15284
2020-12-22 20:24:42 | INFO | train_inner | epoch 010:   1269 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.039, loss=4.506, nll_loss=2.183, ppl=4.54, wps=22778, ups=1.92, wpb=11869, bsz=408.5, num_updates=28800, lr=1.49071e-05, gnorm=0.802, train_wall=52, wall=15336
2020-12-22 20:25:34 | INFO | train_inner | epoch 010:   1369 / 3059 symm_kl=0.562, self_kl=0, self_cv=6.034, loss=4.478, nll_loss=2.153, ppl=4.45, wps=22847.2, ups=1.92, wpb=11923.3, bsz=418.5, num_updates=28900, lr=1.48813e-05, gnorm=0.791, train_wall=52, wall=15388
2020-12-22 20:26:27 | INFO | train_inner | epoch 010:   1469 / 3059 symm_kl=0.567, self_kl=0, self_cv=6.04, loss=4.489, nll_loss=2.164, ppl=4.48, wps=22720, ups=1.91, wpb=11869.6, bsz=414.7, num_updates=29000, lr=1.48556e-05, gnorm=0.798, train_wall=52, wall=15441
2020-12-22 20:27:19 | INFO | train_inner | epoch 010:   1569 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.037, loss=4.499, nll_loss=2.176, ppl=4.52, wps=22657.4, ups=1.92, wpb=11820.3, bsz=399.8, num_updates=29100, lr=1.48301e-05, gnorm=0.805, train_wall=52, wall=15493
2020-12-22 20:28:11 | INFO | train_inner | epoch 010:   1669 / 3059 symm_kl=0.572, self_kl=0, self_cv=6.035, loss=4.506, nll_loss=2.184, ppl=4.54, wps=22669.8, ups=1.92, wpb=11834.8, bsz=401.1, num_updates=29200, lr=1.48047e-05, gnorm=0.805, train_wall=52, wall=15545
2020-12-22 20:29:03 | INFO | train_inner | epoch 010:   1769 / 3059 symm_kl=0.569, self_kl=0, self_cv=6.041, loss=4.504, nll_loss=2.181, ppl=4.53, wps=22724.2, ups=1.91, wpb=11872.2, bsz=407.4, num_updates=29300, lr=1.47794e-05, gnorm=0.799, train_wall=52, wall=15597
2020-12-22 20:29:55 | INFO | train_inner | epoch 010:   1869 / 3059 symm_kl=0.563, self_kl=0, self_cv=6.042, loss=4.474, nll_loss=2.148, ppl=4.43, wps=22898.8, ups=1.92, wpb=11925.1, bsz=405.4, num_updates=29400, lr=1.47542e-05, gnorm=0.791, train_wall=52, wall=15649
2020-12-22 20:30:47 | INFO | train_inner | epoch 010:   1969 / 3059 symm_kl=0.564, self_kl=0, self_cv=6.031, loss=4.493, nll_loss=2.17, ppl=4.5, wps=22714.7, ups=1.92, wpb=11814.7, bsz=423, num_updates=29500, lr=1.47292e-05, gnorm=0.806, train_wall=52, wall=15701
2020-12-22 20:31:39 | INFO | train_inner | epoch 010:   2069 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.038, loss=4.496, nll_loss=2.172, ppl=4.51, wps=22665.3, ups=1.92, wpb=11789.9, bsz=406.7, num_updates=29600, lr=1.47043e-05, gnorm=0.804, train_wall=52, wall=15753
2020-12-22 20:32:31 | INFO | train_inner | epoch 010:   2169 / 3059 symm_kl=0.57, self_kl=0, self_cv=6.038, loss=4.505, nll_loss=2.182, ppl=4.54, wps=22762.9, ups=1.92, wpb=11831.8, bsz=404.8, num_updates=29700, lr=1.46795e-05, gnorm=0.803, train_wall=52, wall=15805
2020-12-22 20:33:23 | INFO | train_inner | epoch 010:   2269 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.03, loss=4.517, nll_loss=2.197, ppl=4.58, wps=22757.6, ups=1.93, wpb=11818.3, bsz=387.4, num_updates=29800, lr=1.46549e-05, gnorm=0.806, train_wall=52, wall=15857
2020-12-22 20:34:15 | INFO | train_inner | epoch 010:   2369 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.042, loss=4.501, nll_loss=2.177, ppl=4.52, wps=22823.8, ups=1.93, wpb=11844.1, bsz=385.8, num_updates=29900, lr=1.46303e-05, gnorm=0.802, train_wall=52, wall=15909
2020-12-22 20:35:07 | INFO | train_inner | epoch 010:   2469 / 3059 symm_kl=0.571, self_kl=0, self_cv=6.034, loss=4.508, nll_loss=2.186, ppl=4.55, wps=22604.5, ups=1.91, wpb=11844.4, bsz=395.6, num_updates=30000, lr=1.46059e-05, gnorm=0.806, train_wall=52, wall=15962
2020-12-22 20:35:59 | INFO | train_inner | epoch 010:   2569 / 3059 symm_kl=0.565, self_kl=0, self_cv=6.039, loss=4.493, nll_loss=2.169, ppl=4.5, wps=22792.3, ups=1.93, wpb=11832.6, bsz=413, num_updates=30100, lr=1.45817e-05, gnorm=0.802, train_wall=52, wall=16014
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 120 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
