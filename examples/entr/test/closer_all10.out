nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/closer-all1
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=0.08
extr='--layer-choice normal --warmup-init-lr 1e-07 --noised-with-grad'
2021-01-01 21:50:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:46 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19887
2021-01-01 21:50:46 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:19887
2021-01-01 21:50:46 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-01 21:50:46 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19887
2021-01-01 21:50:46 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-01 21:50:47 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:19887
2021-01-01 21:50:47 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2021-01-01 21:50:47 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-01 21:50:50 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19887', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_with_grad=True, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.08, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-01 21:50:50 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-01 21:50:50 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-01 21:50:50 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-01 21:50:50 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-01 21:50:50 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-01 21:50:51 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-01 21:50:51 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-01 21:50:51 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2021-01-01 21:50:51 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2021-01-01 21:50:51 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-01 21:50:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-01 21:50:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-01 21:50:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-01 21:50:52 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 21:50:52 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 21:50:52 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 21:50:52 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 21:50:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-01 21:50:52 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2021-01-01 21:50:52 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-01 21:50:52 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-01 21:50:52 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-01 21:50:52 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-01 21:50:52 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-01 21:50:52 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-01 21:50:52 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-01 21:50:52 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-01 21:50:52 | INFO | fairseq.trainer | begin training epoch 1
2021-01-01 21:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:50:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:50:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:51:58 | INFO | train_inner | epoch 001:    100 / 421 symm_mse=22.835, loss=5.296, nll_loss=0.845, ppl=1.8, wps=23243.8, ups=1.64, wpb=14145.4, bsz=485.9, num_updates=100, lr=1.43e-06, gnorm=3.854, train_wall=61, wall=66
2021-01-01 21:52:59 | INFO | train_inner | epoch 001:    200 / 421 symm_mse=19.899, loss=4.961, nll_loss=0.863, ppl=1.82, wps=22908.5, ups=1.64, wpb=13995.2, bsz=501.2, num_updates=200, lr=2.76e-06, gnorm=3.467, train_wall=61, wall=127
2021-01-01 21:54:01 | INFO | train_inner | epoch 001:    300 / 421 symm_mse=17.367, loss=4.695, nll_loss=0.915, ppl=1.89, wps=22533.1, ups=1.61, wpb=13972.8, bsz=508.7, num_updates=300, lr=4.09e-06, gnorm=2.925, train_wall=62, wall=189
2021-01-01 21:55:03 | INFO | train_inner | epoch 001:    400 / 421 symm_mse=16.651, loss=4.637, nll_loss=0.948, ppl=1.93, wps=22320.4, ups=1.61, wpb=13886.9, bsz=487.4, num_updates=400, lr=5.42e-06, gnorm=2.411, train_wall=62, wall=251
2021-01-01 21:55:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:55:33 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_mse 0 | loss 5.625 | nll_loss 4.094 | ppl 17.07 | bleu 22.36 | wps 5870.2 | wpb 10324.2 | bsz 375 | num_updates 421
2021-01-01 21:55:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:55:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 1 @ 421 updates, score 22.36) (writing took 2.2269167602062225 seconds)
2021-01-01 21:55:35 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-01 21:55:35 | INFO | train | epoch 001 | symm_mse 19.135 | loss 4.894 | nll_loss 0.897 | ppl 1.86 | wps 21125.1 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 421 | lr 5.6993e-06 | gnorm 3.131 | train_wall 259 | wall 284
2021-01-01 21:55:35 | INFO | fairseq.trainer | begin training epoch 2
2021-01-01 21:55:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:55:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:56:27 | INFO | train_inner | epoch 002:     79 / 421 symm_mse=16.264, loss=4.614, nll_loss=0.986, ppl=1.98, wps=16490.1, ups=1.19, wpb=13861.1, bsz=487.7, num_updates=500, lr=6.75e-06, gnorm=2.269, train_wall=61, wall=335
2021-01-01 21:57:29 | INFO | train_inner | epoch 002:    179 / 421 symm_mse=14.634, loss=4.439, nll_loss=1.012, ppl=2.02, wps=22429.9, ups=1.6, wpb=14007.7, bsz=489.9, num_updates=600, lr=8.08e-06, gnorm=1.924, train_wall=62, wall=398
2021-01-01 21:58:32 | INFO | train_inner | epoch 002:    279 / 421 symm_mse=14.222, loss=4.418, nll_loss=1.047, ppl=2.07, wps=22525.4, ups=1.61, wpb=14033.1, bsz=489, num_updates=700, lr=9.41e-06, gnorm=1.863, train_wall=62, wall=460
2021-01-01 21:59:34 | INFO | train_inner | epoch 002:    379 / 421 symm_mse=13.482, loss=4.369, nll_loss=1.095, ppl=2.14, wps=22518.4, ups=1.62, wpb=13928.4, bsz=496.1, num_updates=800, lr=1.074e-05, gnorm=1.756, train_wall=62, wall=522
2021-01-01 21:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:00:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:00:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:00:16 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_mse 0 | loss 5.47 | nll_loss 3.994 | ppl 15.93 | bleu 22.39 | wps 5860.2 | wpb 10324.2 | bsz 375 | num_updates 842 | best_bleu 22.39
2021-01-01 22:00:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:00:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:00:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 2 @ 842 updates, score 22.39) (writing took 4.701911697164178 seconds)
2021-01-01 22:00:21 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-01 22:00:21 | INFO | train | epoch 002 | symm_mse 14.388 | loss 4.436 | nll_loss 1.045 | ppl 2.06 | wps 20560.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 842 | lr 1.12986e-05 | gnorm 1.911 | train_wall 260 | wall 570
2021-01-01 22:00:21 | INFO | fairseq.trainer | begin training epoch 3
2021-01-01 22:00:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:00:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:01:00 | INFO | train_inner | epoch 003:     58 / 421 symm_mse=13.224, loss=4.349, nll_loss=1.114, ppl=2.16, wps=16127.7, ups=1.16, wpb=13895.5, bsz=482.9, num_updates=900, lr=1.207e-05, gnorm=1.719, train_wall=61, wall=608
2021-01-01 22:02:02 | INFO | train_inner | epoch 003:    158 / 421 symm_mse=12.137, loss=4.232, nll_loss=1.117, ppl=2.17, wps=22405.9, ups=1.61, wpb=13938, bsz=507, num_updates=1000, lr=1.34e-05, gnorm=1.56, train_wall=62, wall=670
2021-01-01 22:03:04 | INFO | train_inner | epoch 003:    258 / 421 symm_mse=12.641, loss=4.314, nll_loss=1.149, ppl=2.22, wps=22456.3, ups=1.61, wpb=13957.9, bsz=482.5, num_updates=1100, lr=1.473e-05, gnorm=1.604, train_wall=62, wall=733
2021-01-01 22:04:07 | INFO | train_inner | epoch 003:    358 / 421 symm_mse=11.744, loss=4.206, nll_loss=1.142, ppl=2.21, wps=22584, ups=1.6, wpb=14135, bsz=507.8, num_updates=1200, lr=1.606e-05, gnorm=1.491, train_wall=62, wall=795
2021-01-01 22:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:05:04 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_mse 0 | loss 5.42 | nll_loss 3.958 | ppl 15.54 | bleu 22.59 | wps 5316.8 | wpb 10324.2 | bsz 375 | num_updates 1263 | best_bleu 22.59
2021-01-01 22:05:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:05:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:05:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:05:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:05:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:05:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:05:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 3 @ 1263 updates, score 22.59) (writing took 4.73355109244585 seconds)
2021-01-01 22:05:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-01 22:05:09 | INFO | train | epoch 003 | symm_mse 12.278 | loss 4.263 | nll_loss 1.139 | ppl 2.2 | wps 20427.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 1263 | lr 1.68979e-05 | gnorm 1.568 | train_wall 261 | wall 857
2021-01-01 22:05:09 | INFO | fairseq.trainer | begin training epoch 4
2021-01-01 22:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:05:35 | INFO | train_inner | epoch 004:     37 / 421 symm_mse=12.231, loss=4.288, nll_loss=1.177, ppl=2.26, wps=15782.3, ups=1.14, wpb=13896.1, bsz=467.7, num_updates=1300, lr=1.739e-05, gnorm=1.568, train_wall=62, wall=883
2021-01-01 22:06:37 | INFO | train_inner | epoch 004:    137 / 421 symm_mse=11.391, loss=4.187, nll_loss=1.167, ppl=2.25, wps=22061.5, ups=1.61, wpb=13735.4, bsz=496.9, num_updates=1400, lr=1.872e-05, gnorm=1.488, train_wall=62, wall=945
2021-01-01 22:07:40 | INFO | train_inner | epoch 004:    237 / 421 symm_mse=11.345, loss=4.191, nll_loss=1.178, ppl=2.26, wps=22427.6, ups=1.59, wpb=14090.6, bsz=475.5, num_updates=1500, lr=2.005e-05, gnorm=1.459, train_wall=63, wall=1008
2021-01-01 22:08:42 | INFO | train_inner | epoch 004:    337 / 421 symm_mse=10.779, loss=4.119, nll_loss=1.166, ppl=2.24, wps=22516.2, ups=1.6, wpb=14057, bsz=499.7, num_updates=1600, lr=2.138e-05, gnorm=1.395, train_wall=62, wall=1071
2021-01-01 22:09:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:09:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:09:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:09:50 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.931 | ppl 15.26 | bleu 22.54 | wps 6660.7 | wpb 10324.2 | bsz 375 | num_updates 1684 | best_bleu 22.59
2021-01-01 22:09:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:09:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:09:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 4 @ 1684 updates, score 22.54) (writing took 2.9107613638043404 seconds)
2021-01-01 22:09:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-01 22:09:53 | INFO | train | epoch 004 | symm_mse 11.1 | loss 4.157 | nll_loss 1.17 | ppl 2.25 | wps 20689.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 1684 | lr 2.24972e-05 | gnorm 1.438 | train_wall 262 | wall 1142
2021-01-01 22:09:53 | INFO | fairseq.trainer | begin training epoch 5
2021-01-01 22:09:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:09:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:10:06 | INFO | train_inner | epoch 005:     16 / 421 symm_mse=10.485, loss=4.084, nll_loss=1.164, ppl=2.24, wps=16647, ups=1.19, wpb=13973, bsz=503, num_updates=1700, lr=2.271e-05, gnorm=1.365, train_wall=62, wall=1155
2021-01-01 22:11:08 | INFO | train_inner | epoch 005:    116 / 421 symm_mse=10.424, loss=4.074, nll_loss=1.158, ppl=2.23, wps=22450.4, ups=1.61, wpb=13947.8, bsz=499.7, num_updates=1800, lr=2.404e-05, gnorm=1.364, train_wall=62, wall=1217
2021-01-01 22:12:11 | INFO | train_inner | epoch 005:    216 / 421 symm_mse=10.472, loss=4.097, nll_loss=1.18, ppl=2.27, wps=22488, ups=1.6, wpb=14054.4, bsz=484.6, num_updates=1900, lr=2.537e-05, gnorm=1.349, train_wall=62, wall=1279
2021-01-01 22:13:13 | INFO | train_inner | epoch 005:    316 / 421 symm_mse=10.162, loss=4.066, nll_loss=1.185, ppl=2.27, wps=22303.8, ups=1.6, wpb=13937.1, bsz=501.2, num_updates=2000, lr=2.67e-05, gnorm=1.334, train_wall=62, wall=1342
2021-01-01 22:14:16 | INFO | train_inner | epoch 005:    416 / 421 symm_mse=10.139, loss=4.049, nll_loss=1.166, ppl=2.24, wps=22333.3, ups=1.59, wpb=14006.7, bsz=492.6, num_updates=2100, lr=2.803e-05, gnorm=1.345, train_wall=63, wall=1404
2021-01-01 22:14:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:14:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:14:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:14:34 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_mse 0 | loss 5.39 | nll_loss 3.919 | ppl 15.12 | bleu 22.57 | wps 6698.2 | wpb 10324.2 | bsz 375 | num_updates 2105 | best_bleu 22.59
2021-01-01 22:14:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:14:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:14:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 5 @ 2105 updates, score 22.57) (writing took 2.884013319388032 seconds)
2021-01-01 22:14:37 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-01 22:14:37 | INFO | train | epoch 005 | symm_mse 10.345 | loss 4.078 | nll_loss 1.174 | ppl 2.26 | wps 20709.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 2105 | lr 2.80965e-05 | gnorm 1.353 | train_wall 262 | wall 1426
2021-01-01 22:14:37 | INFO | fairseq.trainer | begin training epoch 6
2021-01-01 22:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:15:39 | INFO | train_inner | epoch 006:     95 / 421 symm_mse=9.805, loss=4.015, nll_loss=1.171, ppl=2.25, wps=16627.9, ups=1.2, wpb=13827.8, bsz=506.6, num_updates=2200, lr=2.936e-05, gnorm=1.301, train_wall=61, wall=1488
2021-01-01 22:16:43 | INFO | train_inner | epoch 006:    195 / 421 symm_mse=9.843, loss=4.029, nll_loss=1.181, ppl=2.27, wps=22223.9, ups=1.58, wpb=14087.8, bsz=489.6, num_updates=2300, lr=3.069e-05, gnorm=1.296, train_wall=63, wall=1551
2021-01-01 22:17:45 | INFO | train_inner | epoch 006:    295 / 421 symm_mse=9.918, loss=4.052, nll_loss=1.198, ppl=2.29, wps=22593.5, ups=1.61, wpb=14036.1, bsz=477.8, num_updates=2400, lr=3.202e-05, gnorm=1.287, train_wall=62, wall=1613
2021-01-01 22:18:47 | INFO | train_inner | epoch 006:    395 / 421 symm_mse=9.32, loss=3.962, nll_loss=1.172, ppl=2.25, wps=22434.5, ups=1.6, wpb=13994.5, bsz=506.2, num_updates=2500, lr=3.335e-05, gnorm=1.265, train_wall=62, wall=1676
2021-01-01 22:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:19:18 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_mse 0 | loss 5.371 | nll_loss 3.889 | ppl 14.82 | bleu 22.69 | wps 6860.8 | wpb 10324.2 | bsz 375 | num_updates 2526 | best_bleu 22.69
2021-01-01 22:19:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:19:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 6 @ 2526 updates, score 22.69) (writing took 4.823668641969562 seconds)
2021-01-01 22:19:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-01 22:19:23 | INFO | train | epoch 006 | symm_mse 9.729 | loss 4.017 | nll_loss 1.182 | ppl 2.27 | wps 20565.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 2526 | lr 3.36958e-05 | gnorm 1.288 | train_wall 262 | wall 1712
2021-01-01 22:19:23 | INFO | fairseq.trainer | begin training epoch 7
2021-01-01 22:19:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:19:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:20:12 | INFO | train_inner | epoch 007:     74 / 421 symm_mse=9.424, loss=3.985, nll_loss=1.184, ppl=2.27, wps=16219.6, ups=1.18, wpb=13772.9, bsz=486.4, num_updates=2600, lr=3.468e-05, gnorm=1.274, train_wall=62, wall=1760
2021-01-01 22:21:14 | INFO | train_inner | epoch 007:    174 / 421 symm_mse=9.379, loss=3.983, nll_loss=1.187, ppl=2.28, wps=22396.1, ups=1.61, wpb=13910.1, bsz=482.8, num_updates=2700, lr=3.601e-05, gnorm=1.245, train_wall=62, wall=1823
2021-01-01 22:22:17 | INFO | train_inner | epoch 007:    274 / 421 symm_mse=9.298, loss=3.97, nll_loss=1.181, ppl=2.27, wps=22205.2, ups=1.6, wpb=13879.3, bsz=492.2, num_updates=2800, lr=3.734e-05, gnorm=1.256, train_wall=62, wall=1885
2021-01-01 22:23:19 | INFO | train_inner | epoch 007:    374 / 421 symm_mse=9.263, loss=3.972, nll_loss=1.189, ppl=2.28, wps=22571.6, ups=1.6, wpb=14151.4, bsz=491.7, num_updates=2900, lr=3.867e-05, gnorm=1.268, train_wall=62, wall=1948
2021-01-01 22:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:23:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:23:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:23:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:23:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:23:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:24:04 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_mse 0 | loss 5.365 | nll_loss 3.881 | ppl 14.73 | bleu 22.48 | wps 6725.9 | wpb 10324.2 | bsz 375 | num_updates 2947 | best_bleu 22.69
2021-01-01 22:24:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:24:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:24:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:24:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:24:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:24:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:24:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:24:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 7 @ 2947 updates, score 22.48) (writing took 2.9480608105659485 seconds)
2021-01-01 22:24:07 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-01 22:24:07 | INFO | train | epoch 007 | symm_mse 9.257 | loss 3.967 | nll_loss 1.184 | ppl 2.27 | wps 20735.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 2947 | lr 3.92951e-05 | gnorm 1.251 | train_wall 261 | wall 1995
2021-01-01 22:24:07 | INFO | fairseq.trainer | begin training epoch 8
2021-01-01 22:24:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:24:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:24:43 | INFO | train_inner | epoch 008:     53 / 421 symm_mse=8.843, loss=3.916, nll_loss=1.18, ppl=2.27, wps=16896.3, ups=1.2, wpb=14109.9, bsz=506.2, num_updates=3000, lr=4e-05, gnorm=1.21, train_wall=62, wall=2031
2021-01-01 22:25:45 | INFO | train_inner | epoch 008:    153 / 421 symm_mse=9.28, loss=3.98, nll_loss=1.195, ppl=2.29, wps=22320.4, ups=1.6, wpb=13958.9, bsz=475.6, num_updates=3100, lr=3.93496e-05, gnorm=1.285, train_wall=62, wall=2094
2021-01-01 22:26:48 | INFO | train_inner | epoch 008:    253 / 421 symm_mse=8.692, loss=3.901, nll_loss=1.178, ppl=2.26, wps=22077.9, ups=1.59, wpb=13843, bsz=502.1, num_updates=3200, lr=3.87298e-05, gnorm=1.192, train_wall=62, wall=2156
2021-01-01 22:27:51 | INFO | train_inner | epoch 008:    353 / 421 symm_mse=8.401, loss=3.855, nll_loss=1.165, ppl=2.24, wps=22489.4, ups=1.59, wpb=14141.4, bsz=514.8, num_updates=3300, lr=3.81385e-05, gnorm=1.148, train_wall=63, wall=2219
2021-01-01 22:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:28:49 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_mse 0 | loss 5.361 | nll_loss 3.877 | ppl 14.69 | bleu 22.6 | wps 6698.1 | wpb 10324.2 | bsz 375 | num_updates 3368 | best_bleu 22.69
2021-01-01 22:28:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:28:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 8 @ 3368 updates, score 22.6) (writing took 2.936206176877022 seconds)
2021-01-01 22:28:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-01 22:28:52 | INFO | train | epoch 008 | symm_mse 8.852 | loss 3.923 | nll_loss 1.185 | ppl 2.27 | wps 20622.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 3368 | lr 3.77515e-05 | gnorm 1.212 | train_wall 263 | wall 2280
2021-01-01 22:28:52 | INFO | fairseq.trainer | begin training epoch 9
2021-01-01 22:28:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:28:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:29:15 | INFO | train_inner | epoch 009:     32 / 421 symm_mse=8.883, loss=3.942, nll_loss=1.203, ppl=2.3, wps=16436.2, ups=1.19, wpb=13812.4, bsz=478.6, num_updates=3400, lr=3.75735e-05, gnorm=1.203, train_wall=62, wall=2303
2021-01-01 22:30:18 | INFO | train_inner | epoch 009:    132 / 421 symm_mse=8.952, loss=3.942, nll_loss=1.194, ppl=2.29, wps=22352.1, ups=1.6, wpb=13981.2, bsz=478.7, num_updates=3500, lr=3.70328e-05, gnorm=1.227, train_wall=62, wall=2366
2021-01-01 22:31:21 | INFO | train_inner | epoch 009:    232 / 421 symm_mse=8.445, loss=3.87, nll_loss=1.176, ppl=2.26, wps=22257.8, ups=1.59, wpb=14040.5, bsz=491.4, num_updates=3600, lr=3.65148e-05, gnorm=1.162, train_wall=63, wall=2429
2021-01-01 22:32:24 | INFO | train_inner | epoch 009:    332 / 421 symm_mse=8.265, loss=3.851, nll_loss=1.177, ppl=2.26, wps=22202.4, ups=1.58, wpb=14010.8, bsz=506.9, num_updates=3700, lr=3.6018e-05, gnorm=1.163, train_wall=63, wall=2492
2021-01-01 22:33:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:33:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:33:36 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_mse 0 | loss 5.352 | nll_loss 3.863 | ppl 14.55 | bleu 22.46 | wps 5909.2 | wpb 10324.2 | bsz 375 | num_updates 3789 | best_bleu 22.69
2021-01-01 22:33:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 9 @ 3789 updates, score 22.46) (writing took 2.842873826622963 seconds)
2021-01-01 22:33:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-01 22:33:39 | INFO | train | epoch 009 | symm_mse 8.533 | loss 3.887 | nll_loss 1.183 | ppl 2.27 | wps 20471.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 3789 | lr 3.55925e-05 | gnorm 1.185 | train_wall 263 | wall 2568
2021-01-01 22:33:39 | INFO | fairseq.trainer | begin training epoch 10
2021-01-01 22:33:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:33:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:33:49 | INFO | train_inner | epoch 010:     11 / 421 symm_mse=8.428, loss=3.882, nll_loss=1.191, ppl=2.28, wps=16072.2, ups=1.17, wpb=13765.2, bsz=485.4, num_updates=3800, lr=3.55409e-05, gnorm=1.191, train_wall=62, wall=2578
2021-01-01 22:34:51 | INFO | train_inner | epoch 010:    111 / 421 symm_mse=8.367, loss=3.865, nll_loss=1.178, ppl=2.26, wps=22720, ups=1.61, wpb=14110.2, bsz=485.5, num_updates=3900, lr=3.50823e-05, gnorm=1.141, train_wall=62, wall=2640
2021-01-01 22:35:54 | INFO | train_inner | epoch 010:    211 / 421 symm_mse=8.587, loss=3.904, nll_loss=1.195, ppl=2.29, wps=22129.4, ups=1.59, wpb=13947.9, bsz=478.8, num_updates=4000, lr=3.4641e-05, gnorm=1.185, train_wall=63, wall=2703
2021-01-01 22:36:57 | INFO | train_inner | epoch 010:    311 / 421 symm_mse=7.988, loss=3.807, nll_loss=1.163, ppl=2.24, wps=22278.7, ups=1.59, wpb=14032.4, bsz=515.1, num_updates=4100, lr=3.4216e-05, gnorm=1.112, train_wall=63, wall=2766
2021-01-01 22:38:00 | INFO | train_inner | epoch 010:    411 / 421 symm_mse=8.17, loss=3.845, nll_loss=1.182, ppl=2.27, wps=22147.9, ups=1.59, wpb=13937.4, bsz=501.3, num_updates=4200, lr=3.38062e-05, gnorm=1.138, train_wall=63, wall=2829
2021-01-01 22:38:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:38:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:38:25 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_mse 0 | loss 5.334 | nll_loss 3.849 | ppl 14.41 | bleu 22.38 | wps 5197.3 | wpb 10324.2 | bsz 375 | num_updates 4210 | best_bleu 22.69
2021-01-01 22:38:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:38:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:38:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 10 @ 4210 updates, score 22.38) (writing took 2.881867477670312 seconds)
2021-01-01 22:38:28 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-01 22:38:28 | INFO | train | epoch 010 | symm_mse 8.296 | loss 3.858 | nll_loss 1.18 | ppl 2.27 | wps 20367.5 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 4210 | lr 3.3766e-05 | gnorm 1.149 | train_wall 263 | wall 2856
2021-01-01 22:38:28 | INFO | fairseq.trainer | begin training epoch 11
2021-01-01 22:38:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:38:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:39:26 | INFO | train_inner | epoch 011:     90 / 421 symm_mse=8.438, loss=3.88, nll_loss=1.186, ppl=2.28, wps=16046, ups=1.16, wpb=13795, bsz=468.2, num_updates=4300, lr=3.34108e-05, gnorm=1.173, train_wall=61, wall=2915
2021-01-01 22:40:29 | INFO | train_inner | epoch 011:    190 / 421 symm_mse=7.939, loss=3.803, nll_loss=1.164, ppl=2.24, wps=22411.6, ups=1.59, wpb=14072.6, bsz=503.6, num_updates=4400, lr=3.30289e-05, gnorm=1.113, train_wall=63, wall=2978
2021-01-01 22:41:32 | INFO | train_inner | epoch 011:    290 / 421 symm_mse=8.262, loss=3.88, nll_loss=1.21, ppl=2.31, wps=22066.8, ups=1.6, wpb=13821.1, bsz=488.1, num_updates=4500, lr=3.26599e-05, gnorm=1.148, train_wall=62, wall=3040
2021-01-01 22:42:35 | INFO | train_inner | epoch 011:    390 / 421 symm_mse=7.95, loss=3.808, nll_loss=1.166, ppl=2.24, wps=22459.3, ups=1.58, wpb=14171.3, bsz=501.5, num_updates=4600, lr=3.23029e-05, gnorm=1.104, train_wall=63, wall=3103
2021-01-01 22:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:43:11 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_mse 0 | loss 5.336 | nll_loss 3.842 | ppl 14.34 | bleu 22.25 | wps 5856.6 | wpb 10324.2 | bsz 375 | num_updates 4631 | best_bleu 22.69
2021-01-01 22:43:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:43:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:43:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:43:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:43:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 11 @ 4631 updates, score 22.25) (writing took 2.9139465615153313 seconds)
2021-01-01 22:43:14 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-01 22:43:14 | INFO | train | epoch 011 | symm_mse 8.091 | loss 3.834 | nll_loss 1.179 | ppl 2.26 | wps 20545.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 4631 | lr 3.21946e-05 | gnorm 1.128 | train_wall 262 | wall 3143
2021-01-01 22:43:14 | INFO | fairseq.trainer | begin training epoch 12
2021-01-01 22:43:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:43:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:44:00 | INFO | train_inner | epoch 012:     69 / 421 symm_mse=7.883, loss=3.799, nll_loss=1.165, ppl=2.24, wps=16309.8, ups=1.18, wpb=13876.4, bsz=488.4, num_updates=4700, lr=3.19574e-05, gnorm=1.116, train_wall=62, wall=3188
2021-01-01 22:45:02 | INFO | train_inner | epoch 012:    169 / 421 symm_mse=8.064, loss=3.831, nll_loss=1.178, ppl=2.26, wps=22364.6, ups=1.61, wpb=13884.5, bsz=497.5, num_updates=4800, lr=3.16228e-05, gnorm=1.116, train_wall=62, wall=3251
2021-01-01 22:46:05 | INFO | train_inner | epoch 012:    269 / 421 symm_mse=7.794, loss=3.784, nll_loss=1.159, ppl=2.23, wps=22720.2, ups=1.6, wpb=14222.4, bsz=501.7, num_updates=4900, lr=3.12984e-05, gnorm=1.091, train_wall=62, wall=3313
2021-01-01 22:47:07 | INFO | train_inner | epoch 012:    369 / 421 symm_mse=7.923, loss=3.826, nll_loss=1.192, ppl=2.29, wps=22291.6, ups=1.6, wpb=13899.9, bsz=487.4, num_updates=5000, lr=3.09839e-05, gnorm=1.108, train_wall=62, wall=3375
2021-01-01 22:47:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:47:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:47:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:47:57 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_mse 0 | loss 5.325 | nll_loss 3.827 | ppl 14.19 | bleu 22.32 | wps 5902.5 | wpb 10324.2 | bsz 375 | num_updates 5052 | best_bleu 22.69
2021-01-01 22:47:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:47:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:47:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:48:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 12 @ 5052 updates, score 22.32) (writing took 2.8689876794815063 seconds)
2021-01-01 22:48:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-01 22:48:00 | INFO | train | epoch 012 | symm_mse 7.934 | loss 3.813 | nll_loss 1.175 | ppl 2.26 | wps 20619.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 5052 | lr 3.0824e-05 | gnorm 1.11 | train_wall 261 | wall 3428
2021-01-01 22:48:00 | INFO | fairseq.trainer | begin training epoch 13
2021-01-01 22:48:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:48:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:48:32 | INFO | train_inner | epoch 013:     48 / 421 symm_mse=8.133, loss=3.841, nll_loss=1.183, ppl=2.27, wps=16321.7, ups=1.18, wpb=13889.5, bsz=482.8, num_updates=5100, lr=3.06786e-05, gnorm=1.144, train_wall=62, wall=3461
2021-01-01 22:49:35 | INFO | train_inner | epoch 013:    148 / 421 symm_mse=7.957, loss=3.82, nll_loss=1.178, ppl=2.26, wps=22320.7, ups=1.58, wpb=14105.2, bsz=473, num_updates=5200, lr=3.03822e-05, gnorm=1.114, train_wall=63, wall=3524
2021-01-01 22:50:39 | INFO | train_inner | epoch 013:    248 / 421 symm_mse=7.7, loss=3.781, nll_loss=1.168, ppl=2.25, wps=22035.6, ups=1.58, wpb=13944.9, bsz=504.8, num_updates=5300, lr=3.00942e-05, gnorm=1.074, train_wall=63, wall=3587
2021-01-01 22:51:41 | INFO | train_inner | epoch 013:    348 / 421 symm_mse=7.542, loss=3.757, nll_loss=1.161, ppl=2.24, wps=22385.9, ups=1.59, wpb=14073.3, bsz=500.6, num_updates=5400, lr=2.98142e-05, gnorm=1.05, train_wall=63, wall=3650
2021-01-01 22:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:52:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:52:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:52:45 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_mse 0 | loss 5.315 | nll_loss 3.822 | ppl 14.14 | bleu 22.17 | wps 5448.9 | wpb 10324.2 | bsz 375 | num_updates 5473 | best_bleu 22.69
2021-01-01 22:52:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:52:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 13 @ 5473 updates, score 22.17) (writing took 2.9144023582339287 seconds)
2021-01-01 22:52:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-01 22:52:48 | INFO | train | epoch 013 | symm_mse 7.786 | loss 3.796 | nll_loss 1.173 | ppl 2.26 | wps 20375.8 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 5473 | lr 2.96147e-05 | gnorm 1.093 | train_wall 263 | wall 3717
2021-01-01 22:52:48 | INFO | fairseq.trainer | begin training epoch 14
2021-01-01 22:52:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:52:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:53:08 | INFO | train_inner | epoch 014:     27 / 421 symm_mse=7.763, loss=3.798, nll_loss=1.18, ppl=2.27, wps=15968.6, ups=1.16, wpb=13804.5, bsz=491.3, num_updates=5500, lr=2.9542e-05, gnorm=1.106, train_wall=62, wall=3736
2021-01-01 22:54:11 | INFO | train_inner | epoch 014:    127 / 421 symm_mse=7.812, loss=3.798, nll_loss=1.173, ppl=2.25, wps=22498.8, ups=1.59, wpb=14140.2, bsz=491.6, num_updates=5600, lr=2.9277e-05, gnorm=1.093, train_wall=63, wall=3799
2021-01-01 22:55:13 | INFO | train_inner | epoch 014:    227 / 421 symm_mse=7.688, loss=3.782, nll_loss=1.17, ppl=2.25, wps=22220.9, ups=1.6, wpb=13893.4, bsz=488.5, num_updates=5700, lr=2.90191e-05, gnorm=1.092, train_wall=62, wall=3862
2021-01-01 22:56:16 | INFO | train_inner | epoch 014:    327 / 421 symm_mse=7.756, loss=3.802, nll_loss=1.185, ppl=2.27, wps=22183.9, ups=1.6, wpb=13861.2, bsz=485.7, num_updates=5800, lr=2.87678e-05, gnorm=1.087, train_wall=62, wall=3924
2021-01-01 22:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 22:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 22:57:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 22:57:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 22:57:32 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_mse 0 | loss 5.309 | nll_loss 3.808 | ppl 14.01 | bleu 22.48 | wps 5969.9 | wpb 10324.2 | bsz 375 | num_updates 5894 | best_bleu 22.69
2021-01-01 22:57:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 22:57:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 14 @ 5894 updates, score 22.48) (writing took 2.848714392632246 seconds)
2021-01-01 22:57:34 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-01 22:57:34 | INFO | train | epoch 014 | symm_mse 7.661 | loss 3.78 | nll_loss 1.171 | ppl 2.25 | wps 20547.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 5894 | lr 2.85375e-05 | gnorm 1.081 | train_wall 262 | wall 4003
2021-01-01 22:57:34 | INFO | fairseq.trainer | begin training epoch 15
2021-01-01 22:57:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 22:57:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 22:57:41 | INFO | train_inner | epoch 015:      6 / 421 symm_mse=7.35, loss=3.735, nll_loss=1.16, ppl=2.23, wps=16310.6, ups=1.17, wpb=13964.8, bsz=507.2, num_updates=5900, lr=2.8523e-05, gnorm=1.051, train_wall=63, wall=4010
2021-01-01 22:58:43 | INFO | train_inner | epoch 015:    106 / 421 symm_mse=7.625, loss=3.778, nll_loss=1.173, ppl=2.25, wps=22530.1, ups=1.62, wpb=13909.8, bsz=489.7, num_updates=6000, lr=2.82843e-05, gnorm=1.057, train_wall=62, wall=4072
2021-01-01 22:59:46 | INFO | train_inner | epoch 015:    206 / 421 symm_mse=7.451, loss=3.737, nll_loss=1.149, ppl=2.22, wps=22172.5, ups=1.58, wpb=14025.9, bsz=508.1, num_updates=6100, lr=2.80515e-05, gnorm=1.079, train_wall=63, wall=4135
2021-01-01 23:00:49 | INFO | train_inner | epoch 015:    306 / 421 symm_mse=7.519, loss=3.762, nll_loss=1.168, ppl=2.25, wps=22242.2, ups=1.59, wpb=13965.9, bsz=486.5, num_updates=6200, lr=2.78243e-05, gnorm=1.067, train_wall=63, wall=4198
2021-01-01 23:01:51 | INFO | train_inner | epoch 015:    406 / 421 symm_mse=7.661, loss=3.791, nll_loss=1.185, ppl=2.27, wps=22556.9, ups=1.61, wpb=14000.5, bsz=488.7, num_updates=6300, lr=2.76026e-05, gnorm=1.088, train_wall=62, wall=4260
2021-01-01 23:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:02:18 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_mse 0 | loss 5.309 | nll_loss 3.811 | ppl 14.03 | bleu 22.29 | wps 5522.7 | wpb 10324.2 | bsz 375 | num_updates 6315 | best_bleu 22.69
2021-01-01 23:02:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:02:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:02:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 15 @ 6315 updates, score 22.29) (writing took 2.8854882549494505 seconds)
2021-01-01 23:02:21 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-01 23:02:21 | INFO | train | epoch 015 | symm_mse 7.56 | loss 3.765 | nll_loss 1.167 | ppl 2.25 | wps 20501 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 6315 | lr 2.75698e-05 | gnorm 1.072 | train_wall 262 | wall 4290
2021-01-01 23:02:21 | INFO | fairseq.trainer | begin training epoch 16
2021-01-01 23:02:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:02:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:03:17 | INFO | train_inner | epoch 016:     85 / 421 symm_mse=7.409, loss=3.739, nll_loss=1.157, ppl=2.23, wps=16230.4, ups=1.17, wpb=13920.5, bsz=499.7, num_updates=6400, lr=2.73861e-05, gnorm=1.036, train_wall=62, wall=4345
2021-01-01 23:04:20 | INFO | train_inner | epoch 016:    185 / 421 symm_mse=7.566, loss=3.771, nll_loss=1.173, ppl=2.26, wps=22285.4, ups=1.6, wpb=13953.5, bsz=496.1, num_updates=6500, lr=2.71746e-05, gnorm=1.09, train_wall=62, wall=4408
2021-01-01 23:05:22 | INFO | train_inner | epoch 016:    285 / 421 symm_mse=7.265, loss=3.717, nll_loss=1.15, ppl=2.22, wps=22393.6, ups=1.59, wpb=14059.6, bsz=504.2, num_updates=6600, lr=2.6968e-05, gnorm=1.037, train_wall=63, wall=4471
2021-01-01 23:06:25 | INFO | train_inner | epoch 016:    385 / 421 symm_mse=7.574, loss=3.773, nll_loss=1.173, ppl=2.26, wps=22330.8, ups=1.6, wpb=13933.8, bsz=478.3, num_updates=6700, lr=2.6766e-05, gnorm=1.058, train_wall=62, wall=4533
2021-01-01 23:06:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:07:04 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_mse 0 | loss 5.3 | nll_loss 3.8 | ppl 13.93 | bleu 22.17 | wps 5795.5 | wpb 10324.2 | bsz 375 | num_updates 6736 | best_bleu 22.69
2021-01-01 23:07:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:07:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 16 @ 6736 updates, score 22.17) (writing took 2.9534740056842566 seconds)
2021-01-01 23:07:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-01 23:07:07 | INFO | train | epoch 016 | symm_mse 7.459 | loss 3.754 | nll_loss 1.167 | ppl 2.25 | wps 20558.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 6736 | lr 2.66944e-05 | gnorm 1.058 | train_wall 262 | wall 4576
2021-01-01 23:07:07 | INFO | fairseq.trainer | begin training epoch 17
2021-01-01 23:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:07:50 | INFO | train_inner | epoch 017:     64 / 421 symm_mse=7.439, loss=3.76, nll_loss=1.177, ppl=2.26, wps=16288, ups=1.17, wpb=13884.8, bsz=477.7, num_updates=6800, lr=2.65684e-05, gnorm=1.064, train_wall=62, wall=4618
2021-01-01 23:08:53 | INFO | train_inner | epoch 017:    164 / 421 symm_mse=7.302, loss=3.71, nll_loss=1.137, ppl=2.2, wps=22470.8, ups=1.59, wpb=14155.6, bsz=509.1, num_updates=6900, lr=2.63752e-05, gnorm=1.042, train_wall=63, wall=4681
2021-01-01 23:09:56 | INFO | train_inner | epoch 017:    264 / 421 symm_mse=7.364, loss=3.735, nll_loss=1.157, ppl=2.23, wps=22264, ups=1.59, wpb=14019.6, bsz=491.8, num_updates=7000, lr=2.61861e-05, gnorm=1.033, train_wall=63, wall=4744
2021-01-01 23:10:58 | INFO | train_inner | epoch 017:    364 / 421 symm_mse=7.326, loss=3.746, nll_loss=1.175, ppl=2.26, wps=22298.9, ups=1.6, wpb=13911.3, bsz=494.4, num_updates=7100, lr=2.60011e-05, gnorm=1.029, train_wall=62, wall=4807
2021-01-01 23:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:11:51 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_mse 0 | loss 5.299 | nll_loss 3.802 | ppl 13.95 | bleu 22.25 | wps 5935.9 | wpb 10324.2 | bsz 375 | num_updates 7157 | best_bleu 22.69
2021-01-01 23:11:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:11:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 17 @ 7157 updates, score 22.25) (writing took 2.886580303311348 seconds)
2021-01-01 23:11:54 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-01 23:11:54 | INFO | train | epoch 017 | symm_mse 7.372 | loss 3.742 | nll_loss 1.165 | ppl 2.24 | wps 20529.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 7157 | lr 2.58973e-05 | gnorm 1.041 | train_wall 263 | wall 4862
2021-01-01 23:11:54 | INFO | fairseq.trainer | begin training epoch 18
2021-01-01 23:11:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:11:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:12:23 | INFO | train_inner | epoch 018:     43 / 421 symm_mse=7.499, loss=3.77, nll_loss=1.18, ppl=2.27, wps=16315, ups=1.18, wpb=13876, bsz=478.1, num_updates=7200, lr=2.58199e-05, gnorm=1.078, train_wall=62, wall=4892
2021-01-01 23:13:27 | INFO | train_inner | epoch 018:    143 / 421 symm_mse=7.358, loss=3.738, nll_loss=1.162, ppl=2.24, wps=22231.1, ups=1.59, wpb=14024.4, bsz=489.7, num_updates=7300, lr=2.56424e-05, gnorm=1.071, train_wall=63, wall=4955
2021-01-01 23:14:30 | INFO | train_inner | epoch 018:    243 / 421 symm_mse=7.319, loss=3.736, nll_loss=1.164, ppl=2.24, wps=21903.4, ups=1.58, wpb=13842.7, bsz=494.3, num_updates=7400, lr=2.54686e-05, gnorm=1.045, train_wall=63, wall=5018
2021-01-01 23:15:32 | INFO | train_inner | epoch 018:    343 / 421 symm_mse=7.085, loss=3.701, nll_loss=1.154, ppl=2.23, wps=22463, ups=1.6, wpb=14074.6, bsz=501.3, num_updates=7500, lr=2.52982e-05, gnorm=1.011, train_wall=62, wall=5081
2021-01-01 23:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:16:38 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_mse 0 | loss 5.295 | nll_loss 3.794 | ppl 13.88 | bleu 22.36 | wps 5963.8 | wpb 10324.2 | bsz 375 | num_updates 7578 | best_bleu 22.69
2021-01-01 23:16:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 18 @ 7578 updates, score 22.36) (writing took 2.864797007292509 seconds)
2021-01-01 23:16:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-01 23:16:41 | INFO | train | epoch 018 | symm_mse 7.29 | loss 3.732 | nll_loss 1.163 | ppl 2.24 | wps 20503.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 7578 | lr 2.51677e-05 | gnorm 1.049 | train_wall 263 | wall 5149
2021-01-01 23:16:41 | INFO | fairseq.trainer | begin training epoch 19
2021-01-01 23:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:16:57 | INFO | train_inner | epoch 019:     22 / 421 symm_mse=7.259, loss=3.736, nll_loss=1.173, ppl=2.25, wps=16267.2, ups=1.18, wpb=13829.2, bsz=486.7, num_updates=7600, lr=2.51312e-05, gnorm=1.042, train_wall=62, wall=5166
2021-01-01 23:18:00 | INFO | train_inner | epoch 019:    122 / 421 symm_mse=7.1, loss=3.7, nll_loss=1.15, ppl=2.22, wps=22471.9, ups=1.6, wpb=14075, bsz=504.1, num_updates=7700, lr=2.49675e-05, gnorm=1.026, train_wall=62, wall=5228
2021-01-01 23:19:03 | INFO | train_inner | epoch 019:    222 / 421 symm_mse=7.299, loss=3.728, nll_loss=1.157, ppl=2.23, wps=22283.1, ups=1.59, wpb=14017.6, bsz=488.3, num_updates=7800, lr=2.48069e-05, gnorm=1.038, train_wall=63, wall=5291
2021-01-01 23:20:05 | INFO | train_inner | epoch 019:    322 / 421 symm_mse=7.364, loss=3.75, nll_loss=1.174, ppl=2.26, wps=22271.2, ups=1.6, wpb=13925.5, bsz=485.6, num_updates=7900, lr=2.46494e-05, gnorm=1.031, train_wall=62, wall=5354
2021-01-01 23:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:21:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:21:24 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_mse 0 | loss 5.292 | nll_loss 3.791 | ppl 13.84 | bleu 22.41 | wps 5916.6 | wpb 10324.2 | bsz 375 | num_updates 7999 | best_bleu 22.69
2021-01-01 23:21:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 19 @ 7999 updates, score 22.41) (writing took 2.8794733081012964 seconds)
2021-01-01 23:21:27 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-01 23:21:27 | INFO | train | epoch 019 | symm_mse 7.215 | loss 3.722 | nll_loss 1.161 | ppl 2.24 | wps 20520.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 7999 | lr 2.44964e-05 | gnorm 1.032 | train_wall 263 | wall 5436
2021-01-01 23:21:27 | INFO | fairseq.trainer | begin training epoch 20
2021-01-01 23:21:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:21:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:21:31 | INFO | train_inner | epoch 020:      1 / 421 symm_mse=7.127, loss=3.712, nll_loss=1.163, ppl=2.24, wps=16263.4, ups=1.17, wpb=13936.5, bsz=492.6, num_updates=8000, lr=2.44949e-05, gnorm=1.035, train_wall=63, wall=5440
2021-01-01 23:22:33 | INFO | train_inner | epoch 020:    101 / 421 symm_mse=7.182, loss=3.718, nll_loss=1.16, ppl=2.23, wps=22578.9, ups=1.61, wpb=14022.1, bsz=493.9, num_updates=8100, lr=2.43432e-05, gnorm=1.01, train_wall=62, wall=5502
2021-01-01 23:23:37 | INFO | train_inner | epoch 020:    201 / 421 symm_mse=7.181, loss=3.712, nll_loss=1.154, ppl=2.23, wps=22090.5, ups=1.58, wpb=13989.7, bsz=488, num_updates=8200, lr=2.41943e-05, gnorm=1.047, train_wall=63, wall=5565
2021-01-01 23:24:39 | INFO | train_inner | epoch 020:    301 / 421 symm_mse=7.165, loss=3.719, nll_loss=1.164, ppl=2.24, wps=22601, ups=1.6, wpb=14118.4, bsz=497.3, num_updates=8300, lr=2.40481e-05, gnorm=1.015, train_wall=62, wall=5628
2021-01-01 23:25:42 | INFO | train_inner | epoch 020:    401 / 421 symm_mse=7.15, loss=3.717, nll_loss=1.165, ppl=2.24, wps=22083.8, ups=1.6, wpb=13791.4, bsz=493.2, num_updates=8400, lr=2.39046e-05, gnorm=1.032, train_wall=62, wall=5690
2021-01-01 23:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:26:11 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_mse 0 | loss 5.294 | nll_loss 3.792 | ppl 13.86 | bleu 22.27 | wps 5957.5 | wpb 10324.2 | bsz 375 | num_updates 8420 | best_bleu 22.69
2021-01-01 23:26:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:26:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:26:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:26:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:26:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 20 @ 8420 updates, score 22.27) (writing took 2.8987419083714485 seconds)
2021-01-01 23:26:14 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-01 23:26:14 | INFO | train | epoch 020 | symm_mse 7.158 | loss 3.715 | nll_loss 1.16 | ppl 2.23 | wps 20525.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 8420 | lr 2.38762e-05 | gnorm 1.027 | train_wall 263 | wall 5722
2021-01-01 23:26:14 | INFO | fairseq.trainer | begin training epoch 21
2021-01-01 23:26:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:26:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:26:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:26:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:26:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:27:06 | INFO | train_inner | epoch 021:     80 / 421 symm_mse=7.118, loss=3.717, nll_loss=1.168, ppl=2.25, wps=16151.4, ups=1.18, wpb=13696.4, bsz=490.3, num_updates=8500, lr=2.37635e-05, gnorm=1.044, train_wall=62, wall=5775
2021-01-01 23:28:10 | INFO | train_inner | epoch 021:    180 / 421 symm_mse=7.222, loss=3.723, nll_loss=1.163, ppl=2.24, wps=22273.9, ups=1.58, wpb=14078.8, bsz=487.9, num_updates=8600, lr=2.3625e-05, gnorm=1.044, train_wall=63, wall=5838
2021-01-01 23:29:12 | INFO | train_inner | epoch 021:    280 / 421 symm_mse=6.833, loss=3.655, nll_loss=1.134, ppl=2.19, wps=22658.9, ups=1.6, wpb=14138.2, bsz=508.5, num_updates=8700, lr=2.34888e-05, gnorm=0.973, train_wall=62, wall=5900
2021-01-01 23:30:14 | INFO | train_inner | epoch 021:    380 / 421 symm_mse=7.09, loss=3.709, nll_loss=1.162, ppl=2.24, wps=22407.6, ups=1.6, wpb=14011.6, bsz=493.4, num_updates=8800, lr=2.3355e-05, gnorm=1.013, train_wall=62, wall=5963
2021-01-01 23:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:30:57 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_mse 0 | loss 5.286 | nll_loss 3.779 | ppl 13.73 | bleu 22.33 | wps 5905.3 | wpb 10324.2 | bsz 375 | num_updates 8841 | best_bleu 22.69
2021-01-01 23:30:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:31:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 21 @ 8841 updates, score 22.33) (writing took 2.9230681397020817 seconds)
2021-01-01 23:31:00 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-01 23:31:00 | INFO | train | epoch 021 | symm_mse 7.086 | loss 3.706 | nll_loss 1.159 | ppl 2.23 | wps 20579.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 8841 | lr 2.33008e-05 | gnorm 1.018 | train_wall 262 | wall 6008
2021-01-01 23:31:00 | INFO | fairseq.trainer | begin training epoch 22
2021-01-01 23:31:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:31:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:31:39 | INFO | train_inner | epoch 022:     59 / 421 symm_mse=7.126, loss=3.717, nll_loss=1.166, ppl=2.24, wps=16435.7, ups=1.18, wpb=13937.8, bsz=479, num_updates=8900, lr=2.32234e-05, gnorm=0.999, train_wall=61, wall=6048
2021-01-01 23:32:42 | INFO | train_inner | epoch 022:    159 / 421 symm_mse=6.844, loss=3.66, nll_loss=1.136, ppl=2.2, wps=22335.5, ups=1.59, wpb=14049.6, bsz=507.4, num_updates=9000, lr=2.3094e-05, gnorm=0.996, train_wall=63, wall=6111
2021-01-01 23:33:45 | INFO | train_inner | epoch 022:    259 / 421 symm_mse=7.12, loss=3.717, nll_loss=1.168, ppl=2.25, wps=21971.3, ups=1.58, wpb=13902.5, bsz=489.7, num_updates=9100, lr=2.29668e-05, gnorm=1.015, train_wall=63, wall=6174
2021-01-01 23:34:47 | INFO | train_inner | epoch 022:    359 / 421 symm_mse=7.049, loss=3.698, nll_loss=1.155, ppl=2.23, wps=22646.4, ups=1.61, wpb=14032.6, bsz=487.8, num_updates=9200, lr=2.28416e-05, gnorm=1.002, train_wall=62, wall=6236
2021-01-01 23:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:35:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:35:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:35:43 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_mse 0 | loss 5.284 | nll_loss 3.776 | ppl 13.7 | bleu 22.39 | wps 5898.5 | wpb 10324.2 | bsz 375 | num_updates 9262 | best_bleu 22.69
2021-01-01 23:35:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:35:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 22 @ 9262 updates, score 22.39) (writing took 2.3445695620030165 seconds)
2021-01-01 23:35:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-01 23:35:45 | INFO | train | epoch 022 | symm_mse 7.03 | loss 3.697 | nll_loss 1.157 | ppl 2.23 | wps 20592.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 9262 | lr 2.2765e-05 | gnorm 1.003 | train_wall 262 | wall 6294
2021-01-01 23:35:45 | INFO | fairseq.trainer | begin training epoch 23
2021-01-01 23:35:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:35:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:35:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:36:12 | INFO | train_inner | epoch 023:     38 / 421 symm_mse=7.366, loss=3.759, nll_loss=1.184, ppl=2.27, wps=16414.7, ups=1.19, wpb=13834.9, bsz=470.7, num_updates=9300, lr=2.27185e-05, gnorm=1.041, train_wall=62, wall=6320
2021-01-01 23:37:14 | INFO | train_inner | epoch 023:    138 / 421 symm_mse=6.876, loss=3.676, nll_loss=1.151, ppl=2.22, wps=22316.1, ups=1.6, wpb=13964.3, bsz=496.2, num_updates=9400, lr=2.25973e-05, gnorm=0.989, train_wall=62, wall=6383
2021-01-01 23:38:17 | INFO | train_inner | epoch 023:    238 / 421 symm_mse=7.126, loss=3.721, nll_loss=1.173, ppl=2.26, wps=22303.6, ups=1.59, wpb=13991.9, bsz=478, num_updates=9500, lr=2.24781e-05, gnorm=1.015, train_wall=63, wall=6445
2021-01-01 23:39:19 | INFO | train_inner | epoch 023:    338 / 421 symm_mse=6.803, loss=3.661, nll_loss=1.144, ppl=2.21, wps=22369.6, ups=1.6, wpb=13972.2, bsz=506.8, num_updates=9600, lr=2.23607e-05, gnorm=0.98, train_wall=62, wall=6508
2021-01-01 23:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:40:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:40:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:40:28 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_mse 0 | loss 5.28 | nll_loss 3.777 | ppl 13.71 | bleu 22.25 | wps 5884.6 | wpb 10324.2 | bsz 375 | num_updates 9683 | best_bleu 22.69
2021-01-01 23:40:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 23 @ 9683 updates, score 22.25) (writing took 2.920317891985178 seconds)
2021-01-01 23:40:31 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-01 23:40:31 | INFO | train | epoch 023 | symm_mse 6.975 | loss 3.69 | nll_loss 1.155 | ppl 2.23 | wps 20574.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 9683 | lr 2.22646e-05 | gnorm 1.003 | train_wall 262 | wall 6579
2021-01-01 23:40:31 | INFO | fairseq.trainer | begin training epoch 24
2021-01-01 23:40:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:40:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:40:45 | INFO | train_inner | epoch 024:     17 / 421 symm_mse=6.749, loss=3.647, nll_loss=1.135, ppl=2.2, wps=16382.9, ups=1.17, wpb=13990.7, bsz=508.2, num_updates=9700, lr=2.22451e-05, gnorm=0.981, train_wall=62, wall=6593
2021-01-01 23:41:47 | INFO | train_inner | epoch 024:    117 / 421 symm_mse=7.086, loss=3.705, nll_loss=1.157, ppl=2.23, wps=22488.2, ups=1.6, wpb=14043.6, bsz=489.9, num_updates=9800, lr=2.21313e-05, gnorm=1.002, train_wall=62, wall=6656
2021-01-01 23:42:50 | INFO | train_inner | epoch 024:    217 / 421 symm_mse=6.928, loss=3.679, nll_loss=1.148, ppl=2.22, wps=22267, ups=1.59, wpb=13992.3, bsz=488.8, num_updates=9900, lr=2.20193e-05, gnorm=0.993, train_wall=63, wall=6719
2021-01-01 23:43:53 | INFO | train_inner | epoch 024:    317 / 421 symm_mse=6.945, loss=3.697, nll_loss=1.167, ppl=2.25, wps=21934.1, ups=1.6, wpb=13726.7, bsz=487.9, num_updates=10000, lr=2.19089e-05, gnorm=1.003, train_wall=62, wall=6781
2021-01-01 23:44:56 | INFO | train_inner | epoch 024:    417 / 421 symm_mse=6.822, loss=3.666, nll_loss=1.149, ppl=2.22, wps=22543.8, ups=1.59, wpb=14165.6, bsz=504.5, num_updates=10100, lr=2.18002e-05, gnorm=0.992, train_wall=63, wall=6844
2021-01-01 23:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:44:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:44:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:44:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:44:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:45:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:45:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:45:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:45:15 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_mse 0 | loss 5.282 | nll_loss 3.775 | ppl 13.69 | bleu 22.32 | wps 5898.4 | wpb 10324.2 | bsz 375 | num_updates 10104 | best_bleu 22.69
2021-01-01 23:45:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:45:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:45:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:45:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:45:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 24 @ 10104 updates, score 22.32) (writing took 3.0888526272028685 seconds)
2021-01-01 23:45:18 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-01 23:45:18 | INFO | train | epoch 024 | symm_mse 6.925 | loss 3.684 | nll_loss 1.154 | ppl 2.23 | wps 20500 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 10104 | lr 2.17959e-05 | gnorm 0.995 | train_wall 263 | wall 6866
2021-01-01 23:45:18 | INFO | fairseq.trainer | begin training epoch 25
2021-01-01 23:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:46:21 | INFO | train_inner | epoch 025:     96 / 421 symm_mse=7.038, loss=3.691, nll_loss=1.148, ppl=2.22, wps=16312.4, ups=1.17, wpb=13965.8, bsz=477.5, num_updates=10200, lr=2.1693e-05, gnorm=1.036, train_wall=62, wall=6930
2021-01-01 23:47:24 | INFO | train_inner | epoch 025:    196 / 421 symm_mse=6.76, loss=3.665, nll_loss=1.154, ppl=2.23, wps=21985, ups=1.58, wpb=13916.1, bsz=500.1, num_updates=10300, lr=2.15875e-05, gnorm=0.992, train_wall=63, wall=6993
2021-01-01 23:48:27 | INFO | train_inner | epoch 025:    296 / 421 symm_mse=6.948, loss=3.69, nll_loss=1.158, ppl=2.23, wps=22424.7, ups=1.59, wpb=14064.9, bsz=487, num_updates=10400, lr=2.14834e-05, gnorm=1.003, train_wall=63, wall=7056
2021-01-01 23:49:30 | INFO | train_inner | epoch 025:    396 / 421 symm_mse=6.713, loss=3.652, nll_loss=1.146, ppl=2.21, wps=22399.9, ups=1.6, wpb=14008.2, bsz=508.2, num_updates=10500, lr=2.13809e-05, gnorm=0.967, train_wall=62, wall=7118
2021-01-01 23:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:50:02 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_mse 0 | loss 5.276 | nll_loss 3.772 | ppl 13.67 | bleu 22.12 | wps 5783.5 | wpb 10324.2 | bsz 375 | num_updates 10525 | best_bleu 22.69
2021-01-01 23:50:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:50:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 25 @ 10525 updates, score 22.12) (writing took 3.0232956781983376 seconds)
2021-01-01 23:50:05 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-01 23:50:05 | INFO | train | epoch 025 | symm_mse 6.891 | loss 3.678 | nll_loss 1.153 | ppl 2.22 | wps 20463.6 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 10525 | lr 2.13555e-05 | gnorm 1.003 | train_wall 263 | wall 7154
2021-01-01 23:50:05 | INFO | fairseq.trainer | begin training epoch 26
2021-01-01 23:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:50:55 | INFO | train_inner | epoch 026:     75 / 421 symm_mse=6.74, loss=3.646, nll_loss=1.135, ppl=2.2, wps=16126.7, ups=1.17, wpb=13766.8, bsz=507.2, num_updates=10600, lr=2.12798e-05, gnorm=1.017, train_wall=62, wall=7204
2021-01-01 23:51:58 | INFO | train_inner | epoch 026:    175 / 421 symm_mse=6.819, loss=3.664, nll_loss=1.145, ppl=2.21, wps=22413.1, ups=1.59, wpb=14063.1, bsz=499.9, num_updates=10700, lr=2.11801e-05, gnorm=0.985, train_wall=63, wall=7266
2021-01-01 23:53:01 | INFO | train_inner | epoch 026:    275 / 421 symm_mse=6.882, loss=3.682, nll_loss=1.158, ppl=2.23, wps=22428.3, ups=1.59, wpb=14095.9, bsz=494.4, num_updates=10800, lr=2.10819e-05, gnorm=0.991, train_wall=63, wall=7329
2021-01-01 23:54:04 | INFO | train_inner | epoch 026:    375 / 421 symm_mse=6.993, loss=3.7, nll_loss=1.164, ppl=2.24, wps=22045.7, ups=1.58, wpb=13928.6, bsz=473.6, num_updates=10900, lr=2.09849e-05, gnorm=1.008, train_wall=63, wall=7392
2021-01-01 23:54:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:54:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:54:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:54:50 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_mse 0 | loss 5.271 | nll_loss 3.767 | ppl 13.61 | bleu 22.38 | wps 5855.4 | wpb 10324.2 | bsz 375 | num_updates 10946 | best_bleu 22.69
2021-01-01 23:54:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 26 @ 10946 updates, score 22.38) (writing took 2.617731235921383 seconds)
2021-01-01 23:54:52 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-01 23:54:52 | INFO | train | epoch 026 | symm_mse 6.84 | loss 3.672 | nll_loss 1.151 | ppl 2.22 | wps 20500.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 10946 | lr 2.09408e-05 | gnorm 0.997 | train_wall 263 | wall 7441
2021-01-01 23:54:52 | INFO | fairseq.trainer | begin training epoch 27
2021-01-01 23:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:54:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:55:29 | INFO | train_inner | epoch 027:     54 / 421 symm_mse=6.821, loss=3.678, nll_loss=1.16, ppl=2.23, wps=16341.6, ups=1.18, wpb=13886.4, bsz=483.5, num_updates=11000, lr=2.08893e-05, gnorm=0.985, train_wall=62, wall=7477
2021-01-01 23:56:32 | INFO | train_inner | epoch 027:    154 / 421 symm_mse=6.961, loss=3.692, nll_loss=1.159, ppl=2.23, wps=22328.6, ups=1.59, wpb=14008.5, bsz=483.7, num_updates=11100, lr=2.0795e-05, gnorm=1.001, train_wall=63, wall=7540
2021-01-01 23:57:34 | INFO | train_inner | epoch 027:    254 / 421 symm_mse=6.685, loss=3.646, nll_loss=1.143, ppl=2.21, wps=22490.2, ups=1.6, wpb=14065.1, bsz=498.8, num_updates=11200, lr=2.0702e-05, gnorm=0.972, train_wall=62, wall=7603
2021-01-01 23:58:37 | INFO | train_inner | epoch 027:    354 / 421 symm_mse=6.757, loss=3.658, nll_loss=1.146, ppl=2.21, wps=22230, ups=1.6, wpb=13936.2, bsz=501.4, num_updates=11300, lr=2.06102e-05, gnorm=0.998, train_wall=62, wall=7665
2021-01-01 23:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 23:59:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 23:59:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 23:59:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 23:59:35 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_mse 0 | loss 5.269 | nll_loss 3.764 | ppl 13.58 | bleu 22.46 | wps 5917.5 | wpb 10324.2 | bsz 375 | num_updates 11367 | best_bleu 22.69
2021-01-01 23:59:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 23:59:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 27 @ 11367 updates, score 22.46) (writing took 3.1334638837724924 seconds)
2021-01-01 23:59:38 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-01 23:59:38 | INFO | train | epoch 027 | symm_mse 6.797 | loss 3.667 | nll_loss 1.152 | ppl 2.22 | wps 20560.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 11367 | lr 2.05493e-05 | gnorm 0.988 | train_wall 262 | wall 7727
2021-01-01 23:59:38 | INFO | fairseq.trainer | begin training epoch 28
2021-01-01 23:59:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 23:59:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 23:59:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:00:02 | INFO | train_inner | epoch 028:     33 / 421 symm_mse=6.691, loss=3.644, nll_loss=1.14, ppl=2.2, wps=16192.1, ups=1.18, wpb=13736.5, bsz=489.8, num_updates=11400, lr=2.05196e-05, gnorm=0.988, train_wall=61, wall=7750
2021-01-02 00:01:04 | INFO | train_inner | epoch 028:    133 / 421 symm_mse=6.955, loss=3.69, nll_loss=1.158, ppl=2.23, wps=22459.7, ups=1.61, wpb=13970.3, bsz=473.9, num_updates=11500, lr=2.04302e-05, gnorm=0.989, train_wall=62, wall=7812
2021-01-02 00:02:07 | INFO | train_inner | epoch 028:    233 / 421 symm_mse=6.748, loss=3.656, nll_loss=1.144, ppl=2.21, wps=22402.2, ups=1.59, wpb=14051.6, bsz=492.8, num_updates=11600, lr=2.03419e-05, gnorm=0.962, train_wall=63, wall=7875
2021-01-02 00:03:09 | INFO | train_inner | epoch 028:    333 / 421 symm_mse=6.687, loss=3.662, nll_loss=1.161, ppl=2.24, wps=22421.5, ups=1.59, wpb=14062.4, bsz=500.3, num_updates=11700, lr=2.02548e-05, gnorm=0.966, train_wall=63, wall=7938
2021-01-02 00:04:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:04:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:04:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:04:21 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_mse 0 | loss 5.27 | nll_loss 3.761 | ppl 13.56 | bleu 22.49 | wps 5904 | wpb 10324.2 | bsz 375 | num_updates 11788 | best_bleu 22.69
2021-01-02 00:04:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:04:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 28 @ 11788 updates, score 22.49) (writing took 3.0695812217891216 seconds)
2021-01-02 00:04:24 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-02 00:04:24 | INFO | train | epoch 028 | symm_mse 6.744 | loss 3.659 | nll_loss 1.15 | ppl 2.22 | wps 20567.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 11788 | lr 2.0179e-05 | gnorm 0.972 | train_wall 262 | wall 8013
2021-01-02 00:04:24 | INFO | fairseq.trainer | begin training epoch 29
2021-01-02 00:04:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:04:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:04:35 | INFO | train_inner | epoch 029:     12 / 421 symm_mse=6.733, loss=3.661, nll_loss=1.152, ppl=2.22, wps=16227.1, ups=1.17, wpb=13890.5, bsz=495.7, num_updates=11800, lr=2.01688e-05, gnorm=0.98, train_wall=62, wall=8023
2021-01-02 00:05:37 | INFO | train_inner | epoch 029:    112 / 421 symm_mse=6.565, loss=3.632, nll_loss=1.141, ppl=2.21, wps=22634.3, ups=1.6, wpb=14129.1, bsz=498.7, num_updates=11900, lr=2.00839e-05, gnorm=0.945, train_wall=62, wall=8086
2021-01-02 00:06:40 | INFO | train_inner | epoch 029:    212 / 421 symm_mse=6.852, loss=3.675, nll_loss=1.154, ppl=2.23, wps=22341.2, ups=1.59, wpb=14014.8, bsz=486.4, num_updates=12000, lr=2e-05, gnorm=1.006, train_wall=63, wall=8149
2021-01-02 00:07:43 | INFO | train_inner | epoch 029:    312 / 421 symm_mse=6.549, loss=3.629, nll_loss=1.141, ppl=2.21, wps=21988.7, ups=1.58, wpb=13916.7, bsz=507.4, num_updates=12100, lr=1.99172e-05, gnorm=0.968, train_wall=63, wall=8212
2021-01-02 00:08:46 | INFO | train_inner | epoch 029:    412 / 421 symm_mse=6.835, loss=3.677, nll_loss=1.157, ppl=2.23, wps=21958.3, ups=1.58, wpb=13860.4, bsz=482.7, num_updates=12200, lr=1.98354e-05, gnorm=0.999, train_wall=63, wall=8275
2021-01-02 00:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:09:09 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_mse 0 | loss 5.267 | nll_loss 3.761 | ppl 13.55 | bleu 22.41 | wps 5893.6 | wpb 10324.2 | bsz 375 | num_updates 12209 | best_bleu 22.69
2021-01-02 00:09:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:09:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:09:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:09:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:09:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:09:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:09:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:09:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 29 @ 12209 updates, score 22.41) (writing took 3.1438136510550976 seconds)
2021-01-02 00:09:12 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-02 00:09:12 | INFO | train | epoch 029 | symm_mse 6.715 | loss 3.655 | nll_loss 1.149 | ppl 2.22 | wps 20424.8 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 12209 | lr 1.98281e-05 | gnorm 0.981 | train_wall 264 | wall 8301
2021-01-02 00:09:12 | INFO | fairseq.trainer | begin training epoch 30
2021-01-02 00:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:10:12 | INFO | train_inner | epoch 030:     91 / 421 symm_mse=6.615, loss=3.632, nll_loss=1.134, ppl=2.2, wps=16439.2, ups=1.17, wpb=14010.2, bsz=498.8, num_updates=12300, lr=1.97546e-05, gnorm=0.953, train_wall=62, wall=8360
2021-01-02 00:11:15 | INFO | train_inner | epoch 030:    191 / 421 symm_mse=6.721, loss=3.653, nll_loss=1.146, ppl=2.21, wps=22033.5, ups=1.58, wpb=13933.2, bsz=483.3, num_updates=12400, lr=1.96748e-05, gnorm=0.98, train_wall=63, wall=8423
2021-01-02 00:12:18 | INFO | train_inner | epoch 030:    291 / 421 symm_mse=6.789, loss=3.672, nll_loss=1.159, ppl=2.23, wps=22291.1, ups=1.6, wpb=13950.6, bsz=497.5, num_updates=12500, lr=1.95959e-05, gnorm=1.017, train_wall=62, wall=8486
2021-01-02 00:13:20 | INFO | train_inner | epoch 030:    391 / 421 symm_mse=6.502, loss=3.625, nll_loss=1.14, ppl=2.2, wps=22437.6, ups=1.6, wpb=14044.2, bsz=493.8, num_updates=12600, lr=1.9518e-05, gnorm=0.939, train_wall=62, wall=8549
2021-01-02 00:13:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:13:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:13:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:13:56 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_mse 0 | loss 5.266 | nll_loss 3.759 | ppl 13.54 | bleu 22.28 | wps 5895.6 | wpb 10324.2 | bsz 375 | num_updates 12630 | best_bleu 22.69
2021-01-02 00:13:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:13:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:13:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:13:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 30 @ 12630 updates, score 22.28) (writing took 3.0894641298800707 seconds)
2021-01-02 00:13:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-02 00:13:59 | INFO | train | epoch 030 | symm_mse 6.676 | loss 3.65 | nll_loss 1.147 | ppl 2.22 | wps 20511.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 12630 | lr 1.94948e-05 | gnorm 0.976 | train_wall 263 | wall 8587
2021-01-02 00:13:59 | INFO | fairseq.trainer | begin training epoch 31
2021-01-02 00:14:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:14:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:14:45 | INFO | train_inner | epoch 031:     70 / 421 symm_mse=6.894, loss=3.69, nll_loss=1.165, ppl=2.24, wps=16275.6, ups=1.17, wpb=13873.1, bsz=481.6, num_updates=12700, lr=1.9441e-05, gnorm=1.017, train_wall=62, wall=8634
2021-01-02 00:15:48 | INFO | train_inner | epoch 031:    170 / 421 symm_mse=6.464, loss=3.615, nll_loss=1.135, ppl=2.2, wps=22415.5, ups=1.6, wpb=14050.2, bsz=507.7, num_updates=12800, lr=1.93649e-05, gnorm=0.945, train_wall=62, wall=8696
2021-01-02 00:16:50 | INFO | train_inner | epoch 031:    270 / 421 symm_mse=6.877, loss=3.683, nll_loss=1.158, ppl=2.23, wps=22303.8, ups=1.6, wpb=13924.8, bsz=465.5, num_updates=12900, lr=1.92897e-05, gnorm=0.987, train_wall=62, wall=8759
2021-01-02 00:17:53 | INFO | train_inner | epoch 031:    370 / 421 symm_mse=6.56, loss=3.63, nll_loss=1.14, ppl=2.2, wps=22296.9, ups=1.59, wpb=14000.3, bsz=501.4, num_updates=13000, lr=1.92154e-05, gnorm=0.944, train_wall=63, wall=8822
2021-01-02 00:18:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:18:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:18:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:18:42 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_mse 0 | loss 5.264 | nll_loss 3.756 | ppl 13.51 | bleu 22.31 | wps 5917.9 | wpb 10324.2 | bsz 375 | num_updates 13051 | best_bleu 22.69
2021-01-02 00:18:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:18:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 31 @ 13051 updates, score 22.31) (writing took 2.898180255666375 seconds)
2021-01-02 00:18:45 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-02 00:18:45 | INFO | train | epoch 031 | symm_mse 6.637 | loss 3.645 | nll_loss 1.147 | ppl 2.21 | wps 20555.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 13051 | lr 1.91778e-05 | gnorm 0.967 | train_wall 262 | wall 8873
2021-01-02 00:18:45 | INFO | fairseq.trainer | begin training epoch 32
2021-01-02 00:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:19:18 | INFO | train_inner | epoch 032:     49 / 421 symm_mse=6.476, loss=3.627, nll_loss=1.147, ppl=2.21, wps=16175.9, ups=1.18, wpb=13736.1, bsz=497.8, num_updates=13100, lr=1.91419e-05, gnorm=0.978, train_wall=62, wall=8907
2021-01-02 00:20:20 | INFO | train_inner | epoch 032:    149 / 421 symm_mse=6.54, loss=3.624, nll_loss=1.134, ppl=2.19, wps=22834.5, ups=1.61, wpb=14205.9, bsz=491.4, num_updates=13200, lr=1.90693e-05, gnorm=0.933, train_wall=62, wall=8969
2021-01-02 00:21:23 | INFO | train_inner | epoch 032:    249 / 421 symm_mse=6.619, loss=3.636, nll_loss=1.139, ppl=2.2, wps=22583.9, ups=1.6, wpb=14086.9, bsz=488.9, num_updates=13300, lr=1.89974e-05, gnorm=0.98, train_wall=62, wall=9031
2021-01-02 00:22:25 | INFO | train_inner | epoch 032:    349 / 421 symm_mse=6.962, loss=3.696, nll_loss=1.164, ppl=2.24, wps=22196.3, ups=1.6, wpb=13848.6, bsz=472.5, num_updates=13400, lr=1.89264e-05, gnorm=1.045, train_wall=62, wall=9094
2021-01-02 00:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:23:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:23:29 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_mse 0 | loss 5.262 | nll_loss 3.752 | ppl 13.48 | bleu 22.28 | wps 5154.7 | wpb 10324.2 | bsz 375 | num_updates 13472 | best_bleu 22.69
2021-01-02 00:23:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:23:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 32 @ 13472 updates, score 22.28) (writing took 2.9363818913698196 seconds)
2021-01-02 00:23:31 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-02 00:23:31 | INFO | train | epoch 032 | symm_mse 6.611 | loss 3.64 | nll_loss 1.145 | ppl 2.21 | wps 20524.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 13472 | lr 1.88758e-05 | gnorm 0.98 | train_wall 261 | wall 9160
2021-01-02 00:23:31 | INFO | fairseq.trainer | begin training epoch 33
2021-01-02 00:23:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:23:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:23:52 | INFO | train_inner | epoch 033:     28 / 421 symm_mse=6.28, loss=3.599, nll_loss=1.14, ppl=2.2, wps=16021.1, ups=1.15, wpb=13913.9, bsz=525.4, num_updates=13500, lr=1.88562e-05, gnorm=0.94, train_wall=62, wall=9180
2021-01-02 00:24:54 | INFO | train_inner | epoch 033:    128 / 421 symm_mse=6.342, loss=3.595, nll_loss=1.127, ppl=2.18, wps=22654.7, ups=1.62, wpb=14000, bsz=500, num_updates=13600, lr=1.87867e-05, gnorm=0.943, train_wall=62, wall=9242
2021-01-02 00:25:56 | INFO | train_inner | epoch 033:    228 / 421 symm_mse=6.528, loss=3.627, nll_loss=1.139, ppl=2.2, wps=22407.9, ups=1.6, wpb=14017.7, bsz=502.2, num_updates=13700, lr=1.8718e-05, gnorm=0.956, train_wall=62, wall=9305
2021-01-02 00:26:58 | INFO | train_inner | epoch 033:    328 / 421 symm_mse=6.915, loss=3.689, nll_loss=1.162, ppl=2.24, wps=22484.5, ups=1.61, wpb=13934, bsz=474.8, num_updates=13800, lr=1.86501e-05, gnorm=1.012, train_wall=62, wall=9367
2021-01-02 00:27:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:27:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:27:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:27:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:27:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:28:14 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_mse 0 | loss 5.259 | nll_loss 3.75 | ppl 13.46 | bleu 22.38 | wps 5156.1 | wpb 10324.2 | bsz 375 | num_updates 13893 | best_bleu 22.69
2021-01-02 00:28:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:28:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 33 @ 13893 updates, score 22.38) (writing took 2.921992350369692 seconds)
2021-01-02 00:28:17 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-02 00:28:17 | INFO | train | epoch 033 | symm_mse 6.584 | loss 3.637 | nll_loss 1.144 | ppl 2.21 | wps 20570.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 13893 | lr 1.85876e-05 | gnorm 0.974 | train_wall 260 | wall 9446
2021-01-02 00:28:17 | INFO | fairseq.trainer | begin training epoch 34
2021-01-02 00:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:28:25 | INFO | train_inner | epoch 034:      7 / 421 symm_mse=6.638, loss=3.648, nll_loss=1.152, ppl=2.22, wps=16069.3, ups=1.16, wpb=13912, bsz=487.7, num_updates=13900, lr=1.85829e-05, gnorm=0.998, train_wall=62, wall=9453
2021-01-02 00:29:27 | INFO | train_inner | epoch 034:    107 / 421 symm_mse=6.533, loss=3.632, nll_loss=1.144, ppl=2.21, wps=22518.6, ups=1.61, wpb=13986, bsz=504.2, num_updates=14000, lr=1.85164e-05, gnorm=0.963, train_wall=62, wall=9515
2021-01-02 00:30:29 | INFO | train_inner | epoch 034:    207 / 421 symm_mse=6.577, loss=3.641, nll_loss=1.15, ppl=2.22, wps=22542.9, ups=1.61, wpb=13983.3, bsz=494.8, num_updates=14100, lr=1.84506e-05, gnorm=0.955, train_wall=62, wall=9577
2021-01-02 00:31:31 | INFO | train_inner | epoch 034:    307 / 421 symm_mse=6.325, loss=3.595, nll_loss=1.131, ppl=2.19, wps=22315.2, ups=1.6, wpb=13929, bsz=504.4, num_updates=14200, lr=1.83855e-05, gnorm=0.919, train_wall=62, wall=9640
2021-01-02 00:32:34 | INFO | train_inner | epoch 034:    407 / 421 symm_mse=6.702, loss=3.654, nll_loss=1.149, ppl=2.22, wps=22371.9, ups=1.6, wpb=13997.4, bsz=476.1, num_updates=14300, lr=1.83211e-05, gnorm=0.973, train_wall=62, wall=9702
2021-01-02 00:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:32:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:32:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:32:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:33:00 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_mse 0 | loss 5.256 | nll_loss 3.745 | ppl 13.41 | bleu 22.38 | wps 5835.2 | wpb 10324.2 | bsz 375 | num_updates 14314 | best_bleu 22.69
2021-01-02 00:33:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:33:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 34 @ 14314 updates, score 22.38) (writing took 2.885934157297015 seconds)
2021-01-02 00:33:02 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-02 00:33:02 | INFO | train | epoch 034 | symm_mse 6.548 | loss 3.633 | nll_loss 1.144 | ppl 2.21 | wps 20631 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14314 | lr 1.83122e-05 | gnorm 0.955 | train_wall 261 | wall 9731
2021-01-02 00:33:02 | INFO | fairseq.trainer | begin training epoch 35
2021-01-02 00:33:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:33:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:33:59 | INFO | train_inner | epoch 035:     86 / 421 symm_mse=6.407, loss=3.605, nll_loss=1.13, ppl=2.19, wps=16469.9, ups=1.18, wpb=13962.1, bsz=477.3, num_updates=14400, lr=1.82574e-05, gnorm=0.951, train_wall=61, wall=9787
2021-01-02 00:35:01 | INFO | train_inner | epoch 035:    186 / 421 symm_mse=6.446, loss=3.607, nll_loss=1.129, ppl=2.19, wps=22438.1, ups=1.61, wpb=13963.9, bsz=507.5, num_updates=14500, lr=1.81944e-05, gnorm=0.951, train_wall=62, wall=9849
2021-01-02 00:36:03 | INFO | train_inner | epoch 035:    286 / 421 symm_mse=6.649, loss=3.65, nll_loss=1.15, ppl=2.22, wps=22774, ups=1.61, wpb=14103.9, bsz=479.2, num_updates=14600, lr=1.81319e-05, gnorm=0.977, train_wall=62, wall=9911
2021-01-02 00:37:06 | INFO | train_inner | epoch 035:    386 / 421 symm_mse=6.65, loss=3.653, nll_loss=1.155, ppl=2.23, wps=22220.1, ups=1.6, wpb=13917.1, bsz=499.5, num_updates=14700, lr=1.80702e-05, gnorm=0.965, train_wall=62, wall=9974
2021-01-02 00:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:37:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:37:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:37:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:37:44 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_mse 0 | loss 5.258 | nll_loss 3.747 | ppl 13.43 | bleu 22.5 | wps 5889.4 | wpb 10324.2 | bsz 375 | num_updates 14735 | best_bleu 22.69
2021-01-02 00:37:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:37:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 35 @ 14735 updates, score 22.5) (writing took 2.9539974126964808 seconds)
2021-01-02 00:37:47 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-02 00:37:47 | INFO | train | epoch 035 | symm_mse 6.522 | loss 3.629 | nll_loss 1.143 | ppl 2.21 | wps 20663.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14735 | lr 1.80487e-05 | gnorm 0.96 | train_wall 261 | wall 10016
2021-01-02 00:37:47 | INFO | fairseq.trainer | begin training epoch 36
2021-01-02 00:37:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:37:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:38:31 | INFO | train_inner | epoch 036:     65 / 421 symm_mse=6.525, loss=3.635, nll_loss=1.15, ppl=2.22, wps=16180.6, ups=1.18, wpb=13745.1, bsz=484.9, num_updates=14800, lr=1.8009e-05, gnorm=0.969, train_wall=62, wall=10059
2021-01-02 00:39:33 | INFO | train_inner | epoch 036:    165 / 421 symm_mse=6.574, loss=3.643, nll_loss=1.152, ppl=2.22, wps=22425.2, ups=1.61, wpb=13932.1, bsz=482.5, num_updates=14900, lr=1.79485e-05, gnorm=0.966, train_wall=62, wall=10121
2021-01-02 00:40:35 | INFO | train_inner | epoch 036:    265 / 421 symm_mse=6.344, loss=3.599, nll_loss=1.132, ppl=2.19, wps=22656.2, ups=1.61, wpb=14089.7, bsz=509.1, num_updates=15000, lr=1.78885e-05, gnorm=0.933, train_wall=62, wall=10183
2021-01-02 00:41:38 | INFO | train_inner | epoch 036:    365 / 421 symm_mse=6.41, loss=3.605, nll_loss=1.132, ppl=2.19, wps=22671.8, ups=1.59, wpb=14257.4, bsz=501.1, num_updates=15100, lr=1.78292e-05, gnorm=0.935, train_wall=63, wall=10246
2021-01-02 00:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:42:30 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_mse 0 | loss 5.259 | nll_loss 3.749 | ppl 13.44 | bleu 22.45 | wps 5665.7 | wpb 10324.2 | bsz 375 | num_updates 15156 | best_bleu 22.69
2021-01-02 00:42:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:42:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 36 @ 15156 updates, score 22.45) (writing took 3.0647858157753944 seconds)
2021-01-02 00:42:33 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-02 00:42:33 | INFO | train | epoch 036 | symm_mse 6.49 | loss 3.625 | nll_loss 1.142 | ppl 2.21 | wps 20575.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 15156 | lr 1.77962e-05 | gnorm 0.954 | train_wall 261 | wall 10301
2021-01-02 00:42:33 | INFO | fairseq.trainer | begin training epoch 37
2021-01-02 00:42:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:42:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:43:03 | INFO | train_inner | epoch 037:     44 / 421 symm_mse=6.687, loss=3.662, nll_loss=1.159, ppl=2.23, wps=16126.2, ups=1.17, wpb=13760.8, bsz=468.6, num_updates=15200, lr=1.77705e-05, gnorm=0.975, train_wall=61, wall=10332
2021-01-02 00:44:05 | INFO | train_inner | epoch 037:    144 / 421 symm_mse=6.569, loss=3.643, nll_loss=1.153, ppl=2.22, wps=22199.8, ups=1.6, wpb=13844.6, bsz=487.3, num_updates=15300, lr=1.77123e-05, gnorm=0.971, train_wall=62, wall=10394
2021-01-02 00:45:08 | INFO | train_inner | epoch 037:    244 / 421 symm_mse=6.292, loss=3.59, nll_loss=1.128, ppl=2.19, wps=22588.9, ups=1.61, wpb=14021.7, bsz=500.2, num_updates=15400, lr=1.76547e-05, gnorm=0.913, train_wall=62, wall=10456
2021-01-02 00:46:10 | INFO | train_inner | epoch 037:    344 / 421 symm_mse=6.45, loss=3.615, nll_loss=1.136, ppl=2.2, wps=22758.1, ups=1.61, wpb=14112, bsz=500.6, num_updates=15500, lr=1.75977e-05, gnorm=0.927, train_wall=62, wall=10518
2021-01-02 00:46:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:47:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:47:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:47:15 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_mse 0 | loss 5.257 | nll_loss 3.745 | ppl 13.4 | bleu 22.5 | wps 5274.3 | wpb 10324.2 | bsz 375 | num_updates 15577 | best_bleu 22.69
2021-01-02 00:47:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:47:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:47:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:47:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:47:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 37 @ 15577 updates, score 22.5) (writing took 2.8822649847716093 seconds)
2021-01-02 00:47:18 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-02 00:47:18 | INFO | train | epoch 037 | symm_mse 6.46 | loss 3.621 | nll_loss 1.141 | ppl 2.21 | wps 20618.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15577 | lr 1.75541e-05 | gnorm 0.945 | train_wall 260 | wall 10587
2021-01-02 00:47:18 | INFO | fairseq.trainer | begin training epoch 38
2021-01-02 00:47:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:47:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:47:36 | INFO | train_inner | epoch 038:     23 / 421 symm_mse=6.438, loss=3.627, nll_loss=1.152, ppl=2.22, wps=15994.2, ups=1.16, wpb=13763.3, bsz=486, num_updates=15600, lr=1.75412e-05, gnorm=0.96, train_wall=61, wall=10604
2021-01-02 00:48:37 | INFO | train_inner | epoch 038:    123 / 421 symm_mse=6.353, loss=3.598, nll_loss=1.128, ppl=2.19, wps=22505.1, ups=1.62, wpb=13863.9, bsz=510.3, num_updates=15700, lr=1.74852e-05, gnorm=0.942, train_wall=61, wall=10666
2021-01-02 00:49:39 | INFO | train_inner | epoch 038:    223 / 421 symm_mse=6.325, loss=3.603, nll_loss=1.139, ppl=2.2, wps=22617, ups=1.61, wpb=14077.8, bsz=498.1, num_updates=15800, lr=1.74298e-05, gnorm=0.919, train_wall=62, wall=10728
2021-01-02 00:50:42 | INFO | train_inner | epoch 038:    323 / 421 symm_mse=6.607, loss=3.65, nll_loss=1.156, ppl=2.23, wps=22469.1, ups=1.59, wpb=14103.4, bsz=481.4, num_updates=15900, lr=1.73749e-05, gnorm=0.966, train_wall=63, wall=10791
2021-01-02 00:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:51:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:51:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:51:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:51:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:51:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:51:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:51:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:51:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:51:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:51:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:51:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:51:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:52:00 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_mse 0 | loss 5.258 | nll_loss 3.746 | ppl 13.41 | bleu 22.48 | wps 5955.3 | wpb 10324.2 | bsz 375 | num_updates 15998 | best_bleu 22.69
2021-01-02 00:52:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:52:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:52:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:52:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:52:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 38 @ 15998 updates, score 22.48) (writing took 2.956236280500889 seconds)
2021-01-02 00:52:03 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-02 00:52:03 | INFO | train | epoch 038 | symm_mse 6.438 | loss 3.618 | nll_loss 1.141 | ppl 2.2 | wps 20649.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15998 | lr 1.73216e-05 | gnorm 0.948 | train_wall 261 | wall 10871
2021-01-02 00:52:03 | INFO | fairseq.trainer | begin training epoch 39
2021-01-02 00:52:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:52:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:52:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:52:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:52:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:52:07 | INFO | train_inner | epoch 039:      2 / 421 symm_mse=6.407, loss=3.607, nll_loss=1.132, ppl=2.19, wps=16318.1, ups=1.17, wpb=13913.6, bsz=491.7, num_updates=16000, lr=1.73205e-05, gnorm=0.961, train_wall=62, wall=10876
2021-01-02 00:53:09 | INFO | train_inner | epoch 039:    102 / 421 symm_mse=6.347, loss=3.606, nll_loss=1.139, ppl=2.2, wps=22707.3, ups=1.62, wpb=13976.1, bsz=494.8, num_updates=16100, lr=1.72666e-05, gnorm=0.923, train_wall=61, wall=10937
2021-01-02 00:54:11 | INFO | train_inner | epoch 039:    202 / 421 symm_mse=6.316, loss=3.583, nll_loss=1.117, ppl=2.17, wps=22610.3, ups=1.61, wpb=14067.4, bsz=497.4, num_updates=16200, lr=1.72133e-05, gnorm=0.942, train_wall=62, wall=11000
2021-01-02 00:55:13 | INFO | train_inner | epoch 039:    302 / 421 symm_mse=6.408, loss=3.621, nll_loss=1.149, ppl=2.22, wps=22625.8, ups=1.61, wpb=14046.5, bsz=496.7, num_updates=16300, lr=1.71604e-05, gnorm=0.921, train_wall=62, wall=11062
2021-01-02 00:56:15 | INFO | train_inner | epoch 039:    402 / 421 symm_mse=6.549, loss=3.635, nll_loss=1.147, ppl=2.21, wps=22369.4, ups=1.61, wpb=13881.8, bsz=485.9, num_updates=16400, lr=1.7108e-05, gnorm=0.962, train_wall=62, wall=11124
2021-01-02 00:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 00:56:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 00:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 00:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 00:56:44 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_mse 0 | loss 5.255 | nll_loss 3.746 | ppl 13.42 | bleu 22.3 | wps 5969.5 | wpb 10324.2 | bsz 375 | num_updates 16419 | best_bleu 22.69
2021-01-02 00:56:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 00:56:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:56:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 39 @ 16419 updates, score 22.3) (writing took 2.8934834096580744 seconds)
2021-01-02 00:56:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-02 00:56:47 | INFO | train | epoch 039 | symm_mse 6.417 | loss 3.614 | nll_loss 1.139 | ppl 2.2 | wps 20720.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 16419 | lr 1.70981e-05 | gnorm 0.944 | train_wall 260 | wall 11155
2021-01-02 00:56:47 | INFO | fairseq.trainer | begin training epoch 40
2021-01-02 00:56:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 00:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 00:57:40 | INFO | train_inner | epoch 040:     81 / 421 symm_mse=6.748, loss=3.668, nll_loss=1.158, ppl=2.23, wps=16257.7, ups=1.18, wpb=13775.3, bsz=458.8, num_updates=16500, lr=1.70561e-05, gnorm=0.988, train_wall=62, wall=11209
2021-01-02 00:58:43 | INFO | train_inner | epoch 040:    181 / 421 symm_mse=6.169, loss=3.578, nll_loss=1.13, ppl=2.19, wps=22385.4, ups=1.6, wpb=13989.1, bsz=509.5, num_updates=16600, lr=1.70046e-05, gnorm=0.927, train_wall=62, wall=11271
2021-01-02 00:59:45 | INFO | train_inner | epoch 040:    281 / 421 symm_mse=6.403, loss=3.606, nll_loss=1.133, ppl=2.19, wps=22794.2, ups=1.61, wpb=14114.3, bsz=492.2, num_updates=16700, lr=1.69536e-05, gnorm=0.942, train_wall=62, wall=11333
2021-01-02 01:00:47 | INFO | train_inner | epoch 040:    381 / 421 symm_mse=6.283, loss=3.605, nll_loss=1.147, ppl=2.21, wps=22523.6, ups=1.61, wpb=14010.3, bsz=512.3, num_updates=16800, lr=1.69031e-05, gnorm=0.932, train_wall=62, wall=11395
2021-01-02 01:01:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:01:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:01:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:01:28 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_mse 0 | loss 5.252 | nll_loss 3.739 | ppl 13.36 | bleu 22.47 | wps 5893.9 | wpb 10324.2 | bsz 375 | num_updates 16840 | best_bleu 22.69
2021-01-02 01:01:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:01:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:01:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 40 @ 16840 updates, score 22.47) (writing took 2.907064886763692 seconds)
2021-01-02 01:01:31 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-02 01:01:31 | INFO | train | epoch 040 | symm_mse 6.385 | loss 3.61 | nll_loss 1.139 | ppl 2.2 | wps 20674.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 16840 | lr 1.6883e-05 | gnorm 0.942 | train_wall 261 | wall 11440
2021-01-02 01:01:31 | INFO | fairseq.trainer | begin training epoch 41
2021-01-02 01:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:01:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:02:11 | INFO | train_inner | epoch 041:     60 / 421 symm_mse=6.343, loss=3.601, nll_loss=1.133, ppl=2.19, wps=16379, ups=1.18, wpb=13852.1, bsz=485.2, num_updates=16900, lr=1.6853e-05, gnorm=0.943, train_wall=61, wall=11480
2021-01-02 01:03:14 | INFO | train_inner | epoch 041:    160 / 421 symm_mse=6.393, loss=3.605, nll_loss=1.132, ppl=2.19, wps=22449, ups=1.6, wpb=14070.9, bsz=484.6, num_updates=17000, lr=1.68034e-05, gnorm=0.956, train_wall=62, wall=11542
2021-01-02 01:04:17 | INFO | train_inner | epoch 041:    260 / 421 symm_mse=6.136, loss=3.572, nll_loss=1.129, ppl=2.19, wps=22236.9, ups=1.6, wpb=13910.3, bsz=522.1, num_updates=17100, lr=1.67542e-05, gnorm=0.906, train_wall=62, wall=11605
2021-01-02 01:05:19 | INFO | train_inner | epoch 041:    360 / 421 symm_mse=6.531, loss=3.635, nll_loss=1.148, ppl=2.22, wps=22359.3, ups=1.6, wpb=13988.5, bsz=486.2, num_updates=17200, lr=1.67054e-05, gnorm=0.953, train_wall=62, wall=11668
2021-01-02 01:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:05:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:05:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:05:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:05:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:06:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:06:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:06:15 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_mse 0 | loss 5.252 | nll_loss 3.74 | ppl 13.36 | bleu 22.23 | wps 5831.5 | wpb 10324.2 | bsz 375 | num_updates 17261 | best_bleu 22.69
2021-01-02 01:06:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:06:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:06:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:06:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:06:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 41 @ 17261 updates, score 22.23) (writing took 2.8867900744080544 seconds)
2021-01-02 01:06:18 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-02 01:06:18 | INFO | train | epoch 041 | symm_mse 6.372 | loss 3.608 | nll_loss 1.138 | ppl 2.2 | wps 20533 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 17261 | lr 1.66758e-05 | gnorm 0.939 | train_wall 262 | wall 11726
2021-01-02 01:06:18 | INFO | fairseq.trainer | begin training epoch 42
2021-01-02 01:06:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:06:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:06:45 | INFO | train_inner | epoch 042:     39 / 421 symm_mse=6.495, loss=3.624, nll_loss=1.14, ppl=2.2, wps=16319, ups=1.17, wpb=13992.9, bsz=466.6, num_updates=17300, lr=1.6657e-05, gnorm=0.94, train_wall=62, wall=11753
2021-01-02 01:07:47 | INFO | train_inner | epoch 042:    139 / 421 symm_mse=6.319, loss=3.602, nll_loss=1.138, ppl=2.2, wps=22173.6, ups=1.6, wpb=13864.2, bsz=490.1, num_updates=17400, lr=1.66091e-05, gnorm=0.942, train_wall=62, wall=11816
2021-01-02 01:08:49 | INFO | train_inner | epoch 042:    239 / 421 symm_mse=6.344, loss=3.599, nll_loss=1.131, ppl=2.19, wps=22683.6, ups=1.61, wpb=14054, bsz=497.4, num_updates=17500, lr=1.65616e-05, gnorm=0.93, train_wall=62, wall=11878
2021-01-02 01:09:52 | INFO | train_inner | epoch 042:    339 / 421 symm_mse=6.308, loss=3.598, nll_loss=1.136, ppl=2.2, wps=22616.3, ups=1.61, wpb=14062.4, bsz=506.2, num_updates=17600, lr=1.65145e-05, gnorm=0.926, train_wall=62, wall=11940
2021-01-02 01:10:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:10:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:10:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:10:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:10:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:10:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:10:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:10:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:10:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:11:00 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_mse 0 | loss 5.251 | nll_loss 3.742 | ppl 13.38 | bleu 22.32 | wps 5781.6 | wpb 10324.2 | bsz 375 | num_updates 17682 | best_bleu 22.69
2021-01-02 01:11:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:11:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:11:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:11:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:11:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:11:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:11:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:11:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 42 @ 17682 updates, score 22.32) (writing took 2.983226368203759 seconds)
2021-01-02 01:11:03 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-02 01:11:03 | INFO | train | epoch 042 | symm_mse 6.347 | loss 3.605 | nll_loss 1.138 | ppl 2.2 | wps 20636.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 17682 | lr 1.64761e-05 | gnorm 0.94 | train_wall 261 | wall 12011
2021-01-02 01:11:03 | INFO | fairseq.trainer | begin training epoch 43
2021-01-02 01:11:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:11:17 | INFO | train_inner | epoch 043:     18 / 421 symm_mse=6.326, loss=3.608, nll_loss=1.144, ppl=2.21, wps=16152.7, ups=1.17, wpb=13796.6, bsz=485.9, num_updates=17700, lr=1.64677e-05, gnorm=0.954, train_wall=62, wall=12025
2021-01-02 01:12:19 | INFO | train_inner | epoch 043:    118 / 421 symm_mse=6.334, loss=3.603, nll_loss=1.137, ppl=2.2, wps=22706.1, ups=1.62, wpb=13998.6, bsz=490.6, num_updates=17800, lr=1.64214e-05, gnorm=0.933, train_wall=61, wall=12087
2021-01-02 01:13:21 | INFO | train_inner | epoch 043:    218 / 421 symm_mse=6.501, loss=3.626, nll_loss=1.142, ppl=2.21, wps=22435.1, ups=1.61, wpb=13921.6, bsz=480.3, num_updates=17900, lr=1.63755e-05, gnorm=0.968, train_wall=62, wall=12149
2021-01-02 01:14:23 | INFO | train_inner | epoch 043:    318 / 421 symm_mse=6.208, loss=3.588, nll_loss=1.136, ppl=2.2, wps=22450.1, ups=1.6, wpb=14013.3, bsz=509.1, num_updates=18000, lr=1.63299e-05, gnorm=0.913, train_wall=62, wall=12211
2021-01-02 01:15:26 | INFO | train_inner | epoch 043:    418 / 421 symm_mse=6.277, loss=3.598, nll_loss=1.139, ppl=2.2, wps=22439.8, ups=1.59, wpb=14072.7, bsz=493.7, num_updates=18100, lr=1.62848e-05, gnorm=0.928, train_wall=63, wall=12274
2021-01-02 01:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:15:44 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_mse 0 | loss 5.247 | nll_loss 3.734 | ppl 13.31 | bleu 22.41 | wps 5884.5 | wpb 10324.2 | bsz 375 | num_updates 18103 | best_bleu 22.69
2021-01-02 01:15:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:15:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 43 @ 18103 updates, score 22.41) (writing took 2.905108479782939 seconds)
2021-01-02 01:15:47 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-02 01:15:47 | INFO | train | epoch 043 | symm_mse 6.312 | loss 3.601 | nll_loss 1.137 | ppl 2.2 | wps 20662.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18103 | lr 1.62834e-05 | gnorm 0.936 | train_wall 261 | wall 12296
2021-01-02 01:15:47 | INFO | fairseq.trainer | begin training epoch 44
2021-01-02 01:15:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:15:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:16:51 | INFO | train_inner | epoch 044:     97 / 421 symm_mse=6.291, loss=3.589, nll_loss=1.126, ppl=2.18, wps=16321.8, ups=1.18, wpb=13866.6, bsz=492.2, num_updates=18200, lr=1.624e-05, gnorm=0.946, train_wall=62, wall=12359
2021-01-02 01:17:53 | INFO | train_inner | epoch 044:    197 / 421 symm_mse=6.236, loss=3.572, nll_loss=1.114, ppl=2.16, wps=22508.5, ups=1.6, wpb=14058.6, bsz=492.3, num_updates=18300, lr=1.61955e-05, gnorm=0.932, train_wall=62, wall=12422
2021-01-02 01:18:55 | INFO | train_inner | epoch 044:    297 / 421 symm_mse=6.141, loss=3.583, nll_loss=1.141, ppl=2.2, wps=22430.7, ups=1.61, wpb=13940.4, bsz=512, num_updates=18400, lr=1.61515e-05, gnorm=0.917, train_wall=62, wall=12484
2021-01-02 01:19:57 | INFO | train_inner | epoch 044:    397 / 421 symm_mse=6.539, loss=3.642, nll_loss=1.155, ppl=2.23, wps=22782.6, ups=1.62, wpb=14035.6, bsz=473.9, num_updates=18500, lr=1.61077e-05, gnorm=0.956, train_wall=61, wall=12545
2021-01-02 01:20:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:20:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:20:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:20:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:20:29 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_mse 0 | loss 5.246 | nll_loss 3.733 | ppl 13.29 | bleu 22.51 | wps 5939.1 | wpb 10324.2 | bsz 375 | num_updates 18524 | best_bleu 22.69
2021-01-02 01:20:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:20:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:20:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 44 @ 18524 updates, score 22.51) (writing took 2.9326285421848297 seconds)
2021-01-02 01:20:32 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-02 01:20:32 | INFO | train | epoch 044 | symm_mse 6.303 | loss 3.598 | nll_loss 1.136 | ppl 2.2 | wps 20694.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18524 | lr 1.60973e-05 | gnorm 0.934 | train_wall 260 | wall 12580
2021-01-02 01:20:32 | INFO | fairseq.trainer | begin training epoch 45
2021-01-02 01:20:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:20:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:21:22 | INFO | train_inner | epoch 045:     76 / 421 symm_mse=6.153, loss=3.584, nll_loss=1.14, ppl=2.2, wps=16241.6, ups=1.18, wpb=13771.3, bsz=483, num_updates=18600, lr=1.60644e-05, gnorm=0.912, train_wall=62, wall=12630
2021-01-02 01:22:23 | INFO | train_inner | epoch 045:    176 / 421 symm_mse=6.239, loss=3.592, nll_loss=1.138, ppl=2.2, wps=22691.4, ups=1.62, wpb=13992.1, bsz=510.8, num_updates=18700, lr=1.60214e-05, gnorm=0.935, train_wall=61, wall=12692
2021-01-02 01:23:25 | INFO | train_inner | epoch 045:    276 / 421 symm_mse=6.533, loss=3.634, nll_loss=1.147, ppl=2.21, wps=22508.5, ups=1.61, wpb=13974.4, bsz=476, num_updates=18800, lr=1.59787e-05, gnorm=0.978, train_wall=62, wall=12754
2021-01-02 01:24:28 | INFO | train_inner | epoch 045:    376 / 421 symm_mse=6.227, loss=3.581, nll_loss=1.126, ppl=2.18, wps=22599.3, ups=1.6, wpb=14092, bsz=499.6, num_updates=18900, lr=1.59364e-05, gnorm=0.926, train_wall=62, wall=12816
2021-01-02 01:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:24:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:24:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:24:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:24:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:25:13 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_mse 0 | loss 5.247 | nll_loss 3.735 | ppl 13.32 | bleu 22.47 | wps 5867 | wpb 10324.2 | bsz 375 | num_updates 18945 | best_bleu 22.69
2021-01-02 01:25:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:25:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 45 @ 18945 updates, score 22.47) (writing took 2.893266901373863 seconds)
2021-01-02 01:25:16 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-02 01:25:16 | INFO | train | epoch 045 | symm_mse 6.28 | loss 3.595 | nll_loss 1.135 | ppl 2.2 | wps 20698.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18945 | lr 1.59174e-05 | gnorm 0.942 | train_wall 260 | wall 12864
2021-01-02 01:25:16 | INFO | fairseq.trainer | begin training epoch 46
2021-01-02 01:25:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:25:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:25:53 | INFO | train_inner | epoch 046:     55 / 421 symm_mse=6.319, loss=3.594, nll_loss=1.13, ppl=2.19, wps=16398.8, ups=1.18, wpb=13951.3, bsz=487.3, num_updates=19000, lr=1.58944e-05, gnorm=0.943, train_wall=62, wall=12901
2021-01-02 01:26:55 | INFO | train_inner | epoch 046:    155 / 421 symm_mse=6.201, loss=3.59, nll_loss=1.138, ppl=2.2, wps=22265.5, ups=1.61, wpb=13867.7, bsz=496.7, num_updates=19100, lr=1.58527e-05, gnorm=0.922, train_wall=62, wall=12964
2021-01-02 01:27:58 | INFO | train_inner | epoch 046:    255 / 421 symm_mse=6.302, loss=3.597, nll_loss=1.133, ppl=2.19, wps=22514.1, ups=1.6, wpb=14102.5, bsz=494.7, num_updates=19200, lr=1.58114e-05, gnorm=0.919, train_wall=62, wall=13026
2021-01-02 01:29:00 | INFO | train_inner | epoch 046:    355 / 421 symm_mse=6.275, loss=3.596, nll_loss=1.138, ppl=2.2, wps=22511, ups=1.61, wpb=13996.6, bsz=487.2, num_updates=19300, lr=1.57704e-05, gnorm=0.934, train_wall=62, wall=13088
2021-01-02 01:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:29:58 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_mse 0 | loss 5.247 | nll_loss 3.733 | ppl 13.3 | bleu 22.43 | wps 5926.7 | wpb 10324.2 | bsz 375 | num_updates 19366 | best_bleu 22.69
2021-01-02 01:29:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:29:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:29:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:30:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:30:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:30:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:30:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 46 @ 19366 updates, score 22.43) (writing took 2.8923955019563437 seconds)
2021-01-02 01:30:01 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-02 01:30:01 | INFO | train | epoch 046 | symm_mse 6.258 | loss 3.593 | nll_loss 1.136 | ppl 2.2 | wps 20611.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 19366 | lr 1.57435e-05 | gnorm 0.922 | train_wall 262 | wall 13149
2021-01-02 01:30:01 | INFO | fairseq.trainer | begin training epoch 47
2021-01-02 01:30:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:30:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:30:25 | INFO | train_inner | epoch 047:     34 / 421 symm_mse=6.102, loss=3.571, nll_loss=1.131, ppl=2.19, wps=16411.2, ups=1.18, wpb=13956.4, bsz=499.5, num_updates=19400, lr=1.57297e-05, gnorm=0.903, train_wall=62, wall=13174
2021-01-02 01:31:27 | INFO | train_inner | epoch 047:    134 / 421 symm_mse=6.583, loss=3.645, nll_loss=1.152, ppl=2.22, wps=22626.5, ups=1.62, wpb=13959.6, bsz=462.2, num_updates=19500, lr=1.56893e-05, gnorm=0.965, train_wall=61, wall=13235
2021-01-02 01:32:29 | INFO | train_inner | epoch 047:    234 / 421 symm_mse=6.095, loss=3.568, nll_loss=1.128, ppl=2.19, wps=22600.4, ups=1.61, wpb=13996.9, bsz=506.4, num_updates=19600, lr=1.56492e-05, gnorm=0.922, train_wall=62, wall=13297
2021-01-02 01:33:31 | INFO | train_inner | epoch 047:    334 / 421 symm_mse=6.114, loss=3.571, nll_loss=1.13, ppl=2.19, wps=22520.9, ups=1.61, wpb=13997.8, bsz=500.8, num_updates=19700, lr=1.56094e-05, gnorm=0.911, train_wall=62, wall=13359
2021-01-02 01:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:34:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:34:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:34:44 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_mse 0 | loss 5.243 | nll_loss 3.73 | ppl 13.27 | bleu 22.5 | wps 5075.1 | wpb 10324.2 | bsz 375 | num_updates 19787 | best_bleu 22.69
2021-01-02 01:34:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:34:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 47 @ 19787 updates, score 22.5) (writing took 2.9190427474677563 seconds)
2021-01-02 01:34:47 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-02 01:34:47 | INFO | train | epoch 047 | symm_mse 6.24 | loss 3.59 | nll_loss 1.135 | ppl 2.2 | wps 20579.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 19787 | lr 1.55751e-05 | gnorm 0.931 | train_wall 260 | wall 13435
2021-01-02 01:34:47 | INFO | fairseq.trainer | begin training epoch 48
2021-01-02 01:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:34:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:34:58 | INFO | train_inner | epoch 048:     13 / 421 symm_mse=6.177, loss=3.579, nll_loss=1.131, ppl=2.19, wps=15889.9, ups=1.15, wpb=13865.8, bsz=501.2, num_updates=19800, lr=1.557e-05, gnorm=0.934, train_wall=62, wall=13447
2021-01-02 01:36:00 | INFO | train_inner | epoch 048:    113 / 421 symm_mse=6.395, loss=3.616, nll_loss=1.144, ppl=2.21, wps=22635.6, ups=1.62, wpb=13942.5, bsz=492.8, num_updates=19900, lr=1.55308e-05, gnorm=0.977, train_wall=61, wall=13508
2021-01-02 01:37:02 | INFO | train_inner | epoch 048:    213 / 421 symm_mse=6.258, loss=3.597, nll_loss=1.141, ppl=2.21, wps=22630.8, ups=1.62, wpb=13986.9, bsz=473.4, num_updates=20000, lr=1.54919e-05, gnorm=0.919, train_wall=62, wall=13570
2021-01-02 01:38:04 | INFO | train_inner | epoch 048:    313 / 421 symm_mse=6.163, loss=3.58, nll_loss=1.133, ppl=2.19, wps=22419.6, ups=1.6, wpb=13999.2, bsz=503.7, num_updates=20100, lr=1.54533e-05, gnorm=0.913, train_wall=62, wall=13632
2021-01-02 01:39:06 | INFO | train_inner | epoch 048:    413 / 421 symm_mse=6.113, loss=3.567, nll_loss=1.124, ppl=2.18, wps=22398.2, ups=1.6, wpb=13966.9, bsz=498.5, num_updates=20200, lr=1.5415e-05, gnorm=0.912, train_wall=62, wall=13695
2021-01-02 01:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:39:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:39:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:39:28 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_mse 0 | loss 5.244 | nll_loss 3.731 | ppl 13.28 | bleu 22.55 | wps 5877.5 | wpb 10324.2 | bsz 375 | num_updates 20208 | best_bleu 22.69
2021-01-02 01:39:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:39:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:39:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 48 @ 20208 updates, score 22.55) (writing took 2.8765048664063215 seconds)
2021-01-02 01:39:31 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-02 01:39:31 | INFO | train | epoch 048 | symm_mse 6.219 | loss 3.587 | nll_loss 1.134 | ppl 2.19 | wps 20688.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 20208 | lr 1.5412e-05 | gnorm 0.927 | train_wall 260 | wall 13719
2021-01-02 01:39:31 | INFO | fairseq.trainer | begin training epoch 49
2021-01-02 01:39:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:39:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:40:32 | INFO | train_inner | epoch 049:     92 / 421 symm_mse=6.21, loss=3.577, nll_loss=1.123, ppl=2.18, wps=16352.3, ups=1.17, wpb=13934.4, bsz=503, num_updates=20300, lr=1.5377e-05, gnorm=0.922, train_wall=62, wall=13780
2021-01-02 01:41:33 | INFO | train_inner | epoch 049:    192 / 421 symm_mse=6.46, loss=3.635, nll_loss=1.158, ppl=2.23, wps=22452.1, ups=1.62, wpb=13889.8, bsz=475.2, num_updates=20400, lr=1.53393e-05, gnorm=0.967, train_wall=62, wall=13842
2021-01-02 01:42:36 | INFO | train_inner | epoch 049:    292 / 421 symm_mse=6.04, loss=3.559, nll_loss=1.125, ppl=2.18, wps=22379.2, ups=1.61, wpb=13923.3, bsz=497, num_updates=20500, lr=1.53018e-05, gnorm=0.887, train_wall=62, wall=13904
2021-01-02 01:43:38 | INFO | train_inner | epoch 049:    392 / 421 symm_mse=6.217, loss=3.583, nll_loss=1.129, ppl=2.19, wps=22778.8, ups=1.61, wpb=14132.9, bsz=483.7, num_updates=20600, lr=1.52647e-05, gnorm=0.918, train_wall=62, wall=13966
2021-01-02 01:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:43:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:43:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:43:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:43:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:43:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:43:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:43:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:43:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:44:13 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_mse 0 | loss 5.241 | nll_loss 3.728 | ppl 13.25 | bleu 22.33 | wps 5849.7 | wpb 10324.2 | bsz 375 | num_updates 20629 | best_bleu 22.69
2021-01-02 01:44:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:44:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:44:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:44:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:44:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:44:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:44:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:44:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 49 @ 20629 updates, score 22.33) (writing took 2.9781049098819494 seconds)
2021-01-02 01:44:16 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-02 01:44:16 | INFO | train | epoch 049 | symm_mse 6.204 | loss 3.585 | nll_loss 1.133 | ppl 2.19 | wps 20671 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 20629 | lr 1.52539e-05 | gnorm 0.923 | train_wall 260 | wall 14004
2021-01-02 01:44:16 | INFO | fairseq.trainer | begin training epoch 50
2021-01-02 01:44:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:44:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:45:03 | INFO | train_inner | epoch 050:     71 / 421 symm_mse=6.114, loss=3.571, nll_loss=1.129, ppl=2.19, wps=16365.7, ups=1.17, wpb=13974.8, bsz=500.3, num_updates=20700, lr=1.52277e-05, gnorm=0.922, train_wall=62, wall=14051
2021-01-02 01:46:05 | INFO | train_inner | epoch 050:    171 / 421 symm_mse=6.364, loss=3.608, nll_loss=1.138, ppl=2.2, wps=22355.3, ups=1.61, wpb=13849.1, bsz=471, num_updates=20800, lr=1.51911e-05, gnorm=0.955, train_wall=62, wall=14113
2021-01-02 01:47:08 | INFO | train_inner | epoch 050:    271 / 421 symm_mse=6.011, loss=3.547, nll_loss=1.115, ppl=2.17, wps=22542.6, ups=1.6, wpb=14131.2, bsz=510.4, num_updates=20900, lr=1.51547e-05, gnorm=0.891, train_wall=62, wall=14176
2021-01-02 01:48:10 | INFO | train_inner | epoch 050:    371 / 421 symm_mse=6.078, loss=3.571, nll_loss=1.134, ppl=2.2, wps=22447.7, ups=1.6, wpb=13993.5, bsz=510.5, num_updates=21000, lr=1.51186e-05, gnorm=0.912, train_wall=62, wall=14238
2021-01-02 01:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:48:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:48:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:48:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:48:58 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_mse 0 | loss 5.242 | nll_loss 3.73 | ppl 13.27 | bleu 22.33 | wps 5859.5 | wpb 10324.2 | bsz 375 | num_updates 21050 | best_bleu 22.69
2021-01-02 01:48:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:48:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:48:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:49:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:49:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:49:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:49:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 50 @ 21050 updates, score 22.33) (writing took 2.9367530420422554 seconds)
2021-01-02 01:49:01 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-02 01:49:01 | INFO | train | epoch 050 | symm_mse 6.193 | loss 3.583 | nll_loss 1.132 | ppl 2.19 | wps 20610.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 21050 | lr 1.51006e-05 | gnorm 0.924 | train_wall 261 | wall 14289
2021-01-02 01:49:01 | INFO | fairseq.trainer | begin training epoch 51
2021-01-02 01:49:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:49:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:49:35 | INFO | train_inner | epoch 051:     50 / 421 symm_mse=6.485, loss=3.639, nll_loss=1.159, ppl=2.23, wps=16238.4, ups=1.18, wpb=13798.6, bsz=464.4, num_updates=21100, lr=1.50827e-05, gnorm=0.96, train_wall=62, wall=14323
2021-01-02 01:50:37 | INFO | train_inner | epoch 051:    150 / 421 symm_mse=6.04, loss=3.562, nll_loss=1.129, ppl=2.19, wps=22568.9, ups=1.61, wpb=14049.9, bsz=505.3, num_updates=21200, lr=1.50471e-05, gnorm=0.903, train_wall=62, wall=14386
2021-01-02 01:51:39 | INFO | train_inner | epoch 051:    250 / 421 symm_mse=6.152, loss=3.581, nll_loss=1.136, ppl=2.2, wps=22436.9, ups=1.61, wpb=13949.7, bsz=501, num_updates=21300, lr=1.50117e-05, gnorm=0.92, train_wall=62, wall=14448
2021-01-02 01:52:42 | INFO | train_inner | epoch 051:    350 / 421 symm_mse=6.22, loss=3.58, nll_loss=1.125, ppl=2.18, wps=22538.2, ups=1.61, wpb=13994.4, bsz=484.5, num_updates=21400, lr=1.49766e-05, gnorm=0.921, train_wall=62, wall=14510
2021-01-02 01:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:53:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:53:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:53:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:53:43 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_mse 0 | loss 5.24 | nll_loss 3.726 | ppl 13.24 | bleu 22.37 | wps 5867.4 | wpb 10324.2 | bsz 375 | num_updates 21471 | best_bleu 22.69
2021-01-02 01:53:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:53:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:53:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 51 @ 21471 updates, score 22.37) (writing took 2.936099134385586 seconds)
2021-01-02 01:53:46 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-02 01:53:46 | INFO | train | epoch 051 | symm_mse 6.172 | loss 3.58 | nll_loss 1.132 | ppl 2.19 | wps 20636.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 21471 | lr 1.49518e-05 | gnorm 0.92 | train_wall 261 | wall 14574
2021-01-02 01:53:46 | INFO | fairseq.trainer | begin training epoch 52
2021-01-02 01:53:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:53:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:54:07 | INFO | train_inner | epoch 052:     29 / 421 symm_mse=6.099, loss=3.562, nll_loss=1.12, ppl=2.17, wps=16382.9, ups=1.17, wpb=14005.4, bsz=491.4, num_updates=21500, lr=1.49417e-05, gnorm=0.906, train_wall=62, wall=14595
2021-01-02 01:55:09 | INFO | train_inner | epoch 052:    129 / 421 symm_mse=6.142, loss=3.576, nll_loss=1.131, ppl=2.19, wps=22539.8, ups=1.61, wpb=14011.3, bsz=500.1, num_updates=21600, lr=1.49071e-05, gnorm=0.908, train_wall=62, wall=14658
2021-01-02 01:56:11 | INFO | train_inner | epoch 052:    229 / 421 symm_mse=6.499, loss=3.645, nll_loss=1.164, ppl=2.24, wps=22385.5, ups=1.61, wpb=13925.4, bsz=462.5, num_updates=21700, lr=1.48727e-05, gnorm=0.97, train_wall=62, wall=14720
2021-01-02 01:57:13 | INFO | train_inner | epoch 052:    329 / 421 symm_mse=6.078, loss=3.56, nll_loss=1.121, ppl=2.17, wps=22462.1, ups=1.62, wpb=13906.2, bsz=492.3, num_updates=21800, lr=1.48386e-05, gnorm=0.915, train_wall=62, wall=14782
2021-01-02 01:58:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 01:58:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 01:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 01:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 01:58:27 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_mse 0 | loss 5.241 | nll_loss 3.727 | ppl 13.24 | bleu 22.34 | wps 5945.8 | wpb 10324.2 | bsz 375 | num_updates 21892 | best_bleu 22.69
2021-01-02 01:58:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 01:58:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 52 @ 21892 updates, score 22.34) (writing took 2.9014341607689857 seconds)
2021-01-02 01:58:30 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-02 01:58:30 | INFO | train | epoch 052 | symm_mse 6.155 | loss 3.578 | nll_loss 1.132 | ppl 2.19 | wps 20688.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 21892 | lr 1.48074e-05 | gnorm 0.925 | train_wall 260 | wall 14859
2021-01-02 01:58:30 | INFO | fairseq.trainer | begin training epoch 53
2021-01-02 01:58:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 01:58:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 01:58:38 | INFO | train_inner | epoch 053:      8 / 421 symm_mse=5.813, loss=3.519, nll_loss=1.109, ppl=2.16, wps=16429.6, ups=1.17, wpb=13998.2, bsz=519.4, num_updates=21900, lr=1.48047e-05, gnorm=0.905, train_wall=62, wall=14867
2021-01-02 01:59:40 | INFO | train_inner | epoch 053:    108 / 421 symm_mse=6.259, loss=3.596, nll_loss=1.139, ppl=2.2, wps=22749, ups=1.62, wpb=14041.4, bsz=485.9, num_updates=22000, lr=1.4771e-05, gnorm=0.929, train_wall=62, wall=14929
2021-01-02 02:00:42 | INFO | train_inner | epoch 053:    208 / 421 symm_mse=6.12, loss=3.567, nll_loss=1.123, ppl=2.18, wps=22351.2, ups=1.61, wpb=13904.8, bsz=492.6, num_updates=22100, lr=1.47375e-05, gnorm=0.902, train_wall=62, wall=14991
2021-01-02 02:01:45 | INFO | train_inner | epoch 053:    308 / 421 symm_mse=6.025, loss=3.551, nll_loss=1.119, ppl=2.17, wps=22305.1, ups=1.59, wpb=14051.7, bsz=498.5, num_updates=22200, lr=1.47043e-05, gnorm=0.919, train_wall=63, wall=15054
2021-01-02 02:02:47 | INFO | train_inner | epoch 053:    408 / 421 symm_mse=6.251, loss=3.6, nll_loss=1.145, ppl=2.21, wps=22441.1, ups=1.61, wpb=13918.4, bsz=491.2, num_updates=22300, lr=1.46713e-05, gnorm=0.941, train_wall=62, wall=15116
2021-01-02 02:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:02:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:02:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:02:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:02:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:03:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:03:12 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_mse 0 | loss 5.244 | nll_loss 3.731 | ppl 13.28 | bleu 22.36 | wps 5883.3 | wpb 10324.2 | bsz 375 | num_updates 22313 | best_bleu 22.69
2021-01-02 02:03:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:03:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:03:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:03:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:03:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:03:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:03:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:03:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 53 @ 22313 updates, score 22.36) (writing took 2.879982279613614 seconds)
2021-01-02 02:03:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-02 02:03:15 | INFO | train | epoch 053 | symm_mse 6.14 | loss 3.576 | nll_loss 1.131 | ppl 2.19 | wps 20630.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 22313 | lr 1.4667e-05 | gnorm 0.92 | train_wall 261 | wall 15144
2021-01-02 02:03:15 | INFO | fairseq.trainer | begin training epoch 54
2021-01-02 02:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:04:12 | INFO | train_inner | epoch 054:     87 / 421 symm_mse=6.154, loss=3.576, nll_loss=1.129, ppl=2.19, wps=16459, ups=1.18, wpb=13991.3, bsz=490.6, num_updates=22400, lr=1.46385e-05, gnorm=0.934, train_wall=62, wall=15201
2021-01-02 02:05:15 | INFO | train_inner | epoch 054:    187 / 421 symm_mse=6.108, loss=3.578, nll_loss=1.138, ppl=2.2, wps=22387.9, ups=1.6, wpb=13999.8, bsz=486.5, num_updates=22500, lr=1.46059e-05, gnorm=0.919, train_wall=62, wall=15263
2021-01-02 02:06:17 | INFO | train_inner | epoch 054:    287 / 421 symm_mse=5.995, loss=3.551, nll_loss=1.121, ppl=2.18, wps=22548.7, ups=1.6, wpb=14070.5, bsz=509, num_updates=22600, lr=1.45736e-05, gnorm=0.901, train_wall=62, wall=15326
2021-01-02 02:07:19 | INFO | train_inner | epoch 054:    387 / 421 symm_mse=6.217, loss=3.589, nll_loss=1.136, ppl=2.2, wps=22463.9, ups=1.62, wpb=13862.2, bsz=486.7, num_updates=22700, lr=1.45414e-05, gnorm=0.93, train_wall=62, wall=15388
2021-01-02 02:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:07:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:07:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:07:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:07:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:07:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:07:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:07:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:07:59 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_mse 0 | loss 5.239 | nll_loss 3.726 | ppl 13.23 | bleu 22.49 | wps 5210.3 | wpb 10324.2 | bsz 375 | num_updates 22734 | best_bleu 22.69
2021-01-02 02:07:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:08:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:08:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:08:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:08:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 54 @ 22734 updates, score 22.49) (writing took 2.8460083082318306 seconds)
2021-01-02 02:08:02 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-02 02:08:02 | INFO | train | epoch 054 | symm_mse 6.119 | loss 3.573 | nll_loss 1.13 | ppl 2.19 | wps 20526 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 22734 | lr 1.45306e-05 | gnorm 0.924 | train_wall 261 | wall 15430
2021-01-02 02:08:02 | INFO | fairseq.trainer | begin training epoch 55
2021-01-02 02:08:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:08:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:08:46 | INFO | train_inner | epoch 055:     66 / 421 symm_mse=6.2, loss=3.591, nll_loss=1.141, ppl=2.2, wps=15950.1, ups=1.15, wpb=13841.5, bsz=488.1, num_updates=22800, lr=1.45095e-05, gnorm=0.928, train_wall=62, wall=15474
2021-01-02 02:09:48 | INFO | train_inner | epoch 055:    166 / 421 symm_mse=5.842, loss=3.53, nll_loss=1.118, ppl=2.17, wps=22509.4, ups=1.6, wpb=14052.4, bsz=517.9, num_updates=22900, lr=1.44778e-05, gnorm=0.876, train_wall=62, wall=15537
2021-01-02 02:10:51 | INFO | train_inner | epoch 055:    266 / 421 symm_mse=6.322, loss=3.598, nll_loss=1.132, ppl=2.19, wps=22516.9, ups=1.6, wpb=14060.1, bsz=471.7, num_updates=23000, lr=1.44463e-05, gnorm=0.92, train_wall=62, wall=15599
2021-01-02 02:11:53 | INFO | train_inner | epoch 055:    366 / 421 symm_mse=6.038, loss=3.564, nll_loss=1.131, ppl=2.19, wps=22376, ups=1.61, wpb=13892.3, bsz=496.7, num_updates=23100, lr=1.4415e-05, gnorm=0.925, train_wall=62, wall=15661
2021-01-02 02:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:12:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:12:44 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_mse 0 | loss 5.238 | nll_loss 3.726 | ppl 13.23 | bleu 22.42 | wps 5950.4 | wpb 10324.2 | bsz 375 | num_updates 23155 | best_bleu 22.69
2021-01-02 02:12:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:12:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 55 @ 23155 updates, score 22.42) (writing took 2.3932158425450325 seconds)
2021-01-02 02:12:46 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-02 02:12:46 | INFO | train | epoch 055 | symm_mse 6.106 | loss 3.571 | nll_loss 1.13 | ppl 2.19 | wps 20665.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 23155 | lr 1.43979e-05 | gnorm 0.912 | train_wall 261 | wall 15715
2021-01-02 02:12:46 | INFO | fairseq.trainer | begin training epoch 56
2021-01-02 02:12:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:12:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:12:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:13:17 | INFO | train_inner | epoch 056:     45 / 421 symm_mse=6.208, loss=3.59, nll_loss=1.138, ppl=2.2, wps=16341.8, ups=1.19, wpb=13788.6, bsz=481.8, num_updates=23200, lr=1.43839e-05, gnorm=0.944, train_wall=62, wall=15746
2021-01-02 02:14:20 | INFO | train_inner | epoch 056:    145 / 421 symm_mse=5.787, loss=3.519, nll_loss=1.112, ppl=2.16, wps=22564.2, ups=1.59, wpb=14199.1, bsz=519.6, num_updates=23300, lr=1.4353e-05, gnorm=0.856, train_wall=63, wall=15809
2021-01-02 02:15:22 | INFO | train_inner | epoch 056:    245 / 421 symm_mse=6.076, loss=3.565, nll_loss=1.126, ppl=2.18, wps=22576.1, ups=1.62, wpb=13964, bsz=488.3, num_updates=23400, lr=1.43223e-05, gnorm=0.927, train_wall=62, wall=15870
2021-01-02 02:16:24 | INFO | train_inner | epoch 056:    345 / 421 symm_mse=6.248, loss=3.592, nll_loss=1.137, ppl=2.2, wps=22185.5, ups=1.61, wpb=13808, bsz=486.3, num_updates=23500, lr=1.42918e-05, gnorm=0.952, train_wall=62, wall=15933
2021-01-02 02:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:17:30 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_mse 0 | loss 5.235 | nll_loss 3.722 | ppl 13.2 | bleu 22.32 | wps 5239.5 | wpb 10324.2 | bsz 375 | num_updates 23576 | best_bleu 22.69
2021-01-02 02:17:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:17:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 56 @ 23576 updates, score 22.32) (writing took 2.9751486387103796 seconds)
2021-01-02 02:17:33 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-02 02:17:33 | INFO | train | epoch 056 | symm_mse 6.089 | loss 3.569 | nll_loss 1.13 | ppl 2.19 | wps 20546.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23576 | lr 1.42687e-05 | gnorm 0.918 | train_wall 261 | wall 16001
2021-01-02 02:17:33 | INFO | fairseq.trainer | begin training epoch 57
2021-01-02 02:17:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:17:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:17:51 | INFO | train_inner | epoch 057:     24 / 421 symm_mse=6.101, loss=3.578, nll_loss=1.14, ppl=2.2, wps=16172.8, ups=1.16, wpb=13962, bsz=488.1, num_updates=23600, lr=1.42615e-05, gnorm=0.917, train_wall=61, wall=16019
2021-01-02 02:18:53 | INFO | train_inner | epoch 057:    124 / 421 symm_mse=5.931, loss=3.549, nll_loss=1.127, ppl=2.18, wps=22360.5, ups=1.6, wpb=13953.8, bsz=500.8, num_updates=23700, lr=1.42314e-05, gnorm=0.895, train_wall=62, wall=16081
2021-01-02 02:19:55 | INFO | train_inner | epoch 057:    224 / 421 symm_mse=6.292, loss=3.595, nll_loss=1.133, ppl=2.19, wps=22468.9, ups=1.61, wpb=13941.2, bsz=473.1, num_updates=23800, lr=1.42014e-05, gnorm=0.933, train_wall=62, wall=16143
2021-01-02 02:20:58 | INFO | train_inner | epoch 057:    324 / 421 symm_mse=5.987, loss=3.551, nll_loss=1.123, ppl=2.18, wps=22376.8, ups=1.59, wpb=14044.8, bsz=504.2, num_updates=23900, lr=1.41717e-05, gnorm=0.9, train_wall=63, wall=16206
2021-01-02 02:21:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:22:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:22:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:22:16 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_mse 0 | loss 5.236 | nll_loss 3.721 | ppl 13.18 | bleu 22.62 | wps 5913.9 | wpb 10324.2 | bsz 375 | num_updates 23997 | best_bleu 22.69
2021-01-02 02:22:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 57 @ 23997 updates, score 22.62) (writing took 2.900382161140442 seconds)
2021-01-02 02:22:19 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-02 02:22:19 | INFO | train | epoch 057 | symm_mse 6.074 | loss 3.567 | nll_loss 1.129 | ppl 2.19 | wps 20559.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23997 | lr 1.4143e-05 | gnorm 0.908 | train_wall 262 | wall 16287
2021-01-02 02:22:19 | INFO | fairseq.trainer | begin training epoch 58
2021-01-02 02:22:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:22:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:22:24 | INFO | train_inner | epoch 058:      3 / 421 symm_mse=6.107, loss=3.569, nll_loss=1.127, ppl=2.18, wps=16240.6, ups=1.16, wpb=13965.8, bsz=483.9, num_updates=24000, lr=1.41421e-05, gnorm=0.905, train_wall=63, wall=16292
2021-01-02 02:23:26 | INFO | train_inner | epoch 058:    103 / 421 symm_mse=5.865, loss=3.527, nll_loss=1.11, ppl=2.16, wps=22882, ups=1.62, wpb=14125.7, bsz=513.5, num_updates=24100, lr=1.41128e-05, gnorm=0.884, train_wall=62, wall=16354
2021-01-02 02:24:28 | INFO | train_inner | epoch 058:    203 / 421 symm_mse=6.264, loss=3.605, nll_loss=1.149, ppl=2.22, wps=22163, ups=1.6, wpb=13886.5, bsz=477.5, num_updates=24200, lr=1.40836e-05, gnorm=0.94, train_wall=62, wall=16417
2021-01-02 02:25:31 | INFO | train_inner | epoch 058:    303 / 421 symm_mse=6.155, loss=3.579, nll_loss=1.133, ppl=2.19, wps=22379.2, ups=1.6, wpb=13979.3, bsz=488.4, num_updates=24300, lr=1.40546e-05, gnorm=0.939, train_wall=62, wall=16479
2021-01-02 02:26:33 | INFO | train_inner | epoch 058:    403 / 421 symm_mse=6.085, loss=3.568, nll_loss=1.13, ppl=2.19, wps=22393.4, ups=1.61, wpb=13927.2, bsz=482.3, num_updates=24400, lr=1.40257e-05, gnorm=0.904, train_wall=62, wall=16541
2021-01-02 02:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:26:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:26:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:26:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:27:01 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_mse 0 | loss 5.236 | nll_loss 3.723 | ppl 13.2 | bleu 22.46 | wps 5929.1 | wpb 10324.2 | bsz 375 | num_updates 24418 | best_bleu 22.69
2021-01-02 02:27:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:27:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 58 @ 24418 updates, score 22.46) (writing took 2.869658412411809 seconds)
2021-01-02 02:27:04 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-02 02:27:04 | INFO | train | epoch 058 | symm_mse 6.06 | loss 3.565 | nll_loss 1.129 | ppl 2.19 | wps 20616.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 24418 | lr 1.40206e-05 | gnorm 0.913 | train_wall 261 | wall 16572
2021-01-02 02:27:04 | INFO | fairseq.trainer | begin training epoch 59
2021-01-02 02:27:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:27:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:27:58 | INFO | train_inner | epoch 059:     82 / 421 symm_mse=6.071, loss=3.569, nll_loss=1.132, ppl=2.19, wps=16410.6, ups=1.17, wpb=13975.5, bsz=496.1, num_updates=24500, lr=1.39971e-05, gnorm=0.917, train_wall=62, wall=16626
2021-01-02 02:29:01 | INFO | train_inner | epoch 059:    182 / 421 symm_mse=6.012, loss=3.556, nll_loss=1.126, ppl=2.18, wps=22191.2, ups=1.59, wpb=13932.4, bsz=486.6, num_updates=24600, lr=1.39686e-05, gnorm=0.896, train_wall=63, wall=16689
2021-01-02 02:30:03 | INFO | train_inner | epoch 059:    282 / 421 symm_mse=6.241, loss=3.594, nll_loss=1.139, ppl=2.2, wps=22383.2, ups=1.61, wpb=13891, bsz=476.6, num_updates=24700, lr=1.39403e-05, gnorm=0.954, train_wall=62, wall=16751
2021-01-02 02:31:05 | INFO | train_inner | epoch 059:    382 / 421 symm_mse=5.77, loss=3.516, nll_loss=1.111, ppl=2.16, wps=22719.2, ups=1.61, wpb=14152.9, bsz=520.6, num_updates=24800, lr=1.39122e-05, gnorm=0.861, train_wall=62, wall=16814
2021-01-02 02:31:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:31:46 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_mse 0 | loss 5.233 | nll_loss 3.719 | ppl 13.17 | bleu 22.43 | wps 5880.3 | wpb 10324.2 | bsz 375 | num_updates 24839 | best_bleu 22.69
2021-01-02 02:31:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:31:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 59 @ 24839 updates, score 22.43) (writing took 2.90006803907454 seconds)
2021-01-02 02:31:49 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-02 02:31:49 | INFO | train | epoch 059 | symm_mse 6.049 | loss 3.563 | nll_loss 1.129 | ppl 2.19 | wps 20600.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 24839 | lr 1.39012e-05 | gnorm 0.911 | train_wall 262 | wall 16858
2021-01-02 02:31:49 | INFO | fairseq.trainer | begin training epoch 60
2021-01-02 02:31:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:31:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:32:30 | INFO | train_inner | epoch 060:     61 / 421 symm_mse=5.976, loss=3.555, nll_loss=1.127, ppl=2.18, wps=16386.8, ups=1.18, wpb=13915, bsz=494.5, num_updates=24900, lr=1.38842e-05, gnorm=0.894, train_wall=62, wall=16898
2021-01-02 02:33:32 | INFO | train_inner | epoch 060:    161 / 421 symm_mse=6.099, loss=3.569, nll_loss=1.128, ppl=2.19, wps=22375.6, ups=1.6, wpb=13949.2, bsz=495.3, num_updates=25000, lr=1.38564e-05, gnorm=0.92, train_wall=62, wall=16961
2021-01-02 02:34:35 | INFO | train_inner | epoch 060:    261 / 421 symm_mse=6.185, loss=3.583, nll_loss=1.134, ppl=2.19, wps=22318.3, ups=1.6, wpb=13980, bsz=487, num_updates=25100, lr=1.38288e-05, gnorm=0.915, train_wall=62, wall=17023
2021-01-02 02:35:37 | INFO | train_inner | epoch 060:    361 / 421 symm_mse=5.877, loss=3.539, nll_loss=1.123, ppl=2.18, wps=22717.5, ups=1.62, wpb=14002.2, bsz=497.2, num_updates=25200, lr=1.38013e-05, gnorm=0.893, train_wall=61, wall=17085
2021-01-02 02:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:36:31 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_mse 0 | loss 5.234 | nll_loss 3.722 | ppl 13.19 | bleu 22.3 | wps 5860 | wpb 10324.2 | bsz 375 | num_updates 25260 | best_bleu 22.69
2021-01-02 02:36:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:36:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:36:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 60 @ 25260 updates, score 22.3) (writing took 2.9404144417494535 seconds)
2021-01-02 02:36:34 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-02 02:36:34 | INFO | train | epoch 060 | symm_mse 6.035 | loss 3.56 | nll_loss 1.127 | ppl 2.18 | wps 20640.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 25260 | lr 1.37849e-05 | gnorm 0.912 | train_wall 261 | wall 17143
2021-01-02 02:36:34 | INFO | fairseq.trainer | begin training epoch 61
2021-01-02 02:36:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:36:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:37:02 | INFO | train_inner | epoch 061:     40 / 421 symm_mse=5.93, loss=3.543, nll_loss=1.121, ppl=2.18, wps=16313.9, ups=1.17, wpb=13960.9, bsz=497.4, num_updates=25300, lr=1.3774e-05, gnorm=0.922, train_wall=62, wall=17171
2021-01-02 02:38:05 | INFO | train_inner | epoch 061:    140 / 421 symm_mse=5.979, loss=3.556, nll_loss=1.129, ppl=2.19, wps=22125.4, ups=1.59, wpb=13876.7, bsz=488.5, num_updates=25400, lr=1.37469e-05, gnorm=0.887, train_wall=63, wall=17233
2021-01-02 02:39:07 | INFO | train_inner | epoch 061:    240 / 421 symm_mse=6.081, loss=3.57, nll_loss=1.132, ppl=2.19, wps=22515.9, ups=1.62, wpb=13922.9, bsz=496.8, num_updates=25500, lr=1.37199e-05, gnorm=0.933, train_wall=62, wall=17295
2021-01-02 02:40:09 | INFO | train_inner | epoch 061:    340 / 421 symm_mse=6.161, loss=3.573, nll_loss=1.126, ppl=2.18, wps=22741.7, ups=1.61, wpb=14164, bsz=486.6, num_updates=25600, lr=1.36931e-05, gnorm=0.916, train_wall=62, wall=17358
2021-01-02 02:41:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:41:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:41:18 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_mse 0 | loss 5.234 | nll_loss 3.721 | ppl 13.19 | bleu 22.35 | wps 5300.2 | wpb 10324.2 | bsz 375 | num_updates 25681 | best_bleu 22.69
2021-01-02 02:41:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 61 @ 25681 updates, score 22.35) (writing took 2.8718447759747505 seconds)
2021-01-02 02:41:21 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-02 02:41:21 | INFO | train | epoch 061 | symm_mse 6.026 | loss 3.56 | nll_loss 1.127 | ppl 2.18 | wps 20522.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 25681 | lr 1.36715e-05 | gnorm 0.91 | train_wall 261 | wall 17429
2021-01-02 02:41:21 | INFO | fairseq.trainer | begin training epoch 62
2021-01-02 02:41:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:41:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:41:36 | INFO | train_inner | epoch 062:     19 / 421 symm_mse=5.96, loss=3.553, nll_loss=1.129, ppl=2.19, wps=15959.4, ups=1.15, wpb=13835, bsz=493.3, num_updates=25700, lr=1.36664e-05, gnorm=0.913, train_wall=62, wall=17444
2021-01-02 02:42:38 | INFO | train_inner | epoch 062:    119 / 421 symm_mse=6.139, loss=3.571, nll_loss=1.125, ppl=2.18, wps=22481.8, ups=1.61, wpb=13986.2, bsz=487.6, num_updates=25800, lr=1.36399e-05, gnorm=0.947, train_wall=62, wall=17506
2021-01-02 02:43:40 | INFO | train_inner | epoch 062:    219 / 421 symm_mse=5.969, loss=3.553, nll_loss=1.127, ppl=2.18, wps=22581.1, ups=1.61, wpb=14050.9, bsz=497.9, num_updates=25900, lr=1.36135e-05, gnorm=0.895, train_wall=62, wall=17569
2021-01-02 02:44:43 | INFO | train_inner | epoch 062:    319 / 421 symm_mse=5.898, loss=3.54, nll_loss=1.121, ppl=2.18, wps=22287.4, ups=1.6, wpb=13962.5, bsz=495.5, num_updates=26000, lr=1.35873e-05, gnorm=0.899, train_wall=62, wall=17631
2021-01-02 02:45:45 | INFO | train_inner | epoch 062:    419 / 421 symm_mse=6.061, loss=3.569, nll_loss=1.133, ppl=2.19, wps=22426.5, ups=1.6, wpb=13977.3, bsz=489.4, num_updates=26100, lr=1.35613e-05, gnorm=0.904, train_wall=62, wall=17694
2021-01-02 02:45:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:45:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:45:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:45:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:45:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:45:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:45:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:45:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:45:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:46:03 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_mse 0 | loss 5.234 | nll_loss 3.72 | ppl 13.18 | bleu 22.38 | wps 5868.6 | wpb 10324.2 | bsz 375 | num_updates 26102 | best_bleu 22.69
2021-01-02 02:46:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:46:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 62 @ 26102 updates, score 22.38) (writing took 2.8822261933237314 seconds)
2021-01-02 02:46:06 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-02 02:46:06 | INFO | train | epoch 062 | symm_mse 6.009 | loss 3.557 | nll_loss 1.127 | ppl 2.18 | wps 20615.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 26102 | lr 1.35608e-05 | gnorm 0.911 | train_wall 261 | wall 17715
2021-01-02 02:46:06 | INFO | fairseq.trainer | begin training epoch 63
2021-01-02 02:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:46:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:47:10 | INFO | train_inner | epoch 063:     98 / 421 symm_mse=6.175, loss=3.572, nll_loss=1.121, ppl=2.18, wps=16544, ups=1.17, wpb=14090.7, bsz=472.6, num_updates=26200, lr=1.35354e-05, gnorm=0.937, train_wall=62, wall=17779
2021-01-02 02:48:13 | INFO | train_inner | epoch 063:    198 / 421 symm_mse=5.995, loss=3.561, nll_loss=1.132, ppl=2.19, wps=22171.1, ups=1.6, wpb=13897.7, bsz=494.5, num_updates=26300, lr=1.35096e-05, gnorm=0.898, train_wall=62, wall=17841
2021-01-02 02:49:15 | INFO | train_inner | epoch 063:    298 / 421 symm_mse=5.705, loss=3.514, nll_loss=1.117, ppl=2.17, wps=22321, ups=1.6, wpb=13936.7, bsz=513.1, num_updates=26400, lr=1.3484e-05, gnorm=0.862, train_wall=62, wall=17904
2021-01-02 02:50:18 | INFO | train_inner | epoch 063:    398 / 421 symm_mse=6.032, loss=3.568, nll_loss=1.137, ppl=2.2, wps=22414, ups=1.61, wpb=13957.4, bsz=498.3, num_updates=26500, lr=1.34585e-05, gnorm=0.899, train_wall=62, wall=17966
2021-01-02 02:50:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:50:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:50:49 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_mse 0 | loss 5.232 | nll_loss 3.718 | ppl 13.16 | bleu 22.59 | wps 5885.9 | wpb 10324.2 | bsz 375 | num_updates 26523 | best_bleu 22.69
2021-01-02 02:50:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:50:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 63 @ 26523 updates, score 22.59) (writing took 2.857803026214242 seconds)
2021-01-02 02:50:52 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-02 02:50:52 | INFO | train | epoch 063 | symm_mse 5.992 | loss 3.555 | nll_loss 1.126 | ppl 2.18 | wps 20598.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 26523 | lr 1.34527e-05 | gnorm 0.899 | train_wall 262 | wall 18000
2021-01-02 02:50:52 | INFO | fairseq.trainer | begin training epoch 64
2021-01-02 02:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:51:43 | INFO | train_inner | epoch 064:     77 / 421 symm_mse=6.044, loss=3.551, nll_loss=1.114, ppl=2.17, wps=16414, ups=1.18, wpb=13916.8, bsz=482.8, num_updates=26600, lr=1.34332e-05, gnorm=0.905, train_wall=62, wall=18051
2021-01-02 02:52:45 | INFO | train_inner | epoch 064:    177 / 421 symm_mse=5.977, loss=3.557, nll_loss=1.132, ppl=2.19, wps=22442.5, ups=1.61, wpb=13975.7, bsz=487.3, num_updates=26700, lr=1.3408e-05, gnorm=0.889, train_wall=62, wall=18113
2021-01-02 02:53:48 | INFO | train_inner | epoch 064:    277 / 421 symm_mse=5.959, loss=3.55, nll_loss=1.125, ppl=2.18, wps=22467.5, ups=1.59, wpb=14128.6, bsz=497.4, num_updates=26800, lr=1.3383e-05, gnorm=0.908, train_wall=63, wall=18176
2021-01-02 02:54:51 | INFO | train_inner | epoch 064:    377 / 421 symm_mse=6.061, loss=3.57, nll_loss=1.135, ppl=2.2, wps=21928.9, ups=1.58, wpb=13883.7, bsz=493.9, num_updates=26900, lr=1.33581e-05, gnorm=0.915, train_wall=63, wall=18239
2021-01-02 02:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 02:55:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 02:55:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 02:55:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 02:55:37 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_mse 0 | loss 5.231 | nll_loss 3.716 | ppl 13.14 | bleu 22.44 | wps 5194.6 | wpb 10324.2 | bsz 375 | num_updates 26944 | best_bleu 22.69
2021-01-02 02:55:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 02:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:55:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 64 @ 26944 updates, score 22.44) (writing took 2.9517556317150593 seconds)
2021-01-02 02:55:40 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-02 02:55:40 | INFO | train | epoch 064 | symm_mse 5.98 | loss 3.553 | nll_loss 1.125 | ppl 2.18 | wps 20387.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 26944 | lr 1.33472e-05 | gnorm 0.9 | train_wall 263 | wall 18289
2021-01-02 02:55:40 | INFO | fairseq.trainer | begin training epoch 65
2021-01-02 02:55:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 02:55:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 02:56:18 | INFO | train_inner | epoch 065:     56 / 421 symm_mse=5.94, loss=3.549, nll_loss=1.125, ppl=2.18, wps=15879.3, ups=1.15, wpb=13770.9, bsz=483.4, num_updates=27000, lr=1.33333e-05, gnorm=0.896, train_wall=62, wall=18326
2021-01-02 02:57:20 | INFO | train_inner | epoch 065:    156 / 421 symm_mse=6.096, loss=3.566, nll_loss=1.126, ppl=2.18, wps=22468.2, ups=1.6, wpb=14040.4, bsz=480.6, num_updates=27100, lr=1.33087e-05, gnorm=0.911, train_wall=62, wall=18389
2021-01-02 02:58:23 | INFO | train_inner | epoch 065:    256 / 421 symm_mse=5.905, loss=3.545, nll_loss=1.126, ppl=2.18, wps=22335.5, ups=1.6, wpb=13927, bsz=516.4, num_updates=27200, lr=1.32842e-05, gnorm=0.906, train_wall=62, wall=18451
2021-01-02 02:59:26 | INFO | train_inner | epoch 065:    356 / 421 symm_mse=5.915, loss=3.546, nll_loss=1.125, ppl=2.18, wps=22303.6, ups=1.59, wpb=14052, bsz=486.2, num_updates=27300, lr=1.32599e-05, gnorm=0.874, train_wall=63, wall=18514
2021-01-02 03:00:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:00:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:00:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:00:23 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_mse 0 | loss 5.23 | nll_loss 3.715 | ppl 13.13 | bleu 22.55 | wps 5914.3 | wpb 10324.2 | bsz 375 | num_updates 27365 | best_bleu 22.69
2021-01-02 03:00:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:00:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 65 @ 27365 updates, score 22.55) (writing took 2.9203968662768602 seconds)
2021-01-02 03:00:26 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-02 03:00:26 | INFO | train | epoch 065 | symm_mse 5.963 | loss 3.552 | nll_loss 1.126 | ppl 2.18 | wps 20557.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27365 | lr 1.32441e-05 | gnorm 0.902 | train_wall 262 | wall 18575
2021-01-02 03:00:26 | INFO | fairseq.trainer | begin training epoch 66
2021-01-02 03:00:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:00:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:00:51 | INFO | train_inner | epoch 066:     35 / 421 symm_mse=5.888, loss=3.544, nll_loss=1.127, ppl=2.18, wps=16268.9, ups=1.17, wpb=13884.4, bsz=501.7, num_updates=27400, lr=1.32357e-05, gnorm=0.931, train_wall=62, wall=18599
2021-01-02 03:01:54 | INFO | train_inner | epoch 066:    135 / 421 symm_mse=5.948, loss=3.538, nll_loss=1.112, ppl=2.16, wps=22301.5, ups=1.59, wpb=14031.2, bsz=494.3, num_updates=27500, lr=1.32116e-05, gnorm=0.895, train_wall=63, wall=18662
2021-01-02 03:02:57 | INFO | train_inner | epoch 066:    235 / 421 symm_mse=5.891, loss=3.534, nll_loss=1.115, ppl=2.17, wps=22214.3, ups=1.59, wpb=14005.9, bsz=498.6, num_updates=27600, lr=1.31876e-05, gnorm=0.884, train_wall=63, wall=18725
2021-01-02 03:03:59 | INFO | train_inner | epoch 066:    335 / 421 symm_mse=6.014, loss=3.558, nll_loss=1.127, ppl=2.18, wps=22591.5, ups=1.6, wpb=14104.2, bsz=488.2, num_updates=27700, lr=1.31638e-05, gnorm=0.899, train_wall=62, wall=18788
2021-01-02 03:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:04:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:04:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:04:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:05:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:05:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:05:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:05:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:05:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:05:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:05:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:05:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:05:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:05:10 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_mse 0 | loss 5.231 | nll_loss 3.717 | ppl 13.15 | bleu 22.5 | wps 5845.4 | wpb 10324.2 | bsz 375 | num_updates 27786 | best_bleu 22.69
2021-01-02 03:05:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:05:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:05:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:05:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:05:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 66 @ 27786 updates, score 22.5) (writing took 2.91766107827425 seconds)
2021-01-02 03:05:13 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-02 03:05:13 | INFO | train | epoch 066 | symm_mse 5.958 | loss 3.55 | nll_loss 1.125 | ppl 2.18 | wps 20525.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27786 | lr 1.31434e-05 | gnorm 0.9 | train_wall 262 | wall 18861
2021-01-02 03:05:13 | INFO | fairseq.trainer | begin training epoch 67
2021-01-02 03:05:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:05:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:05:25 | INFO | train_inner | epoch 067:     14 / 421 symm_mse=5.973, loss=3.559, nll_loss=1.133, ppl=2.19, wps=16194.3, ups=1.17, wpb=13844.4, bsz=484.9, num_updates=27800, lr=1.31401e-05, gnorm=0.905, train_wall=62, wall=18873
2021-01-02 03:06:27 | INFO | train_inner | epoch 067:    114 / 421 symm_mse=6.179, loss=3.595, nll_loss=1.147, ppl=2.21, wps=22276, ups=1.6, wpb=13962.4, bsz=482.9, num_updates=27900, lr=1.31165e-05, gnorm=0.928, train_wall=62, wall=18936
2021-01-02 03:07:31 | INFO | train_inner | epoch 067:    214 / 421 symm_mse=5.834, loss=3.531, nll_loss=1.118, ppl=2.17, wps=22227.6, ups=1.58, wpb=14065.6, bsz=497.8, num_updates=28000, lr=1.30931e-05, gnorm=0.899, train_wall=63, wall=18999
2021-01-02 03:08:34 | INFO | train_inner | epoch 067:    314 / 421 symm_mse=5.94, loss=3.539, nll_loss=1.115, ppl=2.17, wps=22322.7, ups=1.59, wpb=14011.8, bsz=495.5, num_updates=28100, lr=1.30698e-05, gnorm=0.913, train_wall=63, wall=19062
2021-01-02 03:09:36 | INFO | train_inner | epoch 067:    414 / 421 symm_mse=5.819, loss=3.533, nll_loss=1.123, ppl=2.18, wps=22108.3, ups=1.6, wpb=13845.2, bsz=495.4, num_updates=28200, lr=1.30466e-05, gnorm=0.875, train_wall=62, wall=19125
2021-01-02 03:09:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:09:57 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_mse 0 | loss 5.23 | nll_loss 3.716 | ppl 13.14 | bleu 22.39 | wps 5856.2 | wpb 10324.2 | bsz 375 | num_updates 28207 | best_bleu 22.69
2021-01-02 03:09:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:09:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:09:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:10:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:10:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:10:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:10:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 67 @ 28207 updates, score 22.39) (writing took 2.8709782995283604 seconds)
2021-01-02 03:10:00 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-02 03:10:00 | INFO | train | epoch 067 | symm_mse 5.941 | loss 3.548 | nll_loss 1.124 | ppl 2.18 | wps 20451.4 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 28207 | lr 1.30449e-05 | gnorm 0.905 | train_wall 264 | wall 19149
2021-01-02 03:10:00 | INFO | fairseq.trainer | begin training epoch 68
2021-01-02 03:10:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:10:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:11:01 | INFO | train_inner | epoch 068:     93 / 421 symm_mse=5.754, loss=3.511, nll_loss=1.106, ppl=2.15, wps=16320.6, ups=1.17, wpb=13895.4, bsz=502.8, num_updates=28300, lr=1.30235e-05, gnorm=0.883, train_wall=62, wall=19210
2021-01-02 03:12:04 | INFO | train_inner | epoch 068:    193 / 421 symm_mse=5.923, loss=3.543, nll_loss=1.121, ppl=2.18, wps=22176, ups=1.59, wpb=13941.5, bsz=498.7, num_updates=28400, lr=1.30005e-05, gnorm=0.918, train_wall=63, wall=19273
2021-01-02 03:13:07 | INFO | train_inner | epoch 068:    293 / 421 symm_mse=6.069, loss=3.57, nll_loss=1.133, ppl=2.19, wps=22400.6, ups=1.59, wpb=14061.8, bsz=483.7, num_updates=28500, lr=1.29777e-05, gnorm=0.909, train_wall=63, wall=19335
2021-01-02 03:14:09 | INFO | train_inner | epoch 068:    393 / 421 symm_mse=5.989, loss=3.559, nll_loss=1.131, ppl=2.19, wps=22509.8, ups=1.61, wpb=14016.9, bsz=485.7, num_updates=28600, lr=1.2955e-05, gnorm=0.905, train_wall=62, wall=19398
2021-01-02 03:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:14:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:14:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:14:43 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_mse 0 | loss 5.231 | nll_loss 3.716 | ppl 13.15 | bleu 22.46 | wps 5885.1 | wpb 10324.2 | bsz 375 | num_updates 28628 | best_bleu 22.69
2021-01-02 03:14:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:14:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:14:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 68 @ 28628 updates, score 22.46) (writing took 2.9382755886763334 seconds)
2021-01-02 03:14:46 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-02 03:14:46 | INFO | train | epoch 068 | symm_mse 5.936 | loss 3.547 | nll_loss 1.124 | ppl 2.18 | wps 20557.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 28628 | lr 1.29487e-05 | gnorm 0.903 | train_wall 262 | wall 19435
2021-01-02 03:14:46 | INFO | fairseq.trainer | begin training epoch 69
2021-01-02 03:14:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:14:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:15:35 | INFO | train_inner | epoch 069:     72 / 421 symm_mse=5.931, loss=3.55, nll_loss=1.129, ppl=2.19, wps=16238.3, ups=1.17, wpb=13856.8, bsz=488.3, num_updates=28700, lr=1.29324e-05, gnorm=0.911, train_wall=62, wall=19483
2021-01-02 03:16:37 | INFO | train_inner | epoch 069:    172 / 421 symm_mse=5.878, loss=3.538, nll_loss=1.121, ppl=2.18, wps=22473.6, ups=1.61, wpb=13998.6, bsz=496.1, num_updates=28800, lr=1.29099e-05, gnorm=0.899, train_wall=62, wall=19545
2021-01-02 03:17:40 | INFO | train_inner | epoch 069:    272 / 421 symm_mse=5.847, loss=3.529, nll_loss=1.116, ppl=2.17, wps=22590.6, ups=1.6, wpb=14161.2, bsz=500.7, num_updates=28900, lr=1.28876e-05, gnorm=0.895, train_wall=62, wall=19608
2021-01-02 03:18:42 | INFO | train_inner | epoch 069:    372 / 421 symm_mse=5.926, loss=3.549, nll_loss=1.129, ppl=2.19, wps=22205.4, ups=1.6, wpb=13844.5, bsz=482.7, num_updates=29000, lr=1.28654e-05, gnorm=0.884, train_wall=62, wall=19670
2021-01-02 03:19:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:19:31 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_mse 0 | loss 5.227 | nll_loss 3.712 | ppl 13.11 | bleu 22.56 | wps 5164.9 | wpb 10324.2 | bsz 375 | num_updates 29049 | best_bleu 22.69
2021-01-02 03:19:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 69 @ 29049 updates, score 22.56) (writing took 2.879014765843749 seconds)
2021-01-02 03:19:34 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-02 03:19:34 | INFO | train | epoch 069 | symm_mse 5.921 | loss 3.545 | nll_loss 1.125 | ppl 2.18 | wps 20444.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 29049 | lr 1.28545e-05 | gnorm 0.902 | train_wall 262 | wall 19723
2021-01-02 03:19:34 | INFO | fairseq.trainer | begin training epoch 70
2021-01-02 03:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:19:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:19:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:20:09 | INFO | train_inner | epoch 070:     51 / 421 symm_mse=5.924, loss=3.538, nll_loss=1.116, ppl=2.17, wps=16028.3, ups=1.15, wpb=13940.7, bsz=497.4, num_updates=29100, lr=1.28432e-05, gnorm=0.912, train_wall=62, wall=19757
2021-01-02 03:21:11 | INFO | train_inner | epoch 070:    151 / 421 symm_mse=5.948, loss=3.546, nll_loss=1.121, ppl=2.18, wps=22498.4, ups=1.6, wpb=14040.9, bsz=491.7, num_updates=29200, lr=1.28212e-05, gnorm=0.899, train_wall=62, wall=19820
2021-01-02 03:22:14 | INFO | train_inner | epoch 070:    251 / 421 symm_mse=5.975, loss=3.562, nll_loss=1.136, ppl=2.2, wps=22365.1, ups=1.6, wpb=13968.4, bsz=492.8, num_updates=29300, lr=1.27993e-05, gnorm=0.908, train_wall=62, wall=19882
2021-01-02 03:23:17 | INFO | train_inner | epoch 070:    351 / 421 symm_mse=5.814, loss=3.52, nll_loss=1.109, ppl=2.16, wps=22329.8, ups=1.59, wpb=14021.5, bsz=494.1, num_updates=29400, lr=1.27775e-05, gnorm=0.873, train_wall=63, wall=19945
2021-01-02 03:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:24:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:24:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:24:17 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_mse 0 | loss 5.224 | nll_loss 3.712 | ppl 13.1 | bleu 22.42 | wps 5869.5 | wpb 10324.2 | bsz 375 | num_updates 29470 | best_bleu 22.69
2021-01-02 03:24:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 70 @ 29470 updates, score 22.42) (writing took 2.977942943572998 seconds)
2021-01-02 03:24:20 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-02 03:24:20 | INFO | train | epoch 070 | symm_mse 5.909 | loss 3.543 | nll_loss 1.123 | ppl 2.18 | wps 20550.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 29470 | lr 1.27623e-05 | gnorm 0.9 | train_wall 262 | wall 20009
2021-01-02 03:24:20 | INFO | fairseq.trainer | begin training epoch 71
2021-01-02 03:24:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:24:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:24:42 | INFO | train_inner | epoch 071:     30 / 421 symm_mse=6.099, loss=3.586, nll_loss=1.149, ppl=2.22, wps=16146.8, ups=1.17, wpb=13792.5, bsz=475.6, num_updates=29500, lr=1.27559e-05, gnorm=0.933, train_wall=62, wall=20030
2021-01-02 03:25:45 | INFO | train_inner | epoch 071:    130 / 421 symm_mse=5.853, loss=3.536, nll_loss=1.124, ppl=2.18, wps=22429.4, ups=1.6, wpb=14042.9, bsz=493.1, num_updates=29600, lr=1.27343e-05, gnorm=0.886, train_wall=62, wall=20093
2021-01-02 03:26:47 | INFO | train_inner | epoch 071:    230 / 421 symm_mse=5.818, loss=3.535, nll_loss=1.126, ppl=2.18, wps=22222.8, ups=1.59, wpb=13948, bsz=496.8, num_updates=29700, lr=1.27128e-05, gnorm=0.885, train_wall=63, wall=20156
2021-01-02 03:27:50 | INFO | train_inner | epoch 071:    330 / 421 symm_mse=5.889, loss=3.542, nll_loss=1.124, ppl=2.18, wps=22597.4, ups=1.61, wpb=14070.8, bsz=493.8, num_updates=29800, lr=1.26915e-05, gnorm=0.881, train_wall=62, wall=20218
2021-01-02 03:28:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:28:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:28:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:28:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:28:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:28:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:28:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:28:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:28:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:28:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:28:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:28:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:29:03 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_mse 0 | loss 5.231 | nll_loss 3.716 | ppl 13.14 | bleu 22.53 | wps 5907.4 | wpb 10324.2 | bsz 375 | num_updates 29891 | best_bleu 22.69
2021-01-02 03:29:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:29:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:29:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:29:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:29:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:29:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:29:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:29:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 71 @ 29891 updates, score 22.53) (writing took 2.867797262966633 seconds)
2021-01-02 03:29:06 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-02 03:29:06 | INFO | train | epoch 071 | symm_mse 5.904 | loss 3.542 | nll_loss 1.124 | ppl 2.18 | wps 20576.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 29891 | lr 1.26722e-05 | gnorm 0.894 | train_wall 262 | wall 20295
2021-01-02 03:29:06 | INFO | fairseq.trainer | begin training epoch 72
2021-01-02 03:29:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:29:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:29:15 | INFO | train_inner | epoch 072:      9 / 421 symm_mse=5.945, loss=3.536, nll_loss=1.111, ppl=2.16, wps=16199.4, ups=1.17, wpb=13812, bsz=497.7, num_updates=29900, lr=1.26702e-05, gnorm=0.92, train_wall=62, wall=20303
2021-01-02 03:30:17 | INFO | train_inner | epoch 072:    109 / 421 symm_mse=5.822, loss=3.534, nll_loss=1.123, ppl=2.18, wps=22378.2, ups=1.61, wpb=13899.2, bsz=500.5, num_updates=30000, lr=1.26491e-05, gnorm=0.89, train_wall=62, wall=20365
2021-01-02 03:31:20 | INFO | train_inner | epoch 072:    209 / 421 symm_mse=6.028, loss=3.57, nll_loss=1.138, ppl=2.2, wps=22313.6, ups=1.59, wpb=14056.5, bsz=479, num_updates=30100, lr=1.26281e-05, gnorm=0.909, train_wall=63, wall=20428
2021-01-02 03:32:23 | INFO | train_inner | epoch 072:    309 / 421 symm_mse=5.849, loss=3.519, nll_loss=1.104, ppl=2.15, wps=22439.7, ups=1.59, wpb=14090.8, bsz=503.7, num_updates=30200, lr=1.26072e-05, gnorm=0.889, train_wall=63, wall=20491
2021-01-02 03:33:25 | INFO | train_inner | epoch 072:    409 / 421 symm_mse=5.878, loss=3.536, nll_loss=1.119, ppl=2.17, wps=22297.5, ups=1.6, wpb=13971.2, bsz=488.9, num_updates=30300, lr=1.25863e-05, gnorm=0.922, train_wall=62, wall=20554
2021-01-02 03:33:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:33:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:33:50 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_mse 0 | loss 5.229 | nll_loss 3.714 | ppl 13.12 | bleu 22.61 | wps 5932.2 | wpb 10324.2 | bsz 375 | num_updates 30312 | best_bleu 22.69
2021-01-02 03:33:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:33:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:33:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 72 @ 30312 updates, score 22.61) (writing took 3.0205498691648245 seconds)
2021-01-02 03:33:53 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-02 03:33:53 | INFO | train | epoch 072 | symm_mse 5.887 | loss 3.539 | nll_loss 1.122 | ppl 2.18 | wps 20531.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 30312 | lr 1.25838e-05 | gnorm 0.902 | train_wall 262 | wall 20581
2021-01-02 03:33:53 | INFO | fairseq.trainer | begin training epoch 73
2021-01-02 03:33:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:34:50 | INFO | train_inner | epoch 073:     88 / 421 symm_mse=5.886, loss=3.544, nll_loss=1.127, ppl=2.18, wps=16239.3, ups=1.18, wpb=13804.3, bsz=486.8, num_updates=30400, lr=1.25656e-05, gnorm=0.882, train_wall=62, wall=20639
2021-01-02 03:35:53 | INFO | train_inner | epoch 073:    188 / 421 symm_mse=5.945, loss=3.548, nll_loss=1.124, ppl=2.18, wps=22239.5, ups=1.6, wpb=13883.7, bsz=482.3, num_updates=30500, lr=1.2545e-05, gnorm=0.902, train_wall=62, wall=20701
2021-01-02 03:36:56 | INFO | train_inner | epoch 073:    288 / 421 symm_mse=5.672, loss=3.493, nll_loss=1.099, ppl=2.14, wps=22502.1, ups=1.6, wpb=14107.1, bsz=514.2, num_updates=30600, lr=1.25245e-05, gnorm=0.864, train_wall=62, wall=20764
2021-01-02 03:37:58 | INFO | train_inner | epoch 073:    388 / 421 symm_mse=5.922, loss=3.55, nll_loss=1.129, ppl=2.19, wps=22423.9, ups=1.6, wpb=14021.4, bsz=489.6, num_updates=30700, lr=1.25041e-05, gnorm=0.886, train_wall=62, wall=20826
2021-01-02 03:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:38:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:38:35 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_mse 0 | loss 5.225 | nll_loss 3.712 | ppl 13.1 | bleu 22.62 | wps 5955.2 | wpb 10324.2 | bsz 375 | num_updates 30733 | best_bleu 22.69
2021-01-02 03:38:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:38:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:38:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 73 @ 30733 updates, score 22.62) (writing took 2.762493845075369 seconds)
2021-01-02 03:38:38 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-02 03:38:38 | INFO | train | epoch 073 | symm_mse 5.871 | loss 3.537 | nll_loss 1.122 | ppl 2.18 | wps 20596.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 30733 | lr 1.24974e-05 | gnorm 0.886 | train_wall 262 | wall 20867
2021-01-02 03:38:38 | INFO | fairseq.trainer | begin training epoch 74
2021-01-02 03:38:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:38:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:39:23 | INFO | train_inner | epoch 074:     67 / 421 symm_mse=5.905, loss=3.549, nll_loss=1.131, ppl=2.19, wps=16459.8, ups=1.18, wpb=13947.4, bsz=487, num_updates=30800, lr=1.24838e-05, gnorm=0.89, train_wall=62, wall=20911
2021-01-02 03:40:26 | INFO | train_inner | epoch 074:    167 / 421 symm_mse=5.772, loss=3.514, nll_loss=1.107, ppl=2.15, wps=22352.7, ups=1.58, wpb=14112, bsz=503.8, num_updates=30900, lr=1.24635e-05, gnorm=0.886, train_wall=63, wall=20974
2021-01-02 03:41:28 | INFO | train_inner | epoch 074:    267 / 421 symm_mse=5.981, loss=3.564, nll_loss=1.137, ppl=2.2, wps=22361.4, ups=1.6, wpb=13972.7, bsz=481.2, num_updates=31000, lr=1.24434e-05, gnorm=0.894, train_wall=62, wall=21037
2021-01-02 03:42:31 | INFO | train_inner | epoch 074:    367 / 421 symm_mse=5.898, loss=3.547, nll_loss=1.129, ppl=2.19, wps=22036.6, ups=1.59, wpb=13875.6, bsz=488.2, num_updates=31100, lr=1.24234e-05, gnorm=0.892, train_wall=63, wall=21100
2021-01-02 03:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:43:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:43:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:43:22 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_mse 0 | loss 5.228 | nll_loss 3.711 | ppl 13.1 | bleu 22.57 | wps 6014.7 | wpb 10324.2 | bsz 375 | num_updates 31154 | best_bleu 22.69
2021-01-02 03:43:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:43:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 74 @ 31154 updates, score 22.57) (writing took 2.9017398357391357 seconds)
2021-01-02 03:43:24 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-02 03:43:24 | INFO | train | epoch 074 | symm_mse 5.865 | loss 3.537 | nll_loss 1.122 | ppl 2.18 | wps 20534.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 31154 | lr 1.24126e-05 | gnorm 0.89 | train_wall 263 | wall 21153
2021-01-02 03:43:24 | INFO | fairseq.trainer | begin training epoch 75
2021-01-02 03:43:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:43:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:43:56 | INFO | train_inner | epoch 075:     46 / 421 symm_mse=5.72, loss=3.506, nll_loss=1.105, ppl=2.15, wps=16365.9, ups=1.18, wpb=13850.8, bsz=498.8, num_updates=31200, lr=1.24035e-05, gnorm=0.886, train_wall=62, wall=21184
2021-01-02 03:44:59 | INFO | train_inner | epoch 075:    146 / 421 symm_mse=5.85, loss=3.524, nll_loss=1.11, ppl=2.16, wps=22508.6, ups=1.59, wpb=14149.4, bsz=504.3, num_updates=31300, lr=1.23836e-05, gnorm=0.891, train_wall=63, wall=21247
2021-01-02 03:46:01 | INFO | train_inner | epoch 075:    246 / 421 symm_mse=6.002, loss=3.564, nll_loss=1.134, ppl=2.2, wps=22374.3, ups=1.6, wpb=13988.6, bsz=487.1, num_updates=31400, lr=1.23639e-05, gnorm=0.909, train_wall=62, wall=21310
2021-01-02 03:47:04 | INFO | train_inner | epoch 075:    346 / 421 symm_mse=5.879, loss=3.539, nll_loss=1.122, ppl=2.18, wps=22030.3, ups=1.59, wpb=13842.7, bsz=481.1, num_updates=31500, lr=1.23443e-05, gnorm=0.898, train_wall=63, wall=21373
2021-01-02 03:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:47:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:47:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:47:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:47:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:47:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:47:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:47:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:47:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:47:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:47:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:47:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:47:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:48:07 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_mse 0 | loss 5.225 | nll_loss 3.711 | ppl 13.1 | bleu 22.46 | wps 5935.6 | wpb 10324.2 | bsz 375 | num_updates 31575 | best_bleu 22.69
2021-01-02 03:48:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:48:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:48:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:48:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:48:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:48:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:48:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:48:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 75 @ 31575 updates, score 22.46) (writing took 2.9531910978257656 seconds)
2021-01-02 03:48:10 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-02 03:48:10 | INFO | train | epoch 075 | symm_mse 5.862 | loss 3.535 | nll_loss 1.12 | ppl 2.17 | wps 20566.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 31575 | lr 1.23296e-05 | gnorm 0.897 | train_wall 262 | wall 21439
2021-01-02 03:48:10 | INFO | fairseq.trainer | begin training epoch 76
2021-01-02 03:48:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:48:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:48:29 | INFO | train_inner | epoch 076:     25 / 421 symm_mse=5.689, loss=3.512, nll_loss=1.117, ppl=2.17, wps=16370.6, ups=1.18, wpb=13898.5, bsz=505.8, num_updates=31600, lr=1.23247e-05, gnorm=0.89, train_wall=62, wall=21458
2021-01-02 03:49:31 | INFO | train_inner | epoch 076:    125 / 421 symm_mse=5.661, loss=3.507, nll_loss=1.114, ppl=2.16, wps=22602.3, ups=1.6, wpb=14096.4, bsz=515.1, num_updates=31700, lr=1.23053e-05, gnorm=0.864, train_wall=62, wall=21520
2021-01-02 03:50:34 | INFO | train_inner | epoch 076:    225 / 421 symm_mse=5.903, loss=3.537, nll_loss=1.117, ppl=2.17, wps=22260.1, ups=1.59, wpb=13973.1, bsz=492.4, num_updates=31800, lr=1.22859e-05, gnorm=0.893, train_wall=63, wall=21583
2021-01-02 03:51:37 | INFO | train_inner | epoch 076:    325 / 421 symm_mse=6.161, loss=3.58, nll_loss=1.132, ppl=2.19, wps=22244.5, ups=1.59, wpb=13963, bsz=461.8, num_updates=31900, lr=1.22666e-05, gnorm=0.925, train_wall=63, wall=21645
2021-01-02 03:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:52:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:52:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:52:54 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_mse 0 | loss 5.224 | nll_loss 3.709 | ppl 13.08 | bleu 22.56 | wps 5884.1 | wpb 10324.2 | bsz 375 | num_updates 31996 | best_bleu 22.69
2021-01-02 03:52:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:52:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:52:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 76 @ 31996 updates, score 22.56) (writing took 2.9392998963594437 seconds)
2021-01-02 03:52:57 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-02 03:52:57 | INFO | train | epoch 076 | symm_mse 5.848 | loss 3.534 | nll_loss 1.121 | ppl 2.18 | wps 20523.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 31996 | lr 1.22482e-05 | gnorm 0.887 | train_wall 263 | wall 21725
2021-01-02 03:52:57 | INFO | fairseq.trainer | begin training epoch 77
2021-01-02 03:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:52:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:53:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:53:03 | INFO | train_inner | epoch 077:      4 / 421 symm_mse=5.867, loss=3.545, nll_loss=1.132, ppl=2.19, wps=16147.5, ups=1.17, wpb=13844.7, bsz=485, num_updates=32000, lr=1.22474e-05, gnorm=0.885, train_wall=62, wall=21731
2021-01-02 03:54:05 | INFO | train_inner | epoch 077:    104 / 421 symm_mse=5.668, loss=3.502, nll_loss=1.107, ppl=2.15, wps=22556.4, ups=1.61, wpb=13985.2, bsz=500.6, num_updates=32100, lr=1.22284e-05, gnorm=0.883, train_wall=62, wall=21793
2021-01-02 03:55:08 | INFO | train_inner | epoch 077:    204 / 421 symm_mse=5.888, loss=3.536, nll_loss=1.118, ppl=2.17, wps=22306, ups=1.59, wpb=14043.8, bsz=486.1, num_updates=32200, lr=1.22094e-05, gnorm=0.877, train_wall=63, wall=21856
2021-01-02 03:56:11 | INFO | train_inner | epoch 077:    304 / 421 symm_mse=5.953, loss=3.556, nll_loss=1.132, ppl=2.19, wps=21985.1, ups=1.58, wpb=13904.1, bsz=480.2, num_updates=32300, lr=1.21904e-05, gnorm=0.914, train_wall=63, wall=21919
2021-01-02 03:57:14 | INFO | train_inner | epoch 077:    404 / 421 symm_mse=5.772, loss=3.53, nll_loss=1.127, ppl=2.18, wps=22390.7, ups=1.59, wpb=14050.1, bsz=507.9, num_updates=32400, lr=1.21716e-05, gnorm=0.856, train_wall=63, wall=21982
2021-01-02 03:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 03:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 03:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 03:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 03:57:41 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_mse 0 | loss 5.224 | nll_loss 3.708 | ppl 13.07 | bleu 22.6 | wps 5885.1 | wpb 10324.2 | bsz 375 | num_updates 32417 | best_bleu 22.69
2021-01-02 03:57:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 03:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:57:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 77 @ 32417 updates, score 22.6) (writing took 2.870095567777753 seconds)
2021-01-02 03:57:44 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-02 03:57:44 | INFO | train | epoch 077 | symm_mse 5.832 | loss 3.532 | nll_loss 1.121 | ppl 2.17 | wps 20486.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 32417 | lr 1.21684e-05 | gnorm 0.887 | train_wall 263 | wall 22013
2021-01-02 03:57:44 | INFO | fairseq.trainer | begin training epoch 78
2021-01-02 03:57:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 03:57:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 03:58:39 | INFO | train_inner | epoch 078:     83 / 421 symm_mse=5.878, loss=3.543, nll_loss=1.127, ppl=2.18, wps=16138.7, ups=1.17, wpb=13787.4, bsz=490.6, num_updates=32500, lr=1.21529e-05, gnorm=0.887, train_wall=62, wall=22068
2021-01-02 03:59:42 | INFO | train_inner | epoch 078:    183 / 421 symm_mse=5.978, loss=3.553, nll_loss=1.125, ppl=2.18, wps=22362.8, ups=1.59, wpb=14097.3, bsz=477.3, num_updates=32600, lr=1.21342e-05, gnorm=0.905, train_wall=63, wall=22131
2021-01-02 04:00:45 | INFO | train_inner | epoch 078:    283 / 421 symm_mse=5.715, loss=3.503, nll_loss=1.104, ppl=2.15, wps=22225.7, ups=1.59, wpb=13992.1, bsz=506.1, num_updates=32700, lr=1.21157e-05, gnorm=0.885, train_wall=63, wall=22194
2021-01-02 04:01:48 | INFO | train_inner | epoch 078:    383 / 421 symm_mse=5.66, loss=3.514, nll_loss=1.122, ppl=2.18, wps=22466, ups=1.6, wpb=14019.5, bsz=506.6, num_updates=32800, lr=1.20972e-05, gnorm=0.865, train_wall=62, wall=22256
2021-01-02 04:02:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:02:28 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_mse 0 | loss 5.225 | nll_loss 3.71 | ppl 13.08 | bleu 22.65 | wps 5953.2 | wpb 10324.2 | bsz 375 | num_updates 32838 | best_bleu 22.69
2021-01-02 04:02:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:02:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:02:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 78 @ 32838 updates, score 22.65) (writing took 2.9675823654979467 seconds)
2021-01-02 04:02:31 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-02 04:02:31 | INFO | train | epoch 078 | symm_mse 5.829 | loss 3.531 | nll_loss 1.12 | ppl 2.17 | wps 20481.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 32838 | lr 1.20902e-05 | gnorm 0.887 | train_wall 263 | wall 22300
2021-01-02 04:02:31 | INFO | fairseq.trainer | begin training epoch 79
2021-01-02 04:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:03:12 | INFO | train_inner | epoch 079:     62 / 421 symm_mse=6.038, loss=3.561, nll_loss=1.127, ppl=2.18, wps=16251.3, ups=1.18, wpb=13781.3, bsz=469.8, num_updates=32900, lr=1.20788e-05, gnorm=0.922, train_wall=62, wall=22341
2021-01-02 04:04:15 | INFO | train_inner | epoch 079:    162 / 421 symm_mse=5.774, loss=3.521, nll_loss=1.115, ppl=2.17, wps=22358.1, ups=1.59, wpb=14053.4, bsz=495.8, num_updates=33000, lr=1.20605e-05, gnorm=0.878, train_wall=63, wall=22404
2021-01-02 04:05:18 | INFO | train_inner | epoch 079:    262 / 421 symm_mse=5.742, loss=3.515, nll_loss=1.114, ppl=2.16, wps=22244.6, ups=1.58, wpb=14036.6, bsz=502, num_updates=33100, lr=1.20422e-05, gnorm=0.877, train_wall=63, wall=22467
2021-01-02 04:06:21 | INFO | train_inner | epoch 079:    362 / 421 symm_mse=5.875, loss=3.542, nll_loss=1.127, ppl=2.18, wps=22049.6, ups=1.58, wpb=13914.4, bsz=489, num_updates=33200, lr=1.20241e-05, gnorm=0.9, train_wall=63, wall=22530
2021-01-02 04:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:07:15 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_mse 0 | loss 5.224 | nll_loss 3.709 | ppl 13.07 | bleu 22.49 | wps 5854.8 | wpb 10324.2 | bsz 375 | num_updates 33259 | best_bleu 22.69
2021-01-02 04:07:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 79 @ 33259 updates, score 22.49) (writing took 2.8687444422394037 seconds)
2021-01-02 04:07:18 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-02 04:07:18 | INFO | train | epoch 079 | symm_mse 5.825 | loss 3.531 | nll_loss 1.12 | ppl 2.17 | wps 20496.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 33259 | lr 1.20134e-05 | gnorm 0.894 | train_wall 263 | wall 22587
2021-01-02 04:07:18 | INFO | fairseq.trainer | begin training epoch 80
2021-01-02 04:07:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:07:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:07:47 | INFO | train_inner | epoch 080:     41 / 421 symm_mse=5.777, loss=3.522, nll_loss=1.117, ppl=2.17, wps=16325.6, ups=1.17, wpb=13895.2, bsz=496.2, num_updates=33300, lr=1.2006e-05, gnorm=0.897, train_wall=62, wall=22615
2021-01-02 04:08:50 | INFO | train_inner | epoch 080:    141 / 421 symm_mse=5.811, loss=3.526, nll_loss=1.117, ppl=2.17, wps=22237.3, ups=1.59, wpb=14015.3, bsz=502.4, num_updates=33400, lr=1.1988e-05, gnorm=0.877, train_wall=63, wall=22678
2021-01-02 04:09:53 | INFO | train_inner | epoch 080:    241 / 421 symm_mse=5.8, loss=3.532, nll_loss=1.125, ppl=2.18, wps=22383.7, ups=1.59, wpb=14112, bsz=494.1, num_updates=33500, lr=1.19701e-05, gnorm=0.878, train_wall=63, wall=22741
2021-01-02 04:10:56 | INFO | train_inner | epoch 080:    341 / 421 symm_mse=5.869, loss=3.536, nll_loss=1.12, ppl=2.17, wps=22169.8, ups=1.59, wpb=13952.8, bsz=476.2, num_updates=33600, lr=1.19523e-05, gnorm=0.879, train_wall=63, wall=22804
2021-01-02 04:11:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:12:02 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_mse 0 | loss 5.223 | nll_loss 3.707 | ppl 13.06 | bleu 22.59 | wps 5912.7 | wpb 10324.2 | bsz 375 | num_updates 33680 | best_bleu 22.69
2021-01-02 04:12:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:12:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 80 @ 33680 updates, score 22.59) (writing took 2.8787535205483437 seconds)
2021-01-02 04:12:05 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-02 04:12:05 | INFO | train | epoch 080 | symm_mse 5.805 | loss 3.528 | nll_loss 1.12 | ppl 2.17 | wps 20486.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 33680 | lr 1.19381e-05 | gnorm 0.878 | train_wall 263 | wall 22874
2021-01-02 04:12:05 | INFO | fairseq.trainer | begin training epoch 81
2021-01-02 04:12:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:12:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:12:21 | INFO | train_inner | epoch 081:     20 / 421 symm_mse=5.818, loss=3.531, nll_loss=1.121, ppl=2.18, wps=16271, ups=1.17, wpb=13861.8, bsz=491.9, num_updates=33700, lr=1.19345e-05, gnorm=0.893, train_wall=62, wall=22889
2021-01-02 04:13:23 | INFO | train_inner | epoch 081:    120 / 421 symm_mse=5.686, loss=3.507, nll_loss=1.11, ppl=2.16, wps=22511.7, ups=1.61, wpb=13969.8, bsz=502, num_updates=33800, lr=1.19169e-05, gnorm=0.87, train_wall=62, wall=22951
2021-01-02 04:14:26 | INFO | train_inner | epoch 081:    220 / 421 symm_mse=5.766, loss=3.525, nll_loss=1.12, ppl=2.17, wps=22038.8, ups=1.59, wpb=13903.9, bsz=499.9, num_updates=33900, lr=1.18993e-05, gnorm=0.897, train_wall=63, wall=23014
2021-01-02 04:15:29 | INFO | train_inner | epoch 081:    320 / 421 symm_mse=5.835, loss=3.531, nll_loss=1.12, ppl=2.17, wps=22449.5, ups=1.59, wpb=14120.4, bsz=487, num_updates=34000, lr=1.18818e-05, gnorm=0.88, train_wall=63, wall=23077
2021-01-02 04:16:32 | INFO | train_inner | epoch 081:    420 / 421 symm_mse=5.825, loss=3.531, nll_loss=1.12, ppl=2.17, wps=22041.8, ups=1.58, wpb=13969.7, bsz=489.8, num_updates=34100, lr=1.18643e-05, gnorm=0.882, train_wall=63, wall=23141
2021-01-02 04:16:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:16:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:16:49 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_mse 0 | loss 5.223 | nll_loss 3.708 | ppl 13.07 | bleu 22.69 | wps 5982.2 | wpb 10324.2 | bsz 375 | num_updates 34101 | best_bleu 22.69
2021-01-02 04:16:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:16:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 81 @ 34101 updates, score 22.69) (writing took 4.49645684286952 seconds)
2021-01-02 04:16:54 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-02 04:16:54 | INFO | train | epoch 081 | symm_mse 5.801 | loss 3.527 | nll_loss 1.119 | ppl 2.17 | wps 20376.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 34101 | lr 1.18642e-05 | gnorm 0.886 | train_wall 263 | wall 23162
2021-01-02 04:16:54 | INFO | fairseq.trainer | begin training epoch 82
2021-01-02 04:16:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:17:59 | INFO | train_inner | epoch 082:     99 / 421 symm_mse=5.907, loss=3.546, nll_loss=1.126, ppl=2.18, wps=16118.4, ups=1.16, wpb=13925.6, bsz=481.2, num_updates=34200, lr=1.1847e-05, gnorm=0.905, train_wall=62, wall=23227
2021-01-02 04:19:01 | INFO | train_inner | epoch 082:    199 / 421 symm_mse=5.922, loss=3.545, nll_loss=1.124, ppl=2.18, wps=22288.6, ups=1.59, wpb=13999.5, bsz=488, num_updates=34300, lr=1.18297e-05, gnorm=0.913, train_wall=63, wall=23290
2021-01-02 04:20:04 | INFO | train_inner | epoch 082:    299 / 421 symm_mse=5.779, loss=3.524, nll_loss=1.118, ppl=2.17, wps=22222.8, ups=1.59, wpb=13976.5, bsz=488.2, num_updates=34400, lr=1.18125e-05, gnorm=0.878, train_wall=63, wall=23353
2021-01-02 04:21:07 | INFO | train_inner | epoch 082:    399 / 421 symm_mse=5.545, loss=3.489, nll_loss=1.108, ppl=2.16, wps=22257, ups=1.59, wpb=13959.8, bsz=515.1, num_updates=34500, lr=1.17954e-05, gnorm=0.868, train_wall=63, wall=23415
2021-01-02 04:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:21:38 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_mse 0 | loss 5.221 | nll_loss 3.706 | ppl 13.05 | bleu 22.72 | wps 5853 | wpb 10324.2 | bsz 375 | num_updates 34522 | best_bleu 22.72
2021-01-02 04:21:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:21:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 82 @ 34522 updates, score 22.72) (writing took 4.7284638248384 seconds)
2021-01-02 04:21:42 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-02 04:21:42 | INFO | train | epoch 082 | symm_mse 5.784 | loss 3.525 | nll_loss 1.118 | ppl 2.17 | wps 20380.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 34522 | lr 1.17916e-05 | gnorm 0.891 | train_wall 263 | wall 23451
2021-01-02 04:21:42 | INFO | fairseq.trainer | begin training epoch 83
2021-01-02 04:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:22:34 | INFO | train_inner | epoch 083:     78 / 421 symm_mse=5.863, loss=3.54, nll_loss=1.126, ppl=2.18, wps=16004.6, ups=1.15, wpb=13862, bsz=480.6, num_updates=34600, lr=1.17783e-05, gnorm=0.88, train_wall=61, wall=23502
2021-01-02 04:23:36 | INFO | train_inner | epoch 083:    178 / 421 symm_mse=5.809, loss=3.533, nll_loss=1.124, ppl=2.18, wps=22138, ups=1.59, wpb=13916.6, bsz=488.6, num_updates=34700, lr=1.17613e-05, gnorm=0.883, train_wall=63, wall=23565
2021-01-02 04:24:40 | INFO | train_inner | epoch 083:    278 / 421 symm_mse=5.758, loss=3.521, nll_loss=1.117, ppl=2.17, wps=22178.4, ups=1.59, wpb=13982, bsz=495, num_updates=34800, lr=1.17444e-05, gnorm=0.876, train_wall=63, wall=23628
2021-01-02 04:25:42 | INFO | train_inner | epoch 083:    378 / 421 symm_mse=5.669, loss=3.504, nll_loss=1.111, ppl=2.16, wps=22353.1, ups=1.59, wpb=14049.8, bsz=508, num_updates=34900, lr=1.17276e-05, gnorm=0.871, train_wall=63, wall=23691
2021-01-02 04:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:26:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:26:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:26:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:26:27 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_mse 0 | loss 5.225 | nll_loss 3.708 | ppl 13.07 | bleu 22.6 | wps 5364.2 | wpb 10324.2 | bsz 375 | num_updates 34943 | best_bleu 22.72
2021-01-02 04:26:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:26:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 83 @ 34943 updates, score 22.6) (writing took 3.117708493024111 seconds)
2021-01-02 04:26:31 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-02 04:26:31 | INFO | train | epoch 083 | symm_mse 5.784 | loss 3.524 | nll_loss 1.118 | ppl 2.17 | wps 20408.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 34943 | lr 1.17203e-05 | gnorm 0.878 | train_wall 263 | wall 23739
2021-01-02 04:26:31 | INFO | fairseq.trainer | begin training epoch 84
2021-01-02 04:26:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:26:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:27:09 | INFO | train_inner | epoch 084:     57 / 421 symm_mse=5.886, loss=3.532, nll_loss=1.113, ppl=2.16, wps=16103.8, ups=1.15, wpb=13975, bsz=481.6, num_updates=35000, lr=1.17108e-05, gnorm=0.901, train_wall=62, wall=23778
2021-01-02 04:28:12 | INFO | train_inner | epoch 084:    157 / 421 symm_mse=5.801, loss=3.529, nll_loss=1.121, ppl=2.17, wps=22101.3, ups=1.59, wpb=13918.9, bsz=487.9, num_updates=35100, lr=1.16941e-05, gnorm=0.877, train_wall=63, wall=23841
2021-01-02 04:29:15 | INFO | train_inner | epoch 084:    257 / 421 symm_mse=5.741, loss=3.516, nll_loss=1.113, ppl=2.16, wps=22183.9, ups=1.59, wpb=13950.9, bsz=496.1, num_updates=35200, lr=1.16775e-05, gnorm=0.889, train_wall=63, wall=23903
2021-01-02 04:30:18 | INFO | train_inner | epoch 084:    357 / 421 symm_mse=5.747, loss=3.522, nll_loss=1.12, ppl=2.17, wps=22456.2, ups=1.6, wpb=14065.2, bsz=497.5, num_updates=35300, lr=1.16609e-05, gnorm=0.883, train_wall=62, wall=23966
2021-01-02 04:30:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:30:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:30:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:30:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:30:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:31:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:31:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:31:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:31:15 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_mse 0 | loss 5.223 | nll_loss 3.709 | ppl 13.07 | bleu 22.45 | wps 5819.2 | wpb 10324.2 | bsz 375 | num_updates 35364 | best_bleu 22.72
2021-01-02 04:31:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:31:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 84 @ 35364 updates, score 22.45) (writing took 2.7841231115162373 seconds)
2021-01-02 04:31:18 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-02 04:31:18 | INFO | train | epoch 084 | symm_mse 5.774 | loss 3.523 | nll_loss 1.117 | ppl 2.17 | wps 20497.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 35364 | lr 1.16504e-05 | gnorm 0.887 | train_wall 263 | wall 24026
2021-01-02 04:31:18 | INFO | fairseq.trainer | begin training epoch 85
2021-01-02 04:31:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:31:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:31:43 | INFO | train_inner | epoch 085:     36 / 421 symm_mse=5.593, loss=3.483, nll_loss=1.095, ppl=2.14, wps=16353.2, ups=1.17, wpb=13976.4, bsz=506.2, num_updates=35400, lr=1.16445e-05, gnorm=0.873, train_wall=62, wall=24052
2021-01-02 04:32:46 | INFO | train_inner | epoch 085:    136 / 421 symm_mse=5.61, loss=3.493, nll_loss=1.104, ppl=2.15, wps=22243.4, ups=1.59, wpb=13980.4, bsz=498.1, num_updates=35500, lr=1.1628e-05, gnorm=0.864, train_wall=63, wall=24114
2021-01-02 04:33:49 | INFO | train_inner | epoch 085:    236 / 421 symm_mse=5.771, loss=3.523, nll_loss=1.119, ppl=2.17, wps=22378, ups=1.59, wpb=14089.2, bsz=493.7, num_updates=35600, lr=1.16117e-05, gnorm=0.882, train_wall=63, wall=24177
2021-01-02 04:34:52 | INFO | train_inner | epoch 085:    336 / 421 symm_mse=5.841, loss=3.541, nll_loss=1.13, ppl=2.19, wps=22240, ups=1.6, wpb=13923.5, bsz=483.7, num_updates=35700, lr=1.15954e-05, gnorm=0.892, train_wall=62, wall=24240
2021-01-02 04:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:35:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:35:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:35:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:35:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:35:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:35:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:35:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:35:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:36:02 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_mse 0 | loss 5.222 | nll_loss 3.707 | ppl 13.06 | bleu 22.56 | wps 5863 | wpb 10324.2 | bsz 375 | num_updates 35785 | best_bleu 22.72
2021-01-02 04:36:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:36:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 85 @ 35785 updates, score 22.56) (writing took 2.9248168915510178 seconds)
2021-01-02 04:36:04 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-02 04:36:04 | INFO | train | epoch 085 | symm_mse 5.765 | loss 3.522 | nll_loss 1.118 | ppl 2.17 | wps 20493.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 35785 | lr 1.15816e-05 | gnorm 0.89 | train_wall 263 | wall 24313
2021-01-02 04:36:04 | INFO | fairseq.trainer | begin training epoch 86
2021-01-02 04:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:36:17 | INFO | train_inner | epoch 086:     15 / 421 symm_mse=6.02, loss=3.565, nll_loss=1.133, ppl=2.19, wps=16233.9, ups=1.17, wpb=13875.6, bsz=480.7, num_updates=35800, lr=1.15792e-05, gnorm=0.946, train_wall=62, wall=24325
2021-01-02 04:37:19 | INFO | train_inner | epoch 086:    115 / 421 symm_mse=5.729, loss=3.508, nll_loss=1.106, ppl=2.15, wps=22613.6, ups=1.62, wpb=13984.8, bsz=487.7, num_updates=35900, lr=1.15631e-05, gnorm=0.893, train_wall=62, wall=24387
2021-01-02 04:38:21 | INFO | train_inner | epoch 086:    215 / 421 symm_mse=5.662, loss=3.509, nll_loss=1.116, ppl=2.17, wps=22252.8, ups=1.6, wpb=13938.5, bsz=498.3, num_updates=36000, lr=1.1547e-05, gnorm=0.865, train_wall=62, wall=24450
2021-01-02 04:39:24 | INFO | train_inner | epoch 086:    315 / 421 symm_mse=5.855, loss=3.548, nll_loss=1.136, ppl=2.2, wps=22238.1, ups=1.59, wpb=14000.8, bsz=477.6, num_updates=36100, lr=1.1531e-05, gnorm=0.895, train_wall=63, wall=24513
2021-01-02 04:40:27 | INFO | train_inner | epoch 086:    415 / 421 symm_mse=5.673, loss=3.505, nll_loss=1.11, ppl=2.16, wps=22287.5, ups=1.59, wpb=14012.9, bsz=511.5, num_updates=36200, lr=1.15151e-05, gnorm=0.868, train_wall=63, wall=24576
2021-01-02 04:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:40:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:40:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:40:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:40:48 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_mse 0 | loss 5.224 | nll_loss 3.707 | ppl 13.06 | bleu 22.55 | wps 5895.2 | wpb 10324.2 | bsz 375 | num_updates 36206 | best_bleu 22.72
2021-01-02 04:40:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:40:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:40:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 86 @ 36206 updates, score 22.55) (writing took 2.9156273994594812 seconds)
2021-01-02 04:40:51 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-02 04:40:51 | INFO | train | epoch 086 | symm_mse 5.75 | loss 3.52 | nll_loss 1.117 | ppl 2.17 | wps 20546.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 36206 | lr 1.15141e-05 | gnorm 0.887 | train_wall 262 | wall 24599
2021-01-02 04:40:51 | INFO | fairseq.trainer | begin training epoch 87
2021-01-02 04:40:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:40:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:41:52 | INFO | train_inner | epoch 087:     94 / 421 symm_mse=5.76, loss=3.518, nll_loss=1.115, ppl=2.17, wps=16219.5, ups=1.18, wpb=13761.8, bsz=487.9, num_updates=36300, lr=1.14992e-05, gnorm=0.895, train_wall=62, wall=24661
2021-01-02 04:42:55 | INFO | train_inner | epoch 087:    194 / 421 symm_mse=5.836, loss=3.525, nll_loss=1.111, ppl=2.16, wps=22335.7, ups=1.59, wpb=14010.6, bsz=480, num_updates=36400, lr=1.14834e-05, gnorm=0.905, train_wall=63, wall=24723
2021-01-02 04:43:58 | INFO | train_inner | epoch 087:    294 / 421 symm_mse=5.833, loss=3.53, nll_loss=1.118, ppl=2.17, wps=22365.2, ups=1.58, wpb=14131.5, bsz=494.4, num_updates=36500, lr=1.14676e-05, gnorm=0.9, train_wall=63, wall=24787
2021-01-02 04:45:01 | INFO | train_inner | epoch 087:    394 / 421 symm_mse=5.562, loss=3.502, nll_loss=1.121, ppl=2.17, wps=22199, ups=1.59, wpb=13941.3, bsz=509.6, num_updates=36600, lr=1.1452e-05, gnorm=0.859, train_wall=63, wall=24849
2021-01-02 04:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:45:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:45:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:45:35 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_mse 0 | loss 5.221 | nll_loss 3.705 | ppl 13.04 | bleu 22.54 | wps 5948.2 | wpb 10324.2 | bsz 375 | num_updates 36627 | best_bleu 22.72
2021-01-02 04:45:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:45:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:45:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 87 @ 36627 updates, score 22.54) (writing took 2.9394761603325605 seconds)
2021-01-02 04:45:38 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-02 04:45:38 | INFO | train | epoch 087 | symm_mse 5.743 | loss 3.518 | nll_loss 1.116 | ppl 2.17 | wps 20503.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 36627 | lr 1.14477e-05 | gnorm 0.886 | train_wall 263 | wall 24886
2021-01-02 04:45:38 | INFO | fairseq.trainer | begin training epoch 88
2021-01-02 04:45:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:45:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:46:26 | INFO | train_inner | epoch 088:     73 / 421 symm_mse=5.703, loss=3.514, nll_loss=1.116, ppl=2.17, wps=16286.8, ups=1.17, wpb=13922, bsz=483, num_updates=36700, lr=1.14364e-05, gnorm=0.882, train_wall=62, wall=24935
2021-01-02 04:47:29 | INFO | train_inner | epoch 088:    173 / 421 symm_mse=5.715, loss=3.505, nll_loss=1.104, ppl=2.15, wps=21928.9, ups=1.58, wpb=13837.3, bsz=483.8, num_updates=36800, lr=1.14208e-05, gnorm=0.888, train_wall=63, wall=24998
2021-01-02 04:48:33 | INFO | train_inner | epoch 088:    273 / 421 symm_mse=5.748, loss=3.526, nll_loss=1.124, ppl=2.18, wps=22354.9, ups=1.59, wpb=14095, bsz=506.8, num_updates=36900, lr=1.14053e-05, gnorm=0.882, train_wall=63, wall=25061
2021-01-02 04:49:36 | INFO | train_inner | epoch 088:    373 / 421 symm_mse=5.867, loss=3.54, nll_loss=1.125, ppl=2.18, wps=22166.4, ups=1.59, wpb=13973.8, bsz=490.4, num_updates=37000, lr=1.13899e-05, gnorm=0.889, train_wall=63, wall=25124
2021-01-02 04:50:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:50:22 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_mse 0 | loss 5.22 | nll_loss 3.705 | ppl 13.05 | bleu 22.44 | wps 5876.3 | wpb 10324.2 | bsz 375 | num_updates 37048 | best_bleu 22.72
2021-01-02 04:50:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:50:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 88 @ 37048 updates, score 22.44) (writing took 2.6798462849110365 seconds)
2021-01-02 04:50:25 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-02 04:50:25 | INFO | train | epoch 088 | symm_mse 5.736 | loss 3.518 | nll_loss 1.116 | ppl 2.17 | wps 20463.4 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 37048 | lr 1.13825e-05 | gnorm 0.882 | train_wall 264 | wall 25173
2021-01-02 04:50:25 | INFO | fairseq.trainer | begin training epoch 89
2021-01-02 04:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:51:00 | INFO | train_inner | epoch 089:     52 / 421 symm_mse=5.581, loss=3.491, nll_loss=1.106, ppl=2.15, wps=16474.1, ups=1.18, wpb=13986.3, bsz=500.3, num_updates=37100, lr=1.13745e-05, gnorm=0.867, train_wall=62, wall=25209
2021-01-02 04:52:03 | INFO | train_inner | epoch 089:    152 / 421 symm_mse=5.649, loss=3.496, nll_loss=1.103, ppl=2.15, wps=22198.4, ups=1.59, wpb=13927.9, bsz=491.8, num_updates=37200, lr=1.13592e-05, gnorm=0.878, train_wall=63, wall=25272
2021-01-02 04:53:06 | INFO | train_inner | epoch 089:    252 / 421 symm_mse=5.754, loss=3.531, nll_loss=1.129, ppl=2.19, wps=22245.2, ups=1.59, wpb=13999.4, bsz=495.4, num_updates=37300, lr=1.1344e-05, gnorm=0.871, train_wall=63, wall=25335
2021-01-02 04:54:09 | INFO | train_inner | epoch 089:    352 / 421 symm_mse=6.001, loss=3.554, nll_loss=1.123, ppl=2.18, wps=22499.5, ups=1.6, wpb=14106.2, bsz=471, num_updates=37400, lr=1.13288e-05, gnorm=0.914, train_wall=63, wall=25397
2021-01-02 04:54:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:54:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:54:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:54:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:54:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:54:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:54:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:54:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:54:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:55:09 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_mse 0 | loss 5.222 | nll_loss 3.705 | ppl 13.04 | bleu 22.58 | wps 5900.3 | wpb 10324.2 | bsz 375 | num_updates 37469 | best_bleu 22.72
2021-01-02 04:55:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:55:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:55:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:55:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:55:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 89 @ 37469 updates, score 22.58) (writing took 2.9045486841350794 seconds)
2021-01-02 04:55:11 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-02 04:55:11 | INFO | train | epoch 089 | symm_mse 5.733 | loss 3.517 | nll_loss 1.116 | ppl 2.17 | wps 20529.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 37469 | lr 1.13184e-05 | gnorm 0.885 | train_wall 263 | wall 25460
2021-01-02 04:55:11 | INFO | fairseq.trainer | begin training epoch 90
2021-01-02 04:55:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:55:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:55:34 | INFO | train_inner | epoch 090:     31 / 421 symm_mse=5.698, loss=3.512, nll_loss=1.116, ppl=2.17, wps=16287.7, ups=1.18, wpb=13829.1, bsz=499.7, num_updates=37500, lr=1.13137e-05, gnorm=0.886, train_wall=62, wall=25482
2021-01-02 04:56:37 | INFO | train_inner | epoch 090:    131 / 421 symm_mse=5.868, loss=3.534, nll_loss=1.117, ppl=2.17, wps=22199.8, ups=1.59, wpb=13971.9, bsz=468, num_updates=37600, lr=1.12987e-05, gnorm=0.891, train_wall=63, wall=25545
2021-01-02 04:57:40 | INFO | train_inner | epoch 090:    231 / 421 symm_mse=5.58, loss=3.495, nll_loss=1.112, ppl=2.16, wps=22234.8, ups=1.58, wpb=14079.9, bsz=510.4, num_updates=37700, lr=1.12837e-05, gnorm=0.854, train_wall=63, wall=25608
2021-01-02 04:58:42 | INFO | train_inner | epoch 090:    331 / 421 symm_mse=5.719, loss=3.522, nll_loss=1.123, ppl=2.18, wps=22201.6, ups=1.61, wpb=13773.4, bsz=492.6, num_updates=37800, lr=1.12687e-05, gnorm=0.884, train_wall=62, wall=25670
2021-01-02 04:59:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 04:59:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 04:59:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 04:59:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 04:59:56 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_mse 0 | loss 5.221 | nll_loss 3.708 | ppl 13.07 | bleu 22.41 | wps 5852.5 | wpb 10324.2 | bsz 375 | num_updates 37890 | best_bleu 22.72
2021-01-02 04:59:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 04:59:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 04:59:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 04:59:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 90 @ 37890 updates, score 22.41) (writing took 2.903710948303342 seconds)
2021-01-02 04:59:59 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-02 04:59:59 | INFO | train | epoch 090 | symm_mse 5.723 | loss 3.515 | nll_loss 1.116 | ppl 2.17 | wps 20486.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 37890 | lr 1.12553e-05 | gnorm 0.875 | train_wall 263 | wall 25747
2021-01-02 04:59:59 | INFO | fairseq.trainer | begin training epoch 91
2021-01-02 04:59:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:00:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:00:08 | INFO | train_inner | epoch 091:     10 / 421 symm_mse=5.634, loss=3.503, nll_loss=1.114, ppl=2.16, wps=16324, ups=1.16, wpb=14051.5, bsz=501.5, num_updates=37900, lr=1.12538e-05, gnorm=0.862, train_wall=63, wall=25757
2021-01-02 05:01:10 | INFO | train_inner | epoch 091:    110 / 421 symm_mse=5.647, loss=3.507, nll_loss=1.117, ppl=2.17, wps=22491.5, ups=1.61, wpb=13964.4, bsz=498.9, num_updates=38000, lr=1.1239e-05, gnorm=0.873, train_wall=62, wall=25819
2021-01-02 05:02:13 | INFO | train_inner | epoch 091:    210 / 421 symm_mse=5.679, loss=3.518, nll_loss=1.125, ppl=2.18, wps=22187.1, ups=1.6, wpb=13883.8, bsz=503.3, num_updates=38100, lr=1.12243e-05, gnorm=0.895, train_wall=62, wall=25881
2021-01-02 05:03:16 | INFO | train_inner | epoch 091:    310 / 421 symm_mse=5.843, loss=3.521, nll_loss=1.105, ppl=2.15, wps=22423.7, ups=1.59, wpb=14130.5, bsz=481.8, num_updates=38200, lr=1.12096e-05, gnorm=0.896, train_wall=63, wall=25944
2021-01-02 05:04:18 | INFO | train_inner | epoch 091:    410 / 421 symm_mse=5.692, loss=3.514, nll_loss=1.118, ppl=2.17, wps=22368.1, ups=1.6, wpb=13954.4, bsz=488, num_updates=38300, lr=1.11949e-05, gnorm=0.894, train_wall=62, wall=26007
2021-01-02 05:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:04:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:04:42 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_mse 0 | loss 5.22 | nll_loss 3.705 | ppl 13.04 | bleu 22.45 | wps 5800.9 | wpb 10324.2 | bsz 375 | num_updates 38311 | best_bleu 22.72
2021-01-02 05:04:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:04:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 91 @ 38311 updates, score 22.45) (writing took 2.8825582079589367 seconds)
2021-01-02 05:04:45 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-02 05:04:45 | INFO | train | epoch 091 | symm_mse 5.715 | loss 3.515 | nll_loss 1.116 | ppl 2.17 | wps 20530.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 38311 | lr 1.11933e-05 | gnorm 0.888 | train_wall 262 | wall 26033
2021-01-02 05:04:45 | INFO | fairseq.trainer | begin training epoch 92
2021-01-02 05:04:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:04:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:05:43 | INFO | train_inner | epoch 092:     89 / 421 symm_mse=5.78, loss=3.521, nll_loss=1.114, ppl=2.16, wps=16260.4, ups=1.17, wpb=13872.5, bsz=487.1, num_updates=38400, lr=1.11803e-05, gnorm=0.888, train_wall=62, wall=26092
2021-01-02 05:06:46 | INFO | train_inner | epoch 092:    189 / 421 symm_mse=5.778, loss=3.527, nll_loss=1.122, ppl=2.18, wps=22204.9, ups=1.59, wpb=13956.8, bsz=481.9, num_updates=38500, lr=1.11658e-05, gnorm=0.879, train_wall=63, wall=26155
2021-01-02 05:07:50 | INFO | train_inner | epoch 092:    289 / 421 symm_mse=5.624, loss=3.501, nll_loss=1.112, ppl=2.16, wps=22285.7, ups=1.58, wpb=14079.7, bsz=508.1, num_updates=38600, lr=1.11513e-05, gnorm=0.866, train_wall=63, wall=26218
2021-01-02 05:08:52 | INFO | train_inner | epoch 092:    389 / 421 symm_mse=5.617, loss=3.503, nll_loss=1.116, ppl=2.17, wps=22264.6, ups=1.6, wpb=13919.4, bsz=495.5, num_updates=38700, lr=1.11369e-05, gnorm=0.865, train_wall=62, wall=26280
2021-01-02 05:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:09:30 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_mse 0 | loss 5.222 | nll_loss 3.706 | ppl 13.05 | bleu 22.43 | wps 5485.8 | wpb 10324.2 | bsz 375 | num_updates 38732 | best_bleu 22.72
2021-01-02 05:09:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:09:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 92 @ 38732 updates, score 22.43) (writing took 2.3955791871994734 seconds)
2021-01-02 05:09:32 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-02 05:09:32 | INFO | train | epoch 092 | symm_mse 5.704 | loss 3.513 | nll_loss 1.115 | ppl 2.17 | wps 20466.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 38732 | lr 1.11323e-05 | gnorm 0.875 | train_wall 263 | wall 26321
2021-01-02 05:09:32 | INFO | fairseq.trainer | begin training epoch 93
2021-01-02 05:09:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:10:18 | INFO | train_inner | epoch 093:     68 / 421 symm_mse=5.586, loss=3.489, nll_loss=1.104, ppl=2.15, wps=16295.2, ups=1.17, wpb=13931.8, bsz=499.8, num_updates=38800, lr=1.11226e-05, gnorm=0.863, train_wall=62, wall=26366
2021-01-02 05:11:20 | INFO | train_inner | epoch 093:    168 / 421 symm_mse=5.707, loss=3.508, nll_loss=1.109, ppl=2.16, wps=22086, ups=1.59, wpb=13886.1, bsz=487.4, num_updates=38900, lr=1.11083e-05, gnorm=0.876, train_wall=63, wall=26429
2021-01-02 05:12:23 | INFO | train_inner | epoch 093:    268 / 421 symm_mse=5.84, loss=3.54, nll_loss=1.128, ppl=2.19, wps=22147.8, ups=1.6, wpb=13850.6, bsz=485.8, num_updates=39000, lr=1.1094e-05, gnorm=0.901, train_wall=62, wall=26491
2021-01-02 05:13:25 | INFO | train_inner | epoch 093:    368 / 421 symm_mse=5.774, loss=3.518, nll_loss=1.111, ppl=2.16, wps=22849.6, ups=1.61, wpb=14222.5, bsz=489.6, num_updates=39100, lr=1.10798e-05, gnorm=0.9, train_wall=62, wall=26554
2021-01-02 05:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:14:15 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_mse 0 | loss 5.219 | nll_loss 3.704 | ppl 13.03 | bleu 22.54 | wps 5791.5 | wpb 10324.2 | bsz 375 | num_updates 39153 | best_bleu 22.72
2021-01-02 05:14:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 93 @ 39153 updates, score 22.54) (writing took 3.1866504717618227 seconds)
2021-01-02 05:14:19 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-02 05:14:19 | INFO | train | epoch 093 | symm_mse 5.704 | loss 3.512 | nll_loss 1.115 | ppl 2.17 | wps 20549.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 39153 | lr 1.10723e-05 | gnorm 0.884 | train_wall 262 | wall 26607
2021-01-02 05:14:19 | INFO | fairseq.trainer | begin training epoch 94
2021-01-02 05:14:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:14:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:14:51 | INFO | train_inner | epoch 094:     47 / 421 symm_mse=5.651, loss=3.501, nll_loss=1.11, ppl=2.16, wps=16379.1, ups=1.17, wpb=13992.9, bsz=499.8, num_updates=39200, lr=1.10657e-05, gnorm=0.882, train_wall=62, wall=26639
2021-01-02 05:15:53 | INFO | train_inner | epoch 094:    147 / 421 symm_mse=5.653, loss=3.501, nll_loss=1.107, ppl=2.15, wps=22331.9, ups=1.59, wpb=14008.8, bsz=483.8, num_updates=39300, lr=1.10516e-05, gnorm=0.865, train_wall=63, wall=26702
2021-01-02 05:16:56 | INFO | train_inner | epoch 094:    247 / 421 symm_mse=5.735, loss=3.519, nll_loss=1.118, ppl=2.17, wps=22494.8, ups=1.6, wpb=14073.1, bsz=497.6, num_updates=39400, lr=1.10375e-05, gnorm=0.874, train_wall=62, wall=26764
2021-01-02 05:17:59 | INFO | train_inner | epoch 094:    347 / 421 symm_mse=5.769, loss=3.539, nll_loss=1.138, ppl=2.2, wps=21980.5, ups=1.59, wpb=13782.1, bsz=478.6, num_updates=39500, lr=1.10236e-05, gnorm=0.874, train_wall=63, wall=26827
2021-01-02 05:18:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:19:02 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_mse 0 | loss 5.22 | nll_loss 3.704 | ppl 13.03 | bleu 22.37 | wps 5889 | wpb 10324.2 | bsz 375 | num_updates 39574 | best_bleu 22.72
2021-01-02 05:19:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:19:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:19:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:19:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:19:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:19:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:19:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:19:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 94 @ 39574 updates, score 22.37) (writing took 3.104796851053834 seconds)
2021-01-02 05:19:05 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-02 05:19:05 | INFO | train | epoch 094 | symm_mse 5.697 | loss 3.512 | nll_loss 1.115 | ppl 2.17 | wps 20502.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 39574 | lr 1.10133e-05 | gnorm 0.878 | train_wall 263 | wall 26894
2021-01-02 05:19:05 | INFO | fairseq.trainer | begin training epoch 95
2021-01-02 05:19:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:19:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:19:25 | INFO | train_inner | epoch 095:     26 / 421 symm_mse=5.519, loss=3.479, nll_loss=1.1, ppl=2.14, wps=16261.2, ups=1.16, wpb=13986.8, bsz=521.4, num_updates=39600, lr=1.10096e-05, gnorm=0.887, train_wall=63, wall=26913
2021-01-02 05:20:27 | INFO | train_inner | epoch 095:    126 / 421 symm_mse=5.903, loss=3.547, nll_loss=1.127, ppl=2.18, wps=22186.3, ups=1.6, wpb=13861.6, bsz=476.5, num_updates=39700, lr=1.09958e-05, gnorm=0.887, train_wall=62, wall=26976
2021-01-02 05:21:30 | INFO | train_inner | epoch 095:    226 / 421 symm_mse=5.751, loss=3.512, nll_loss=1.108, ppl=2.16, wps=22098.1, ups=1.59, wpb=13904.9, bsz=482.6, num_updates=39800, lr=1.09819e-05, gnorm=0.89, train_wall=63, wall=27038
2021-01-02 05:22:33 | INFO | train_inner | epoch 095:    326 / 421 symm_mse=5.469, loss=3.479, nll_loss=1.108, ppl=2.16, wps=22600.7, ups=1.59, wpb=14181.4, bsz=509.4, num_updates=39900, lr=1.09682e-05, gnorm=0.836, train_wall=63, wall=27101
2021-01-02 05:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:23:49 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_mse 0 | loss 5.22 | nll_loss 3.705 | ppl 13.04 | bleu 22.52 | wps 5936.1 | wpb 10324.2 | bsz 375 | num_updates 39995 | best_bleu 22.72
2021-01-02 05:23:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:23:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 95 @ 39995 updates, score 22.52) (writing took 3.05028073489666 seconds)
2021-01-02 05:23:52 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-02 05:23:52 | INFO | train | epoch 095 | symm_mse 5.686 | loss 3.51 | nll_loss 1.115 | ppl 2.17 | wps 20507.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 39995 | lr 1.09551e-05 | gnorm 0.877 | train_wall 263 | wall 27181
2021-01-02 05:23:52 | INFO | fairseq.trainer | begin training epoch 96
2021-01-02 05:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:23:59 | INFO | train_inner | epoch 096:      5 / 421 symm_mse=5.696, loss=3.516, nll_loss=1.12, ppl=2.17, wps=16172.7, ups=1.17, wpb=13880.1, bsz=488.7, num_updates=40000, lr=1.09545e-05, gnorm=0.892, train_wall=63, wall=27187
2021-01-02 05:25:01 | INFO | train_inner | epoch 096:    105 / 421 symm_mse=5.639, loss=3.495, nll_loss=1.102, ppl=2.15, wps=22683.7, ups=1.6, wpb=14175.6, bsz=499, num_updates=40100, lr=1.09408e-05, gnorm=0.86, train_wall=62, wall=27250
2021-01-02 05:26:04 | INFO | train_inner | epoch 096:    205 / 421 symm_mse=5.59, loss=3.502, nll_loss=1.118, ppl=2.17, wps=21943.9, ups=1.59, wpb=13801.8, bsz=502.2, num_updates=40200, lr=1.09272e-05, gnorm=0.862, train_wall=63, wall=27312
2021-01-02 05:27:07 | INFO | train_inner | epoch 096:    305 / 421 symm_mse=5.777, loss=3.53, nll_loss=1.125, ppl=2.18, wps=22269.5, ups=1.59, wpb=14033.4, bsz=490.9, num_updates=40300, lr=1.09136e-05, gnorm=0.872, train_wall=63, wall=27375
2021-01-02 05:28:09 | INFO | train_inner | epoch 096:    405 / 421 symm_mse=5.731, loss=3.515, nll_loss=1.114, ppl=2.17, wps=22363, ups=1.6, wpb=13939.7, bsz=479.9, num_updates=40400, lr=1.09001e-05, gnorm=0.876, train_wall=62, wall=27438
2021-01-02 05:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:28:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:28:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:28:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:28:36 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_mse 0 | loss 5.218 | nll_loss 3.704 | ppl 13.03 | bleu 22.47 | wps 5898.3 | wpb 10324.2 | bsz 375 | num_updates 40416 | best_bleu 22.72
2021-01-02 05:28:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:28:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 96 @ 40416 updates, score 22.47) (writing took 3.0570671409368515 seconds)
2021-01-02 05:28:40 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-02 05:28:40 | INFO | train | epoch 096 | symm_mse 5.684 | loss 3.51 | nll_loss 1.114 | ppl 2.16 | wps 20466.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 40416 | lr 1.08979e-05 | gnorm 0.867 | train_wall 263 | wall 27468
2021-01-02 05:28:40 | INFO | fairseq.trainer | begin training epoch 97
2021-01-02 05:28:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:28:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:29:35 | INFO | train_inner | epoch 097:     84 / 421 symm_mse=5.535, loss=3.49, nll_loss=1.112, ppl=2.16, wps=16381, ups=1.16, wpb=14091.7, bsz=512.2, num_updates=40500, lr=1.08866e-05, gnorm=0.85, train_wall=63, wall=27524
2021-01-02 05:30:38 | INFO | train_inner | epoch 097:    184 / 421 symm_mse=5.879, loss=3.545, nll_loss=1.13, ppl=2.19, wps=22046.2, ups=1.6, wpb=13802.1, bsz=472.5, num_updates=40600, lr=1.08732e-05, gnorm=0.895, train_wall=62, wall=27586
2021-01-02 05:31:41 | INFO | train_inner | epoch 097:    284 / 421 symm_mse=5.527, loss=3.484, nll_loss=1.105, ppl=2.15, wps=22340.5, ups=1.59, wpb=14048.6, bsz=500.6, num_updates=40700, lr=1.08598e-05, gnorm=0.847, train_wall=63, wall=27649
2021-01-02 05:32:44 | INFO | train_inner | epoch 097:    384 / 421 symm_mse=5.656, loss=3.501, nll_loss=1.109, ppl=2.16, wps=22311.4, ups=1.59, wpb=13995.7, bsz=497.8, num_updates=40800, lr=1.08465e-05, gnorm=0.881, train_wall=63, wall=27712
2021-01-02 05:33:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:33:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:33:24 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_mse 0 | loss 5.217 | nll_loss 3.702 | ppl 13.02 | bleu 22.58 | wps 5807.5 | wpb 10324.2 | bsz 375 | num_updates 40837 | best_bleu 22.72
2021-01-02 05:33:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:33:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:33:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 97 @ 40837 updates, score 22.58) (writing took 3.1383609529584646 seconds)
2021-01-02 05:33:27 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-02 05:33:27 | INFO | train | epoch 097 | symm_mse 5.67 | loss 3.508 | nll_loss 1.114 | ppl 2.16 | wps 20457.5 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 40837 | lr 1.08416e-05 | gnorm 0.875 | train_wall 263 | wall 27755
2021-01-02 05:33:27 | INFO | fairseq.trainer | begin training epoch 98
2021-01-02 05:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:34:09 | INFO | train_inner | epoch 098:     63 / 421 symm_mse=5.636, loss=3.492, nll_loss=1.1, ppl=2.14, wps=16144.4, ups=1.17, wpb=13843.1, bsz=482.2, num_updates=40900, lr=1.08333e-05, gnorm=0.879, train_wall=62, wall=27798
2021-01-02 05:35:12 | INFO | train_inner | epoch 098:    163 / 421 symm_mse=5.601, loss=3.496, nll_loss=1.11, ppl=2.16, wps=22229.8, ups=1.6, wpb=13929.2, bsz=503.2, num_updates=41000, lr=1.082e-05, gnorm=0.891, train_wall=62, wall=27860
2021-01-02 05:36:14 | INFO | train_inner | epoch 098:    263 / 421 symm_mse=5.738, loss=3.532, nll_loss=1.132, ppl=2.19, wps=22363.8, ups=1.6, wpb=13939.5, bsz=483, num_updates=41100, lr=1.08069e-05, gnorm=0.88, train_wall=62, wall=27923
2021-01-02 05:37:17 | INFO | train_inner | epoch 098:    363 / 421 symm_mse=5.698, loss=3.505, nll_loss=1.107, ppl=2.15, wps=22535.1, ups=1.6, wpb=14068.5, bsz=492.2, num_updates=41200, lr=1.07937e-05, gnorm=0.884, train_wall=62, wall=27985
2021-01-02 05:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:37:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:37:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:37:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:37:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:37:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:37:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:37:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:37:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:37:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:37:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:37:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:38:09 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_mse 0 | loss 5.219 | nll_loss 3.703 | ppl 13.02 | bleu 22.41 | wps 5889.5 | wpb 10324.2 | bsz 375 | num_updates 41258 | best_bleu 22.72
2021-01-02 05:38:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:38:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:38:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:38:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:38:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 98 @ 41258 updates, score 22.41) (writing took 3.0387536995112896 seconds)
2021-01-02 05:38:12 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-02 05:38:12 | INFO | train | epoch 098 | symm_mse 5.664 | loss 3.507 | nll_loss 1.114 | ppl 2.16 | wps 20603.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 41258 | lr 1.07862e-05 | gnorm 0.877 | train_wall 261 | wall 28041
2021-01-02 05:38:12 | INFO | fairseq.trainer | begin training epoch 99
2021-01-02 05:38:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:38:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:38:42 | INFO | train_inner | epoch 099:     42 / 421 symm_mse=5.683, loss=3.508, nll_loss=1.113, ppl=2.16, wps=16351.8, ups=1.18, wpb=13872, bsz=489.7, num_updates=41300, lr=1.07807e-05, gnorm=0.875, train_wall=61, wall=28070
2021-01-02 05:39:44 | INFO | train_inner | epoch 099:    142 / 421 symm_mse=5.567, loss=3.493, nll_loss=1.109, ppl=2.16, wps=22395.9, ups=1.6, wpb=14015.1, bsz=501.8, num_updates=41400, lr=1.07676e-05, gnorm=0.867, train_wall=62, wall=28133
2021-01-02 05:40:47 | INFO | train_inner | epoch 099:    242 / 421 symm_mse=5.791, loss=3.525, nll_loss=1.119, ppl=2.17, wps=22299.8, ups=1.59, wpb=14021.7, bsz=480, num_updates=41500, lr=1.07547e-05, gnorm=0.894, train_wall=63, wall=28196
2021-01-02 05:41:50 | INFO | train_inner | epoch 099:    342 / 421 symm_mse=5.807, loss=3.536, nll_loss=1.127, ppl=2.18, wps=22272.1, ups=1.6, wpb=13915.6, bsz=489.7, num_updates=41600, lr=1.07417e-05, gnorm=0.888, train_wall=62, wall=28258
2021-01-02 05:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:42:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:42:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:42:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:42:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:42:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:42:56 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_mse 0 | loss 5.218 | nll_loss 3.701 | ppl 13 | bleu 22.39 | wps 5850.3 | wpb 10324.2 | bsz 375 | num_updates 41679 | best_bleu 22.72
2021-01-02 05:42:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:42:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 99 @ 41679 updates, score 22.39) (writing took 3.1388554833829403 seconds)
2021-01-02 05:42:59 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-02 05:42:59 | INFO | train | epoch 099 | symm_mse 5.652 | loss 3.505 | nll_loss 1.113 | ppl 2.16 | wps 20516.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 41679 | lr 1.07315e-05 | gnorm 0.876 | train_wall 262 | wall 28328
2021-01-02 05:42:59 | INFO | fairseq.trainer | begin training epoch 100
2021-01-02 05:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:43:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:43:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:43:15 | INFO | train_inner | epoch 100:     21 / 421 symm_mse=5.47, loss=3.472, nll_loss=1.1, ppl=2.14, wps=16303.6, ups=1.17, wpb=13985.8, bsz=496.4, num_updates=41700, lr=1.07288e-05, gnorm=0.854, train_wall=62, wall=28344
2021-01-02 05:44:17 | INFO | train_inner | epoch 100:    121 / 421 symm_mse=5.484, loss=3.466, nll_loss=1.089, ppl=2.13, wps=22740, ups=1.61, wpb=14118.8, bsz=510.7, num_updates=41800, lr=1.0716e-05, gnorm=0.839, train_wall=62, wall=28406
2021-01-02 05:45:20 | INFO | train_inner | epoch 100:    221 / 421 symm_mse=5.655, loss=3.502, nll_loss=1.108, ppl=2.16, wps=22526.1, ups=1.61, wpb=14026.1, bsz=489.2, num_updates=41900, lr=1.07032e-05, gnorm=0.87, train_wall=62, wall=28468
2021-01-02 05:46:22 | INFO | train_inner | epoch 100:    321 / 421 symm_mse=5.673, loss=3.516, nll_loss=1.122, ppl=2.18, wps=22293.1, ups=1.59, wpb=13989.4, bsz=484.4, num_updates=42000, lr=1.06904e-05, gnorm=0.885, train_wall=63, wall=28531
2021-01-02 05:47:25 | INFO | train_inner | epoch 100:    421 / 421 symm_mse=5.796, loss=3.54, nll_loss=1.134, ppl=2.2, wps=21955.6, ups=1.6, wpb=13759.9, bsz=487.8, num_updates=42100, lr=1.06777e-05, gnorm=0.897, train_wall=62, wall=28594
2021-01-02 05:47:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:47:42 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_mse 0 | loss 5.218 | nll_loss 3.703 | ppl 13.02 | bleu 22.29 | wps 5896.6 | wpb 10324.2 | bsz 375 | num_updates 42100 | best_bleu 22.72
2021-01-02 05:47:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:47:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 100 @ 42100 updates, score 22.29) (writing took 3.0319664403796196 seconds)
2021-01-02 05:47:45 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-02 05:47:45 | INFO | train | epoch 100 | symm_mse 5.646 | loss 3.504 | nll_loss 1.112 | ppl 2.16 | wps 20576.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 42100 | lr 1.06777e-05 | gnorm 0.871 | train_wall 262 | wall 28613
2021-01-02 05:47:45 | INFO | fairseq.trainer | begin training epoch 101
2021-01-02 05:47:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:47:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:48:50 | INFO | train_inner | epoch 101:    100 / 421 symm_mse=5.506, loss=3.473, nll_loss=1.096, ppl=2.14, wps=16273.3, ups=1.17, wpb=13883.2, bsz=499.1, num_updates=42200, lr=1.06651e-05, gnorm=0.884, train_wall=62, wall=28679
2021-01-02 05:49:52 | INFO | train_inner | epoch 101:    200 / 421 symm_mse=5.759, loss=3.529, nll_loss=1.126, ppl=2.18, wps=22689.1, ups=1.61, wpb=14079.7, bsz=491, num_updates=42300, lr=1.06525e-05, gnorm=0.893, train_wall=62, wall=28741
2021-01-02 05:50:55 | INFO | train_inner | epoch 101:    300 / 421 symm_mse=5.664, loss=3.503, nll_loss=1.108, ppl=2.16, wps=22525.1, ups=1.6, wpb=14068.5, bsz=485.1, num_updates=42400, lr=1.06399e-05, gnorm=0.869, train_wall=62, wall=28803
2021-01-02 05:51:58 | INFO | train_inner | epoch 101:    400 / 421 symm_mse=5.623, loss=3.508, nll_loss=1.12, ppl=2.17, wps=22180.3, ups=1.6, wpb=13890.9, bsz=496, num_updates=42500, lr=1.06274e-05, gnorm=0.87, train_wall=62, wall=28866
2021-01-02 05:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:52:28 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_mse 0 | loss 5.218 | nll_loss 3.702 | ppl 13.01 | bleu 22.48 | wps 5876.6 | wpb 10324.2 | bsz 375 | num_updates 42521 | best_bleu 22.72
2021-01-02 05:52:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:52:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 101 @ 42521 updates, score 22.48) (writing took 3.0703533180058002 seconds)
2021-01-02 05:52:31 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-02 05:52:31 | INFO | train | epoch 101 | symm_mse 5.64 | loss 3.503 | nll_loss 1.112 | ppl 2.16 | wps 20588 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 42521 | lr 1.06248e-05 | gnorm 0.879 | train_wall 262 | wall 28899
2021-01-02 05:52:31 | INFO | fairseq.trainer | begin training epoch 102
2021-01-02 05:52:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:52:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:53:23 | INFO | train_inner | epoch 102:     79 / 421 symm_mse=5.862, loss=3.54, nll_loss=1.125, ppl=2.18, wps=16432.8, ups=1.18, wpb=13969.1, bsz=481.5, num_updates=42600, lr=1.06149e-05, gnorm=0.922, train_wall=62, wall=28951
2021-01-02 05:54:25 | INFO | train_inner | epoch 102:    179 / 421 symm_mse=5.549, loss=3.485, nll_loss=1.105, ppl=2.15, wps=22397.6, ups=1.61, wpb=13930.9, bsz=507, num_updates=42700, lr=1.06025e-05, gnorm=0.877, train_wall=62, wall=29013
2021-01-02 05:55:27 | INFO | train_inner | epoch 102:    279 / 421 symm_mse=5.588, loss=3.494, nll_loss=1.108, ppl=2.16, wps=22254.9, ups=1.6, wpb=13897.1, bsz=483, num_updates=42800, lr=1.05901e-05, gnorm=0.856, train_wall=62, wall=29076
2021-01-02 05:56:31 | INFO | train_inner | epoch 102:    379 / 421 symm_mse=5.475, loss=3.477, nll_loss=1.104, ppl=2.15, wps=22142.3, ups=1.58, wpb=14020.9, bsz=498.7, num_updates=42900, lr=1.05777e-05, gnorm=0.853, train_wall=63, wall=29139
2021-01-02 05:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 05:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 05:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 05:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 05:57:14 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_mse 0 | loss 5.213 | nll_loss 3.696 | ppl 12.96 | bleu 22.5 | wps 5859.6 | wpb 10324.2 | bsz 375 | num_updates 42942 | best_bleu 22.72
2021-01-02 05:57:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 05:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:57:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:57:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:57:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:57:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 102 @ 42942 updates, score 22.5) (writing took 3.0541649144142866 seconds)
2021-01-02 05:57:17 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-02 05:57:17 | INFO | train | epoch 102 | symm_mse 5.63 | loss 3.502 | nll_loss 1.112 | ppl 2.16 | wps 20553.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 42942 | lr 1.05725e-05 | gnorm 0.878 | train_wall 262 | wall 29185
2021-01-02 05:57:17 | INFO | fairseq.trainer | begin training epoch 103
2021-01-02 05:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 05:57:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 05:57:56 | INFO | train_inner | epoch 103:     58 / 421 symm_mse=5.703, loss=3.516, nll_loss=1.119, ppl=2.17, wps=16406.7, ups=1.17, wpb=14003.3, bsz=502, num_updates=43000, lr=1.05654e-05, gnorm=0.913, train_wall=62, wall=29224
2021-01-02 05:58:58 | INFO | train_inner | epoch 103:    158 / 421 symm_mse=5.549, loss=3.485, nll_loss=1.104, ppl=2.15, wps=22389.9, ups=1.61, wpb=13884.4, bsz=487.8, num_updates=43100, lr=1.05531e-05, gnorm=0.869, train_wall=62, wall=29286
2021-01-02 06:00:01 | INFO | train_inner | epoch 103:    258 / 421 symm_mse=5.728, loss=3.513, nll_loss=1.112, ppl=2.16, wps=22475.2, ups=1.6, wpb=14078.5, bsz=477.8, num_updates=43200, lr=1.05409e-05, gnorm=0.882, train_wall=62, wall=29349
2021-01-02 06:01:02 | INFO | train_inner | epoch 103:    358 / 421 symm_mse=5.631, loss=3.507, nll_loss=1.118, ppl=2.17, wps=22738.1, ups=1.62, wpb=14069.8, bsz=494.8, num_updates=43300, lr=1.05287e-05, gnorm=0.86, train_wall=62, wall=29411
2021-01-02 06:01:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:01:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:01:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:01:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:01:59 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_mse 0 | loss 5.213 | nll_loss 3.698 | ppl 12.98 | bleu 22.58 | wps 5966.6 | wpb 10324.2 | bsz 375 | num_updates 43363 | best_bleu 22.72
2021-01-02 06:01:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:02:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 103 @ 43363 updates, score 22.58) (writing took 2.5881562270224094 seconds)
2021-01-02 06:02:01 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-02 06:02:01 | INFO | train | epoch 103 | symm_mse 5.629 | loss 3.502 | nll_loss 1.113 | ppl 2.16 | wps 20676.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 43363 | lr 1.05211e-05 | gnorm 0.88 | train_wall 261 | wall 29470
2021-01-02 06:02:01 | INFO | fairseq.trainer | begin training epoch 104
2021-01-02 06:02:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:02:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:02:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:02:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:02:27 | INFO | train_inner | epoch 104:     37 / 421 symm_mse=5.603, loss=3.505, nll_loss=1.119, ppl=2.17, wps=16342.8, ups=1.18, wpb=13858.5, bsz=500.9, num_updates=43400, lr=1.05166e-05, gnorm=0.869, train_wall=62, wall=29496
2021-01-02 06:03:29 | INFO | train_inner | epoch 104:    137 / 421 symm_mse=5.71, loss=3.505, nll_loss=1.104, ppl=2.15, wps=22496, ups=1.61, wpb=13991.8, bsz=485.7, num_updates=43500, lr=1.05045e-05, gnorm=0.883, train_wall=62, wall=29558
2021-01-02 06:04:32 | INFO | train_inner | epoch 104:    237 / 421 symm_mse=5.544, loss=3.49, nll_loss=1.109, ppl=2.16, wps=22489.3, ups=1.61, wpb=13997.3, bsz=499.1, num_updates=43600, lr=1.04925e-05, gnorm=0.87, train_wall=62, wall=29620
2021-01-02 06:05:34 | INFO | train_inner | epoch 104:    337 / 421 symm_mse=5.619, loss=3.503, nll_loss=1.116, ppl=2.17, wps=22478.4, ups=1.61, wpb=13981.3, bsz=497, num_updates=43700, lr=1.04804e-05, gnorm=0.865, train_wall=62, wall=29682
2021-01-02 06:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:06:43 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_mse 0 | loss 5.215 | nll_loss 3.699 | ppl 12.99 | bleu 22.56 | wps 5883.5 | wpb 10324.2 | bsz 375 | num_updates 43784 | best_bleu 22.72
2021-01-02 06:06:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:06:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:06:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 104 @ 43784 updates, score 22.56) (writing took 3.1207898780703545 seconds)
2021-01-02 06:06:46 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-02 06:06:46 | INFO | train | epoch 104 | symm_mse 5.621 | loss 3.5 | nll_loss 1.111 | ppl 2.16 | wps 20624.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 43784 | lr 1.04704e-05 | gnorm 0.869 | train_wall 261 | wall 29755
2021-01-02 06:06:46 | INFO | fairseq.trainer | begin training epoch 105
2021-01-02 06:06:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:06:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:07:00 | INFO | train_inner | epoch 105:     16 / 421 symm_mse=5.575, loss=3.493, nll_loss=1.11, ppl=2.16, wps=16171.6, ups=1.17, wpb=13856.2, bsz=485.6, num_updates=43800, lr=1.04685e-05, gnorm=0.858, train_wall=62, wall=29768
2021-01-02 06:08:01 | INFO | train_inner | epoch 105:    116 / 421 symm_mse=5.543, loss=3.487, nll_loss=1.108, ppl=2.15, wps=22586.8, ups=1.61, wpb=13988.2, bsz=507, num_updates=43900, lr=1.04565e-05, gnorm=0.88, train_wall=62, wall=29830
2021-01-02 06:09:04 | INFO | train_inner | epoch 105:    216 / 421 symm_mse=5.709, loss=3.512, nll_loss=1.113, ppl=2.16, wps=22498.8, ups=1.61, wpb=13991.7, bsz=483.1, num_updates=44000, lr=1.04447e-05, gnorm=0.887, train_wall=62, wall=29892
2021-01-02 06:10:06 | INFO | train_inner | epoch 105:    316 / 421 symm_mse=5.682, loss=3.515, nll_loss=1.121, ppl=2.17, wps=22560, ups=1.61, wpb=14043.8, bsz=485.8, num_updates=44100, lr=1.04328e-05, gnorm=0.883, train_wall=62, wall=29954
2021-01-02 06:11:08 | INFO | train_inner | epoch 105:    416 / 421 symm_mse=5.58, loss=3.495, nll_loss=1.111, ppl=2.16, wps=22379.9, ups=1.6, wpb=13989, bsz=497.6, num_updates=44200, lr=1.0421e-05, gnorm=0.862, train_wall=62, wall=30017
2021-01-02 06:11:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:11:28 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_mse 0 | loss 5.212 | nll_loss 3.695 | ppl 12.95 | bleu 22.54 | wps 6046 | wpb 10324.2 | bsz 375 | num_updates 44205 | best_bleu 22.72
2021-01-02 06:11:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:11:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 105 @ 44205 updates, score 22.54) (writing took 3.0912521108984947 seconds)
2021-01-02 06:11:31 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-02 06:11:31 | INFO | train | epoch 105 | symm_mse 5.613 | loss 3.5 | nll_loss 1.112 | ppl 2.16 | wps 20652.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 44205 | lr 1.04204e-05 | gnorm 0.878 | train_wall 261 | wall 30040
2021-01-02 06:11:31 | INFO | fairseq.trainer | begin training epoch 106
2021-01-02 06:11:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:11:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:12:33 | INFO | train_inner | epoch 106:     95 / 421 symm_mse=5.693, loss=3.511, nll_loss=1.113, ppl=2.16, wps=16328.7, ups=1.18, wpb=13822.1, bsz=478.9, num_updates=44300, lr=1.04092e-05, gnorm=0.884, train_wall=62, wall=30102
2021-01-02 06:13:35 | INFO | train_inner | epoch 106:    195 / 421 symm_mse=5.362, loss=3.455, nll_loss=1.095, ppl=2.14, wps=22509.5, ups=1.61, wpb=14001.9, bsz=513.3, num_updates=44400, lr=1.03975e-05, gnorm=0.843, train_wall=62, wall=30164
2021-01-02 06:14:38 | INFO | train_inner | epoch 106:    295 / 421 symm_mse=5.73, loss=3.529, nll_loss=1.13, ppl=2.19, wps=22423.3, ups=1.61, wpb=13964, bsz=483.5, num_updates=44500, lr=1.03858e-05, gnorm=0.887, train_wall=62, wall=30226
2021-01-02 06:15:40 | INFO | train_inner | epoch 106:    395 / 421 symm_mse=5.589, loss=3.488, nll_loss=1.101, ppl=2.15, wps=22311, ups=1.59, wpb=14026.1, bsz=500.2, num_updates=44600, lr=1.03742e-05, gnorm=0.865, train_wall=63, wall=30289
2021-01-02 06:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:16:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:16:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:16:13 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_mse 0 | loss 5.216 | nll_loss 3.698 | ppl 12.98 | bleu 22.65 | wps 6045 | wpb 10324.2 | bsz 375 | num_updates 44626 | best_bleu 22.72
2021-01-02 06:16:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:16:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:16:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:16:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:16:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 106 @ 44626 updates, score 22.65) (writing took 3.1103581450879574 seconds)
2021-01-02 06:16:16 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-02 06:16:16 | INFO | train | epoch 106 | symm_mse 5.608 | loss 3.498 | nll_loss 1.111 | ppl 2.16 | wps 20627.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 44626 | lr 1.03711e-05 | gnorm 0.869 | train_wall 261 | wall 30325
2021-01-02 06:16:16 | INFO | fairseq.trainer | begin training epoch 107
2021-01-02 06:16:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:16:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:17:05 | INFO | train_inner | epoch 107:     74 / 421 symm_mse=5.66, loss=3.507, nll_loss=1.113, ppl=2.16, wps=16500.4, ups=1.18, wpb=14000, bsz=476.7, num_updates=44700, lr=1.03626e-05, gnorm=0.854, train_wall=62, wall=30374
2021-01-02 06:18:08 | INFO | train_inner | epoch 107:    174 / 421 symm_mse=5.583, loss=3.49, nll_loss=1.105, ppl=2.15, wps=22505.3, ups=1.6, wpb=14039.2, bsz=499.8, num_updates=44800, lr=1.0351e-05, gnorm=0.888, train_wall=62, wall=30436
2021-01-02 06:19:10 | INFO | train_inner | epoch 107:    274 / 421 symm_mse=5.578, loss=3.496, nll_loss=1.112, ppl=2.16, wps=22507.1, ups=1.61, wpb=13956.4, bsz=498.3, num_updates=44900, lr=1.03395e-05, gnorm=0.887, train_wall=62, wall=30498
2021-01-02 06:20:12 | INFO | train_inner | epoch 107:    374 / 421 symm_mse=5.549, loss=3.498, nll_loss=1.12, ppl=2.17, wps=22208.5, ups=1.61, wpb=13812.2, bsz=496, num_updates=45000, lr=1.0328e-05, gnorm=0.854, train_wall=62, wall=30560
2021-01-02 06:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:20:58 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_mse 0 | loss 5.215 | nll_loss 3.698 | ppl 12.98 | bleu 22.54 | wps 6013.7 | wpb 10324.2 | bsz 375 | num_updates 45047 | best_bleu 22.72
2021-01-02 06:20:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:21:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 107 @ 45047 updates, score 22.54) (writing took 3.1291315779089928 seconds)
2021-01-02 06:21:01 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-02 06:21:01 | INFO | train | epoch 107 | symm_mse 5.599 | loss 3.498 | nll_loss 1.111 | ppl 2.16 | wps 20656.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 45047 | lr 1.03226e-05 | gnorm 0.871 | train_wall 261 | wall 30609
2021-01-02 06:21:01 | INFO | fairseq.trainer | begin training epoch 108
2021-01-02 06:21:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:21:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:21:37 | INFO | train_inner | epoch 108:     53 / 421 symm_mse=5.482, loss=3.468, nll_loss=1.094, ppl=2.13, wps=16629.5, ups=1.18, wpb=14121.7, bsz=497.7, num_updates=45100, lr=1.03165e-05, gnorm=0.865, train_wall=62, wall=30645
2021-01-02 06:22:39 | INFO | train_inner | epoch 108:    153 / 421 symm_mse=5.747, loss=3.519, nll_loss=1.116, ppl=2.17, wps=22485.8, ups=1.62, wpb=13887.2, bsz=471.7, num_updates=45200, lr=1.03051e-05, gnorm=0.88, train_wall=62, wall=30707
2021-01-02 06:23:41 | INFO | train_inner | epoch 108:    253 / 421 symm_mse=5.656, loss=3.506, nll_loss=1.113, ppl=2.16, wps=22333.1, ups=1.6, wpb=13926.7, bsz=491.3, num_updates=45300, lr=1.02937e-05, gnorm=0.902, train_wall=62, wall=30769
2021-01-02 06:24:43 | INFO | train_inner | epoch 108:    353 / 421 symm_mse=5.623, loss=3.504, nll_loss=1.115, ppl=2.17, wps=22432.2, ups=1.61, wpb=13971.7, bsz=485, num_updates=45400, lr=1.02824e-05, gnorm=0.882, train_wall=62, wall=30832
2021-01-02 06:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:25:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:25:44 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_mse 0 | loss 5.214 | nll_loss 3.696 | ppl 12.96 | bleu 22.64 | wps 5450 | wpb 10324.2 | bsz 375 | num_updates 45468 | best_bleu 22.72
2021-01-02 06:25:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:25:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 108 @ 45468 updates, score 22.64) (writing took 3.0921927858144045 seconds)
2021-01-02 06:25:47 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-02 06:25:47 | INFO | train | epoch 108 | symm_mse 5.593 | loss 3.496 | nll_loss 1.111 | ppl 2.16 | wps 20580.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 45468 | lr 1.02747e-05 | gnorm 0.879 | train_wall 261 | wall 30895
2021-01-02 06:25:47 | INFO | fairseq.trainer | begin training epoch 109
2021-01-02 06:25:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:25:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:26:10 | INFO | train_inner | epoch 109:     32 / 421 symm_mse=5.537, loss=3.493, nll_loss=1.115, ppl=2.17, wps=16093.3, ups=1.16, wpb=13911, bsz=510.6, num_updates=45500, lr=1.02711e-05, gnorm=0.878, train_wall=62, wall=30918
2021-01-02 06:27:12 | INFO | train_inner | epoch 109:    132 / 421 symm_mse=5.56, loss=3.498, nll_loss=1.116, ppl=2.17, wps=22552.4, ups=1.61, wpb=13987.2, bsz=497.9, num_updates=45600, lr=1.02598e-05, gnorm=0.855, train_wall=62, wall=30980
2021-01-02 06:28:13 | INFO | train_inner | epoch 109:    232 / 421 symm_mse=5.568, loss=3.492, nll_loss=1.109, ppl=2.16, wps=22589.1, ups=1.62, wpb=13961.6, bsz=489.7, num_updates=45700, lr=1.02486e-05, gnorm=0.868, train_wall=62, wall=31042
2021-01-02 06:29:15 | INFO | train_inner | epoch 109:    332 / 421 symm_mse=5.695, loss=3.504, nll_loss=1.106, ppl=2.15, wps=22627.3, ups=1.62, wpb=13998.7, bsz=486.6, num_updates=45800, lr=1.02374e-05, gnorm=0.895, train_wall=62, wall=31104
2021-01-02 06:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:30:28 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_mse 0 | loss 5.217 | nll_loss 3.702 | ppl 13.01 | bleu 22.53 | wps 5896.8 | wpb 10324.2 | bsz 375 | num_updates 45889 | best_bleu 22.72
2021-01-02 06:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 109 @ 45889 updates, score 22.53) (writing took 3.1016241423785686 seconds)
2021-01-02 06:30:31 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-02 06:30:31 | INFO | train | epoch 109 | symm_mse 5.589 | loss 3.496 | nll_loss 1.11 | ppl 2.16 | wps 20672.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 45889 | lr 1.02274e-05 | gnorm 0.869 | train_wall 260 | wall 31180
2021-01-02 06:30:31 | INFO | fairseq.trainer | begin training epoch 110
2021-01-02 06:30:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:30:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:30:41 | INFO | train_inner | epoch 110:     11 / 421 symm_mse=5.468, loss=3.479, nll_loss=1.107, ppl=2.15, wps=16258.4, ups=1.16, wpb=13957.2, bsz=495.4, num_updates=45900, lr=1.02262e-05, gnorm=0.842, train_wall=62, wall=31190
2021-01-02 06:31:43 | INFO | train_inner | epoch 110:    111 / 421 symm_mse=5.675, loss=3.506, nll_loss=1.11, ppl=2.16, wps=22575.9, ups=1.62, wpb=13930.8, bsz=481.6, num_updates=46000, lr=1.02151e-05, gnorm=0.885, train_wall=62, wall=31251
2021-01-02 06:32:45 | INFO | train_inner | epoch 110:    211 / 421 symm_mse=5.665, loss=3.503, nll_loss=1.108, ppl=2.16, wps=22423.7, ups=1.6, wpb=14039.4, bsz=478.6, num_updates=46100, lr=1.0204e-05, gnorm=0.876, train_wall=62, wall=31314
2021-01-02 06:33:48 | INFO | train_inner | epoch 110:    311 / 421 symm_mse=5.492, loss=3.491, nll_loss=1.117, ppl=2.17, wps=22307.9, ups=1.61, wpb=13881.6, bsz=507.3, num_updates=46200, lr=1.01929e-05, gnorm=0.846, train_wall=62, wall=31376
2021-01-02 06:34:50 | INFO | train_inner | epoch 110:    411 / 421 symm_mse=5.509, loss=3.48, nll_loss=1.104, ppl=2.15, wps=22683.7, ups=1.61, wpb=14092.8, bsz=505.5, num_updates=46300, lr=1.01819e-05, gnorm=0.859, train_wall=62, wall=31438
2021-01-02 06:34:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:34:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:34:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:34:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:34:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:34:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:34:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:34:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:34:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:35:13 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_mse 0 | loss 5.216 | nll_loss 3.699 | ppl 12.98 | bleu 22.51 | wps 5883.7 | wpb 10324.2 | bsz 375 | num_updates 46310 | best_bleu 22.72
2021-01-02 06:35:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:35:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 110 @ 46310 updates, score 22.51) (writing took 3.1321099046617746 seconds)
2021-01-02 06:35:16 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-02 06:35:16 | INFO | train | epoch 110 | symm_mse 5.579 | loss 3.494 | nll_loss 1.11 | ppl 2.16 | wps 20644.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 46310 | lr 1.01808e-05 | gnorm 0.867 | train_wall 261 | wall 31465
2021-01-02 06:35:16 | INFO | fairseq.trainer | begin training epoch 111
2021-01-02 06:35:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:35:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:36:15 | INFO | train_inner | epoch 111:     90 / 421 symm_mse=5.549, loss=3.488, nll_loss=1.106, ppl=2.15, wps=16657.5, ups=1.18, wpb=14116, bsz=495.6, num_updates=46400, lr=1.0171e-05, gnorm=0.848, train_wall=61, wall=31523
2021-01-02 06:37:16 | INFO | train_inner | epoch 111:    190 / 421 symm_mse=5.564, loss=3.495, nll_loss=1.113, ppl=2.16, wps=22609.9, ups=1.62, wpb=13991.1, bsz=489.8, num_updates=46500, lr=1.016e-05, gnorm=0.847, train_wall=62, wall=31585
2021-01-02 06:38:19 | INFO | train_inner | epoch 111:    290 / 421 symm_mse=5.587, loss=3.499, nll_loss=1.113, ppl=2.16, wps=22311.4, ups=1.6, wpb=13902.4, bsz=483.1, num_updates=46600, lr=1.01491e-05, gnorm=0.876, train_wall=62, wall=31647
2021-01-02 06:39:22 | INFO | train_inner | epoch 111:    390 / 421 symm_mse=5.558, loss=3.492, nll_loss=1.111, ppl=2.16, wps=22077.6, ups=1.58, wpb=13943.7, bsz=496.4, num_updates=46700, lr=1.01382e-05, gnorm=0.865, train_wall=63, wall=31710
2021-01-02 06:39:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:39:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:39:58 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_mse 0 | loss 5.214 | nll_loss 3.697 | ppl 12.97 | bleu 22.58 | wps 5901.8 | wpb 10324.2 | bsz 375 | num_updates 46731 | best_bleu 22.72
2021-01-02 06:39:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:39:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:39:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:40:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 111 @ 46731 updates, score 22.58) (writing took 3.1005166098475456 seconds)
2021-01-02 06:40:01 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-02 06:40:01 | INFO | train | epoch 111 | symm_mse 5.571 | loss 3.494 | nll_loss 1.11 | ppl 2.16 | wps 20630.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 46731 | lr 1.01349e-05 | gnorm 0.863 | train_wall 261 | wall 31750
2021-01-02 06:40:01 | INFO | fairseq.trainer | begin training epoch 112
2021-01-02 06:40:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:40:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:40:47 | INFO | train_inner | epoch 112:     69 / 421 symm_mse=5.679, loss=3.504, nll_loss=1.108, ppl=2.16, wps=16258.5, ups=1.17, wpb=13896.2, bsz=496.3, num_updates=46800, lr=1.01274e-05, gnorm=0.89, train_wall=62, wall=31796
2021-01-02 06:41:50 | INFO | train_inner | epoch 112:    169 / 421 symm_mse=5.566, loss=3.49, nll_loss=1.107, ppl=2.15, wps=22447, ups=1.61, wpb=13971.6, bsz=489.3, num_updates=46900, lr=1.01166e-05, gnorm=0.874, train_wall=62, wall=31858
2021-01-02 06:42:52 | INFO | train_inner | epoch 112:    269 / 421 symm_mse=5.713, loss=3.521, nll_loss=1.122, ppl=2.18, wps=22221.9, ups=1.6, wpb=13883.7, bsz=472.8, num_updates=47000, lr=1.01058e-05, gnorm=0.882, train_wall=62, wall=31921
2021-01-02 06:43:54 | INFO | train_inner | epoch 112:    369 / 421 symm_mse=5.429, loss=3.47, nll_loss=1.102, ppl=2.15, wps=22554.1, ups=1.61, wpb=14044.4, bsz=503.4, num_updates=47100, lr=1.00951e-05, gnorm=0.851, train_wall=62, wall=31983
2021-01-02 06:44:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:44:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:44:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:44:44 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_mse 0 | loss 5.211 | nll_loss 3.696 | ppl 12.96 | bleu 22.56 | wps 5973.3 | wpb 10324.2 | bsz 375 | num_updates 47152 | best_bleu 22.72
2021-01-02 06:44:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:44:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 112 @ 47152 updates, score 22.56) (writing took 2.9072811864316463 seconds)
2021-01-02 06:44:47 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-02 06:44:47 | INFO | train | epoch 112 | symm_mse 5.569 | loss 3.493 | nll_loss 1.11 | ppl 2.16 | wps 20606.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 47152 | lr 1.00895e-05 | gnorm 0.87 | train_wall 262 | wall 32035
2021-01-02 06:44:47 | INFO | fairseq.trainer | begin training epoch 113
2021-01-02 06:44:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:44:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:45:19 | INFO | train_inner | epoch 113:     48 / 421 symm_mse=5.619, loss=3.505, nll_loss=1.117, ppl=2.17, wps=16427.9, ups=1.18, wpb=13952, bsz=495.1, num_updates=47200, lr=1.00844e-05, gnorm=0.876, train_wall=62, wall=32068
2021-01-02 06:46:22 | INFO | train_inner | epoch 113:    148 / 421 symm_mse=5.397, loss=3.466, nll_loss=1.101, ppl=2.14, wps=22512.2, ups=1.61, wpb=14018, bsz=501.3, num_updates=47300, lr=1.00737e-05, gnorm=0.841, train_wall=62, wall=32130
2021-01-02 06:47:24 | INFO | train_inner | epoch 113:    248 / 421 symm_mse=5.473, loss=3.468, nll_loss=1.094, ppl=2.13, wps=22507.1, ups=1.6, wpb=14084.4, bsz=507.6, num_updates=47400, lr=1.00631e-05, gnorm=0.847, train_wall=62, wall=32193
2021-01-02 06:48:27 | INFO | train_inner | epoch 113:    348 / 421 symm_mse=5.442, loss=3.475, nll_loss=1.105, ppl=2.15, wps=22157.1, ups=1.59, wpb=13916.6, bsz=498.7, num_updates=47500, lr=1.00525e-05, gnorm=0.853, train_wall=63, wall=32255
2021-01-02 06:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:49:30 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_mse 0 | loss 5.214 | nll_loss 3.696 | ppl 12.96 | bleu 22.57 | wps 5622.5 | wpb 10324.2 | bsz 375 | num_updates 47573 | best_bleu 22.72
2021-01-02 06:49:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:49:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 113 @ 47573 updates, score 22.57) (writing took 3.1240983679890633 seconds)
2021-01-02 06:49:33 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-02 06:49:33 | INFO | train | epoch 113 | symm_mse 5.56 | loss 3.491 | nll_loss 1.109 | ppl 2.16 | wps 20502.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 47573 | lr 1.00448e-05 | gnorm 0.862 | train_wall 262 | wall 32322
2021-01-02 06:49:33 | INFO | fairseq.trainer | begin training epoch 114
2021-01-02 06:49:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:49:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:49:53 | INFO | train_inner | epoch 114:     27 / 421 symm_mse=5.805, loss=3.537, nll_loss=1.129, ppl=2.19, wps=16099, ups=1.16, wpb=13894.5, bsz=464.8, num_updates=47600, lr=1.00419e-05, gnorm=0.894, train_wall=62, wall=32342
2021-01-02 06:50:55 | INFO | train_inner | epoch 114:    127 / 421 symm_mse=5.693, loss=3.526, nll_loss=1.13, ppl=2.19, wps=22494.7, ups=1.62, wpb=13900.1, bsz=483, num_updates=47700, lr=1.00314e-05, gnorm=0.887, train_wall=62, wall=32404
2021-01-02 06:51:58 | INFO | train_inner | epoch 114:    227 / 421 symm_mse=5.601, loss=3.498, nll_loss=1.111, ppl=2.16, wps=22470, ups=1.6, wpb=14030.9, bsz=489.1, num_updates=47800, lr=1.00209e-05, gnorm=0.879, train_wall=62, wall=32466
2021-01-02 06:53:00 | INFO | train_inner | epoch 114:    327 / 421 symm_mse=5.697, loss=3.508, nll_loss=1.111, ppl=2.16, wps=22138.2, ups=1.59, wpb=13927.4, bsz=480.9, num_updates=47900, lr=1.00104e-05, gnorm=0.88, train_wall=63, wall=32529
2021-01-02 06:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:54:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:54:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:54:16 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_mse 0 | loss 5.212 | nll_loss 3.694 | ppl 12.94 | bleu 22.69 | wps 5858.6 | wpb 10324.2 | bsz 375 | num_updates 47994 | best_bleu 22.72
2021-01-02 06:54:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 114 @ 47994 updates, score 22.69) (writing took 3.06917866691947 seconds)
2021-01-02 06:54:19 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-02 06:54:19 | INFO | train | epoch 114 | symm_mse 5.556 | loss 3.491 | nll_loss 1.109 | ppl 2.16 | wps 20569 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 47994 | lr 1.00006e-05 | gnorm 0.87 | train_wall 262 | wall 32608
2021-01-02 06:54:19 | INFO | fairseq.trainer | begin training epoch 115
2021-01-02 06:54:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:54:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:54:26 | INFO | train_inner | epoch 115:      6 / 421 symm_mse=5.201, loss=3.426, nll_loss=1.082, ppl=2.12, wps=16244.3, ups=1.16, wpb=13966.9, bsz=528.5, num_updates=48000, lr=1e-05, gnorm=0.845, train_wall=62, wall=32615
2021-01-02 06:55:28 | INFO | train_inner | epoch 115:    106 / 421 symm_mse=5.623, loss=3.503, nll_loss=1.114, ppl=2.16, wps=22673.2, ups=1.62, wpb=13974.8, bsz=486.3, num_updates=48100, lr=9.9896e-06, gnorm=0.864, train_wall=61, wall=32676
2021-01-02 06:56:30 | INFO | train_inner | epoch 115:    206 / 421 symm_mse=5.537, loss=3.49, nll_loss=1.111, ppl=2.16, wps=22418.1, ups=1.61, wpb=13948.4, bsz=491.4, num_updates=48200, lr=9.97923e-06, gnorm=0.859, train_wall=62, wall=32739
2021-01-02 06:57:32 | INFO | train_inner | epoch 115:    306 / 421 symm_mse=5.56, loss=3.484, nll_loss=1.101, ppl=2.14, wps=22416.5, ups=1.61, wpb=13922.8, bsz=493, num_updates=48300, lr=9.9689e-06, gnorm=0.867, train_wall=62, wall=32801
2021-01-02 06:58:35 | INFO | train_inner | epoch 115:    406 / 421 symm_mse=5.493, loss=3.481, nll_loss=1.106, ppl=2.15, wps=22609.3, ups=1.6, wpb=14107, bsz=498.6, num_updates=48400, lr=9.95859e-06, gnorm=0.852, train_wall=62, wall=32863
2021-01-02 06:58:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 06:58:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:58:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:58:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:58:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:58:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:58:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:58:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:58:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:58:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:58:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 06:58:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 06:58:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 06:59:01 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_mse 0 | loss 5.212 | nll_loss 3.695 | ppl 12.95 | bleu 22.63 | wps 5890.9 | wpb 10324.2 | bsz 375 | num_updates 48415 | best_bleu 22.72
2021-01-02 06:59:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 06:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:59:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:59:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:59:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 06:59:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 115 @ 48415 updates, score 22.63) (writing took 3.0957080274820328 seconds)
2021-01-02 06:59:04 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-02 06:59:04 | INFO | train | epoch 115 | symm_mse 5.549 | loss 3.49 | nll_loss 1.108 | ppl 2.16 | wps 20663.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48415 | lr 9.95705e-06 | gnorm 0.862 | train_wall 261 | wall 32892
2021-01-02 06:59:04 | INFO | fairseq.trainer | begin training epoch 116
2021-01-02 06:59:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 06:59:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:00:00 | INFO | train_inner | epoch 116:     85 / 421 symm_mse=5.485, loss=3.477, nll_loss=1.104, ppl=2.15, wps=16443.2, ups=1.18, wpb=13963.5, bsz=489.8, num_updates=48500, lr=9.94832e-06, gnorm=0.855, train_wall=62, wall=32948
2021-01-02 07:01:02 | INFO | train_inner | epoch 116:    185 / 421 symm_mse=5.477, loss=3.466, nll_loss=1.09, ppl=2.13, wps=22576.5, ups=1.59, wpb=14169.6, bsz=503.2, num_updates=48600, lr=9.93808e-06, gnorm=0.843, train_wall=63, wall=33011
2021-01-02 07:02:05 | INFO | train_inner | epoch 116:    285 / 421 symm_mse=5.681, loss=3.518, nll_loss=1.123, ppl=2.18, wps=22008.2, ups=1.61, wpb=13708, bsz=476.4, num_updates=48700, lr=9.92787e-06, gnorm=0.905, train_wall=62, wall=33073
2021-01-02 07:03:07 | INFO | train_inner | epoch 116:    385 / 421 symm_mse=5.516, loss=3.487, nll_loss=1.109, ppl=2.16, wps=22544.4, ups=1.61, wpb=14043.3, bsz=500.8, num_updates=48800, lr=9.91769e-06, gnorm=0.848, train_wall=62, wall=33135
2021-01-02 07:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:03:46 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_mse 0 | loss 5.212 | nll_loss 3.696 | ppl 12.96 | bleu 22.61 | wps 5845.4 | wpb 10324.2 | bsz 375 | num_updates 48836 | best_bleu 22.72
2021-01-02 07:03:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:03:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 116 @ 48836 updates, score 22.61) (writing took 2.6182256136089563 seconds)
2021-01-02 07:03:49 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-02 07:03:49 | INFO | train | epoch 116 | symm_mse 5.548 | loss 3.489 | nll_loss 1.108 | ppl 2.16 | wps 20640.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48836 | lr 9.91404e-06 | gnorm 0.865 | train_wall 261 | wall 33177
2021-01-02 07:03:49 | INFO | fairseq.trainer | begin training epoch 117
2021-01-02 07:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:03:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:03:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:04:32 | INFO | train_inner | epoch 117:     64 / 421 symm_mse=5.766, loss=3.535, nll_loss=1.132, ppl=2.19, wps=16400.9, ups=1.18, wpb=13880.7, bsz=472.5, num_updates=48900, lr=9.90755e-06, gnorm=0.905, train_wall=62, wall=33220
2021-01-02 07:05:34 | INFO | train_inner | epoch 117:    164 / 421 symm_mse=5.424, loss=3.462, nll_loss=1.094, ppl=2.13, wps=22492.1, ups=1.6, wpb=14082.2, bsz=504.9, num_updates=49000, lr=9.89743e-06, gnorm=0.855, train_wall=62, wall=33283
2021-01-02 07:06:36 | INFO | train_inner | epoch 117:    264 / 421 symm_mse=5.485, loss=3.478, nll_loss=1.104, ppl=2.15, wps=22641.8, ups=1.62, wpb=14012, bsz=506.2, num_updates=49100, lr=9.88735e-06, gnorm=0.86, train_wall=62, wall=33345
2021-01-02 07:07:38 | INFO | train_inner | epoch 117:    364 / 421 symm_mse=5.517, loss=3.484, nll_loss=1.105, ppl=2.15, wps=22301.2, ups=1.61, wpb=13879.7, bsz=483.9, num_updates=49200, lr=9.8773e-06, gnorm=0.848, train_wall=62, wall=33407
2021-01-02 07:08:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:08:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:08:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:08:31 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.692 | ppl 12.92 | bleu 22.56 | wps 5870.8 | wpb 10324.2 | bsz 375 | num_updates 49257 | best_bleu 22.72
2021-01-02 07:08:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:08:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:08:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 117 @ 49257 updates, score 22.56) (writing took 3.08154752291739 seconds)
2021-01-02 07:08:34 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-02 07:08:34 | INFO | train | epoch 117 | symm_mse 5.542 | loss 3.489 | nll_loss 1.108 | ppl 2.16 | wps 20620.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 49257 | lr 9.87158e-06 | gnorm 0.864 | train_wall 261 | wall 33463
2021-01-02 07:08:34 | INFO | fairseq.trainer | begin training epoch 118
2021-01-02 07:08:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:08:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:09:04 | INFO | train_inner | epoch 118:     43 / 421 symm_mse=5.559, loss=3.497, nll_loss=1.116, ppl=2.17, wps=16318.6, ups=1.17, wpb=13980.3, bsz=496.7, num_updates=49300, lr=9.86727e-06, gnorm=0.864, train_wall=62, wall=33493
2021-01-02 07:10:06 | INFO | train_inner | epoch 118:    143 / 421 symm_mse=5.593, loss=3.496, nll_loss=1.11, ppl=2.16, wps=22469.6, ups=1.61, wpb=13971.2, bsz=478.3, num_updates=49400, lr=9.85728e-06, gnorm=0.87, train_wall=62, wall=33555
2021-01-02 07:11:09 | INFO | train_inner | epoch 118:    243 / 421 symm_mse=5.681, loss=3.511, nll_loss=1.115, ppl=2.17, wps=22308.6, ups=1.6, wpb=13927.7, bsz=482.2, num_updates=49500, lr=9.84732e-06, gnorm=0.908, train_wall=62, wall=33617
2021-01-02 07:12:11 | INFO | train_inner | epoch 118:    343 / 421 symm_mse=5.382, loss=3.468, nll_loss=1.105, ppl=2.15, wps=22163, ups=1.6, wpb=13889.3, bsz=503.8, num_updates=49600, lr=9.83739e-06, gnorm=0.84, train_wall=62, wall=33680
2021-01-02 07:13:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:13:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:13:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:13:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:13:17 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_mse 0 | loss 5.212 | nll_loss 3.695 | ppl 12.95 | bleu 22.56 | wps 5772.3 | wpb 10324.2 | bsz 375 | num_updates 49678 | best_bleu 22.72
2021-01-02 07:13:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 118 @ 49678 updates, score 22.56) (writing took 3.140084905549884 seconds)
2021-01-02 07:13:21 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-02 07:13:21 | INFO | train | epoch 118 | symm_mse 5.531 | loss 3.487 | nll_loss 1.108 | ppl 2.16 | wps 20528.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 49678 | lr 9.82966e-06 | gnorm 0.866 | train_wall 262 | wall 33749
2021-01-02 07:13:21 | INFO | fairseq.trainer | begin training epoch 119
2021-01-02 07:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:13:37 | INFO | train_inner | epoch 119:     22 / 421 symm_mse=5.445, loss=3.475, nll_loss=1.106, ppl=2.15, wps=16161.5, ups=1.16, wpb=13910.5, bsz=498.5, num_updates=49700, lr=9.82749e-06, gnorm=0.86, train_wall=62, wall=33766
2021-01-02 07:14:39 | INFO | train_inner | epoch 119:    122 / 421 symm_mse=5.537, loss=3.482, nll_loss=1.1, ppl=2.14, wps=22705.8, ups=1.62, wpb=14040.1, bsz=488.4, num_updates=49800, lr=9.81761e-06, gnorm=0.848, train_wall=62, wall=33828
2021-01-02 07:15:41 | INFO | train_inner | epoch 119:    222 / 421 symm_mse=5.739, loss=3.519, nll_loss=1.118, ppl=2.17, wps=22577.3, ups=1.61, wpb=14013, bsz=472.1, num_updates=49900, lr=9.80777e-06, gnorm=0.891, train_wall=62, wall=33890
2021-01-02 07:16:44 | INFO | train_inner | epoch 119:    322 / 421 symm_mse=5.411, loss=3.478, nll_loss=1.114, ppl=2.16, wps=22455.2, ups=1.61, wpb=13969, bsz=510.7, num_updates=50000, lr=9.79796e-06, gnorm=0.851, train_wall=62, wall=33952
2021-01-02 07:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:17:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:17:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:17:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:17:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:17:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:18:02 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_mse 0 | loss 5.209 | nll_loss 3.694 | ppl 12.94 | bleu 22.47 | wps 5844.1 | wpb 10324.2 | bsz 375 | num_updates 50099 | best_bleu 22.72
2021-01-02 07:18:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:18:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:18:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:18:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:18:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:18:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:18:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:18:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 119 @ 50099 updates, score 22.47) (writing took 3.0708052925765514 seconds)
2021-01-02 07:18:05 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-02 07:18:05 | INFO | train | epoch 119 | symm_mse 5.527 | loss 3.486 | nll_loss 1.108 | ppl 2.16 | wps 20693.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 50099 | lr 9.78827e-06 | gnorm 0.865 | train_wall 260 | wall 34033
2021-01-02 07:18:05 | INFO | fairseq.trainer | begin training epoch 120
2021-01-02 07:18:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:18:09 | INFO | train_inner | epoch 120:      1 / 421 symm_mse=5.441, loss=3.464, nll_loss=1.094, ppl=2.13, wps=16395.7, ups=1.17, wpb=13969.4, bsz=502.6, num_updates=50100, lr=9.78818e-06, gnorm=0.861, train_wall=62, wall=34037
2021-01-02 07:19:11 | INFO | train_inner | epoch 120:    101 / 421 symm_mse=5.468, loss=3.474, nll_loss=1.1, ppl=2.14, wps=22649.4, ups=1.61, wpb=14059.1, bsz=491.9, num_updates=50200, lr=9.77842e-06, gnorm=0.868, train_wall=62, wall=34099
2021-01-02 07:20:13 | INFO | train_inner | epoch 120:    201 / 421 symm_mse=5.471, loss=3.475, nll_loss=1.102, ppl=2.15, wps=22421.9, ups=1.61, wpb=13908.9, bsz=492.2, num_updates=50300, lr=9.7687e-06, gnorm=0.877, train_wall=62, wall=34161
2021-01-02 07:21:15 | INFO | train_inner | epoch 120:    301 / 421 symm_mse=5.72, loss=3.51, nll_loss=1.11, ppl=2.16, wps=22542.8, ups=1.61, wpb=13996.1, bsz=476, num_updates=50400, lr=9.759e-06, gnorm=0.878, train_wall=62, wall=34223
2021-01-02 07:22:17 | INFO | train_inner | epoch 120:    401 / 421 symm_mse=5.486, loss=3.493, nll_loss=1.121, ppl=2.17, wps=22427.9, ups=1.6, wpb=14006, bsz=494.6, num_updates=50500, lr=9.74933e-06, gnorm=0.84, train_wall=62, wall=34286
2021-01-02 07:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:22:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:22:48 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_mse 0 | loss 5.21 | nll_loss 3.692 | ppl 12.92 | bleu 22.59 | wps 5364.5 | wpb 10324.2 | bsz 375 | num_updates 50520 | best_bleu 22.72
2021-01-02 07:22:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:22:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 120 @ 50520 updates, score 22.59) (writing took 3.107303064316511 seconds)
2021-01-02 07:22:51 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-02 07:22:51 | INFO | train | epoch 120 | symm_mse 5.521 | loss 3.485 | nll_loss 1.107 | ppl 2.15 | wps 20542 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50520 | lr 9.7474e-06 | gnorm 0.866 | train_wall 261 | wall 34320
2021-01-02 07:22:51 | INFO | fairseq.trainer | begin training epoch 121
2021-01-02 07:22:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:23:44 | INFO | train_inner | epoch 121:     80 / 421 symm_mse=5.504, loss=3.475, nll_loss=1.098, ppl=2.14, wps=15987.1, ups=1.15, wpb=13853.9, bsz=502, num_updates=50600, lr=9.7397e-06, gnorm=0.873, train_wall=62, wall=34372
2021-01-02 07:24:46 | INFO | train_inner | epoch 121:    180 / 421 symm_mse=5.536, loss=3.49, nll_loss=1.11, ppl=2.16, wps=22238.1, ups=1.6, wpb=13886.3, bsz=493, num_updates=50700, lr=9.73009e-06, gnorm=0.884, train_wall=62, wall=34435
2021-01-02 07:25:49 | INFO | train_inner | epoch 121:    280 / 421 symm_mse=5.403, loss=3.465, nll_loss=1.1, ppl=2.14, wps=22473, ups=1.59, wpb=14115.8, bsz=502.8, num_updates=50800, lr=9.7205e-06, gnorm=0.839, train_wall=63, wall=34498
2021-01-02 07:26:52 | INFO | train_inner | epoch 121:    380 / 421 symm_mse=5.625, loss=3.508, nll_loss=1.12, ppl=2.17, wps=22334.1, ups=1.6, wpb=13937.7, bsz=495.9, num_updates=50900, lr=9.71095e-06, gnorm=0.885, train_wall=62, wall=34560
2021-01-02 07:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:27:34 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_mse 0 | loss 5.21 | nll_loss 3.694 | ppl 12.94 | bleu 22.54 | wps 5826.4 | wpb 10324.2 | bsz 375 | num_updates 50941 | best_bleu 22.72
2021-01-02 07:27:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:27:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:27:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 121 @ 50941 updates, score 22.54) (writing took 3.1171567738056183 seconds)
2021-01-02 07:27:37 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-02 07:27:37 | INFO | train | epoch 121 | symm_mse 5.52 | loss 3.485 | nll_loss 1.107 | ppl 2.15 | wps 20546.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50941 | lr 9.70704e-06 | gnorm 0.87 | train_wall 262 | wall 34606
2021-01-02 07:27:37 | INFO | fairseq.trainer | begin training epoch 122
2021-01-02 07:27:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:28:17 | INFO | train_inner | epoch 122:     59 / 421 symm_mse=5.516, loss=3.483, nll_loss=1.105, ppl=2.15, wps=16368.9, ups=1.17, wpb=13933, bsz=477.6, num_updates=51000, lr=9.70143e-06, gnorm=0.868, train_wall=62, wall=34645
2021-01-02 07:29:19 | INFO | train_inner | epoch 122:    159 / 421 symm_mse=5.463, loss=3.476, nll_loss=1.104, ppl=2.15, wps=22612.5, ups=1.6, wpb=14155.5, bsz=499.6, num_updates=51100, lr=9.69193e-06, gnorm=0.838, train_wall=62, wall=34708
2021-01-02 07:30:22 | INFO | train_inner | epoch 122:    259 / 421 symm_mse=5.354, loss=3.46, nll_loss=1.1, ppl=2.14, wps=22540.2, ups=1.61, wpb=14008.1, bsz=513.3, num_updates=51200, lr=9.68246e-06, gnorm=0.834, train_wall=62, wall=34770
2021-01-02 07:31:24 | INFO | train_inner | epoch 122:    359 / 421 symm_mse=5.526, loss=3.48, nll_loss=1.1, ppl=2.14, wps=22433.3, ups=1.6, wpb=14063.1, bsz=482.8, num_updates=51300, lr=9.67302e-06, gnorm=0.867, train_wall=62, wall=34833
2021-01-02 07:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:32:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:32:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:32:20 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_mse 0 | loss 5.209 | nll_loss 3.693 | ppl 12.93 | bleu 22.41 | wps 5871.5 | wpb 10324.2 | bsz 375 | num_updates 51362 | best_bleu 22.72
2021-01-02 07:32:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 122 @ 51362 updates, score 22.41) (writing took 3.068610394373536 seconds)
2021-01-02 07:32:23 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2021-01-02 07:32:23 | INFO | train | epoch 122 | symm_mse 5.508 | loss 3.483 | nll_loss 1.106 | ppl 2.15 | wps 20617.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 51362 | lr 9.66718e-06 | gnorm 0.864 | train_wall 261 | wall 34891
2021-01-02 07:32:23 | INFO | fairseq.trainer | begin training epoch 123
2021-01-02 07:32:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:32:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:32:49 | INFO | train_inner | epoch 123:     38 / 421 symm_mse=5.669, loss=3.517, nll_loss=1.125, ppl=2.18, wps=16065.1, ups=1.18, wpb=13655.5, bsz=474.9, num_updates=51400, lr=9.6636e-06, gnorm=0.918, train_wall=62, wall=34918
2021-01-02 07:33:51 | INFO | train_inner | epoch 123:    138 / 421 symm_mse=5.567, loss=3.495, nll_loss=1.111, ppl=2.16, wps=22536.8, ups=1.61, wpb=13976.4, bsz=481.8, num_updates=51500, lr=9.65422e-06, gnorm=0.929, train_wall=62, wall=34980
2021-01-02 07:34:54 | INFO | train_inner | epoch 123:    238 / 421 symm_mse=5.532, loss=3.485, nll_loss=1.106, ppl=2.15, wps=22494, ups=1.6, wpb=14039.3, bsz=500.6, num_updates=51600, lr=9.64486e-06, gnorm=0.854, train_wall=62, wall=35042
2021-01-02 07:35:56 | INFO | train_inner | epoch 123:    338 / 421 symm_mse=5.391, loss=3.465, nll_loss=1.101, ppl=2.15, wps=22574.9, ups=1.61, wpb=13996.7, bsz=503.3, num_updates=51700, lr=9.63552e-06, gnorm=0.835, train_wall=62, wall=35104
2021-01-02 07:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:36:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:36:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:36:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:36:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:36:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:36:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:36:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:36:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:36:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:36:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:36:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:37:04 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_mse 0 | loss 5.21 | nll_loss 3.692 | ppl 12.92 | bleu 22.56 | wps 5926.2 | wpb 10324.2 | bsz 375 | num_updates 51783 | best_bleu 22.72
2021-01-02 07:37:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:37:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:37:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:37:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:37:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:37:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:37:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:37:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 123 @ 51783 updates, score 22.56) (writing took 3.142751032486558 seconds)
2021-01-02 07:37:08 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2021-01-02 07:37:08 | INFO | train | epoch 123 | symm_mse 5.509 | loss 3.483 | nll_loss 1.107 | ppl 2.15 | wps 20638.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 51783 | lr 9.6278e-06 | gnorm 0.875 | train_wall 261 | wall 35176
2021-01-02 07:37:08 | INFO | fairseq.trainer | begin training epoch 124
2021-01-02 07:37:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:37:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:37:21 | INFO | train_inner | epoch 124:     17 / 421 symm_mse=5.558, loss=3.488, nll_loss=1.107, ppl=2.15, wps=16215, ups=1.17, wpb=13896, bsz=488.3, num_updates=51800, lr=9.62622e-06, gnorm=0.879, train_wall=62, wall=35190
2021-01-02 07:38:23 | INFO | train_inner | epoch 124:    117 / 421 symm_mse=5.526, loss=3.48, nll_loss=1.101, ppl=2.15, wps=22890.3, ups=1.63, wpb=14064.5, bsz=493.2, num_updates=51900, lr=9.61694e-06, gnorm=0.873, train_wall=61, wall=35251
2021-01-02 07:39:25 | INFO | train_inner | epoch 124:    217 / 421 symm_mse=5.435, loss=3.464, nll_loss=1.095, ppl=2.14, wps=22437.4, ups=1.6, wpb=14007.4, bsz=492.8, num_updates=52000, lr=9.60769e-06, gnorm=0.855, train_wall=62, wall=35314
2021-01-02 07:40:28 | INFO | train_inner | epoch 124:    317 / 421 symm_mse=5.365, loss=3.466, nll_loss=1.106, ppl=2.15, wps=22351, ups=1.6, wpb=13972, bsz=515, num_updates=52100, lr=9.59846e-06, gnorm=0.84, train_wall=62, wall=35376
2021-01-02 07:41:30 | INFO | train_inner | epoch 124:    417 / 421 symm_mse=5.617, loss=3.51, nll_loss=1.122, ppl=2.18, wps=22289.5, ups=1.6, wpb=13928.4, bsz=475.2, num_updates=52200, lr=9.58927e-06, gnorm=0.892, train_wall=62, wall=35439
2021-01-02 07:41:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:41:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:41:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:41:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:41:50 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.692 | ppl 12.93 | bleu 22.52 | wps 5846.5 | wpb 10324.2 | bsz 375 | num_updates 52204 | best_bleu 22.72
2021-01-02 07:41:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:41:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 124 @ 52204 updates, score 22.52) (writing took 3.099220050498843 seconds)
2021-01-02 07:41:53 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2021-01-02 07:41:53 | INFO | train | epoch 124 | symm_mse 5.501 | loss 3.482 | nll_loss 1.107 | ppl 2.15 | wps 20619.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 52204 | lr 9.5889e-06 | gnorm 0.867 | train_wall 261 | wall 35461
2021-01-02 07:41:53 | INFO | fairseq.trainer | begin training epoch 125
2021-01-02 07:41:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:42:55 | INFO | train_inner | epoch 125:     96 / 421 symm_mse=5.579, loss=3.491, nll_loss=1.106, ppl=2.15, wps=16273.5, ups=1.18, wpb=13835, bsz=484.3, num_updates=52300, lr=9.58009e-06, gnorm=0.908, train_wall=61, wall=35524
2021-01-02 07:43:58 | INFO | train_inner | epoch 125:    196 / 421 symm_mse=5.425, loss=3.473, nll_loss=1.106, ppl=2.15, wps=22380.4, ups=1.6, wpb=13957.3, bsz=502.6, num_updates=52400, lr=9.57095e-06, gnorm=0.84, train_wall=62, wall=35586
2021-01-02 07:45:00 | INFO | train_inner | epoch 125:    296 / 421 symm_mse=5.625, loss=3.497, nll_loss=1.107, ppl=2.15, wps=22656.5, ups=1.61, wpb=14079, bsz=480.6, num_updates=52500, lr=9.56183e-06, gnorm=0.888, train_wall=62, wall=35648
2021-01-02 07:46:02 | INFO | train_inner | epoch 125:    396 / 421 symm_mse=5.409, loss=3.471, nll_loss=1.106, ppl=2.15, wps=22387.4, ups=1.6, wpb=14031.2, bsz=496.6, num_updates=52600, lr=9.55274e-06, gnorm=0.838, train_wall=62, wall=35711
2021-01-02 07:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:46:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:46:35 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.691 | ppl 12.91 | bleu 22.52 | wps 5912.5 | wpb 10324.2 | bsz 375 | num_updates 52625 | best_bleu 22.72
2021-01-02 07:46:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:46:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 125 @ 52625 updates, score 22.52) (writing took 3.3801000993698835 seconds)
2021-01-02 07:46:38 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2021-01-02 07:46:38 | INFO | train | epoch 125 | symm_mse 5.488 | loss 3.481 | nll_loss 1.106 | ppl 2.15 | wps 20606.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 52625 | lr 9.55047e-06 | gnorm 0.865 | train_wall 261 | wall 35747
2021-01-02 07:46:38 | INFO | fairseq.trainer | begin training epoch 126
2021-01-02 07:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:47:27 | INFO | train_inner | epoch 126:     75 / 421 symm_mse=5.346, loss=3.456, nll_loss=1.097, ppl=2.14, wps=16471.7, ups=1.18, wpb=13991, bsz=505.5, num_updates=52700, lr=9.54367e-06, gnorm=0.837, train_wall=61, wall=35796
2021-01-02 07:48:29 | INFO | train_inner | epoch 126:    175 / 421 symm_mse=5.337, loss=3.456, nll_loss=1.098, ppl=2.14, wps=22578.4, ups=1.61, wpb=14009.5, bsz=506.2, num_updates=52800, lr=9.53463e-06, gnorm=0.838, train_wall=62, wall=35858
2021-01-02 07:49:32 | INFO | train_inner | epoch 126:    275 / 421 symm_mse=5.652, loss=3.519, nll_loss=1.128, ppl=2.19, wps=22189.7, ups=1.61, wpb=13824.1, bsz=477.8, num_updates=52900, lr=9.52561e-06, gnorm=0.892, train_wall=62, wall=35920
2021-01-02 07:50:34 | INFO | train_inner | epoch 126:    375 / 421 symm_mse=5.537, loss=3.486, nll_loss=1.107, ppl=2.15, wps=22288.2, ups=1.6, wpb=13908.7, bsz=493.2, num_updates=53000, lr=9.51662e-06, gnorm=0.871, train_wall=62, wall=35983
2021-01-02 07:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:51:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:51:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:51:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:51:21 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_mse 0 | loss 5.209 | nll_loss 3.693 | ppl 12.93 | bleu 22.63 | wps 5292.6 | wpb 10324.2 | bsz 375 | num_updates 53046 | best_bleu 22.72
2021-01-02 07:51:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 126 @ 53046 updates, score 22.63) (writing took 3.1572402101010084 seconds)
2021-01-02 07:51:24 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2021-01-02 07:51:24 | INFO | train | epoch 126 | symm_mse 5.491 | loss 3.481 | nll_loss 1.106 | ppl 2.15 | wps 20551.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53046 | lr 9.51249e-06 | gnorm 0.859 | train_wall 261 | wall 36033
2021-01-02 07:51:24 | INFO | fairseq.trainer | begin training epoch 127
2021-01-02 07:51:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:51:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:51:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:52:01 | INFO | train_inner | epoch 127:     54 / 421 symm_mse=5.581, loss=3.492, nll_loss=1.106, ppl=2.15, wps=16120.4, ups=1.16, wpb=13942.5, bsz=473, num_updates=53100, lr=9.50765e-06, gnorm=0.863, train_wall=62, wall=36069
2021-01-02 07:53:03 | INFO | train_inner | epoch 127:    154 / 421 symm_mse=5.498, loss=3.481, nll_loss=1.105, ppl=2.15, wps=22280.7, ups=1.59, wpb=13969.6, bsz=493.5, num_updates=53200, lr=9.49871e-06, gnorm=0.853, train_wall=63, wall=36132
2021-01-02 07:54:06 | INFO | train_inner | epoch 127:    254 / 421 symm_mse=5.372, loss=3.459, nll_loss=1.097, ppl=2.14, wps=22522.4, ups=1.61, wpb=14016.7, bsz=501.6, num_updates=53300, lr=9.4898e-06, gnorm=0.848, train_wall=62, wall=36194
2021-01-02 07:55:08 | INFO | train_inner | epoch 127:    354 / 421 symm_mse=5.528, loss=3.486, nll_loss=1.108, ppl=2.16, wps=22558.4, ups=1.61, wpb=14029.2, bsz=490.6, num_updates=53400, lr=9.48091e-06, gnorm=0.856, train_wall=62, wall=36256
2021-01-02 07:55:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 07:55:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:55:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:55:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:55:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:55:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:55:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:55:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:55:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:55:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 07:55:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 07:55:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 07:56:08 | INFO | valid | epoch 127 | valid on 'valid' subset | symm_mse 0 | loss 5.21 | nll_loss 3.693 | ppl 12.94 | bleu 22.49 | wps 5402.4 | wpb 10324.2 | bsz 375 | num_updates 53467 | best_bleu 22.72
2021-01-02 07:56:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 07:56:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:56:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:56:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:56:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:56:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:56:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:56:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 127 @ 53467 updates, score 22.49) (writing took 3.123561942949891 seconds)
2021-01-02 07:56:11 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2021-01-02 07:56:11 | INFO | train | epoch 127 | symm_mse 5.482 | loss 3.48 | nll_loss 1.106 | ppl 2.15 | wps 20539.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53467 | lr 9.47497e-06 | gnorm 0.856 | train_wall 261 | wall 36319
2021-01-02 07:56:11 | INFO | fairseq.trainer | begin training epoch 128
2021-01-02 07:56:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 07:56:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 07:56:34 | INFO | train_inner | epoch 128:     33 / 421 symm_mse=5.491, loss=3.481, nll_loss=1.107, ppl=2.15, wps=16194.3, ups=1.16, wpb=13966.9, bsz=496.8, num_updates=53500, lr=9.47204e-06, gnorm=0.863, train_wall=61, wall=36342
2021-01-02 07:57:36 | INFO | train_inner | epoch 128:    133 / 421 symm_mse=5.539, loss=3.491, nll_loss=1.112, ppl=2.16, wps=22402.4, ups=1.61, wpb=13921.2, bsz=488, num_updates=53600, lr=9.4632e-06, gnorm=0.866, train_wall=62, wall=36405
2021-01-02 07:58:39 | INFO | train_inner | epoch 128:    233 / 421 symm_mse=5.489, loss=3.479, nll_loss=1.103, ppl=2.15, wps=22542.7, ups=1.6, wpb=14080.6, bsz=490.6, num_updates=53700, lr=9.45439e-06, gnorm=0.861, train_wall=62, wall=36467
2021-01-02 07:59:41 | INFO | train_inner | epoch 128:    333 / 421 symm_mse=5.455, loss=3.475, nll_loss=1.105, ppl=2.15, wps=22356.6, ups=1.6, wpb=13957.7, bsz=489, num_updates=53800, lr=9.4456e-06, gnorm=0.862, train_wall=62, wall=36530
2021-01-02 08:00:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:00:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:00:53 | INFO | valid | epoch 128 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.692 | ppl 12.92 | bleu 22.53 | wps 5824.9 | wpb 10324.2 | bsz 375 | num_updates 53888 | best_bleu 22.72
2021-01-02 08:00:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:00:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 128 @ 53888 updates, score 22.53) (writing took 3.150372987613082 seconds)
2021-01-02 08:00:56 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2021-01-02 08:00:56 | INFO | train | epoch 128 | symm_mse 5.481 | loss 3.48 | nll_loss 1.106 | ppl 2.15 | wps 20589.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53888 | lr 9.43788e-06 | gnorm 0.863 | train_wall 261 | wall 36605
2021-01-02 08:00:56 | INFO | fairseq.trainer | begin training epoch 129
2021-01-02 08:00:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:00:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:01:07 | INFO | train_inner | epoch 129:     12 / 421 symm_mse=5.396, loss=3.472, nll_loss=1.109, ppl=2.16, wps=16093.9, ups=1.16, wpb=13849.5, bsz=502.1, num_updates=53900, lr=9.43683e-06, gnorm=0.876, train_wall=62, wall=36616
2021-01-02 08:02:09 | INFO | train_inner | epoch 129:    112 / 421 symm_mse=5.422, loss=3.466, nll_loss=1.099, ppl=2.14, wps=22444.3, ups=1.61, wpb=13914.1, bsz=501.1, num_updates=54000, lr=9.42809e-06, gnorm=0.857, train_wall=62, wall=36678
2021-01-02 08:03:11 | INFO | train_inner | epoch 129:    212 / 421 symm_mse=5.354, loss=3.456, nll_loss=1.095, ppl=2.14, wps=22722.5, ups=1.63, wpb=13959, bsz=522, num_updates=54100, lr=9.41937e-06, gnorm=0.858, train_wall=61, wall=36739
2021-01-02 08:04:13 | INFO | train_inner | epoch 129:    312 / 421 symm_mse=5.594, loss=3.492, nll_loss=1.105, ppl=2.15, wps=22771.2, ups=1.6, wpb=14206.1, bsz=474.5, num_updates=54200, lr=9.41068e-06, gnorm=0.862, train_wall=62, wall=36801
2021-01-02 08:05:15 | INFO | train_inner | epoch 129:    412 / 421 symm_mse=5.528, loss=3.496, nll_loss=1.119, ppl=2.17, wps=22373.1, ups=1.61, wpb=13899.2, bsz=478.6, num_updates=54300, lr=9.40201e-06, gnorm=0.874, train_wall=62, wall=36864
2021-01-02 08:05:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:05:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:05:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:05:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:05:37 | INFO | valid | epoch 129 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.691 | ppl 12.91 | bleu 22.61 | wps 5965.4 | wpb 10324.2 | bsz 375 | num_updates 54309 | best_bleu 22.72
2021-01-02 08:05:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:05:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 129 @ 54309 updates, score 22.61) (writing took 3.193006783723831 seconds)
2021-01-02 08:05:41 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2021-01-02 08:05:41 | INFO | train | epoch 129 | symm_mse 5.477 | loss 3.478 | nll_loss 1.105 | ppl 2.15 | wps 20691.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 54309 | lr 9.40123e-06 | gnorm 0.868 | train_wall 260 | wall 36889
2021-01-02 08:05:41 | INFO | fairseq.trainer | begin training epoch 130
2021-01-02 08:05:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:05:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:06:40 | INFO | train_inner | epoch 130:     91 / 421 symm_mse=5.403, loss=3.463, nll_loss=1.097, ppl=2.14, wps=16483.7, ups=1.17, wpb=14042.1, bsz=491.2, num_updates=54400, lr=9.39336e-06, gnorm=0.842, train_wall=62, wall=36949
2021-01-02 08:07:42 | INFO | train_inner | epoch 130:    191 / 421 symm_mse=5.462, loss=3.479, nll_loss=1.106, ppl=2.15, wps=22395.4, ups=1.62, wpb=13866.6, bsz=486.7, num_updates=54500, lr=9.38474e-06, gnorm=0.848, train_wall=62, wall=37011
2021-01-02 08:08:45 | INFO | train_inner | epoch 130:    291 / 421 symm_mse=5.609, loss=3.504, nll_loss=1.118, ppl=2.17, wps=22330.6, ups=1.6, wpb=13919.2, bsz=479.9, num_updates=54600, lr=9.37614e-06, gnorm=0.888, train_wall=62, wall=37073
2021-01-02 08:09:47 | INFO | train_inner | epoch 130:    391 / 421 symm_mse=5.362, loss=3.457, nll_loss=1.096, ppl=2.14, wps=22569.7, ups=1.61, wpb=14053.1, bsz=508.8, num_updates=54700, lr=9.36757e-06, gnorm=0.847, train_wall=62, wall=37135
2021-01-02 08:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:10:22 | INFO | valid | epoch 130 | valid on 'valid' subset | symm_mse 0 | loss 5.207 | nll_loss 3.69 | ppl 12.91 | bleu 22.62 | wps 5829.1 | wpb 10324.2 | bsz 375 | num_updates 54730 | best_bleu 22.72
2021-01-02 08:10:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:10:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 130 @ 54730 updates, score 22.62) (writing took 3.1607185918837786 seconds)
2021-01-02 08:10:26 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2021-01-02 08:10:26 | INFO | train | epoch 130 | symm_mse 5.464 | loss 3.477 | nll_loss 1.105 | ppl 2.15 | wps 20632.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 54730 | lr 9.365e-06 | gnorm 0.858 | train_wall 261 | wall 37174
2021-01-02 08:10:26 | INFO | fairseq.trainer | begin training epoch 131
2021-01-02 08:10:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:10:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:10:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:11:12 | INFO | train_inner | epoch 131:     70 / 421 symm_mse=5.418, loss=3.464, nll_loss=1.096, ppl=2.14, wps=16369.5, ups=1.17, wpb=13975.1, bsz=498, num_updates=54800, lr=9.35902e-06, gnorm=0.855, train_wall=62, wall=37221
2021-01-02 08:12:14 | INFO | train_inner | epoch 131:    170 / 421 symm_mse=5.609, loss=3.502, nll_loss=1.116, ppl=2.17, wps=22385.5, ups=1.61, wpb=13871.8, bsz=491, num_updates=54900, lr=9.35049e-06, gnorm=0.884, train_wall=62, wall=37283
2021-01-02 08:13:16 | INFO | train_inner | epoch 131:    270 / 421 symm_mse=5.436, loss=3.478, nll_loss=1.11, ppl=2.16, wps=22607.2, ups=1.62, wpb=13997.9, bsz=492.2, num_updates=55000, lr=9.34199e-06, gnorm=0.852, train_wall=62, wall=37344
2021-01-02 08:14:19 | INFO | train_inner | epoch 131:    370 / 421 symm_mse=5.426, loss=3.465, nll_loss=1.097, ppl=2.14, wps=22482.4, ups=1.59, wpb=14128.9, bsz=490.2, num_updates=55100, lr=9.33351e-06, gnorm=0.85, train_wall=63, wall=37407
2021-01-02 08:14:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:14:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:14:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:14:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:15:07 | INFO | valid | epoch 131 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.69 | ppl 12.91 | bleu 22.66 | wps 5881.3 | wpb 10324.2 | bsz 375 | num_updates 55151 | best_bleu 22.72
2021-01-02 08:15:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:15:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:15:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:15:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:15:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:15:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:15:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:15:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 131 @ 55151 updates, score 22.66) (writing took 3.1762500926852226 seconds)
2021-01-02 08:15:11 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2021-01-02 08:15:11 | INFO | train | epoch 131 | symm_mse 5.467 | loss 3.477 | nll_loss 1.105 | ppl 2.15 | wps 20632.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 55151 | lr 9.32919e-06 | gnorm 0.856 | train_wall 261 | wall 37459
2021-01-02 08:15:11 | INFO | fairseq.trainer | begin training epoch 132
2021-01-02 08:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:15:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:15:44 | INFO | train_inner | epoch 132:     49 / 421 symm_mse=5.544, loss=3.493, nll_loss=1.113, ppl=2.16, wps=16285.6, ups=1.17, wpb=13873.5, bsz=484.8, num_updates=55200, lr=9.32505e-06, gnorm=0.851, train_wall=62, wall=37493
2021-01-02 08:16:47 | INFO | train_inner | epoch 132:    149 / 421 symm_mse=5.441, loss=3.477, nll_loss=1.108, ppl=2.16, wps=22491.8, ups=1.6, wpb=14065.3, bsz=496.2, num_updates=55300, lr=9.31661e-06, gnorm=0.862, train_wall=62, wall=37555
2021-01-02 08:17:49 | INFO | train_inner | epoch 132:    249 / 421 symm_mse=5.382, loss=3.452, nll_loss=1.087, ppl=2.12, wps=22774, ups=1.6, wpb=14214.4, bsz=496.5, num_updates=55400, lr=9.3082e-06, gnorm=0.843, train_wall=62, wall=37617
2021-01-02 08:18:51 | INFO | train_inner | epoch 132:    349 / 421 symm_mse=5.414, loss=3.472, nll_loss=1.105, ppl=2.15, wps=22180.4, ups=1.6, wpb=13841.7, bsz=502.2, num_updates=55500, lr=9.29981e-06, gnorm=0.867, train_wall=62, wall=37680
2021-01-02 08:19:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:19:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:19:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:19:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:19:53 | INFO | valid | epoch 132 | valid on 'valid' subset | symm_mse 0 | loss 5.207 | nll_loss 3.691 | ppl 12.91 | bleu 22.71 | wps 5917.4 | wpb 10324.2 | bsz 375 | num_updates 55572 | best_bleu 22.72
2021-01-02 08:19:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:19:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 132 @ 55572 updates, score 22.71) (writing took 3.1494356263428926 seconds)
2021-01-02 08:19:56 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2021-01-02 08:19:56 | INFO | train | epoch 132 | symm_mse 5.462 | loss 3.476 | nll_loss 1.105 | ppl 2.15 | wps 20597.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 55572 | lr 9.29378e-06 | gnorm 0.86 | train_wall 261 | wall 37745
2021-01-02 08:19:56 | INFO | fairseq.trainer | begin training epoch 133
2021-01-02 08:19:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:19:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:20:17 | INFO | train_inner | epoch 133:     28 / 421 symm_mse=5.52, loss=3.482, nll_loss=1.104, ppl=2.15, wps=16073, ups=1.17, wpb=13721.1, bsz=484.5, num_updates=55600, lr=9.29144e-06, gnorm=0.876, train_wall=62, wall=37765
2021-01-02 08:21:19 | INFO | train_inner | epoch 133:    128 / 421 symm_mse=5.403, loss=3.471, nll_loss=1.107, ppl=2.15, wps=22593.3, ups=1.61, wpb=14001, bsz=506.6, num_updates=55700, lr=9.2831e-06, gnorm=0.852, train_wall=62, wall=37827
2021-01-02 08:22:21 | INFO | train_inner | epoch 133:    228 / 421 symm_mse=5.669, loss=3.511, nll_loss=1.118, ppl=2.17, wps=22223.6, ups=1.6, wpb=13881.1, bsz=469.8, num_updates=55800, lr=9.27478e-06, gnorm=0.904, train_wall=62, wall=37890
2021-01-02 08:23:24 | INFO | train_inner | epoch 133:    328 / 421 symm_mse=5.266, loss=3.454, nll_loss=1.106, ppl=2.15, wps=22413.8, ups=1.6, wpb=14043.3, bsz=512.9, num_updates=55900, lr=9.26648e-06, gnorm=0.823, train_wall=62, wall=37952
2021-01-02 08:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:24:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:24:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:24:39 | INFO | valid | epoch 133 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.691 | ppl 12.91 | bleu 22.56 | wps 5875 | wpb 10324.2 | bsz 375 | num_updates 55993 | best_bleu 22.72
2021-01-02 08:24:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 133 @ 55993 updates, score 22.56) (writing took 3.169230217114091 seconds)
2021-01-02 08:24:42 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2021-01-02 08:24:42 | INFO | train | epoch 133 | symm_mse 5.456 | loss 3.475 | nll_loss 1.105 | ppl 2.15 | wps 20550.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 55993 | lr 9.25878e-06 | gnorm 0.864 | train_wall 262 | wall 38031
2021-01-02 08:24:42 | INFO | fairseq.trainer | begin training epoch 134
2021-01-02 08:24:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:24:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:24:50 | INFO | train_inner | epoch 134:      7 / 421 symm_mse=5.487, loss=3.473, nll_loss=1.098, ppl=2.14, wps=16134.4, ups=1.16, wpb=13889.3, bsz=476, num_updates=56000, lr=9.2582e-06, gnorm=0.872, train_wall=63, wall=38038
2021-01-02 08:25:52 | INFO | train_inner | epoch 134:    107 / 421 symm_mse=5.359, loss=3.451, nll_loss=1.09, ppl=2.13, wps=22521.5, ups=1.61, wpb=13987.8, bsz=506.9, num_updates=56100, lr=9.24995e-06, gnorm=0.863, train_wall=62, wall=38101
2021-01-02 08:26:54 | INFO | train_inner | epoch 134:    207 / 421 symm_mse=5.595, loss=3.506, nll_loss=1.121, ppl=2.18, wps=22207, ups=1.61, wpb=13824.7, bsz=481.8, num_updates=56200, lr=9.24171e-06, gnorm=0.889, train_wall=62, wall=38163
2021-01-02 08:27:57 | INFO | train_inner | epoch 134:    307 / 421 symm_mse=5.584, loss=3.496, nll_loss=1.111, ppl=2.16, wps=22506.4, ups=1.61, wpb=14007.4, bsz=477.7, num_updates=56300, lr=9.2335e-06, gnorm=0.876, train_wall=62, wall=38225
2021-01-02 08:28:59 | INFO | train_inner | epoch 134:    407 / 421 symm_mse=5.297, loss=3.45, nll_loss=1.095, ppl=2.14, wps=22628.1, ups=1.6, wpb=14152.7, bsz=501.6, num_updates=56400, lr=9.22531e-06, gnorm=0.834, train_wall=62, wall=38288
2021-01-02 08:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:29:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:29:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:29:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:29:25 | INFO | valid | epoch 134 | valid on 'valid' subset | symm_mse 0 | loss 5.208 | nll_loss 3.69 | ppl 12.91 | bleu 22.68 | wps 5925.2 | wpb 10324.2 | bsz 375 | num_updates 56414 | best_bleu 22.72
2021-01-02 08:29:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:29:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 134 @ 56414 updates, score 22.68) (writing took 3.1612909231334925 seconds)
2021-01-02 08:29:28 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2021-01-02 08:29:28 | INFO | train | epoch 134 | symm_mse 5.444 | loss 3.474 | nll_loss 1.104 | ppl 2.15 | wps 20603.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 56414 | lr 9.22417e-06 | gnorm 0.864 | train_wall 261 | wall 38316
2021-01-02 08:29:28 | INFO | fairseq.trainer | begin training epoch 135
2021-01-02 08:29:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:29:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:30:24 | INFO | train_inner | epoch 135:     86 / 421 symm_mse=5.484, loss=3.491, nll_loss=1.118, ppl=2.17, wps=16307.6, ups=1.18, wpb=13853.9, bsz=490.2, num_updates=56500, lr=9.21714e-06, gnorm=0.858, train_wall=62, wall=38373
2021-01-02 08:31:27 | INFO | train_inner | epoch 135:    186 / 421 symm_mse=5.362, loss=3.465, nll_loss=1.107, ppl=2.15, wps=22512.8, ups=1.6, wpb=14083.1, bsz=505, num_updates=56600, lr=9.209e-06, gnorm=0.841, train_wall=62, wall=38435
2021-01-02 08:32:29 | INFO | train_inner | epoch 135:    286 / 421 symm_mse=5.487, loss=3.477, nll_loss=1.101, ppl=2.15, wps=22480.7, ups=1.6, wpb=14089, bsz=481.4, num_updates=56700, lr=9.20087e-06, gnorm=0.872, train_wall=62, wall=38498
2021-01-02 08:33:31 | INFO | train_inner | epoch 135:    386 / 421 symm_mse=5.346, loss=3.455, nll_loss=1.097, ppl=2.14, wps=22290.9, ups=1.61, wpb=13823.2, bsz=497.7, num_updates=56800, lr=9.19277e-06, gnorm=0.846, train_wall=62, wall=38560
2021-01-02 08:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:34:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:34:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:34:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:34:11 | INFO | valid | epoch 135 | valid on 'valid' subset | symm_mse 0 | loss 5.209 | nll_loss 3.69 | ppl 12.91 | bleu 22.64 | wps 5282.2 | wpb 10324.2 | bsz 375 | num_updates 56835 | best_bleu 22.72
2021-01-02 08:34:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:34:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 135 @ 56835 updates, score 22.64) (writing took 3.1808229852467775 seconds)
2021-01-02 08:34:15 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2021-01-02 08:34:15 | INFO | train | epoch 135 | symm_mse 5.445 | loss 3.474 | nll_loss 1.104 | ppl 2.15 | wps 20506.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 56835 | lr 9.18994e-06 | gnorm 0.857 | train_wall 261 | wall 38603
2021-01-02 08:34:15 | INFO | fairseq.trainer | begin training epoch 136
2021-01-02 08:34:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:34:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:34:58 | INFO | train_inner | epoch 136:     65 / 421 symm_mse=5.638, loss=3.502, nll_loss=1.111, ppl=2.16, wps=15837.9, ups=1.15, wpb=13757.8, bsz=476.9, num_updates=56900, lr=9.18469e-06, gnorm=0.884, train_wall=62, wall=38647
2021-01-02 08:36:01 | INFO | train_inner | epoch 136:    165 / 421 symm_mse=5.555, loss=3.495, nll_loss=1.115, ppl=2.17, wps=22275.2, ups=1.59, wpb=13990.1, bsz=480.3, num_updates=57000, lr=9.17663e-06, gnorm=0.87, train_wall=63, wall=38709
2021-01-02 08:37:04 | INFO | train_inner | epoch 136:    265 / 421 symm_mse=5.174, loss=3.427, nll_loss=1.086, ppl=2.12, wps=22521.9, ups=1.6, wpb=14083, bsz=516.5, num_updates=57100, lr=9.16859e-06, gnorm=0.825, train_wall=62, wall=38772
2021-01-02 08:38:06 | INFO | train_inner | epoch 136:    365 / 421 symm_mse=5.48, loss=3.471, nll_loss=1.095, ppl=2.14, wps=22456.1, ups=1.6, wpb=14023.1, bsz=493, num_updates=57200, lr=9.16057e-06, gnorm=0.862, train_wall=62, wall=38834
2021-01-02 08:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:38:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:38:58 | INFO | valid | epoch 136 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.688 | ppl 12.89 | bleu 22.68 | wps 5964.7 | wpb 10324.2 | bsz 375 | num_updates 57256 | best_bleu 22.72
2021-01-02 08:38:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:39:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:39:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:39:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 136 @ 57256 updates, score 22.68) (writing took 3.1047705840319395 seconds)
2021-01-02 08:39:01 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2021-01-02 08:39:01 | INFO | train | epoch 136 | symm_mse 5.437 | loss 3.472 | nll_loss 1.103 | ppl 2.15 | wps 20542.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 57256 | lr 9.15609e-06 | gnorm 0.858 | train_wall 262 | wall 38889
2021-01-02 08:39:01 | INFO | fairseq.trainer | begin training epoch 137
2021-01-02 08:39:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:39:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:39:31 | INFO | train_inner | epoch 137:     44 / 421 symm_mse=5.534, loss=3.489, nll_loss=1.11, ppl=2.16, wps=16314.7, ups=1.17, wpb=13902.7, bsz=482.4, num_updates=57300, lr=9.15258e-06, gnorm=0.884, train_wall=62, wall=38920
2021-01-02 08:40:34 | INFO | train_inner | epoch 137:    144 / 421 symm_mse=5.378, loss=3.461, nll_loss=1.098, ppl=2.14, wps=22452.6, ups=1.6, wpb=14012.8, bsz=500.2, num_updates=57400, lr=9.1446e-06, gnorm=0.84, train_wall=62, wall=38982
2021-01-02 08:41:36 | INFO | train_inner | epoch 137:    244 / 421 symm_mse=5.307, loss=3.444, nll_loss=1.087, ppl=2.13, wps=22677, ups=1.61, wpb=14122.8, bsz=507.5, num_updates=57500, lr=9.13664e-06, gnorm=0.847, train_wall=62, wall=39044
2021-01-02 08:42:38 | INFO | train_inner | epoch 137:    344 / 421 symm_mse=5.342, loss=3.461, nll_loss=1.103, ppl=2.15, wps=22587.3, ups=1.61, wpb=14032.3, bsz=505, num_updates=57600, lr=9.12871e-06, gnorm=0.839, train_wall=62, wall=39106
2021-01-02 08:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:43:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:43:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:43:43 | INFO | valid | epoch 137 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.66 | wps 5829.1 | wpb 10324.2 | bsz 375 | num_updates 57677 | best_bleu 22.72
2021-01-02 08:43:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:43:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 137 @ 57677 updates, score 22.66) (writing took 3.234858635812998 seconds)
2021-01-02 08:43:46 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2021-01-02 08:43:46 | INFO | train | epoch 137 | symm_mse 5.438 | loss 3.472 | nll_loss 1.103 | ppl 2.15 | wps 20641.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 57677 | lr 9.12261e-06 | gnorm 0.863 | train_wall 261 | wall 39174
2021-01-02 08:43:46 | INFO | fairseq.trainer | begin training epoch 138
2021-01-02 08:43:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:43:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:44:03 | INFO | train_inner | epoch 138:     23 / 421 symm_mse=5.657, loss=3.506, nll_loss=1.113, ppl=2.16, wps=16158, ups=1.17, wpb=13779.6, bsz=461.5, num_updates=57700, lr=9.1208e-06, gnorm=0.908, train_wall=62, wall=39192
2021-01-02 08:45:05 | INFO | train_inner | epoch 138:    123 / 421 symm_mse=5.341, loss=3.46, nll_loss=1.101, ppl=2.15, wps=22508.4, ups=1.62, wpb=13936.9, bsz=493.2, num_updates=57800, lr=9.1129e-06, gnorm=0.836, train_wall=62, wall=39254
2021-01-02 08:46:08 | INFO | train_inner | epoch 138:    223 / 421 symm_mse=5.334, loss=3.448, nll_loss=1.089, ppl=2.13, wps=22363.4, ups=1.6, wpb=14018.1, bsz=501.2, num_updates=57900, lr=9.10503e-06, gnorm=0.839, train_wall=62, wall=39316
2021-01-02 08:47:11 | INFO | train_inner | epoch 138:    323 / 421 symm_mse=5.56, loss=3.502, nll_loss=1.121, ppl=2.18, wps=22164.5, ups=1.59, wpb=13958.9, bsz=491.7, num_updates=58000, lr=9.09718e-06, gnorm=0.881, train_wall=63, wall=39379
2021-01-02 08:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:48:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:48:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:48:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:48:28 | INFO | valid | epoch 138 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.689 | ppl 12.9 | bleu 22.65 | wps 5998.7 | wpb 10324.2 | bsz 375 | num_updates 58098 | best_bleu 22.72
2021-01-02 08:48:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 138 @ 58098 updates, score 22.65) (writing took 3.115048887208104 seconds)
2021-01-02 08:48:32 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2021-01-02 08:48:32 | INFO | train | epoch 138 | symm_mse 5.432 | loss 3.472 | nll_loss 1.103 | ppl 2.15 | wps 20576.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 58098 | lr 9.0895e-06 | gnorm 0.855 | train_wall 262 | wall 39460
2021-01-02 08:48:32 | INFO | fairseq.trainer | begin training epoch 139
2021-01-02 08:48:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:48:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:48:36 | INFO | train_inner | epoch 139:      2 / 421 symm_mse=5.437, loss=3.473, nll_loss=1.105, ppl=2.15, wps=16319.5, ups=1.17, wpb=13931.4, bsz=490.7, num_updates=58100, lr=9.08934e-06, gnorm=0.86, train_wall=62, wall=39465
2021-01-02 08:49:38 | INFO | train_inner | epoch 139:    102 / 421 symm_mse=5.284, loss=3.442, nll_loss=1.088, ppl=2.13, wps=22500.7, ups=1.61, wpb=13998, bsz=503, num_updates=58200, lr=9.08153e-06, gnorm=0.845, train_wall=62, wall=39527
2021-01-02 08:50:41 | INFO | train_inner | epoch 139:    202 / 421 symm_mse=5.607, loss=3.494, nll_loss=1.107, ppl=2.15, wps=22502.7, ups=1.59, wpb=14152.5, bsz=481.4, num_updates=58300, lr=9.07374e-06, gnorm=0.879, train_wall=63, wall=39590
2021-01-02 08:51:44 | INFO | train_inner | epoch 139:    302 / 421 symm_mse=5.383, loss=3.474, nll_loss=1.113, ppl=2.16, wps=22109.2, ups=1.59, wpb=13937.9, bsz=507.2, num_updates=58400, lr=9.06597e-06, gnorm=0.845, train_wall=63, wall=39653
2021-01-02 08:52:47 | INFO | train_inner | epoch 139:    402 / 421 symm_mse=5.427, loss=3.479, nll_loss=1.113, ppl=2.16, wps=22114.4, ups=1.59, wpb=13928.6, bsz=484.1, num_updates=58500, lr=9.05822e-06, gnorm=0.856, train_wall=63, wall=39716
2021-01-02 08:52:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:53:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:53:16 | INFO | valid | epoch 139 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.687 | ppl 12.88 | bleu 22.67 | wps 5865.1 | wpb 10324.2 | bsz 375 | num_updates 58519 | best_bleu 22.72
2021-01-02 08:53:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:53:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 139 @ 58519 updates, score 22.67) (writing took 3.150045981630683 seconds)
2021-01-02 08:53:19 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2021-01-02 08:53:19 | INFO | train | epoch 139 | symm_mse 5.42 | loss 3.47 | nll_loss 1.103 | ppl 2.15 | wps 20436.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 58519 | lr 9.05675e-06 | gnorm 0.858 | train_wall 264 | wall 39748
2021-01-02 08:53:19 | INFO | fairseq.trainer | begin training epoch 140
2021-01-02 08:53:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:53:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:54:13 | INFO | train_inner | epoch 140:     81 / 421 symm_mse=5.47, loss=3.474, nll_loss=1.101, ppl=2.15, wps=16210.1, ups=1.17, wpb=13850.3, bsz=481.8, num_updates=58600, lr=9.05048e-06, gnorm=0.877, train_wall=62, wall=39801
2021-01-02 08:55:16 | INFO | train_inner | epoch 140:    181 / 421 symm_mse=5.339, loss=3.452, nll_loss=1.092, ppl=2.13, wps=22246.8, ups=1.59, wpb=13955.5, bsz=503.3, num_updates=58700, lr=9.04277e-06, gnorm=0.862, train_wall=63, wall=39864
2021-01-02 08:56:18 | INFO | train_inner | epoch 140:    281 / 421 symm_mse=5.328, loss=3.46, nll_loss=1.104, ppl=2.15, wps=22403.1, ups=1.6, wpb=14042.3, bsz=499.2, num_updates=58800, lr=9.03508e-06, gnorm=0.845, train_wall=62, wall=39927
2021-01-02 08:57:21 | INFO | train_inner | epoch 140:    381 / 421 symm_mse=5.506, loss=3.487, nll_loss=1.11, ppl=2.16, wps=22309.6, ups=1.6, wpb=13915.2, bsz=478.8, num_updates=58900, lr=9.02741e-06, gnorm=0.873, train_wall=62, wall=39989
2021-01-02 08:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 08:57:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:57:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:57:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:57:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:57:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:57:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:57:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:57:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:57:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 08:57:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 08:57:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 08:58:03 | INFO | valid | epoch 140 | valid on 'valid' subset | symm_mse 0 | loss 5.209 | nll_loss 3.688 | ppl 12.89 | bleu 22.68 | wps 5798.8 | wpb 10324.2 | bsz 375 | num_updates 58940 | best_bleu 22.72
2021-01-02 08:58:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 08:58:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:58:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:58:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:58:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:58:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:58:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:58:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 140 @ 58940 updates, score 22.68) (writing took 3.1402031667530537 seconds)
2021-01-02 08:58:06 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2021-01-02 08:58:06 | INFO | train | epoch 140 | symm_mse 5.414 | loss 3.469 | nll_loss 1.102 | ppl 2.15 | wps 20517 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 58940 | lr 9.02434e-06 | gnorm 0.866 | train_wall 262 | wall 40034
2021-01-02 08:58:06 | INFO | fairseq.trainer | begin training epoch 141
2021-01-02 08:58:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 08:58:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 08:58:46 | INFO | train_inner | epoch 141:     60 / 421 symm_mse=5.438, loss=3.478, nll_loss=1.11, ppl=2.16, wps=16222.8, ups=1.17, wpb=13880.8, bsz=494.8, num_updates=59000, lr=9.01975e-06, gnorm=0.871, train_wall=62, wall=40075
2021-01-02 08:59:49 | INFO | train_inner | epoch 141:    160 / 421 symm_mse=5.369, loss=3.458, nll_loss=1.096, ppl=2.14, wps=22269.8, ups=1.59, wpb=14011, bsz=500.8, num_updates=59100, lr=9.01212e-06, gnorm=0.848, train_wall=63, wall=40138
2021-01-02 09:00:52 | INFO | train_inner | epoch 141:    260 / 421 symm_mse=5.431, loss=3.469, nll_loss=1.1, ppl=2.14, wps=22320.7, ups=1.59, wpb=14051.8, bsz=484.6, num_updates=59200, lr=9.0045e-06, gnorm=0.874, train_wall=63, wall=40200
2021-01-02 09:01:55 | INFO | train_inner | epoch 141:    360 / 421 symm_mse=5.256, loss=3.444, nll_loss=1.094, ppl=2.13, wps=22349.9, ups=1.59, wpb=14045.7, bsz=505.7, num_updates=59300, lr=8.99691e-06, gnorm=0.826, train_wall=63, wall=40263
2021-01-02 09:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:02:50 | INFO | valid | epoch 141 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.66 | wps 5838.5 | wpb 10324.2 | bsz 375 | num_updates 59361 | best_bleu 22.72
2021-01-02 09:02:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:02:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:02:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 141 @ 59361 updates, score 22.66) (writing took 3.1230740286409855 seconds)
2021-01-02 09:02:53 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2021-01-02 09:02:53 | INFO | train | epoch 141 | symm_mse 5.411 | loss 3.468 | nll_loss 1.102 | ppl 2.15 | wps 20464.5 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 59361 | lr 8.99228e-06 | gnorm 0.862 | train_wall 263 | wall 40322
2021-01-02 09:02:53 | INFO | fairseq.trainer | begin training epoch 142
2021-01-02 09:02:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:02:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:03:21 | INFO | train_inner | epoch 142:     39 / 421 symm_mse=5.689, loss=3.509, nll_loss=1.112, ppl=2.16, wps=16160.1, ups=1.16, wpb=13913.4, bsz=478.8, num_updates=59400, lr=8.98933e-06, gnorm=0.927, train_wall=62, wall=40349
2021-01-02 09:04:23 | INFO | train_inner | epoch 142:    139 / 421 symm_mse=5.317, loss=3.457, nll_loss=1.103, ppl=2.15, wps=22533.2, ups=1.6, wpb=14087.6, bsz=504.5, num_updates=59500, lr=8.98177e-06, gnorm=0.822, train_wall=62, wall=40412
2021-01-02 09:05:27 | INFO | train_inner | epoch 142:    239 / 421 symm_mse=5.38, loss=3.46, nll_loss=1.097, ppl=2.14, wps=22090.3, ups=1.58, wpb=13961.6, bsz=495.8, num_updates=59600, lr=8.97424e-06, gnorm=0.855, train_wall=63, wall=40475
2021-01-02 09:06:29 | INFO | train_inner | epoch 142:    339 / 421 symm_mse=5.487, loss=3.476, nll_loss=1.1, ppl=2.14, wps=22461.8, ups=1.6, wpb=14038.2, bsz=476.4, num_updates=59700, lr=8.96672e-06, gnorm=0.848, train_wall=62, wall=40538
2021-01-02 09:07:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:07:37 | INFO | valid | epoch 142 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.689 | ppl 12.89 | bleu 22.58 | wps 5809.6 | wpb 10324.2 | bsz 375 | num_updates 59782 | best_bleu 22.72
2021-01-02 09:07:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 142 @ 59782 updates, score 22.58) (writing took 3.098649760708213 seconds)
2021-01-02 09:07:40 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2021-01-02 09:07:40 | INFO | train | epoch 142 | symm_mse 5.409 | loss 3.468 | nll_loss 1.102 | ppl 2.15 | wps 20495.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 59782 | lr 8.96057e-06 | gnorm 0.851 | train_wall 263 | wall 40609
2021-01-02 09:07:40 | INFO | fairseq.trainer | begin training epoch 143
2021-01-02 09:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:07:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:07:55 | INFO | train_inner | epoch 143:     18 / 421 symm_mse=5.301, loss=3.454, nll_loss=1.101, ppl=2.14, wps=16057.2, ups=1.17, wpb=13735.8, bsz=495.1, num_updates=59800, lr=8.95922e-06, gnorm=0.856, train_wall=62, wall=40623
2021-01-02 09:08:57 | INFO | train_inner | epoch 143:    118 / 421 symm_mse=5.435, loss=3.47, nll_loss=1.1, ppl=2.14, wps=22344.6, ups=1.6, wpb=13974.8, bsz=490.6, num_updates=59900, lr=8.95173e-06, gnorm=0.857, train_wall=62, wall=40686
2021-01-02 09:10:00 | INFO | train_inner | epoch 143:    218 / 421 symm_mse=5.43, loss=3.468, nll_loss=1.099, ppl=2.14, wps=22402.1, ups=1.59, wpb=14124.9, bsz=486.2, num_updates=60000, lr=8.94427e-06, gnorm=0.857, train_wall=63, wall=40749
2021-01-02 09:11:03 | INFO | train_inner | epoch 143:    318 / 421 symm_mse=5.347, loss=3.461, nll_loss=1.102, ppl=2.15, wps=22411.1, ups=1.6, wpb=14031.6, bsz=502.4, num_updates=60100, lr=8.93683e-06, gnorm=0.845, train_wall=62, wall=40811
2021-01-02 09:12:06 | INFO | train_inner | epoch 143:    418 / 421 symm_mse=5.399, loss=3.475, nll_loss=1.112, ppl=2.16, wps=22027.2, ups=1.59, wpb=13815, bsz=497, num_updates=60200, lr=8.9294e-06, gnorm=0.862, train_wall=63, wall=40874
2021-01-02 09:12:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:12:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:12:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:12:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:12:24 | INFO | valid | epoch 143 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.686 | ppl 12.87 | bleu 22.63 | wps 5938.1 | wpb 10324.2 | bsz 375 | num_updates 60203 | best_bleu 22.72
2021-01-02 09:12:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:12:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 143 @ 60203 updates, score 22.63) (writing took 3.1676283441483974 seconds)
2021-01-02 09:12:27 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2021-01-02 09:12:27 | INFO | train | epoch 143 | symm_mse 5.406 | loss 3.468 | nll_loss 1.102 | ppl 2.15 | wps 20491 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 60203 | lr 8.92918e-06 | gnorm 0.859 | train_wall 263 | wall 40896
2021-01-02 09:12:27 | INFO | fairseq.trainer | begin training epoch 144
2021-01-02 09:12:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:12:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:13:31 | INFO | train_inner | epoch 144:     97 / 421 symm_mse=5.289, loss=3.443, nll_loss=1.09, ppl=2.13, wps=16418, ups=1.17, wpb=14045.9, bsz=505.2, num_updates=60300, lr=8.92199e-06, gnorm=0.855, train_wall=62, wall=40960
2021-01-02 09:14:34 | INFO | train_inner | epoch 144:    197 / 421 symm_mse=5.43, loss=3.475, nll_loss=1.108, ppl=2.15, wps=22183.2, ups=1.58, wpb=14002.8, bsz=487.9, num_updates=60400, lr=8.91461e-06, gnorm=0.857, train_wall=63, wall=41023
2021-01-02 09:15:37 | INFO | train_inner | epoch 144:    297 / 421 symm_mse=5.441, loss=3.477, nll_loss=1.107, ppl=2.15, wps=22085.4, ups=1.59, wpb=13873.5, bsz=498.7, num_updates=60500, lr=8.90724e-06, gnorm=0.861, train_wall=63, wall=41086
2021-01-02 09:16:40 | INFO | train_inner | epoch 144:    397 / 421 symm_mse=5.488, loss=3.478, nll_loss=1.103, ppl=2.15, wps=22052.7, ups=1.59, wpb=13908.2, bsz=475.1, num_updates=60600, lr=8.89988e-06, gnorm=0.887, train_wall=63, wall=41149
2021-01-02 09:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:16:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:16:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:16:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:16:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:17:12 | INFO | valid | epoch 144 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.688 | ppl 12.89 | bleu 22.49 | wps 5944.2 | wpb 10324.2 | bsz 375 | num_updates 60624 | best_bleu 22.72
2021-01-02 09:17:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:17:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 144 @ 60624 updates, score 22.49) (writing took 3.1095448210835457 seconds)
2021-01-02 09:17:15 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2021-01-02 09:17:15 | INFO | train | epoch 144 | symm_mse 5.402 | loss 3.467 | nll_loss 1.102 | ppl 2.15 | wps 20446.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 60624 | lr 8.89812e-06 | gnorm 0.858 | train_wall 264 | wall 41183
2021-01-02 09:17:15 | INFO | fairseq.trainer | begin training epoch 145
2021-01-02 09:17:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:17:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:18:06 | INFO | train_inner | epoch 145:     76 / 421 symm_mse=5.351, loss=3.455, nll_loss=1.094, ppl=2.13, wps=16272.6, ups=1.17, wpb=13908.5, bsz=486.6, num_updates=60700, lr=8.89255e-06, gnorm=0.833, train_wall=62, wall=41234
2021-01-02 09:19:08 | INFO | train_inner | epoch 145:    176 / 421 symm_mse=5.269, loss=3.45, nll_loss=1.102, ppl=2.15, wps=22384.7, ups=1.59, wpb=14034.8, bsz=514.8, num_updates=60800, lr=8.88523e-06, gnorm=0.827, train_wall=63, wall=41297
2021-01-02 09:20:11 | INFO | train_inner | epoch 145:    276 / 421 symm_mse=5.44, loss=3.47, nll_loss=1.099, ppl=2.14, wps=22292.8, ups=1.59, wpb=14031.2, bsz=484.8, num_updates=60900, lr=8.87794e-06, gnorm=0.85, train_wall=63, wall=41360
2021-01-02 09:21:14 | INFO | train_inner | epoch 145:    376 / 421 symm_mse=5.612, loss=3.502, nll_loss=1.114, ppl=2.16, wps=22386.9, ups=1.6, wpb=13973.1, bsz=478.3, num_updates=61000, lr=8.87066e-06, gnorm=0.902, train_wall=62, wall=41422
2021-01-02 09:21:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:21:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:21:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:21:59 | INFO | valid | epoch 145 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.54 | wps 5973.8 | wpb 10324.2 | bsz 375 | num_updates 61045 | best_bleu 22.72
2021-01-02 09:21:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:22:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:22:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:22:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:22:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 145 @ 61045 updates, score 22.54) (writing took 3.15342079102993 seconds)
2021-01-02 09:22:02 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2021-01-02 09:22:02 | INFO | train | epoch 145 | symm_mse 5.399 | loss 3.466 | nll_loss 1.102 | ppl 2.15 | wps 20497.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 61045 | lr 8.86739e-06 | gnorm 0.854 | train_wall 263 | wall 41470
2021-01-02 09:22:02 | INFO | fairseq.trainer | begin training epoch 146
2021-01-02 09:22:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:22:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:22:39 | INFO | train_inner | epoch 146:     55 / 421 symm_mse=5.399, loss=3.471, nll_loss=1.107, ppl=2.15, wps=16179.8, ups=1.18, wpb=13767.2, bsz=492.8, num_updates=61100, lr=8.86339e-06, gnorm=0.857, train_wall=62, wall=41507
2021-01-02 09:23:42 | INFO | train_inner | epoch 146:    155 / 421 symm_mse=5.401, loss=3.466, nll_loss=1.101, ppl=2.15, wps=22269.6, ups=1.59, wpb=13968.5, bsz=490, num_updates=61200, lr=8.85615e-06, gnorm=0.863, train_wall=63, wall=41570
2021-01-02 09:24:44 | INFO | train_inner | epoch 146:    255 / 421 symm_mse=5.414, loss=3.471, nll_loss=1.105, ppl=2.15, wps=22439.5, ups=1.59, wpb=14101.8, bsz=501.4, num_updates=61300, lr=8.84892e-06, gnorm=0.846, train_wall=63, wall=41633
2021-01-02 09:25:47 | INFO | train_inner | epoch 146:    355 / 421 symm_mse=5.372, loss=3.458, nll_loss=1.096, ppl=2.14, wps=22499.6, ups=1.6, wpb=14100, bsz=490.3, num_updates=61400, lr=8.84171e-06, gnorm=0.856, train_wall=62, wall=41696
2021-01-02 09:26:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:26:45 | INFO | valid | epoch 146 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.55 | wps 6005.4 | wpb 10324.2 | bsz 375 | num_updates 61466 | best_bleu 22.72
2021-01-02 09:26:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:26:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:26:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 146 @ 61466 updates, score 22.55) (writing took 3.105156620964408 seconds)
2021-01-02 09:26:48 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2021-01-02 09:26:48 | INFO | train | epoch 146 | symm_mse 5.389 | loss 3.465 | nll_loss 1.101 | ppl 2.14 | wps 20534.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 61466 | lr 8.83697e-06 | gnorm 0.86 | train_wall 263 | wall 41757
2021-01-02 09:26:48 | INFO | fairseq.trainer | begin training epoch 147
2021-01-02 09:26:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:26:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:27:13 | INFO | train_inner | epoch 147:     34 / 421 symm_mse=5.272, loss=3.442, nll_loss=1.09, ppl=2.13, wps=16110.6, ups=1.17, wpb=13778.5, bsz=496.5, num_updates=61500, lr=8.83452e-06, gnorm=0.867, train_wall=62, wall=41781
2021-01-02 09:28:15 | INFO | train_inner | epoch 147:    134 / 421 symm_mse=5.44, loss=3.47, nll_loss=1.1, ppl=2.14, wps=22627.6, ups=1.61, wpb=14087.1, bsz=486.1, num_updates=61600, lr=8.82735e-06, gnorm=0.848, train_wall=62, wall=41843
2021-01-02 09:29:17 | INFO | train_inner | epoch 147:    234 / 421 symm_mse=5.473, loss=3.475, nll_loss=1.101, ppl=2.15, wps=22336.9, ups=1.6, wpb=13988.4, bsz=484, num_updates=61700, lr=8.82019e-06, gnorm=0.861, train_wall=62, wall=41906
2021-01-02 09:30:20 | INFO | train_inner | epoch 147:    334 / 421 symm_mse=5.513, loss=3.487, nll_loss=1.11, ppl=2.16, wps=22287.9, ups=1.59, wpb=14045.6, bsz=484.5, num_updates=61800, lr=8.81305e-06, gnorm=0.892, train_wall=63, wall=41969
2021-01-02 09:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:31:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:31:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:31:32 | INFO | valid | epoch 147 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.684 | ppl 12.85 | bleu 22.72 | wps 5936.3 | wpb 10324.2 | bsz 375 | num_updates 61887 | best_bleu 22.72
2021-01-02 09:31:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:31:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 147 @ 61887 updates, score 22.72) (writing took 5.313200147822499 seconds)
2021-01-02 09:31:37 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2021-01-02 09:31:37 | INFO | train | epoch 147 | symm_mse 5.392 | loss 3.466 | nll_loss 1.102 | ppl 2.15 | wps 20349 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 61887 | lr 8.80686e-06 | gnorm 0.86 | train_wall 263 | wall 42046
2021-01-02 09:31:37 | INFO | fairseq.trainer | begin training epoch 148
2021-01-02 09:31:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:31:49 | INFO | train_inner | epoch 148:     13 / 421 symm_mse=5.175, loss=3.44, nll_loss=1.101, ppl=2.15, wps=15611.6, ups=1.13, wpb=13761.7, bsz=507.4, num_updates=61900, lr=8.80593e-06, gnorm=0.839, train_wall=63, wall=42057
2021-01-02 09:32:51 | INFO | train_inner | epoch 148:    113 / 421 symm_mse=5.305, loss=3.452, nll_loss=1.097, ppl=2.14, wps=22353.1, ups=1.61, wpb=13855.9, bsz=500.1, num_updates=62000, lr=8.79883e-06, gnorm=0.857, train_wall=62, wall=42119
2021-01-02 09:33:54 | INFO | train_inner | epoch 148:    213 / 421 symm_mse=5.434, loss=3.466, nll_loss=1.096, ppl=2.14, wps=22263.3, ups=1.58, wpb=14087.8, bsz=498.6, num_updates=62100, lr=8.79174e-06, gnorm=0.876, train_wall=63, wall=42182
2021-01-02 09:34:57 | INFO | train_inner | epoch 148:    313 / 421 symm_mse=5.478, loss=3.479, nll_loss=1.107, ppl=2.15, wps=22344.2, ups=1.59, wpb=14030.2, bsz=488.8, num_updates=62200, lr=8.78467e-06, gnorm=0.856, train_wall=63, wall=42245
2021-01-02 09:35:59 | INFO | train_inner | epoch 148:    413 / 421 symm_mse=5.346, loss=3.459, nll_loss=1.1, ppl=2.14, wps=22507.4, ups=1.61, wpb=14000.2, bsz=485.4, num_updates=62300, lr=8.77762e-06, gnorm=0.843, train_wall=62, wall=42307
2021-01-02 09:36:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:36:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:36:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:36:21 | INFO | valid | epoch 148 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.684 | ppl 12.86 | bleu 22.74 | wps 5979.4 | wpb 10324.2 | bsz 375 | num_updates 62308 | best_bleu 22.74
2021-01-02 09:36:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:36:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 148 @ 62308 updates, score 22.74) (writing took 5.267137547954917 seconds)
2021-01-02 09:36:26 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2021-01-02 09:36:26 | INFO | train | epoch 148 | symm_mse 5.388 | loss 3.464 | nll_loss 1.101 | ppl 2.14 | wps 20384.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 62308 | lr 8.77705e-06 | gnorm 0.857 | train_wall 262 | wall 42334
2021-01-02 09:36:26 | INFO | fairseq.trainer | begin training epoch 149
2021-01-02 09:36:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:36:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:37:26 | INFO | train_inner | epoch 149:     92 / 421 symm_mse=5.559, loss=3.501, nll_loss=1.119, ppl=2.17, wps=15765.4, ups=1.15, wpb=13733.1, bsz=479.8, num_updates=62400, lr=8.77058e-06, gnorm=0.868, train_wall=62, wall=42394
2021-01-02 09:38:28 | INFO | train_inner | epoch 149:    192 / 421 symm_mse=5.317, loss=3.452, nll_loss=1.097, ppl=2.14, wps=22424.5, ups=1.6, wpb=13972.5, bsz=492.8, num_updates=62500, lr=8.76356e-06, gnorm=0.859, train_wall=62, wall=42457
2021-01-02 09:39:31 | INFO | train_inner | epoch 149:    292 / 421 symm_mse=5.362, loss=3.454, nll_loss=1.091, ppl=2.13, wps=22679.5, ups=1.6, wpb=14177.3, bsz=489.4, num_updates=62600, lr=8.75656e-06, gnorm=0.844, train_wall=62, wall=42519
2021-01-02 09:40:33 | INFO | train_inner | epoch 149:    392 / 421 symm_mse=5.304, loss=3.453, nll_loss=1.098, ppl=2.14, wps=22359.9, ups=1.6, wpb=13963.7, bsz=504.2, num_updates=62700, lr=8.74957e-06, gnorm=0.849, train_wall=62, wall=42582
2021-01-02 09:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:40:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:40:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:40:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:40:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:40:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:40:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:40:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:40:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:41:08 | INFO | valid | epoch 149 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.687 | ppl 12.88 | bleu 22.65 | wps 6045.4 | wpb 10324.2 | bsz 375 | num_updates 62729 | best_bleu 22.74
2021-01-02 09:41:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:41:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:41:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:41:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:41:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:41:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:41:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:41:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 149 @ 62729 updates, score 22.65) (writing took 3.152064435184002 seconds)
2021-01-02 09:41:11 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2021-01-02 09:41:11 | INFO | train | epoch 149 | symm_mse 5.384 | loss 3.464 | nll_loss 1.101 | ppl 2.14 | wps 20633.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 62729 | lr 8.74755e-06 | gnorm 0.857 | train_wall 261 | wall 42619
2021-01-02 09:41:11 | INFO | fairseq.trainer | begin training epoch 150
2021-01-02 09:41:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:41:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:41:58 | INFO | train_inner | epoch 150:     71 / 421 symm_mse=5.369, loss=3.461, nll_loss=1.098, ppl=2.14, wps=16474.2, ups=1.18, wpb=13925.5, bsz=493.4, num_updates=62800, lr=8.7426e-06, gnorm=0.869, train_wall=61, wall=42666
2021-01-02 09:43:00 | INFO | train_inner | epoch 150:    171 / 421 symm_mse=5.456, loss=3.479, nll_loss=1.11, ppl=2.16, wps=22305.3, ups=1.61, wpb=13890.2, bsz=489.2, num_updates=62900, lr=8.73565e-06, gnorm=0.864, train_wall=62, wall=42729
2021-01-02 09:44:03 | INFO | train_inner | epoch 150:    271 / 421 symm_mse=5.265, loss=3.444, nll_loss=1.094, ppl=2.13, wps=22060.7, ups=1.58, wpb=13938, bsz=494.2, num_updates=63000, lr=8.72872e-06, gnorm=0.846, train_wall=63, wall=42792
2021-01-02 09:45:06 | INFO | train_inner | epoch 150:    371 / 421 symm_mse=5.302, loss=3.452, nll_loss=1.097, ppl=2.14, wps=22759.4, ups=1.6, wpb=14243.7, bsz=514.6, num_updates=63100, lr=8.7218e-06, gnorm=0.837, train_wall=62, wall=42854
2021-01-02 09:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:45:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:45:54 | INFO | valid | epoch 150 | valid on 'valid' subset | symm_mse 0 | loss 5.201 | nll_loss 3.685 | ppl 12.86 | bleu 22.52 | wps 5926.9 | wpb 10324.2 | bsz 375 | num_updates 63150 | best_bleu 22.74
2021-01-02 09:45:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:45:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:45:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:45:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 150 @ 63150 updates, score 22.52) (writing took 3.0812776647508144 seconds)
2021-01-02 09:45:57 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2021-01-02 09:45:57 | INFO | train | epoch 150 | symm_mse 5.379 | loss 3.463 | nll_loss 1.101 | ppl 2.14 | wps 20558.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 63150 | lr 8.71834e-06 | gnorm 0.86 | train_wall 262 | wall 42905
2021-01-02 09:45:57 | INFO | fairseq.trainer | begin training epoch 151
2021-01-02 09:45:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:46:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:46:31 | INFO | train_inner | epoch 151:     50 / 421 symm_mse=5.457, loss=3.464, nll_loss=1.093, ppl=2.13, wps=16209.3, ups=1.17, wpb=13831.3, bsz=477, num_updates=63200, lr=8.71489e-06, gnorm=0.885, train_wall=62, wall=42940
2021-01-02 09:47:34 | INFO | train_inner | epoch 151:    150 / 421 symm_mse=5.444, loss=3.478, nll_loss=1.108, ppl=2.16, wps=22162.6, ups=1.59, wpb=13920.5, bsz=482.2, num_updates=63300, lr=8.70801e-06, gnorm=0.859, train_wall=63, wall=43002
2021-01-02 09:48:37 | INFO | train_inner | epoch 151:    250 / 421 symm_mse=5.412, loss=3.468, nll_loss=1.101, ppl=2.15, wps=21908.1, ups=1.58, wpb=13831.6, bsz=486.8, num_updates=63400, lr=8.70114e-06, gnorm=0.851, train_wall=63, wall=43066
2021-01-02 09:49:40 | INFO | train_inner | epoch 151:    350 / 421 symm_mse=5.306, loss=3.455, nll_loss=1.101, ppl=2.14, wps=22399, ups=1.58, wpb=14158.8, bsz=499.7, num_updates=63500, lr=8.69428e-06, gnorm=0.87, train_wall=63, wall=43129
2021-01-02 09:50:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:50:43 | INFO | valid | epoch 151 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.62 | wps 5400.7 | wpb 10324.2 | bsz 375 | num_updates 63571 | best_bleu 22.74
2021-01-02 09:50:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:50:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 151 @ 63571 updates, score 22.62) (writing took 3.159591095522046 seconds)
2021-01-02 09:50:47 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2021-01-02 09:50:47 | INFO | train | epoch 151 | symm_mse 5.375 | loss 3.463 | nll_loss 1.1 | ppl 2.14 | wps 20307.1 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 63571 | lr 8.68943e-06 | gnorm 0.86 | train_wall 264 | wall 43195
2021-01-02 09:50:47 | INFO | fairseq.trainer | begin training epoch 152
2021-01-02 09:50:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:51:08 | INFO | train_inner | epoch 152:     29 / 421 symm_mse=5.439, loss=3.471, nll_loss=1.104, ppl=2.15, wps=15985.1, ups=1.15, wpb=13958.1, bsz=492.3, num_updates=63600, lr=8.68744e-06, gnorm=0.872, train_wall=62, wall=43216
2021-01-02 09:52:10 | INFO | train_inner | epoch 152:    129 / 421 symm_mse=5.342, loss=3.45, nll_loss=1.091, ppl=2.13, wps=22270.4, ups=1.59, wpb=13973.5, bsz=496.2, num_updates=63700, lr=8.68062e-06, gnorm=0.86, train_wall=63, wall=43279
2021-01-02 09:53:13 | INFO | train_inner | epoch 152:    229 / 421 symm_mse=5.356, loss=3.462, nll_loss=1.102, ppl=2.15, wps=22349.1, ups=1.6, wpb=14007.1, bsz=494.6, num_updates=63800, lr=8.67382e-06, gnorm=0.854, train_wall=62, wall=43342
2021-01-02 09:54:16 | INFO | train_inner | epoch 152:    329 / 421 symm_mse=5.345, loss=3.46, nll_loss=1.101, ppl=2.15, wps=22343.1, ups=1.59, wpb=14050.2, bsz=488.2, num_updates=63900, lr=8.66703e-06, gnorm=0.849, train_wall=63, wall=43404
2021-01-02 09:55:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 09:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 09:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 09:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 09:55:30 | INFO | valid | epoch 152 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.685 | ppl 12.86 | bleu 22.8 | wps 5924.9 | wpb 10324.2 | bsz 375 | num_updates 63992 | best_bleu 22.8
2021-01-02 09:55:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 09:55:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 152 @ 63992 updates, score 22.8) (writing took 4.987810457125306 seconds)
2021-01-02 09:55:35 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2021-01-02 09:55:35 | INFO | train | epoch 152 | symm_mse 5.369 | loss 3.462 | nll_loss 1.1 | ppl 2.14 | wps 20384.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 63992 | lr 8.6608e-06 | gnorm 0.859 | train_wall 263 | wall 43484
2021-01-02 09:55:35 | INFO | fairseq.trainer | begin training epoch 153
2021-01-02 09:55:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 09:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 09:55:43 | INFO | train_inner | epoch 153:      8 / 421 symm_mse=5.334, loss=3.462, nll_loss=1.104, ppl=2.15, wps=15739.8, ups=1.14, wpb=13748.4, bsz=498.8, num_updates=64000, lr=8.66025e-06, gnorm=0.861, train_wall=62, wall=43492
2021-01-02 09:56:46 | INFO | train_inner | epoch 153:    108 / 421 symm_mse=5.386, loss=3.459, nll_loss=1.094, ppl=2.13, wps=22408.4, ups=1.61, wpb=13934.4, bsz=493.4, num_updates=64100, lr=8.6535e-06, gnorm=0.863, train_wall=62, wall=43554
2021-01-02 09:57:49 | INFO | train_inner | epoch 153:    208 / 421 symm_mse=5.339, loss=3.463, nll_loss=1.106, ppl=2.15, wps=22174.1, ups=1.58, wpb=13992.3, bsz=498.9, num_updates=64200, lr=8.64675e-06, gnorm=0.866, train_wall=63, wall=43617
2021-01-02 09:58:51 | INFO | train_inner | epoch 153:    308 / 421 symm_mse=5.42, loss=3.479, nll_loss=1.113, ppl=2.16, wps=22349.5, ups=1.6, wpb=13947.1, bsz=484.6, num_updates=64300, lr=8.64003e-06, gnorm=0.848, train_wall=62, wall=43679
2021-01-02 09:59:53 | INFO | train_inner | epoch 153:    408 / 421 symm_mse=5.371, loss=3.455, nll_loss=1.092, ppl=2.13, wps=22619.5, ups=1.6, wpb=14111, bsz=491.5, num_updates=64400, lr=8.63332e-06, gnorm=0.852, train_wall=62, wall=43742
2021-01-02 10:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:00:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:00:20 | INFO | valid | epoch 153 | valid on 'valid' subset | symm_mse 0 | loss 5.206 | nll_loss 3.687 | ppl 12.88 | bleu 22.81 | wps 5235.1 | wpb 10324.2 | bsz 375 | num_updates 64413 | best_bleu 22.81
2021-01-02 10:00:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:00:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 153 @ 64413 updates, score 22.81) (writing took 4.931103393435478 seconds)
2021-01-02 10:00:25 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2021-01-02 10:00:25 | INFO | train | epoch 153 | symm_mse 5.364 | loss 3.461 | nll_loss 1.1 | ppl 2.14 | wps 20304.2 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 64413 | lr 8.63245e-06 | gnorm 0.856 | train_wall 262 | wall 43773
2021-01-02 10:00:25 | INFO | fairseq.trainer | begin training epoch 154
2021-01-02 10:00:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:00:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:01:22 | INFO | train_inner | epoch 154:     87 / 421 symm_mse=5.276, loss=3.444, nll_loss=1.092, ppl=2.13, wps=15769, ups=1.13, wpb=13972.4, bsz=496.9, num_updates=64500, lr=8.62662e-06, gnorm=0.838, train_wall=62, wall=43830
2021-01-02 10:02:25 | INFO | train_inner | epoch 154:    187 / 421 symm_mse=5.525, loss=3.487, nll_loss=1.108, ppl=2.16, wps=22095.3, ups=1.59, wpb=13917.9, bsz=477.1, num_updates=64600, lr=8.61994e-06, gnorm=0.88, train_wall=63, wall=43893
2021-01-02 10:03:28 | INFO | train_inner | epoch 154:    287 / 421 symm_mse=5.38, loss=3.457, nll_loss=1.094, ppl=2.13, wps=22506.1, ups=1.58, wpb=14268, bsz=501.1, num_updates=64700, lr=8.61328e-06, gnorm=0.838, train_wall=63, wall=43957
2021-01-02 10:04:31 | INFO | train_inner | epoch 154:    387 / 421 symm_mse=5.276, loss=3.452, nll_loss=1.101, ppl=2.15, wps=22044.3, ups=1.6, wpb=13797, bsz=493.4, num_updates=64800, lr=8.60663e-06, gnorm=0.875, train_wall=62, wall=44019
2021-01-02 10:04:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:04:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:04:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:04:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:05:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:05:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:05:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:05:09 | INFO | valid | epoch 154 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.687 | ppl 12.88 | bleu 22.76 | wps 5775 | wpb 10324.2 | bsz 375 | num_updates 64834 | best_bleu 22.81
2021-01-02 10:05:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:05:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 154 @ 64834 updates, score 22.76) (writing took 3.100993363186717 seconds)
2021-01-02 10:05:12 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2021-01-02 10:05:12 | INFO | train | epoch 154 | symm_mse 5.364 | loss 3.461 | nll_loss 1.1 | ppl 2.14 | wps 20460.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 64834 | lr 8.60437e-06 | gnorm 0.86 | train_wall 263 | wall 44061
2021-01-02 10:05:12 | INFO | fairseq.trainer | begin training epoch 155
2021-01-02 10:05:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:05:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:05:56 | INFO | train_inner | epoch 155:     66 / 421 symm_mse=5.245, loss=3.435, nll_loss=1.086, ppl=2.12, wps=16298.1, ups=1.17, wpb=13886.5, bsz=497.1, num_updates=64900, lr=8.6e-06, gnorm=0.862, train_wall=61, wall=44105
2021-01-02 10:06:59 | INFO | train_inner | epoch 155:    166 / 421 symm_mse=5.225, loss=3.445, nll_loss=1.099, ppl=2.14, wps=22386.3, ups=1.6, wpb=14014.4, bsz=510, num_updates=65000, lr=8.59338e-06, gnorm=0.843, train_wall=62, wall=44167
2021-01-02 10:08:02 | INFO | train_inner | epoch 155:    266 / 421 symm_mse=5.363, loss=3.464, nll_loss=1.103, ppl=2.15, wps=22067.6, ups=1.59, wpb=13876.6, bsz=484, num_updates=65100, lr=8.58678e-06, gnorm=0.849, train_wall=63, wall=44230
2021-01-02 10:09:05 | INFO | train_inner | epoch 155:    366 / 421 symm_mse=5.395, loss=3.453, nll_loss=1.088, ppl=2.13, wps=22321.9, ups=1.59, wpb=14055.2, bsz=498.1, num_updates=65200, lr=8.58019e-06, gnorm=0.854, train_wall=63, wall=44293
2021-01-02 10:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:09:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:09:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:09:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:09:56 | INFO | valid | epoch 155 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.688 | ppl 12.89 | bleu 22.69 | wps 5898.2 | wpb 10324.2 | bsz 375 | num_updates 65255 | best_bleu 22.81
2021-01-02 10:09:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:09:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:09:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 155 @ 65255 updates, score 22.69) (writing took 3.118469227105379 seconds)
2021-01-02 10:09:59 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2021-01-02 10:09:59 | INFO | train | epoch 155 | symm_mse 5.359 | loss 3.46 | nll_loss 1.099 | ppl 2.14 | wps 20497.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 65255 | lr 8.57657e-06 | gnorm 0.855 | train_wall 263 | wall 44348
2021-01-02 10:09:59 | INFO | fairseq.trainer | begin training epoch 156
2021-01-02 10:10:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:10:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:10:30 | INFO | train_inner | epoch 156:     45 / 421 symm_mse=5.465, loss=3.485, nll_loss=1.116, ppl=2.17, wps=16461.3, ups=1.17, wpb=14053.9, bsz=489.1, num_updates=65300, lr=8.57362e-06, gnorm=0.86, train_wall=62, wall=44378
2021-01-02 10:11:33 | INFO | train_inner | epoch 156:    145 / 421 symm_mse=5.409, loss=3.464, nll_loss=1.097, ppl=2.14, wps=22181.3, ups=1.59, wpb=13935.1, bsz=480.3, num_updates=65400, lr=8.56706e-06, gnorm=0.855, train_wall=63, wall=44441
2021-01-02 10:12:35 | INFO | train_inner | epoch 156:    245 / 421 symm_mse=5.321, loss=3.461, nll_loss=1.105, ppl=2.15, wps=22323.4, ups=1.6, wpb=13952.4, bsz=501.6, num_updates=65500, lr=8.56052e-06, gnorm=0.852, train_wall=62, wall=44504
2021-01-02 10:13:38 | INFO | train_inner | epoch 156:    345 / 421 symm_mse=5.35, loss=3.457, nll_loss=1.097, ppl=2.14, wps=22285, ups=1.59, wpb=13986.1, bsz=488.1, num_updates=65600, lr=8.55399e-06, gnorm=0.861, train_wall=63, wall=44567
2021-01-02 10:14:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:14:42 | INFO | valid | epoch 156 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.685 | ppl 12.86 | bleu 22.6 | wps 5956.2 | wpb 10324.2 | bsz 375 | num_updates 65676 | best_bleu 22.81
2021-01-02 10:14:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:14:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 156 @ 65676 updates, score 22.6) (writing took 3.1308501716703176 seconds)
2021-01-02 10:14:45 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2021-01-02 10:14:45 | INFO | train | epoch 156 | symm_mse 5.351 | loss 3.459 | nll_loss 1.099 | ppl 2.14 | wps 20555 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 65676 | lr 8.54904e-06 | gnorm 0.851 | train_wall 262 | wall 44634
2021-01-02 10:14:45 | INFO | fairseq.trainer | begin training epoch 157
2021-01-02 10:14:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:14:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:15:03 | INFO | train_inner | epoch 157:     24 / 421 symm_mse=5.403, loss=3.468, nll_loss=1.103, ppl=2.15, wps=16200.1, ups=1.18, wpb=13777.8, bsz=481.7, num_updates=65700, lr=8.54748e-06, gnorm=0.852, train_wall=62, wall=44652
2021-01-02 10:16:05 | INFO | train_inner | epoch 157:    124 / 421 symm_mse=5.429, loss=3.467, nll_loss=1.098, ppl=2.14, wps=22416.1, ups=1.61, wpb=13963.6, bsz=485.1, num_updates=65800, lr=8.54098e-06, gnorm=0.879, train_wall=62, wall=44714
2021-01-02 10:17:08 | INFO | train_inner | epoch 157:    224 / 421 symm_mse=5.19, loss=3.438, nll_loss=1.096, ppl=2.14, wps=22225.1, ups=1.59, wpb=13994.5, bsz=507.4, num_updates=65900, lr=8.5345e-06, gnorm=0.832, train_wall=63, wall=44777
2021-01-02 10:18:11 | INFO | train_inner | epoch 157:    324 / 421 symm_mse=5.431, loss=3.469, nll_loss=1.1, ppl=2.14, wps=22288.3, ups=1.59, wpb=14031.7, bsz=487.4, num_updates=66000, lr=8.52803e-06, gnorm=0.878, train_wall=63, wall=44840
2021-01-02 10:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:19:29 | INFO | valid | epoch 157 | valid on 'valid' subset | symm_mse 0 | loss 5.202 | nll_loss 3.685 | ppl 12.86 | bleu 22.77 | wps 5972.3 | wpb 10324.2 | bsz 375 | num_updates 66097 | best_bleu 22.81
2021-01-02 10:19:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 157 @ 66097 updates, score 22.77) (writing took 3.131882531568408 seconds)
2021-01-02 10:19:32 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2021-01-02 10:19:32 | INFO | train | epoch 157 | symm_mse 5.348 | loss 3.458 | nll_loss 1.099 | ppl 2.14 | wps 20512.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 66097 | lr 8.52177e-06 | gnorm 0.862 | train_wall 263 | wall 44920
2021-01-02 10:19:32 | INFO | fairseq.trainer | begin training epoch 158
2021-01-02 10:19:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:19:37 | INFO | train_inner | epoch 158:      3 / 421 symm_mse=5.34, loss=3.458, nll_loss=1.1, ppl=2.14, wps=16253.8, ups=1.17, wpb=13946.9, bsz=493.5, num_updates=66100, lr=8.52158e-06, gnorm=0.861, train_wall=62, wall=44926
2021-01-02 10:20:40 | INFO | train_inner | epoch 158:    103 / 421 symm_mse=5.477, loss=3.482, nll_loss=1.11, ppl=2.16, wps=22422, ups=1.6, wpb=13991.6, bsz=486.3, num_updates=66200, lr=8.51514e-06, gnorm=0.883, train_wall=62, wall=44988
2021-01-02 10:21:43 | INFO | train_inner | epoch 158:    203 / 421 symm_mse=5.139, loss=3.41, nll_loss=1.071, ppl=2.1, wps=22417.8, ups=1.59, wpb=14129.6, bsz=509.4, num_updates=66300, lr=8.50871e-06, gnorm=0.817, train_wall=63, wall=45051
2021-01-02 10:22:46 | INFO | train_inner | epoch 158:    303 / 421 symm_mse=5.431, loss=3.481, nll_loss=1.114, ppl=2.16, wps=22102.3, ups=1.59, wpb=13927.7, bsz=477.6, num_updates=66400, lr=8.5023e-06, gnorm=0.866, train_wall=63, wall=45114
2021-01-02 10:23:49 | INFO | train_inner | epoch 158:    403 / 421 symm_mse=5.328, loss=3.455, nll_loss=1.099, ppl=2.14, wps=22020.1, ups=1.58, wpb=13965.3, bsz=500.1, num_updates=66500, lr=8.49591e-06, gnorm=0.847, train_wall=63, wall=45177
2021-01-02 10:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:24:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:24:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:24:17 | INFO | valid | epoch 158 | valid on 'valid' subset | symm_mse 0 | loss 5.207 | nll_loss 3.688 | ppl 12.88 | bleu 22.6 | wps 5917.8 | wpb 10324.2 | bsz 375 | num_updates 66518 | best_bleu 22.81
2021-01-02 10:24:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:24:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 158 @ 66518 updates, score 22.6) (writing took 3.1689446587115526 seconds)
2021-01-02 10:24:20 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2021-01-02 10:24:20 | INFO | train | epoch 158 | symm_mse 5.342 | loss 3.458 | nll_loss 1.099 | ppl 2.14 | wps 20395 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 66518 | lr 8.49476e-06 | gnorm 0.857 | train_wall 264 | wall 45209
2021-01-02 10:24:20 | INFO | fairseq.trainer | begin training epoch 159
2021-01-02 10:24:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:24:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:25:15 | INFO | train_inner | epoch 159:     82 / 421 symm_mse=5.417, loss=3.468, nll_loss=1.099, ppl=2.14, wps=16320.9, ups=1.17, wpb=14000.2, bsz=473, num_updates=66600, lr=8.48953e-06, gnorm=0.864, train_wall=62, wall=45263
2021-01-02 10:26:18 | INFO | train_inner | epoch 159:    182 / 421 symm_mse=5.481, loss=3.482, nll_loss=1.108, ppl=2.16, wps=22094.7, ups=1.58, wpb=13978.4, bsz=491.2, num_updates=66700, lr=8.48316e-06, gnorm=0.892, train_wall=63, wall=45327
2021-01-02 10:27:21 | INFO | train_inner | epoch 159:    282 / 421 symm_mse=5.188, loss=3.433, nll_loss=1.092, ppl=2.13, wps=22074.2, ups=1.59, wpb=13858.3, bsz=500.2, num_updates=66800, lr=8.47681e-06, gnorm=0.828, train_wall=63, wall=45389
2021-01-02 10:28:24 | INFO | train_inner | epoch 159:    382 / 421 symm_mse=5.24, loss=3.437, nll_loss=1.089, ppl=2.13, wps=22318.9, ups=1.59, wpb=14045.1, bsz=505.5, num_updates=66900, lr=8.47047e-06, gnorm=0.855, train_wall=63, wall=45452
2021-01-02 10:28:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:28:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:28:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:28:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:28:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:28:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:28:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:28:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:28:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:28:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:28:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:28:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:29:06 | INFO | valid | epoch 159 | valid on 'valid' subset | symm_mse 0 | loss 5.204 | nll_loss 3.686 | ppl 12.87 | bleu 22.66 | wps 5723.6 | wpb 10324.2 | bsz 375 | num_updates 66939 | best_bleu 22.81
2021-01-02 10:29:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:29:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:29:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:29:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:29:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:29:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:29:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:29:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 159 @ 66939 updates, score 22.66) (writing took 3.175465090200305 seconds)
2021-01-02 10:29:09 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2021-01-02 10:29:09 | INFO | train | epoch 159 | symm_mse 5.336 | loss 3.456 | nll_loss 1.098 | ppl 2.14 | wps 20377.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 66939 | lr 8.468e-06 | gnorm 0.858 | train_wall 264 | wall 45497
2021-01-02 10:29:09 | INFO | fairseq.trainer | begin training epoch 160
2021-01-02 10:29:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:29:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:29:50 | INFO | train_inner | epoch 160:     61 / 421 symm_mse=5.266, loss=3.456, nll_loss=1.108, ppl=2.16, wps=16008.2, ups=1.16, wpb=13806.8, bsz=505.4, num_updates=67000, lr=8.46415e-06, gnorm=0.854, train_wall=62, wall=45538
2021-01-02 10:30:53 | INFO | train_inner | epoch 160:    161 / 421 symm_mse=5.327, loss=3.452, nll_loss=1.094, ppl=2.13, wps=22181.9, ups=1.6, wpb=13868, bsz=480.2, num_updates=67100, lr=8.45784e-06, gnorm=0.855, train_wall=62, wall=45601
2021-01-02 10:31:55 | INFO | train_inner | epoch 160:    261 / 421 symm_mse=5.323, loss=3.462, nll_loss=1.106, ppl=2.15, wps=22361.6, ups=1.59, wpb=14028.5, bsz=487.8, num_updates=67200, lr=8.45154e-06, gnorm=0.826, train_wall=63, wall=45664
2021-01-02 10:32:58 | INFO | train_inner | epoch 160:    361 / 421 symm_mse=5.375, loss=3.455, nll_loss=1.092, ppl=2.13, wps=22331.6, ups=1.59, wpb=14046.4, bsz=487.5, num_updates=67300, lr=8.44526e-06, gnorm=0.869, train_wall=63, wall=45727
2021-01-02 10:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:33:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:33:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:33:53 | INFO | valid | epoch 160 | valid on 'valid' subset | symm_mse 0 | loss 5.204 | nll_loss 3.686 | ppl 12.87 | bleu 22.67 | wps 5898.3 | wpb 10324.2 | bsz 375 | num_updates 67360 | best_bleu 22.81
2021-01-02 10:33:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:33:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 160 @ 67360 updates, score 22.67) (writing took 3.1212795078754425 seconds)
2021-01-02 10:33:56 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2021-01-02 10:33:56 | INFO | train | epoch 160 | symm_mse 5.339 | loss 3.457 | nll_loss 1.099 | ppl 2.14 | wps 20496.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 67360 | lr 8.4415e-06 | gnorm 0.852 | train_wall 263 | wall 45784
2021-01-02 10:33:56 | INFO | fairseq.trainer | begin training epoch 161
2021-01-02 10:33:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:33:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:34:24 | INFO | train_inner | epoch 161:     40 / 421 symm_mse=5.302, loss=3.446, nll_loss=1.092, ppl=2.13, wps=16346.7, ups=1.17, wpb=13980.6, bsz=506.4, num_updates=67400, lr=8.43899e-06, gnorm=0.854, train_wall=62, wall=45812
2021-01-02 10:35:26 | INFO | train_inner | epoch 161:    140 / 421 symm_mse=5.415, loss=3.466, nll_loss=1.099, ppl=2.14, wps=22145.1, ups=1.6, wpb=13831.6, bsz=489.1, num_updates=67500, lr=8.43274e-06, gnorm=0.888, train_wall=62, wall=45875
2021-01-02 10:36:29 | INFO | train_inner | epoch 161:    240 / 421 symm_mse=5.272, loss=3.443, nll_loss=1.091, ppl=2.13, wps=22117, ups=1.58, wpb=13986.9, bsz=505.6, num_updates=67600, lr=8.4265e-06, gnorm=0.849, train_wall=63, wall=45938
2021-01-02 10:37:32 | INFO | train_inner | epoch 161:    340 / 421 symm_mse=5.28, loss=3.454, nll_loss=1.102, ppl=2.15, wps=22231.2, ups=1.59, wpb=14017.4, bsz=485.5, num_updates=67700, lr=8.42028e-06, gnorm=0.823, train_wall=63, wall=46001
2021-01-02 10:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:38:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:38:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:38:40 | INFO | valid | epoch 161 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.686 | ppl 12.87 | bleu 22.63 | wps 5878.2 | wpb 10324.2 | bsz 375 | num_updates 67781 | best_bleu 22.81
2021-01-02 10:38:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:38:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 161 @ 67781 updates, score 22.63) (writing took 3.129941176623106 seconds)
2021-01-02 10:38:43 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2021-01-02 10:38:43 | INFO | train | epoch 161 | symm_mse 5.336 | loss 3.456 | nll_loss 1.098 | ppl 2.14 | wps 20454 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 67781 | lr 8.41524e-06 | gnorm 0.855 | train_wall 263 | wall 46072
2021-01-02 10:38:43 | INFO | fairseq.trainer | begin training epoch 162
2021-01-02 10:38:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:38:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:38:58 | INFO | train_inner | epoch 162:     19 / 421 symm_mse=5.466, loss=3.48, nll_loss=1.109, ppl=2.16, wps=16335.3, ups=1.16, wpb=14029.2, bsz=491.1, num_updates=67800, lr=8.41406e-06, gnorm=0.867, train_wall=62, wall=46087
2021-01-02 10:40:00 | INFO | train_inner | epoch 162:    119 / 421 symm_mse=5.226, loss=3.443, nll_loss=1.098, ppl=2.14, wps=22460.5, ups=1.61, wpb=13948.6, bsz=497.5, num_updates=67900, lr=8.40787e-06, gnorm=0.843, train_wall=62, wall=46149
2021-01-02 10:41:03 | INFO | train_inner | epoch 162:    219 / 421 symm_mse=5.438, loss=3.47, nll_loss=1.101, ppl=2.15, wps=22292.4, ups=1.59, wpb=13976.5, bsz=484, num_updates=68000, lr=8.40168e-06, gnorm=0.873, train_wall=62, wall=46212
2021-01-02 10:42:06 | INFO | train_inner | epoch 162:    319 / 421 symm_mse=5.211, loss=3.43, nll_loss=1.085, ppl=2.12, wps=22297.9, ups=1.59, wpb=14021.5, bsz=510.2, num_updates=68100, lr=8.39551e-06, gnorm=0.827, train_wall=63, wall=46274
2021-01-02 10:43:09 | INFO | train_inner | epoch 162:    419 / 421 symm_mse=5.48, loss=3.486, nll_loss=1.113, ppl=2.16, wps=22299.3, ups=1.59, wpb=14018.9, bsz=474, num_updates=68200, lr=8.38935e-06, gnorm=0.872, train_wall=63, wall=46337
2021-01-02 10:43:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:43:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:43:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:43:27 | INFO | valid | epoch 162 | valid on 'valid' subset | symm_mse 0 | loss 5.205 | nll_loss 3.686 | ppl 12.87 | bleu 22.72 | wps 5742.3 | wpb 10324.2 | bsz 375 | num_updates 68202 | best_bleu 22.81
2021-01-02 10:43:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:43:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:43:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 162 @ 68202 updates, score 22.72) (writing took 3.1865308973938227 seconds)
2021-01-02 10:43:31 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2021-01-02 10:43:31 | INFO | train | epoch 162 | symm_mse 5.33 | loss 3.456 | nll_loss 1.098 | ppl 2.14 | wps 20483.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 68202 | lr 8.38923e-06 | gnorm 0.852 | train_wall 263 | wall 46359
2021-01-02 10:43:31 | INFO | fairseq.trainer | begin training epoch 163
2021-01-02 10:43:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:43:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:44:35 | INFO | train_inner | epoch 163:     98 / 421 symm_mse=5.208, loss=3.429, nll_loss=1.084, ppl=2.12, wps=16204.5, ups=1.17, wpb=13883.1, bsz=488.5, num_updates=68300, lr=8.38321e-06, gnorm=0.83, train_wall=62, wall=46423
2021-01-02 10:45:37 | INFO | train_inner | epoch 163:    198 / 421 symm_mse=5.353, loss=3.455, nll_loss=1.095, ppl=2.14, wps=22296.1, ups=1.6, wpb=13963.1, bsz=488.4, num_updates=68400, lr=8.37708e-06, gnorm=0.857, train_wall=62, wall=46486
2021-01-02 10:46:40 | INFO | train_inner | epoch 163:    298 / 421 symm_mse=5.304, loss=3.454, nll_loss=1.1, ppl=2.14, wps=22125.2, ups=1.59, wpb=13911.9, bsz=489, num_updates=68500, lr=8.37096e-06, gnorm=0.848, train_wall=63, wall=46549
2021-01-02 10:47:43 | INFO | train_inner | epoch 163:    398 / 421 symm_mse=5.439, loss=3.479, nll_loss=1.111, ppl=2.16, wps=22473.1, ups=1.59, wpb=14128.7, bsz=495.4, num_updates=68600, lr=8.36486e-06, gnorm=0.882, train_wall=63, wall=46611
2021-01-02 10:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:48:14 | INFO | valid | epoch 163 | valid on 'valid' subset | symm_mse 0 | loss 5.202 | nll_loss 3.683 | ppl 12.85 | bleu 22.67 | wps 5862 | wpb 10324.2 | bsz 375 | num_updates 68623 | best_bleu 22.81
2021-01-02 10:48:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:48:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 163 @ 68623 updates, score 22.67) (writing took 3.0840325709432364 seconds)
2021-01-02 10:48:17 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2021-01-02 10:48:17 | INFO | train | epoch 163 | symm_mse 5.323 | loss 3.455 | nll_loss 1.098 | ppl 2.14 | wps 20495.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 68623 | lr 8.36346e-06 | gnorm 0.854 | train_wall 263 | wall 46646
2021-01-02 10:48:17 | INFO | fairseq.trainer | begin training epoch 164
2021-01-02 10:48:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:48:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:49:08 | INFO | train_inner | epoch 164:     77 / 421 symm_mse=5.194, loss=3.43, nll_loss=1.087, ppl=2.12, wps=16402.9, ups=1.17, wpb=14026.4, bsz=506.6, num_updates=68700, lr=8.35877e-06, gnorm=0.827, train_wall=62, wall=46697
2021-01-02 10:50:11 | INFO | train_inner | epoch 164:    177 / 421 symm_mse=5.337, loss=3.453, nll_loss=1.093, ppl=2.13, wps=22306.2, ups=1.59, wpb=14042.8, bsz=493.2, num_updates=68800, lr=8.35269e-06, gnorm=0.847, train_wall=63, wall=46760
2021-01-02 10:51:14 | INFO | train_inner | epoch 164:    277 / 421 symm_mse=5.418, loss=3.47, nll_loss=1.103, ppl=2.15, wps=22267.2, ups=1.6, wpb=13956.4, bsz=483, num_updates=68900, lr=8.34663e-06, gnorm=0.861, train_wall=62, wall=46823
2021-01-02 10:52:17 | INFO | train_inner | epoch 164:    377 / 421 symm_mse=5.355, loss=3.467, nll_loss=1.109, ppl=2.16, wps=22211.7, ups=1.6, wpb=13920.9, bsz=490.8, num_updates=69000, lr=8.34058e-06, gnorm=0.844, train_wall=62, wall=46885
2021-01-02 10:52:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:53:01 | INFO | valid | epoch 164 | valid on 'valid' subset | symm_mse 0 | loss 5.2 | nll_loss 3.682 | ppl 12.83 | bleu 22.85 | wps 6015.5 | wpb 10324.2 | bsz 375 | num_updates 69044 | best_bleu 22.85
2021-01-02 10:53:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:53:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:53:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:53:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:53:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_best.pt (epoch 164 @ 69044 updates, score 22.85) (writing took 5.004554934799671 seconds)
2021-01-02 10:53:06 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2021-01-02 10:53:06 | INFO | train | epoch 164 | symm_mse 5.326 | loss 3.455 | nll_loss 1.098 | ppl 2.14 | wps 20368 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 69044 | lr 8.33792e-06 | gnorm 0.851 | train_wall 263 | wall 46935
2021-01-02 10:53:06 | INFO | fairseq.trainer | begin training epoch 165
2021-01-02 10:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:53:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:53:44 | INFO | train_inner | epoch 165:     56 / 421 symm_mse=5.26, loss=3.447, nll_loss=1.098, ppl=2.14, wps=16041, ups=1.15, wpb=13964.9, bsz=490.1, num_updates=69100, lr=8.33454e-06, gnorm=0.85, train_wall=62, wall=46972
2021-01-02 10:54:46 | INFO | train_inner | epoch 165:    156 / 421 symm_mse=5.387, loss=3.462, nll_loss=1.097, ppl=2.14, wps=22420, ups=1.6, wpb=13980.9, bsz=494.4, num_updates=69200, lr=8.32851e-06, gnorm=0.877, train_wall=62, wall=47035
2021-01-02 10:55:49 | INFO | train_inner | epoch 165:    256 / 421 symm_mse=5.261, loss=3.444, nll_loss=1.094, ppl=2.13, wps=22159.2, ups=1.6, wpb=13843.7, bsz=499.8, num_updates=69300, lr=8.3225e-06, gnorm=0.846, train_wall=62, wall=47097
2021-01-02 10:56:51 | INFO | train_inner | epoch 165:    356 / 421 symm_mse=5.291, loss=3.45, nll_loss=1.097, ppl=2.14, wps=22419.8, ups=1.6, wpb=14028, bsz=499.3, num_updates=69400, lr=8.31651e-06, gnorm=0.862, train_wall=62, wall=47160
2021-01-02 10:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 10:57:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 10:57:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 10:57:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 10:57:50 | INFO | valid | epoch 165 | valid on 'valid' subset | symm_mse 0 | loss 5.201 | nll_loss 3.683 | ppl 12.84 | bleu 22.81 | wps 5502.6 | wpb 10324.2 | bsz 375 | num_updates 69465 | best_bleu 22.85
2021-01-02 10:57:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 10:57:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:57:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 165 @ 69465 updates, score 22.81) (writing took 3.215425418689847 seconds)
2021-01-02 10:57:53 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2021-01-02 10:57:53 | INFO | train | epoch 165 | symm_mse 5.318 | loss 3.453 | nll_loss 1.097 | ppl 2.14 | wps 20516 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 69465 | lr 8.31261e-06 | gnorm 0.858 | train_wall 262 | wall 47221
2021-01-02 10:57:53 | INFO | fairseq.trainer | begin training epoch 166
2021-01-02 10:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 10:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 10:58:18 | INFO | train_inner | epoch 166:     35 / 421 symm_mse=5.336, loss=3.45, nll_loss=1.091, ppl=2.13, wps=15992.9, ups=1.16, wpb=13840.1, bsz=494.2, num_updates=69500, lr=8.31052e-06, gnorm=0.874, train_wall=62, wall=47246
2021-01-02 10:59:20 | INFO | train_inner | epoch 166:    135 / 421 symm_mse=5.353, loss=3.454, nll_loss=1.093, ppl=2.13, wps=22419.3, ups=1.6, wpb=14027.8, bsz=481.8, num_updates=69600, lr=8.30455e-06, gnorm=0.871, train_wall=62, wall=47309
2021-01-02 11:00:23 | INFO | train_inner | epoch 166:    235 / 421 symm_mse=5.23, loss=3.449, nll_loss=1.104, ppl=2.15, wps=22190.7, ups=1.6, wpb=13901.9, bsz=502.8, num_updates=69700, lr=8.29859e-06, gnorm=0.845, train_wall=62, wall=47371
2021-01-02 11:01:26 | INFO | train_inner | epoch 166:    335 / 421 symm_mse=5.454, loss=3.479, nll_loss=1.108, ppl=2.16, wps=22312.8, ups=1.59, wpb=14037.1, bsz=497.1, num_updates=69800, lr=8.29264e-06, gnorm=0.874, train_wall=63, wall=47434
2021-01-02 11:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:02:37 | INFO | valid | epoch 166 | valid on 'valid' subset | symm_mse 0 | loss 5.198 | nll_loss 3.68 | ppl 12.82 | bleu 22.69 | wps 5697.5 | wpb 10324.2 | bsz 375 | num_updates 69886 | best_bleu 22.85
2021-01-02 11:02:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:02:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 166 @ 69886 updates, score 22.69) (writing took 2.8731446489691734 seconds)
2021-01-02 11:02:40 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2021-01-02 11:02:40 | INFO | train | epoch 166 | symm_mse 5.315 | loss 3.453 | nll_loss 1.097 | ppl 2.14 | wps 20506.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 69886 | lr 8.28754e-06 | gnorm 0.86 | train_wall 262 | wall 47508
2021-01-02 11:02:40 | INFO | fairseq.trainer | begin training epoch 167
2021-01-02 11:02:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:02:52 | INFO | train_inner | epoch 167:     14 / 421 symm_mse=5.23, loss=3.432, nll_loss=1.085, ppl=2.12, wps=16287.8, ups=1.17, wpb=13966.2, bsz=479.5, num_updates=69900, lr=8.28671e-06, gnorm=0.846, train_wall=62, wall=47520
2021-01-02 11:03:54 | INFO | train_inner | epoch 167:    114 / 421 symm_mse=5.423, loss=3.475, nll_loss=1.108, ppl=2.16, wps=22532.5, ups=1.61, wpb=13994.7, bsz=486.7, num_updates=70000, lr=8.28079e-06, gnorm=0.85, train_wall=62, wall=47582
2021-01-02 11:04:57 | INFO | train_inner | epoch 167:    214 / 421 symm_mse=5.321, loss=3.453, nll_loss=1.097, ppl=2.14, wps=22283.3, ups=1.59, wpb=14001.9, bsz=488.5, num_updates=70100, lr=8.27488e-06, gnorm=0.858, train_wall=63, wall=47645
2021-01-02 11:05:59 | INFO | train_inner | epoch 167:    314 / 421 symm_mse=5.254, loss=3.444, nll_loss=1.096, ppl=2.14, wps=22465.5, ups=1.59, wpb=14085.7, bsz=503.1, num_updates=70200, lr=8.26898e-06, gnorm=0.836, train_wall=62, wall=47708
2021-01-02 11:07:02 | INFO | train_inner | epoch 167:    414 / 421 symm_mse=5.227, loss=3.435, nll_loss=1.089, ppl=2.13, wps=22269.7, ups=1.6, wpb=13900, bsz=495.6, num_updates=70300, lr=8.2631e-06, gnorm=0.848, train_wall=62, wall=47770
2021-01-02 11:07:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:07:23 | INFO | valid | epoch 167 | valid on 'valid' subset | symm_mse 0 | loss 5.198 | nll_loss 3.68 | ppl 12.82 | bleu 22.71 | wps 5812 | wpb 10324.2 | bsz 375 | num_updates 70307 | best_bleu 22.85
2021-01-02 11:07:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:07:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:07:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 167 @ 70307 updates, score 22.71) (writing took 3.149712208658457 seconds)
2021-01-02 11:07:26 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2021-01-02 11:07:26 | INFO | train | epoch 167 | symm_mse 5.317 | loss 3.453 | nll_loss 1.097 | ppl 2.14 | wps 20524.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 70307 | lr 8.26269e-06 | gnorm 0.852 | train_wall 262 | wall 47795
2021-01-02 11:07:26 | INFO | fairseq.trainer | begin training epoch 168
2021-01-02 11:07:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:07:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:08:27 | INFO | train_inner | epoch 168:     93 / 421 symm_mse=5.432, loss=3.484, nll_loss=1.117, ppl=2.17, wps=16283.5, ups=1.17, wpb=13879.9, bsz=490.7, num_updates=70400, lr=8.25723e-06, gnorm=0.862, train_wall=61, wall=47855
2021-01-02 11:09:29 | INFO | train_inner | epoch 168:    193 / 421 symm_mse=5.457, loss=3.468, nll_loss=1.096, ppl=2.14, wps=22440.3, ups=1.6, wpb=14026.1, bsz=486.1, num_updates=70500, lr=8.25137e-06, gnorm=0.851, train_wall=62, wall=47918
2021-01-02 11:10:32 | INFO | train_inner | epoch 168:    293 / 421 symm_mse=5.273, loss=3.449, nll_loss=1.098, ppl=2.14, wps=22150.8, ups=1.6, wpb=13875.3, bsz=490.4, num_updates=70600, lr=8.24552e-06, gnorm=0.844, train_wall=62, wall=47981
2021-01-02 11:11:35 | INFO | train_inner | epoch 168:    393 / 421 symm_mse=5.138, loss=3.419, nll_loss=1.082, ppl=2.12, wps=22397.4, ups=1.6, wpb=14035.8, bsz=503.1, num_updates=70700, lr=8.23969e-06, gnorm=0.829, train_wall=62, wall=48043
2021-01-02 11:11:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:11:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:11:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:11:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:11:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:11:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:11:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:11:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:11:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:11:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:11:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:11:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:12:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:12:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:12:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:12:09 | INFO | valid | epoch 168 | valid on 'valid' subset | symm_mse 0 | loss 5.201 | nll_loss 3.681 | ppl 12.83 | bleu 22.65 | wps 5868.7 | wpb 10324.2 | bsz 375 | num_updates 70728 | best_bleu 22.85
2021-01-02 11:12:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:12:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 168 @ 70728 updates, score 22.65) (writing took 3.139035377651453 seconds)
2021-01-02 11:12:12 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2021-01-02 11:12:12 | INFO | train | epoch 168 | symm_mse 5.304 | loss 3.452 | nll_loss 1.097 | ppl 2.14 | wps 20565.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 70728 | lr 8.23806e-06 | gnorm 0.842 | train_wall 262 | wall 48081
2021-01-02 11:12:12 | INFO | fairseq.trainer | begin training epoch 169
2021-01-02 11:12:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:12:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:13:00 | INFO | train_inner | epoch 169:     72 / 421 symm_mse=5.239, loss=3.442, nll_loss=1.093, ppl=2.13, wps=16397.4, ups=1.18, wpb=13944.5, bsz=493.4, num_updates=70800, lr=8.23387e-06, gnorm=0.842, train_wall=61, wall=48128
2021-01-02 11:14:03 | INFO | train_inner | epoch 169:    172 / 421 symm_mse=5.285, loss=3.439, nll_loss=1.085, ppl=2.12, wps=22556.8, ups=1.59, wpb=14148.5, bsz=492.2, num_updates=70900, lr=8.22806e-06, gnorm=0.848, train_wall=63, wall=48191
2021-01-02 11:15:05 | INFO | train_inner | epoch 169:    272 / 421 symm_mse=5.325, loss=3.456, nll_loss=1.099, ppl=2.14, wps=22366.4, ups=1.59, wpb=14038.6, bsz=490.2, num_updates=71000, lr=8.22226e-06, gnorm=0.864, train_wall=63, wall=48254
2021-01-02 11:16:08 | INFO | train_inner | epoch 169:    372 / 421 symm_mse=5.444, loss=3.483, nll_loss=1.116, ppl=2.17, wps=22211.2, ups=1.61, wpb=13835.5, bsz=486.9, num_updates=71100, lr=8.21648e-06, gnorm=0.874, train_wall=62, wall=48316
2021-01-02 11:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:16:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:16:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:16:56 | INFO | valid | epoch 169 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.684 | ppl 12.85 | bleu 22.7 | wps 5519.5 | wpb 10324.2 | bsz 375 | num_updates 71149 | best_bleu 22.85
2021-01-02 11:16:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:16:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:16:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 169 @ 71149 updates, score 22.7) (writing took 3.0834026858210564 seconds)
2021-01-02 11:16:59 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2021-01-02 11:16:59 | INFO | train | epoch 169 | symm_mse 5.306 | loss 3.452 | nll_loss 1.097 | ppl 2.14 | wps 20510.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 71149 | lr 8.21365e-06 | gnorm 0.86 | train_wall 262 | wall 48367
2021-01-02 11:16:59 | INFO | fairseq.trainer | begin training epoch 170
2021-01-02 11:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:17:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:17:33 | INFO | train_inner | epoch 170:     51 / 421 symm_mse=5.095, loss=3.423, nll_loss=1.093, ppl=2.13, wps=16078.9, ups=1.16, wpb=13801.9, bsz=502.9, num_updates=71200, lr=8.21071e-06, gnorm=0.847, train_wall=62, wall=48402
2021-01-02 11:18:36 | INFO | train_inner | epoch 170:    151 / 421 symm_mse=5.48, loss=3.483, nll_loss=1.109, ppl=2.16, wps=22486.4, ups=1.61, wpb=13975.6, bsz=482.7, num_updates=71300, lr=8.20495e-06, gnorm=0.88, train_wall=62, wall=48464
2021-01-02 11:19:38 | INFO | train_inner | epoch 170:    251 / 421 symm_mse=5.293, loss=3.451, nll_loss=1.098, ppl=2.14, wps=22343.9, ups=1.6, wpb=13980.4, bsz=490.9, num_updates=71400, lr=8.1992e-06, gnorm=0.879, train_wall=62, wall=48527
2021-01-02 11:20:41 | INFO | train_inner | epoch 170:    351 / 421 symm_mse=5.203, loss=3.429, nll_loss=1.085, ppl=2.12, wps=22235.3, ups=1.6, wpb=13916.7, bsz=501.2, num_updates=71500, lr=8.19346e-06, gnorm=0.848, train_wall=62, wall=48589
2021-01-02 11:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:21:41 | INFO | valid | epoch 170 | valid on 'valid' subset | symm_mse 0 | loss 5.2 | nll_loss 3.681 | ppl 12.83 | bleu 22.68 | wps 5924.6 | wpb 10324.2 | bsz 375 | num_updates 71570 | best_bleu 22.85
2021-01-02 11:21:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:21:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 170 @ 71570 updates, score 22.68) (writing took 3.0702995210886 seconds)
2021-01-02 11:21:44 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2021-01-02 11:21:44 | INFO | train | epoch 170 | symm_mse 5.306 | loss 3.451 | nll_loss 1.097 | ppl 2.14 | wps 20597.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 71570 | lr 8.18946e-06 | gnorm 0.864 | train_wall 262 | wall 48653
2021-01-02 11:21:44 | INFO | fairseq.trainer | begin training epoch 171
2021-01-02 11:21:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:21:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:22:06 | INFO | train_inner | epoch 171:     30 / 421 symm_mse=5.379, loss=3.459, nll_loss=1.095, ppl=2.14, wps=16480.9, ups=1.17, wpb=14090.7, bsz=485, num_updates=71600, lr=8.18774e-06, gnorm=0.863, train_wall=62, wall=48675
2021-01-02 11:23:08 | INFO | train_inner | epoch 171:    130 / 421 symm_mse=5.24, loss=3.446, nll_loss=1.099, ppl=2.14, wps=22610, ups=1.61, wpb=14057.1, bsz=502.6, num_updates=71700, lr=8.18203e-06, gnorm=0.829, train_wall=62, wall=48737
2021-01-02 11:24:11 | INFO | train_inner | epoch 171:    230 / 421 symm_mse=5.349, loss=3.463, nll_loss=1.105, ppl=2.15, wps=22106, ups=1.6, wpb=13844, bsz=488.8, num_updates=71800, lr=8.17633e-06, gnorm=0.869, train_wall=62, wall=48799
2021-01-02 11:25:13 | INFO | train_inner | epoch 171:    330 / 421 symm_mse=5.278, loss=3.439, nll_loss=1.086, ppl=2.12, wps=22443.9, ups=1.6, wpb=14006.2, bsz=499.4, num_updates=71900, lr=8.17064e-06, gnorm=0.862, train_wall=62, wall=48862
2021-01-02 11:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:26:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:26:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:26:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:26:27 | INFO | valid | epoch 171 | valid on 'valid' subset | symm_mse 0 | loss 5.201 | nll_loss 3.683 | ppl 12.85 | bleu 22.66 | wps 5938.6 | wpb 10324.2 | bsz 375 | num_updates 71991 | best_bleu 22.85
2021-01-02 11:26:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 171 @ 71991 updates, score 22.66) (writing took 3.1155334506183863 seconds)
2021-01-02 11:26:30 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2021-01-02 11:26:30 | INFO | train | epoch 171 | symm_mse 5.299 | loss 3.451 | nll_loss 1.097 | ppl 2.14 | wps 20584.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 71991 | lr 8.16548e-06 | gnorm 0.855 | train_wall 262 | wall 48939
2021-01-02 11:26:30 | INFO | fairseq.trainer | begin training epoch 172
2021-01-02 11:26:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:26:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:26:39 | INFO | train_inner | epoch 172:      9 / 421 symm_mse=5.314, loss=3.457, nll_loss=1.102, ppl=2.15, wps=16210.7, ups=1.17, wpb=13852.4, bsz=479.4, num_updates=72000, lr=8.16497e-06, gnorm=0.87, train_wall=62, wall=48947
2021-01-02 11:27:41 | INFO | train_inner | epoch 172:    109 / 421 symm_mse=5.319, loss=3.459, nll_loss=1.104, ppl=2.15, wps=22421.4, ups=1.61, wpb=13907.1, bsz=483.4, num_updates=72100, lr=8.1593e-06, gnorm=0.855, train_wall=62, wall=49009
2021-01-02 11:28:44 | INFO | train_inner | epoch 172:    209 / 421 symm_mse=5.239, loss=3.438, nll_loss=1.09, ppl=2.13, wps=22240.9, ups=1.59, wpb=14003.7, bsz=496.9, num_updates=72200, lr=8.15365e-06, gnorm=0.843, train_wall=63, wall=49072
2021-01-02 11:29:47 | INFO | train_inner | epoch 172:    309 / 421 symm_mse=5.27, loss=3.444, nll_loss=1.093, ppl=2.13, wps=22365.4, ups=1.59, wpb=14031.9, bsz=502.3, num_updates=72300, lr=8.14801e-06, gnorm=0.832, train_wall=63, wall=49135
2021-01-02 11:30:49 | INFO | train_inner | epoch 172:    409 / 421 symm_mse=5.311, loss=3.452, nll_loss=1.096, ppl=2.14, wps=22557.9, ups=1.6, wpb=14074.2, bsz=493.2, num_updates=72400, lr=8.14238e-06, gnorm=0.837, train_wall=62, wall=49197
2021-01-02 11:30:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:31:13 | INFO | valid | epoch 172 | valid on 'valid' subset | symm_mse 0 | loss 5.2 | nll_loss 3.682 | ppl 12.84 | bleu 22.74 | wps 5960.3 | wpb 10324.2 | bsz 375 | num_updates 72412 | best_bleu 22.85
2021-01-02 11:31:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:31:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 172 @ 72412 updates, score 22.74) (writing took 2.524483224377036 seconds)
2021-01-02 11:31:16 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2021-01-02 11:31:16 | INFO | train | epoch 172 | symm_mse 5.295 | loss 3.45 | nll_loss 1.096 | ppl 2.14 | wps 20606.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 72412 | lr 8.1417e-06 | gnorm 0.845 | train_wall 262 | wall 49224
2021-01-02 11:31:16 | INFO | fairseq.trainer | begin training epoch 173
2021-01-02 11:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:32:13 | INFO | train_inner | epoch 173:     88 / 421 symm_mse=5.414, loss=3.462, nll_loss=1.094, ppl=2.13, wps=16492.7, ups=1.19, wpb=13887.6, bsz=469.8, num_updates=72500, lr=8.13676e-06, gnorm=0.855, train_wall=62, wall=49282
2021-01-02 11:33:16 | INFO | train_inner | epoch 173:    188 / 421 symm_mse=5.26, loss=3.456, nll_loss=1.107, ppl=2.15, wps=22452.9, ups=1.61, wpb=13983.5, bsz=492.5, num_updates=72600, lr=8.13116e-06, gnorm=0.827, train_wall=62, wall=49344
2021-01-02 11:34:18 | INFO | train_inner | epoch 173:    288 / 421 symm_mse=5.231, loss=3.442, nll_loss=1.096, ppl=2.14, wps=22385.6, ups=1.61, wpb=13885.1, bsz=508.1, num_updates=72700, lr=8.12556e-06, gnorm=0.828, train_wall=62, wall=49406
2021-01-02 11:35:19 | INFO | train_inner | epoch 173:    388 / 421 symm_mse=5.293, loss=3.444, nll_loss=1.09, ppl=2.13, wps=22858.4, ups=1.62, wpb=14143.7, bsz=497.7, num_updates=72800, lr=8.11998e-06, gnorm=0.845, train_wall=62, wall=49468
2021-01-02 11:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:35:58 | INFO | valid | epoch 173 | valid on 'valid' subset | symm_mse 0 | loss 5.202 | nll_loss 3.682 | ppl 12.84 | bleu 22.77 | wps 5171.2 | wpb 10324.2 | bsz 375 | num_updates 72833 | best_bleu 22.85
2021-01-02 11:35:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:35:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:35:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:36:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 173 @ 72833 updates, score 22.77) (writing took 3.0242960806936026 seconds)
2021-01-02 11:36:01 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2021-01-02 11:36:01 | INFO | train | epoch 173 | symm_mse 5.29 | loss 3.449 | nll_loss 1.096 | ppl 2.14 | wps 20573.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 72833 | lr 8.11814e-06 | gnorm 0.838 | train_wall 260 | wall 49510
2021-01-02 11:36:01 | INFO | fairseq.trainer | begin training epoch 174
2021-01-02 11:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:36:46 | INFO | train_inner | epoch 174:     67 / 421 symm_mse=5.27, loss=3.443, nll_loss=1.092, ppl=2.13, wps=15938.1, ups=1.15, wpb=13800.7, bsz=493, num_updates=72900, lr=8.11441e-06, gnorm=0.841, train_wall=62, wall=49554
2021-01-02 11:37:49 | INFO | train_inner | epoch 174:    167 / 421 symm_mse=5.061, loss=3.422, nll_loss=1.095, ppl=2.14, wps=22095.8, ups=1.58, wpb=13942.9, bsz=519.8, num_updates=73000, lr=8.10885e-06, gnorm=0.809, train_wall=63, wall=49618
2021-01-02 11:38:52 | INFO | train_inner | epoch 174:    267 / 421 symm_mse=5.337, loss=3.446, nll_loss=1.086, ppl=2.12, wps=22474.8, ups=1.59, wpb=14121.8, bsz=490.1, num_updates=73100, lr=8.1033e-06, gnorm=0.838, train_wall=63, wall=49680
2021-01-02 11:39:55 | INFO | train_inner | epoch 174:    367 / 421 symm_mse=5.278, loss=3.447, nll_loss=1.095, ppl=2.14, wps=22286.9, ups=1.59, wpb=13985.4, bsz=488.1, num_updates=73200, lr=8.09776e-06, gnorm=0.828, train_wall=63, wall=49743
2021-01-02 11:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:40:45 | INFO | valid | epoch 174 | valid on 'valid' subset | symm_mse 0 | loss 5.2 | nll_loss 3.683 | ppl 12.84 | bleu 22.54 | wps 6012.4 | wpb 10324.2 | bsz 375 | num_updates 73254 | best_bleu 22.85
2021-01-02 11:40:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:40:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:40:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 174 @ 73254 updates, score 22.54) (writing took 3.0742387641221285 seconds)
2021-01-02 11:40:48 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2021-01-02 11:40:48 | INFO | train | epoch 174 | symm_mse 5.284 | loss 3.449 | nll_loss 1.096 | ppl 2.14 | wps 20506.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 73254 | lr 8.09478e-06 | gnorm 0.835 | train_wall 263 | wall 49797
2021-01-02 11:40:48 | INFO | fairseq.trainer | begin training epoch 175
2021-01-02 11:40:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:40:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:41:20 | INFO | train_inner | epoch 175:     46 / 421 symm_mse=5.469, loss=3.477, nll_loss=1.105, ppl=2.15, wps=16254.2, ups=1.17, wpb=13862.1, bsz=470.2, num_updates=73300, lr=8.09224e-06, gnorm=0.868, train_wall=62, wall=49828
2021-01-02 11:42:22 | INFO | train_inner | epoch 175:    146 / 421 symm_mse=5.34, loss=3.446, nll_loss=1.086, ppl=2.12, wps=22464.7, ups=1.6, wpb=14008.7, bsz=491.7, num_updates=73400, lr=8.08672e-06, gnorm=0.864, train_wall=62, wall=49891
2021-01-02 11:43:25 | INFO | train_inner | epoch 175:    246 / 421 symm_mse=5.05, loss=3.421, nll_loss=1.094, ppl=2.14, wps=22443.6, ups=1.6, wpb=14015.2, bsz=521.2, num_updates=73500, lr=8.08122e-06, gnorm=0.806, train_wall=62, wall=49953
2021-01-02 11:44:27 | INFO | train_inner | epoch 175:    346 / 421 symm_mse=5.524, loss=3.495, nll_loss=1.119, ppl=2.17, wps=22210.7, ups=1.6, wpb=13921.3, bsz=472, num_updates=73600, lr=8.07573e-06, gnorm=0.871, train_wall=62, wall=50016
2021-01-02 11:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:45:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:45:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:45:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:45:33 | INFO | valid | epoch 175 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.686 | ppl 12.87 | bleu 22.68 | wps 5143.7 | wpb 10324.2 | bsz 375 | num_updates 73675 | best_bleu 22.85
2021-01-02 11:45:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:45:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all1/checkpoint_last.pt (epoch 175 @ 73675 updates, score 22.68) (writing took 2.887449663132429 seconds)
2021-01-02 11:45:36 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2021-01-02 11:45:36 | INFO | train | epoch 175 | symm_mse 5.284 | loss 3.448 | nll_loss 1.095 | ppl 2.14 | wps 20443.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 73675 | lr 8.07162e-06 | gnorm 0.845 | train_wall 262 | wall 50084
2021-01-02 11:45:36 | INFO | fairseq.trainer | begin training epoch 176
2021-01-02 11:45:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:45:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:45:55 | INFO | train_inner | epoch 176:     25 / 421 symm_mse=5.181, loss=3.428, nll_loss=1.087, ppl=2.12, wps=16032.7, ups=1.15, wpb=13979.8, bsz=486.9, num_updates=73700, lr=8.07025e-06, gnorm=0.832, train_wall=62, wall=50103
2021-01-02 11:46:57 | INFO | train_inner | epoch 176:    125 / 421 symm_mse=5.397, loss=3.473, nll_loss=1.109, ppl=2.16, wps=22483.7, ups=1.61, wpb=13949.2, bsz=476.7, num_updates=73800, lr=8.06478e-06, gnorm=0.875, train_wall=62, wall=50165
2021-01-02 11:47:59 | INFO | train_inner | epoch 176:    225 / 421 symm_mse=5.179, loss=3.431, nll_loss=1.09, ppl=2.13, wps=22390.9, ups=1.6, wpb=13969.7, bsz=511, num_updates=73900, lr=8.05932e-06, gnorm=0.836, train_wall=62, wall=50228
2021-01-02 11:49:01 | INFO | train_inner | epoch 176:    325 / 421 symm_mse=5.234, loss=3.436, nll_loss=1.089, ppl=2.13, wps=22520.6, ups=1.61, wpb=14016.2, bsz=507, num_updates=74000, lr=8.05387e-06, gnorm=0.859, train_wall=62, wall=50290
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1448 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
