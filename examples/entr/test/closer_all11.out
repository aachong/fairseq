nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/closer-all2
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--layer-choice normal --warmup-init-lr 1e-07 --noised-with-grad'
2021-01-02 11:52:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11807
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11807
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11807
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11807
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-02 11:52:44 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-02 11:52:48 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11807', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_with_grad=True, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-02 11:52:48 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-02 11:52:48 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-02 11:52:48 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-02 11:52:48 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-02 11:52:48 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-02 11:52:49 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-02 11:52:49 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-02 11:52:49 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2021-01-02 11:52:49 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2021-01-02 11:52:49 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-02 11:52:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-02 11:52:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-02 11:52:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-02 11:52:49 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-02 11:52:49 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-02 11:52:49 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-02 11:52:49 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-02 11:52:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-02 11:52:49 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2021-01-02 11:52:49 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-02 11:52:49 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-02 11:52:50 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-02 11:52:50 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-02 11:52:50 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-02 11:52:50 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-02 11:52:50 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-02 11:52:50 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-02 11:52:50 | INFO | fairseq.trainer | begin training epoch 1
2021-01-02 11:52:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:52:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:52:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:53:55 | INFO | train_inner | epoch 001:    100 / 421 symm_mse=0.623, loss=3.559, nll_loss=0.843, ppl=1.79, wps=23300.8, ups=1.65, wpb=14145.4, bsz=485.9, num_updates=100, lr=1.43e-06, gnorm=1.505, train_wall=61, wall=66
2021-01-02 11:54:56 | INFO | train_inner | epoch 001:    200 / 421 symm_mse=0.571, loss=3.483, nll_loss=0.85, ppl=1.8, wps=22912.2, ups=1.64, wpb=13995.2, bsz=501.2, num_updates=200, lr=2.76e-06, gnorm=1.469, train_wall=61, wall=127
2021-01-02 11:55:58 | INFO | train_inner | epoch 001:    300 / 421 symm_mse=0.508, loss=3.401, nll_loss=0.869, ppl=1.83, wps=22462.4, ups=1.61, wpb=13972.8, bsz=508.7, num_updates=300, lr=4.09e-06, gnorm=1.27, train_wall=62, wall=189
2021-01-02 11:57:00 | INFO | train_inner | epoch 001:    400 / 421 symm_mse=0.481, loss=3.369, nll_loss=0.871, ppl=1.83, wps=22594.4, ups=1.63, wpb=13886.9, bsz=487.4, num_updates=400, lr=5.42e-06, gnorm=1.079, train_wall=61, wall=251
2021-01-02 11:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 11:57:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 11:57:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 11:57:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 11:57:29 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_mse 0 | loss 5.716 | nll_loss 4.14 | ppl 17.63 | bleu 22.23 | wps 6199 | wpb 10324.2 | bsz 375 | num_updates 421
2021-01-02 11:57:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 1 @ 421 updates, score 22.23) (writing took 2.2213786840438843 seconds)
2021-01-02 11:57:31 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-02 11:57:31 | INFO | train | epoch 001 | symm_mse 0.543 | loss 3.449 | nll_loss 0.859 | ppl 1.81 | wps 21235.3 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 421 | lr 5.6993e-06 | gnorm 1.318 | train_wall 258 | wall 282
2021-01-02 11:57:31 | INFO | fairseq.trainer | begin training epoch 2
2021-01-02 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:57:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 11:57:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 11:58:23 | INFO | train_inner | epoch 002:     79 / 421 symm_mse=0.466, loss=3.346, nll_loss=0.87, ppl=1.83, wps=16629.1, ups=1.2, wpb=13861.1, bsz=487.7, num_updates=500, lr=6.75e-06, gnorm=1.03, train_wall=61, wall=334
2021-01-02 11:59:25 | INFO | train_inner | epoch 002:    179 / 421 symm_mse=0.446, loss=3.31, nll_loss=0.863, ppl=1.82, wps=22730.2, ups=1.62, wpb=14007.7, bsz=489.9, num_updates=600, lr=8.08e-06, gnorm=0.979, train_wall=61, wall=395
2021-01-02 12:00:27 | INFO | train_inner | epoch 002:    279 / 421 symm_mse=0.439, loss=3.304, nll_loss=0.87, ppl=1.83, wps=22754, ups=1.62, wpb=14033.1, bsz=489, num_updates=700, lr=9.41e-06, gnorm=0.958, train_wall=61, wall=457
2021-01-02 12:01:28 | INFO | train_inner | epoch 002:    379 / 421 symm_mse=0.437, loss=3.315, nll_loss=0.887, ppl=1.85, wps=22586, ups=1.62, wpb=13928.4, bsz=496.1, num_updates=800, lr=1.074e-05, gnorm=0.943, train_wall=61, wall=519
2021-01-02 12:01:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:01:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:01:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:01:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:01:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:01:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:01:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:01:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:02:11 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_mse 0 | loss 5.626 | nll_loss 4.061 | ppl 16.69 | bleu 22.32 | wps 6002.2 | wpb 10324.2 | bsz 375 | num_updates 842 | best_bleu 22.32
2021-01-02 12:02:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:02:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 2 @ 842 updates, score 22.32) (writing took 4.7471251636743546 seconds)
2021-01-02 12:02:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-02 12:02:16 | INFO | train | epoch 002 | symm_mse 0.443 | loss 3.314 | nll_loss 0.873 | ppl 1.83 | wps 20693.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 842 | lr 1.12986e-05 | gnorm 0.971 | train_wall 259 | wall 566
2021-01-02 12:02:16 | INFO | fairseq.trainer | begin training epoch 3
2021-01-02 12:02:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:02:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:02:54 | INFO | train_inner | epoch 003:     58 / 421 symm_mse=0.423, loss=3.283, nll_loss=0.876, ppl=1.83, wps=16196.7, ups=1.17, wpb=13895.5, bsz=482.9, num_updates=900, lr=1.207e-05, gnorm=0.932, train_wall=61, wall=605
2021-01-02 12:03:56 | INFO | train_inner | epoch 003:    158 / 421 symm_mse=0.417, loss=3.282, nll_loss=0.882, ppl=1.84, wps=22616.7, ups=1.62, wpb=13938, bsz=507, num_updates=1000, lr=1.34e-05, gnorm=0.908, train_wall=61, wall=666
2021-01-02 12:04:57 | INFO | train_inner | epoch 003:    258 / 421 symm_mse=0.417, loss=3.285, nll_loss=0.888, ppl=1.85, wps=22638.5, ups=1.62, wpb=13957.9, bsz=482.5, num_updates=1100, lr=1.473e-05, gnorm=0.904, train_wall=61, wall=728
2021-01-02 12:05:59 | INFO | train_inner | epoch 003:    358 / 421 symm_mse=0.404, loss=3.262, nll_loss=0.883, ppl=1.84, wps=22749.1, ups=1.61, wpb=14135, bsz=507.8, num_updates=1200, lr=1.606e-05, gnorm=0.874, train_wall=62, wall=790
2021-01-02 12:06:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:06:55 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_mse 0 | loss 5.571 | nll_loss 4.008 | ppl 16.09 | bleu 22.44 | wps 6009.1 | wpb 10324.2 | bsz 375 | num_updates 1263 | best_bleu 22.44
2021-01-02 12:06:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:06:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:07:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 3 @ 1263 updates, score 22.44) (writing took 4.7726970463991165 seconds)
2021-01-02 12:07:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-02 12:07:00 | INFO | train | epoch 003 | symm_mse 0.412 | loss 3.273 | nll_loss 0.882 | ppl 1.84 | wps 20697.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 1263 | lr 1.68979e-05 | gnorm 0.899 | train_wall 259 | wall 850
2021-01-02 12:07:00 | INFO | fairseq.trainer | begin training epoch 4
2021-01-02 12:07:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:07:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:07:25 | INFO | train_inner | epoch 004:     37 / 421 symm_mse=0.405, loss=3.263, nll_loss=0.884, ppl=1.85, wps=16200.2, ups=1.17, wpb=13896.1, bsz=467.7, num_updates=1300, lr=1.739e-05, gnorm=0.894, train_wall=61, wall=876
2021-01-02 12:08:27 | INFO | train_inner | epoch 004:    137 / 421 symm_mse=0.398, loss=3.251, nll_loss=0.881, ppl=1.84, wps=22309.4, ups=1.62, wpb=13735.4, bsz=496.9, num_updates=1400, lr=1.872e-05, gnorm=0.881, train_wall=61, wall=937
2021-01-02 12:09:28 | INFO | train_inner | epoch 004:    237 / 421 symm_mse=0.394, loss=3.248, nll_loss=0.885, ppl=1.85, wps=22835.2, ups=1.62, wpb=14090.6, bsz=475.5, num_updates=1500, lr=2.005e-05, gnorm=0.859, train_wall=62, wall=999
2021-01-02 12:10:30 | INFO | train_inner | epoch 004:    337 / 421 symm_mse=0.386, loss=3.236, nll_loss=0.885, ppl=1.85, wps=22824.1, ups=1.62, wpb=14057, bsz=499.7, num_updates=1600, lr=2.138e-05, gnorm=0.843, train_wall=61, wall=1061
2021-01-02 12:11:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:11:40 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_mse 0 | loss 5.54 | nll_loss 3.978 | ppl 15.76 | bleu 22.55 | wps 5481.2 | wpb 10324.2 | bsz 375 | num_updates 1684 | best_bleu 22.55
2021-01-02 12:11:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 4 @ 1684 updates, score 22.55) (writing took 4.76730932854116 seconds)
2021-01-02 12:11:44 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-02 12:11:44 | INFO | train | epoch 004 | symm_mse 0.392 | loss 3.244 | nll_loss 0.884 | ppl 1.85 | wps 20663.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 1684 | lr 2.24972e-05 | gnorm 0.86 | train_wall 258 | wall 1135
2021-01-02 12:11:44 | INFO | fairseq.trainer | begin training epoch 5
2021-01-02 12:11:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:11:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:11:57 | INFO | train_inner | epoch 005:     16 / 421 symm_mse=0.382, loss=3.23, nll_loss=0.888, ppl=1.85, wps=16032.4, ups=1.15, wpb=13973, bsz=503, num_updates=1700, lr=2.271e-05, gnorm=0.842, train_wall=61, wall=1148
2021-01-02 12:12:59 | INFO | train_inner | epoch 005:    116 / 421 symm_mse=0.378, loss=3.218, nll_loss=0.877, ppl=1.84, wps=22733.3, ups=1.63, wpb=13947.8, bsz=499.7, num_updates=1800, lr=2.404e-05, gnorm=0.835, train_wall=61, wall=1209
2021-01-02 12:14:00 | INFO | train_inner | epoch 005:    216 / 421 symm_mse=0.378, loss=3.227, nll_loss=0.888, ppl=1.85, wps=22804.9, ups=1.62, wpb=14054.4, bsz=484.6, num_updates=1900, lr=2.537e-05, gnorm=0.83, train_wall=61, wall=1271
2021-01-02 12:15:02 | INFO | train_inner | epoch 005:    316 / 421 symm_mse=0.376, loss=3.226, nll_loss=0.893, ppl=1.86, wps=22591.4, ups=1.62, wpb=13937.1, bsz=501.2, num_updates=2000, lr=2.67e-05, gnorm=0.828, train_wall=62, wall=1333
2021-01-02 12:16:04 | INFO | train_inner | epoch 005:    416 / 421 symm_mse=0.369, loss=3.206, nll_loss=0.879, ppl=1.84, wps=22718, ups=1.62, wpb=14006.7, bsz=492.6, num_updates=2100, lr=2.803e-05, gnorm=0.819, train_wall=61, wall=1394
2021-01-02 12:16:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:16:23 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_mse 0 | loss 5.519 | nll_loss 3.959 | ppl 15.55 | bleu 22.45 | wps 6030.1 | wpb 10324.2 | bsz 375 | num_updates 2105 | best_bleu 22.55
2021-01-02 12:16:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:16:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:16:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 5 @ 2105 updates, score 22.45) (writing took 2.905731564387679 seconds)
2021-01-02 12:16:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-02 12:16:26 | INFO | train | epoch 005 | symm_mse 0.376 | loss 3.221 | nll_loss 0.885 | ppl 1.85 | wps 20871.8 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 2105 | lr 2.80965e-05 | gnorm 0.83 | train_wall 258 | wall 1417
2021-01-02 12:16:26 | INFO | fairseq.trainer | begin training epoch 6
2021-01-02 12:16:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:16:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:17:27 | INFO | train_inner | epoch 006:     95 / 421 symm_mse=0.366, loss=3.203, nll_loss=0.881, ppl=1.84, wps=16474.2, ups=1.19, wpb=13827.8, bsz=506.6, num_updates=2200, lr=2.936e-05, gnorm=0.821, train_wall=61, wall=1478
2021-01-02 12:18:29 | INFO | train_inner | epoch 006:    195 / 421 symm_mse=0.365, loss=3.206, nll_loss=0.886, ppl=1.85, wps=22751.3, ups=1.61, wpb=14087.8, bsz=489.6, num_updates=2300, lr=3.069e-05, gnorm=0.806, train_wall=62, wall=1540
2021-01-02 12:19:31 | INFO | train_inner | epoch 006:    295 / 421 symm_mse=0.365, loss=3.213, nll_loss=0.895, ppl=1.86, wps=22872.9, ups=1.63, wpb=14036.1, bsz=477.8, num_updates=2400, lr=3.202e-05, gnorm=0.809, train_wall=61, wall=1601
2021-01-02 12:20:33 | INFO | train_inner | epoch 006:    395 / 421 symm_mse=0.355, loss=3.189, nll_loss=0.885, ppl=1.85, wps=22613.9, ups=1.62, wpb=13994.5, bsz=506.2, num_updates=2500, lr=3.335e-05, gnorm=0.796, train_wall=62, wall=1663
2021-01-02 12:20:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:21:05 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_mse 0 | loss 5.494 | nll_loss 3.931 | ppl 15.25 | bleu 22.55 | wps 6058.9 | wpb 10324.2 | bsz 375 | num_updates 2526 | best_bleu 22.55
2021-01-02 12:21:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:21:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:21:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:21:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:21:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 6 @ 2526 updates, score 22.55) (writing took 4.784132471308112 seconds)
2021-01-02 12:21:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-02 12:21:10 | INFO | train | epoch 006 | symm_mse 0.363 | loss 3.204 | nll_loss 0.888 | ppl 1.85 | wps 20722.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 2526 | lr 3.36958e-05 | gnorm 0.808 | train_wall 258 | wall 1700
2021-01-02 12:21:10 | INFO | fairseq.trainer | begin training epoch 7
2021-01-02 12:21:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:21:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:21:58 | INFO | train_inner | epoch 007:     74 / 421 symm_mse=0.358, loss=3.198, nll_loss=0.891, ppl=1.85, wps=16054.8, ups=1.17, wpb=13772.9, bsz=486.4, num_updates=2600, lr=3.468e-05, gnorm=0.805, train_wall=61, wall=1749
2021-01-02 12:23:00 | INFO | train_inner | epoch 007:    174 / 421 symm_mse=0.356, loss=3.196, nll_loss=0.892, ppl=1.86, wps=22651.2, ups=1.63, wpb=13910.1, bsz=482.8, num_updates=2700, lr=3.601e-05, gnorm=0.798, train_wall=61, wall=1810
2021-01-02 12:24:01 | INFO | train_inner | epoch 007:    274 / 421 symm_mse=0.353, loss=3.191, nll_loss=0.889, ppl=1.85, wps=22580, ups=1.63, wpb=13879.3, bsz=492.2, num_updates=2800, lr=3.734e-05, gnorm=0.793, train_wall=61, wall=1872
2021-01-02 12:25:03 | INFO | train_inner | epoch 007:    374 / 421 symm_mse=0.349, loss=3.186, nll_loss=0.891, ppl=1.86, wps=22867.1, ups=1.62, wpb=14151.4, bsz=491.7, num_updates=2900, lr=3.867e-05, gnorm=0.787, train_wall=62, wall=1934
2021-01-02 12:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:25:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:25:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:25:49 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_mse 0 | loss 5.481 | nll_loss 3.921 | ppl 15.14 | bleu 22.6 | wps 5869.2 | wpb 10324.2 | bsz 375 | num_updates 2947 | best_bleu 22.6
2021-01-02 12:25:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:25:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:25:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 7 @ 2947 updates, score 22.6) (writing took 4.753893988206983 seconds)
2021-01-02 12:25:54 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-02 12:25:54 | INFO | train | epoch 007 | symm_mse 0.352 | loss 3.19 | nll_loss 0.89 | ppl 1.85 | wps 20699.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 2947 | lr 3.92951e-05 | gnorm 0.792 | train_wall 258 | wall 1985
2021-01-02 12:25:54 | INFO | fairseq.trainer | begin training epoch 8
2021-01-02 12:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:26:30 | INFO | train_inner | epoch 008:     53 / 421 symm_mse=0.344, loss=3.176, nll_loss=0.889, ppl=1.85, wps=16325.9, ups=1.16, wpb=14109.9, bsz=506.2, num_updates=3000, lr=4e-05, gnorm=0.779, train_wall=61, wall=2020
2021-01-02 12:27:31 | INFO | train_inner | epoch 008:    153 / 421 symm_mse=0.348, loss=3.181, nll_loss=0.886, ppl=1.85, wps=22661.8, ups=1.62, wpb=13958.9, bsz=475.6, num_updates=3100, lr=3.93496e-05, gnorm=0.786, train_wall=61, wall=2082
2021-01-02 12:28:33 | INFO | train_inner | epoch 008:    253 / 421 symm_mse=0.343, loss=3.176, nll_loss=0.891, ppl=1.85, wps=22472.6, ups=1.62, wpb=13843, bsz=502.1, num_updates=3200, lr=3.87298e-05, gnorm=0.777, train_wall=61, wall=2143
2021-01-02 12:29:35 | INFO | train_inner | epoch 008:    353 / 421 symm_mse=0.337, loss=3.163, nll_loss=0.888, ppl=1.85, wps=22827.6, ups=1.61, wpb=14141.4, bsz=514.8, num_updates=3300, lr=3.81385e-05, gnorm=0.761, train_wall=62, wall=2205
2021-01-02 12:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:30:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:30:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:30:35 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_mse 0 | loss 5.472 | nll_loss 3.91 | ppl 15.04 | bleu 22.71 | wps 5378.7 | wpb 10324.2 | bsz 375 | num_updates 3368 | best_bleu 22.71
2021-01-02 12:30:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:30:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 8 @ 3368 updates, score 22.71) (writing took 4.751354970037937 seconds)
2021-01-02 12:30:39 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-02 12:30:39 | INFO | train | epoch 008 | symm_mse 0.343 | loss 3.176 | nll_loss 0.891 | ppl 1.85 | wps 20615.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 3368 | lr 3.77515e-05 | gnorm 0.776 | train_wall 258 | wall 2270
2021-01-02 12:30:39 | INFO | fairseq.trainer | begin training epoch 9
2021-01-02 12:30:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:30:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:31:02 | INFO | train_inner | epoch 009:     32 / 421 symm_mse=0.344, loss=3.187, nll_loss=0.901, ppl=1.87, wps=15818.6, ups=1.15, wpb=13812.4, bsz=478.6, num_updates=3400, lr=3.75735e-05, gnorm=0.783, train_wall=61, wall=2293
2021-01-02 12:32:03 | INFO | train_inner | epoch 009:    132 / 421 symm_mse=0.339, loss=3.167, nll_loss=0.885, ppl=1.85, wps=22779.7, ups=1.63, wpb=13981.2, bsz=478.7, num_updates=3500, lr=3.70328e-05, gnorm=0.77, train_wall=61, wall=2354
2021-01-02 12:33:05 | INFO | train_inner | epoch 009:    232 / 421 symm_mse=0.333, loss=3.156, nll_loss=0.885, ppl=1.85, wps=22779.4, ups=1.62, wpb=14040.5, bsz=491.4, num_updates=3600, lr=3.65148e-05, gnorm=0.756, train_wall=61, wall=2416
2021-01-02 12:34:07 | INFO | train_inner | epoch 009:    332 / 421 symm_mse=0.333, loss=3.161, nll_loss=0.891, ppl=1.85, wps=22594.9, ups=1.61, wpb=14010.8, bsz=506.9, num_updates=3700, lr=3.6018e-05, gnorm=0.755, train_wall=62, wall=2478
2021-01-02 12:35:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:35:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:35:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:35:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:35:19 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_mse 0 | loss 5.464 | nll_loss 3.902 | ppl 14.95 | bleu 22.68 | wps 6062 | wpb 10324.2 | bsz 375 | num_updates 3789 | best_bleu 22.71
2021-01-02 12:35:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:35:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 9 @ 3789 updates, score 22.68) (writing took 2.8845180720090866 seconds)
2021-01-02 12:35:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-02 12:35:22 | INFO | train | epoch 009 | symm_mse 0.335 | loss 3.164 | nll_loss 0.889 | ppl 1.85 | wps 20836.7 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 3789 | lr 3.55925e-05 | gnorm 0.763 | train_wall 259 | wall 2552
2021-01-02 12:35:22 | INFO | fairseq.trainer | begin training epoch 10
2021-01-02 12:35:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:35:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:35:32 | INFO | train_inner | epoch 010:     11 / 421 symm_mse=0.334, loss=3.17, nll_loss=0.898, ppl=1.86, wps=16312.4, ups=1.19, wpb=13765.2, bsz=485.4, num_updates=3800, lr=3.55409e-05, gnorm=0.769, train_wall=62, wall=2562
2021-01-02 12:36:33 | INFO | train_inner | epoch 010:    111 / 421 symm_mse=0.331, loss=3.155, nll_loss=0.887, ppl=1.85, wps=22906.7, ups=1.62, wpb=14110.2, bsz=485.5, num_updates=3900, lr=3.50823e-05, gnorm=0.749, train_wall=61, wall=2624
2021-01-02 12:37:35 | INFO | train_inner | epoch 010:    211 / 421 symm_mse=0.334, loss=3.165, nll_loss=0.891, ppl=1.85, wps=22503.1, ups=1.61, wpb=13947.9, bsz=478.8, num_updates=4000, lr=3.4641e-05, gnorm=0.758, train_wall=62, wall=2686
2021-01-02 12:38:37 | INFO | train_inner | epoch 010:    311 / 421 symm_mse=0.323, loss=3.135, nll_loss=0.878, ppl=1.84, wps=22637.9, ups=1.61, wpb=14032.4, bsz=515.1, num_updates=4100, lr=3.4216e-05, gnorm=0.738, train_wall=62, wall=2748
2021-01-02 12:39:39 | INFO | train_inner | epoch 010:    411 / 421 symm_mse=0.328, loss=3.155, nll_loss=0.894, ppl=1.86, wps=22415.7, ups=1.61, wpb=13937.4, bsz=501.3, num_updates=4200, lr=3.38062e-05, gnorm=0.747, train_wall=62, wall=2810
2021-01-02 12:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:40:02 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_mse 0 | loss 5.451 | nll_loss 3.89 | ppl 14.83 | bleu 22.58 | wps 5972.5 | wpb 10324.2 | bsz 375 | num_updates 4210 | best_bleu 22.71
2021-01-02 12:40:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:40:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:40:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:40:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:40:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:40:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:40:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:40:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 10 @ 4210 updates, score 22.58) (writing took 2.8788223639130592 seconds)
2021-01-02 12:40:05 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-02 12:40:05 | INFO | train | epoch 010 | symm_mse 0.329 | loss 3.153 | nll_loss 0.888 | ppl 1.85 | wps 20745.4 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 4210 | lr 3.3766e-05 | gnorm 0.75 | train_wall 260 | wall 2836
2021-01-02 12:40:05 | INFO | fairseq.trainer | begin training epoch 11
2021-01-02 12:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:41:03 | INFO | train_inner | epoch 011:     90 / 421 symm_mse=0.328, loss=3.146, nll_loss=0.881, ppl=1.84, wps=16452.3, ups=1.19, wpb=13795, bsz=468.2, num_updates=4300, lr=3.34108e-05, gnorm=0.758, train_wall=61, wall=2894
2021-01-02 12:42:05 | INFO | train_inner | epoch 011:    190 / 421 symm_mse=0.321, loss=3.131, nll_loss=0.878, ppl=1.84, wps=22797.1, ups=1.62, wpb=14072.6, bsz=503.6, num_updates=4400, lr=3.30289e-05, gnorm=0.735, train_wall=62, wall=2955
2021-01-02 12:43:06 | INFO | train_inner | epoch 011:    290 / 421 symm_mse=0.33, loss=3.17, nll_loss=0.904, ppl=1.87, wps=22425, ups=1.62, wpb=13821.1, bsz=488.1, num_updates=4500, lr=3.26599e-05, gnorm=0.752, train_wall=61, wall=3017
2021-01-02 12:44:08 | INFO | train_inner | epoch 011:    390 / 421 symm_mse=0.321, loss=3.136, nll_loss=0.882, ppl=1.84, wps=22907.3, ups=1.62, wpb=14171.3, bsz=501.5, num_updates=4600, lr=3.23029e-05, gnorm=0.732, train_wall=62, wall=3079
2021-01-02 12:44:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:44:44 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_mse 0 | loss 5.451 | nll_loss 3.89 | ppl 14.82 | bleu 22.58 | wps 6049.3 | wpb 10324.2 | bsz 375 | num_updates 4631 | best_bleu 22.71
2021-01-02 12:44:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:44:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:44:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 11 @ 4631 updates, score 22.58) (writing took 2.8781096655875444 seconds)
2021-01-02 12:44:47 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-02 12:44:47 | INFO | train | epoch 011 | symm_mse 0.324 | loss 3.144 | nll_loss 0.886 | ppl 1.85 | wps 20867.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 4631 | lr 3.21946e-05 | gnorm 0.743 | train_wall 258 | wall 3117
2021-01-02 12:44:47 | INFO | fairseq.trainer | begin training epoch 12
2021-01-02 12:44:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:44:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:45:33 | INFO | train_inner | epoch 012:     69 / 421 symm_mse=0.32, loss=3.131, nll_loss=0.879, ppl=1.84, wps=16464.3, ups=1.19, wpb=13876.4, bsz=488.4, num_updates=4700, lr=3.19574e-05, gnorm=0.736, train_wall=61, wall=3163
2021-01-02 12:46:34 | INFO | train_inner | epoch 012:    169 / 421 symm_mse=0.322, loss=3.141, nll_loss=0.885, ppl=1.85, wps=22582.3, ups=1.63, wpb=13884.5, bsz=497.5, num_updates=4800, lr=3.16228e-05, gnorm=0.736, train_wall=61, wall=3225
2021-01-02 12:47:37 | INFO | train_inner | epoch 012:    269 / 421 symm_mse=0.316, loss=3.122, nll_loss=0.875, ppl=1.83, wps=22761.5, ups=1.6, wpb=14222.4, bsz=501.7, num_updates=4900, lr=3.12984e-05, gnorm=0.722, train_wall=62, wall=3287
2021-01-02 12:48:38 | INFO | train_inner | epoch 012:    369 / 421 symm_mse=0.322, loss=3.148, nll_loss=0.895, ppl=1.86, wps=22566.9, ups=1.62, wpb=13899.9, bsz=487.4, num_updates=5000, lr=3.09839e-05, gnorm=0.737, train_wall=61, wall=3349
2021-01-02 12:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:49:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:49:28 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_mse 0 | loss 5.442 | nll_loss 3.88 | ppl 14.72 | bleu 22.77 | wps 5625.4 | wpb 10324.2 | bsz 375 | num_updates 5052 | best_bleu 22.77
2021-01-02 12:49:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:49:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:49:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 12 @ 5052 updates, score 22.77) (writing took 4.75920489244163 seconds)
2021-01-02 12:49:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-02 12:49:32 | INFO | train | epoch 012 | symm_mse 0.32 | loss 3.136 | nll_loss 0.884 | ppl 1.85 | wps 20605.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 5052 | lr 3.0824e-05 | gnorm 0.733 | train_wall 259 | wall 3403
2021-01-02 12:49:32 | INFO | fairseq.trainer | begin training epoch 13
2021-01-02 12:49:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:49:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:50:05 | INFO | train_inner | epoch 013:     48 / 421 symm_mse=0.319, loss=3.132, nll_loss=0.88, ppl=1.84, wps=16079.2, ups=1.16, wpb=13889.5, bsz=482.8, num_updates=5100, lr=3.06786e-05, gnorm=0.741, train_wall=61, wall=3435
2021-01-02 12:51:06 | INFO | train_inner | epoch 013:    148 / 421 symm_mse=0.318, loss=3.131, nll_loss=0.88, ppl=1.84, wps=22865, ups=1.62, wpb=14105.2, bsz=473, num_updates=5200, lr=3.03822e-05, gnorm=0.726, train_wall=62, wall=3497
2021-01-02 12:52:08 | INFO | train_inner | epoch 013:    248 / 421 symm_mse=0.315, loss=3.125, nll_loss=0.879, ppl=1.84, wps=22537.8, ups=1.62, wpb=13944.9, bsz=504.8, num_updates=5300, lr=3.00942e-05, gnorm=0.728, train_wall=62, wall=3559
2021-01-02 12:53:10 | INFO | train_inner | epoch 013:    348 / 421 symm_mse=0.313, loss=3.124, nll_loss=0.881, ppl=1.84, wps=22905.8, ups=1.63, wpb=14073.3, bsz=500.6, num_updates=5400, lr=2.98142e-05, gnorm=0.718, train_wall=61, wall=3620
2021-01-02 12:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:54:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:54:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:54:11 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_mse 0 | loss 5.433 | nll_loss 3.873 | ppl 14.65 | bleu 22.72 | wps 6197.6 | wpb 10324.2 | bsz 375 | num_updates 5473 | best_bleu 22.77
2021-01-02 12:54:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:54:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:54:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:54:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:54:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:54:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:54:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:54:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 13 @ 5473 updates, score 22.72) (writing took 2.9063167311251163 seconds)
2021-01-02 12:54:14 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-02 12:54:14 | INFO | train | epoch 013 | symm_mse 0.316 | loss 3.129 | nll_loss 0.882 | ppl 1.84 | wps 20887.3 | ups 1.5 | wpb 13969.5 | bsz 492.6 | num_updates 5473 | lr 2.96147e-05 | gnorm 0.728 | train_wall 258 | wall 3684
2021-01-02 12:54:14 | INFO | fairseq.trainer | begin training epoch 14
2021-01-02 12:54:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:54:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:54:34 | INFO | train_inner | epoch 014:     27 / 421 symm_mse=0.316, loss=3.133, nll_loss=0.887, ppl=1.85, wps=16440.2, ups=1.19, wpb=13804.5, bsz=491.3, num_updates=5500, lr=2.9542e-05, gnorm=0.736, train_wall=61, wall=3704
2021-01-02 12:55:35 | INFO | train_inner | epoch 014:    127 / 421 symm_mse=0.314, loss=3.12, nll_loss=0.875, ppl=1.83, wps=22830.2, ups=1.61, wpb=14140.2, bsz=491.6, num_updates=5600, lr=2.9277e-05, gnorm=0.719, train_wall=62, wall=3766
2021-01-02 12:56:38 | INFO | train_inner | epoch 014:    227 / 421 symm_mse=0.313, loss=3.119, nll_loss=0.878, ppl=1.84, wps=22368.6, ups=1.61, wpb=13893.4, bsz=488.5, num_updates=5700, lr=2.90191e-05, gnorm=0.723, train_wall=62, wall=3828
2021-01-02 12:57:39 | INFO | train_inner | epoch 014:    327 / 421 symm_mse=0.317, loss=3.136, nll_loss=0.889, ppl=1.85, wps=22459.1, ups=1.62, wpb=13861.2, bsz=485.7, num_updates=5800, lr=2.87678e-05, gnorm=0.733, train_wall=62, wall=3890
2021-01-02 12:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 12:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 12:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 12:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 12:58:53 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_mse 0 | loss 5.435 | nll_loss 3.872 | ppl 14.65 | bleu 22.89 | wps 6201.9 | wpb 10324.2 | bsz 375 | num_updates 5894 | best_bleu 22.89
2021-01-02 12:58:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 12:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:58:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 14 @ 5894 updates, score 22.89) (writing took 4.771763775497675 seconds)
2021-01-02 12:58:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-02 12:58:58 | INFO | train | epoch 014 | symm_mse 0.313 | loss 3.122 | nll_loss 0.88 | ppl 1.84 | wps 20684.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 5894 | lr 2.85375e-05 | gnorm 0.724 | train_wall 259 | wall 3969
2021-01-02 12:58:58 | INFO | fairseq.trainer | begin training epoch 15
2021-01-02 12:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 12:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 12:59:05 | INFO | train_inner | epoch 015:      6 / 421 symm_mse=0.31, loss=3.118, nll_loss=0.882, ppl=1.84, wps=16269.4, ups=1.17, wpb=13964.8, bsz=507.2, num_updates=5900, lr=2.8523e-05, gnorm=0.723, train_wall=61, wall=3976
2021-01-02 13:00:06 | INFO | train_inner | epoch 015:    106 / 421 symm_mse=0.313, loss=3.125, nll_loss=0.882, ppl=1.84, wps=22731, ups=1.63, wpb=13909.8, bsz=489.7, num_updates=6000, lr=2.82843e-05, gnorm=0.721, train_wall=61, wall=4037
2021-01-02 13:01:08 | INFO | train_inner | epoch 015:    206 / 421 symm_mse=0.306, loss=3.099, nll_loss=0.865, ppl=1.82, wps=22663.5, ups=1.62, wpb=14025.9, bsz=508.1, num_updates=6100, lr=2.80515e-05, gnorm=0.716, train_wall=62, wall=4099
2021-01-02 13:02:10 | INFO | train_inner | epoch 015:    306 / 421 symm_mse=0.31, loss=3.12, nll_loss=0.882, ppl=1.84, wps=22654.6, ups=1.62, wpb=13965.9, bsz=486.5, num_updates=6200, lr=2.78243e-05, gnorm=0.716, train_wall=61, wall=4160
2021-01-02 13:03:12 | INFO | train_inner | epoch 015:    406 / 421 symm_mse=0.312, loss=3.13, nll_loss=0.891, ppl=1.85, wps=22566.7, ups=1.61, wpb=14000.5, bsz=488.7, num_updates=6300, lr=2.76026e-05, gnorm=0.727, train_wall=62, wall=4223
2021-01-02 13:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:03:38 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_mse 0 | loss 5.439 | nll_loss 3.875 | ppl 14.67 | bleu 22.79 | wps 6076.6 | wpb 10324.2 | bsz 375 | num_updates 6315 | best_bleu 22.89
2021-01-02 13:03:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:03:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:03:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 15 @ 6315 updates, score 22.79) (writing took 2.8836941476911306 seconds)
2021-01-02 13:03:40 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-02 13:03:40 | INFO | train | epoch 015 | symm_mse 0.31 | loss 3.116 | nll_loss 0.878 | ppl 1.84 | wps 20838 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 6315 | lr 2.75698e-05 | gnorm 0.72 | train_wall 259 | wall 4251
2021-01-02 13:03:40 | INFO | fairseq.trainer | begin training epoch 16
2021-01-02 13:03:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:03:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:04:36 | INFO | train_inner | epoch 016:     85 / 421 symm_mse=0.306, loss=3.102, nll_loss=0.869, ppl=1.83, wps=16577.7, ups=1.19, wpb=13920.5, bsz=499.7, num_updates=6400, lr=2.73861e-05, gnorm=0.71, train_wall=61, wall=4306
2021-01-02 13:05:38 | INFO | train_inner | epoch 016:    185 / 421 symm_mse=0.309, loss=3.115, nll_loss=0.878, ppl=1.84, wps=22551.6, ups=1.62, wpb=13953.5, bsz=496.1, num_updates=6500, lr=2.71746e-05, gnorm=0.719, train_wall=62, wall=4368
2021-01-02 13:06:40 | INFO | train_inner | epoch 016:    285 / 421 symm_mse=0.303, loss=3.098, nll_loss=0.869, ppl=1.83, wps=22759.8, ups=1.62, wpb=14059.6, bsz=504.2, num_updates=6600, lr=2.6968e-05, gnorm=0.707, train_wall=62, wall=4430
2021-01-02 13:07:41 | INFO | train_inner | epoch 016:    385 / 421 symm_mse=0.309, loss=3.119, nll_loss=0.882, ppl=1.84, wps=22629.5, ups=1.62, wpb=13933.8, bsz=478.3, num_updates=6700, lr=2.6766e-05, gnorm=0.718, train_wall=61, wall=4492
2021-01-02 13:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:08:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:08:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:08:20 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_mse 0 | loss 5.429 | nll_loss 3.867 | ppl 14.6 | bleu 22.73 | wps 6119.2 | wpb 10324.2 | bsz 375 | num_updates 6736 | best_bleu 22.89
2021-01-02 13:08:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:08:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 16 @ 6736 updates, score 22.73) (writing took 2.8481393475085497 seconds)
2021-01-02 13:08:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-02 13:08:23 | INFO | train | epoch 016 | symm_mse 0.308 | loss 3.112 | nll_loss 0.877 | ppl 1.84 | wps 20841.5 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 6736 | lr 2.66944e-05 | gnorm 0.715 | train_wall 259 | wall 4533
2021-01-02 13:08:23 | INFO | fairseq.trainer | begin training epoch 17
2021-01-02 13:08:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:08:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:09:05 | INFO | train_inner | epoch 017:     64 / 421 symm_mse=0.308, loss=3.118, nll_loss=0.883, ppl=1.84, wps=16549.7, ups=1.19, wpb=13884.8, bsz=477.7, num_updates=6800, lr=2.65684e-05, gnorm=0.721, train_wall=61, wall=4576
2021-01-02 13:10:07 | INFO | train_inner | epoch 017:    164 / 421 symm_mse=0.299, loss=3.078, nll_loss=0.854, ppl=1.81, wps=22733, ups=1.61, wpb=14155.6, bsz=509.1, num_updates=6900, lr=2.63752e-05, gnorm=0.697, train_wall=62, wall=4638
2021-01-02 13:11:09 | INFO | train_inner | epoch 017:    264 / 421 symm_mse=0.304, loss=3.1, nll_loss=0.869, ppl=1.83, wps=22825.2, ups=1.63, wpb=14019.6, bsz=491.8, num_updates=7000, lr=2.61861e-05, gnorm=0.705, train_wall=61, wall=4699
2021-01-02 13:12:10 | INFO | train_inner | epoch 017:    364 / 421 symm_mse=0.308, loss=3.123, nll_loss=0.888, ppl=1.85, wps=22627, ups=1.63, wpb=13911.3, bsz=494.4, num_updates=7100, lr=2.60011e-05, gnorm=0.716, train_wall=61, wall=4761
2021-01-02 13:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:12:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:12:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:12:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:12:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:12:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:12:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:12:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:12:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:12:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:12:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:12:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:12:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:13:02 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_mse 0 | loss 5.428 | nll_loss 3.867 | ppl 14.59 | bleu 22.69 | wps 6086.8 | wpb 10324.2 | bsz 375 | num_updates 7157 | best_bleu 22.89
2021-01-02 13:13:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:13:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:13:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:13:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:13:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 17 @ 7157 updates, score 22.69) (writing took 2.876745492219925 seconds)
2021-01-02 13:13:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-02 13:13:05 | INFO | train | epoch 017 | symm_mse 0.305 | loss 3.107 | nll_loss 0.875 | ppl 1.83 | wps 20851.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 7157 | lr 2.58973e-05 | gnorm 0.71 | train_wall 259 | wall 4815
2021-01-02 13:13:05 | INFO | fairseq.trainer | begin training epoch 18
2021-01-02 13:13:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:13:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:13:34 | INFO | train_inner | epoch 018:     43 / 421 symm_mse=0.308, loss=3.117, nll_loss=0.882, ppl=1.84, wps=16524.2, ups=1.19, wpb=13876, bsz=478.1, num_updates=7200, lr=2.58199e-05, gnorm=0.72, train_wall=61, wall=4845
2021-01-02 13:14:36 | INFO | train_inner | epoch 018:    143 / 421 symm_mse=0.303, loss=3.098, nll_loss=0.869, ppl=1.83, wps=22769.4, ups=1.62, wpb=14024.4, bsz=489.7, num_updates=7300, lr=2.56424e-05, gnorm=0.709, train_wall=61, wall=4906
2021-01-02 13:15:37 | INFO | train_inner | epoch 018:    243 / 421 symm_mse=0.304, loss=3.103, nll_loss=0.873, ppl=1.83, wps=22443.9, ups=1.62, wpb=13842.7, bsz=494.3, num_updates=7400, lr=2.54686e-05, gnorm=0.711, train_wall=61, wall=4968
2021-01-02 13:16:39 | INFO | train_inner | epoch 018:    343 / 421 symm_mse=0.301, loss=3.101, nll_loss=0.875, ppl=1.83, wps=22765.6, ups=1.62, wpb=14074.6, bsz=501.3, num_updates=7500, lr=2.52982e-05, gnorm=0.699, train_wall=62, wall=5030
2021-01-02 13:17:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:17:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:17:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:17:44 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_mse 0 | loss 5.423 | nll_loss 3.862 | ppl 14.54 | bleu 22.69 | wps 6089.9 | wpb 10324.2 | bsz 375 | num_updates 7578 | best_bleu 22.89
2021-01-02 13:17:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:17:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:17:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 18 @ 7578 updates, score 22.69) (writing took 2.9551281984895468 seconds)
2021-01-02 13:17:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-02 13:17:47 | INFO | train | epoch 018 | symm_mse 0.303 | loss 3.103 | nll_loss 0.874 | ppl 1.83 | wps 20864.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 7578 | lr 2.51677e-05 | gnorm 0.71 | train_wall 258 | wall 5097
2021-01-02 13:17:47 | INFO | fairseq.trainer | begin training epoch 19
2021-01-02 13:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:17:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:18:03 | INFO | train_inner | epoch 019:     22 / 421 symm_mse=0.304, loss=3.11, nll_loss=0.882, ppl=1.84, wps=16452, ups=1.19, wpb=13829.2, bsz=486.7, num_updates=7600, lr=2.51312e-05, gnorm=0.716, train_wall=61, wall=5114
2021-01-02 13:19:05 | INFO | train_inner | epoch 019:    122 / 421 symm_mse=0.299, loss=3.092, nll_loss=0.868, ppl=1.82, wps=22717.3, ups=1.61, wpb=14075, bsz=504.1, num_updates=7700, lr=2.49675e-05, gnorm=0.7, train_wall=62, wall=5176
2021-01-02 13:20:07 | INFO | train_inner | epoch 019:    222 / 421 symm_mse=0.301, loss=3.095, nll_loss=0.869, ppl=1.83, wps=22661.2, ups=1.62, wpb=14017.6, bsz=488.3, num_updates=7800, lr=2.48069e-05, gnorm=0.701, train_wall=62, wall=5238
2021-01-02 13:21:09 | INFO | train_inner | epoch 019:    322 / 421 symm_mse=0.305, loss=3.109, nll_loss=0.879, ppl=1.84, wps=22567.2, ups=1.62, wpb=13925.5, bsz=485.6, num_updates=7900, lr=2.46494e-05, gnorm=0.708, train_wall=62, wall=5299
2021-01-02 13:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:22:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:22:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:22:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:22:26 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_mse 0 | loss 5.426 | nll_loss 3.865 | ppl 14.57 | bleu 22.77 | wps 6181.7 | wpb 10324.2 | bsz 375 | num_updates 7999 | best_bleu 22.89
2021-01-02 13:22:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 19 @ 7999 updates, score 22.77) (writing took 2.9754859171807766 seconds)
2021-01-02 13:22:29 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-02 13:22:29 | INFO | train | epoch 019 | symm_mse 0.301 | loss 3.098 | nll_loss 0.872 | ppl 1.83 | wps 20822.1 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 7999 | lr 2.44964e-05 | gnorm 0.707 | train_wall 259 | wall 5380
2021-01-02 13:22:29 | INFO | fairseq.trainer | begin training epoch 20
2021-01-02 13:22:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:22:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:22:33 | INFO | train_inner | epoch 020:      1 / 421 symm_mse=0.299, loss=3.096, nll_loss=0.874, ppl=1.83, wps=16589, ups=1.19, wpb=13936.5, bsz=492.6, num_updates=8000, lr=2.44949e-05, gnorm=0.717, train_wall=61, wall=5383
2021-01-02 13:23:34 | INFO | train_inner | epoch 020:    101 / 421 symm_mse=0.3, loss=3.095, nll_loss=0.87, ppl=1.83, wps=22915.1, ups=1.63, wpb=14022.1, bsz=493.9, num_updates=8100, lr=2.43432e-05, gnorm=0.699, train_wall=61, wall=5445
2021-01-02 13:24:36 | INFO | train_inner | epoch 020:    201 / 421 symm_mse=0.299, loss=3.089, nll_loss=0.866, ppl=1.82, wps=22626.5, ups=1.62, wpb=13989.7, bsz=488, num_updates=8200, lr=2.41943e-05, gnorm=0.704, train_wall=62, wall=5506
2021-01-02 13:25:38 | INFO | train_inner | epoch 020:    301 / 421 symm_mse=0.301, loss=3.1, nll_loss=0.876, ppl=1.83, wps=22755.5, ups=1.61, wpb=14118.4, bsz=497.3, num_updates=8300, lr=2.40481e-05, gnorm=0.697, train_wall=62, wall=5569
2021-01-02 13:26:39 | INFO | train_inner | epoch 020:    401 / 421 symm_mse=0.3, loss=3.099, nll_loss=0.875, ppl=1.83, wps=22414.8, ups=1.63, wpb=13791.4, bsz=493.2, num_updates=8400, lr=2.39046e-05, gnorm=0.708, train_wall=61, wall=5630
2021-01-02 13:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:26:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:26:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:26:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:26:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:26:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:26:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:26:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:26:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:27:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:27:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:27:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:27:08 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_mse 0 | loss 5.423 | nll_loss 3.863 | ppl 14.55 | bleu 22.84 | wps 6170.6 | wpb 10324.2 | bsz 375 | num_updates 8420 | best_bleu 22.89
2021-01-02 13:27:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:27:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:27:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:27:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:27:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:27:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:27:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:27:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 20 @ 8420 updates, score 22.84) (writing took 2.9076933301985264 seconds)
2021-01-02 13:27:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-02 13:27:11 | INFO | train | epoch 020 | symm_mse 0.3 | loss 3.095 | nll_loss 0.871 | ppl 1.83 | wps 20850.5 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 8420 | lr 2.38762e-05 | gnorm 0.703 | train_wall 259 | wall 5662
2021-01-02 13:27:11 | INFO | fairseq.trainer | begin training epoch 21
2021-01-02 13:27:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:27:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:28:04 | INFO | train_inner | epoch 021:     80 / 421 symm_mse=0.3, loss=3.098, nll_loss=0.874, ppl=1.83, wps=16285.3, ups=1.19, wpb=13696.4, bsz=490.3, num_updates=8500, lr=2.37635e-05, gnorm=0.714, train_wall=61, wall=5714
2021-01-02 13:29:05 | INFO | train_inner | epoch 021:    180 / 421 symm_mse=0.298, loss=3.089, nll_loss=0.866, ppl=1.82, wps=22760.1, ups=1.62, wpb=14078.8, bsz=487.9, num_updates=8600, lr=2.3625e-05, gnorm=0.698, train_wall=62, wall=5776
2021-01-02 13:30:07 | INFO | train_inner | epoch 021:    280 / 421 symm_mse=0.292, loss=3.071, nll_loss=0.857, ppl=1.81, wps=23021.7, ups=1.63, wpb=14138.2, bsz=508.5, num_updates=8700, lr=2.34888e-05, gnorm=0.682, train_wall=61, wall=5837
2021-01-02 13:31:09 | INFO | train_inner | epoch 021:    380 / 421 symm_mse=0.3, loss=3.099, nll_loss=0.875, ppl=1.83, wps=22669.5, ups=1.62, wpb=14011.6, bsz=493.4, num_updates=8800, lr=2.3355e-05, gnorm=0.7, train_wall=62, wall=5899
2021-01-02 13:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:31:51 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_mse 0 | loss 5.421 | nll_loss 3.86 | ppl 14.52 | bleu 23.03 | wps 5634.7 | wpb 10324.2 | bsz 375 | num_updates 8841 | best_bleu 23.03
2021-01-02 13:31:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:31:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 21 @ 8841 updates, score 23.03) (writing took 4.736504418775439 seconds)
2021-01-02 13:31:56 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-02 13:31:56 | INFO | train | epoch 021 | symm_mse 0.298 | loss 3.091 | nll_loss 0.87 | ppl 1.83 | wps 20640.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 8841 | lr 2.33008e-05 | gnorm 0.699 | train_wall 259 | wall 5947
2021-01-02 13:31:56 | INFO | fairseq.trainer | begin training epoch 22
2021-01-02 13:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:32:35 | INFO | train_inner | epoch 022:     59 / 421 symm_mse=0.298, loss=3.095, nll_loss=0.873, ppl=1.83, wps=16078.2, ups=1.15, wpb=13937.8, bsz=479, num_updates=8900, lr=2.32234e-05, gnorm=0.699, train_wall=61, wall=5986
2021-01-02 13:33:37 | INFO | train_inner | epoch 022:    159 / 421 symm_mse=0.292, loss=3.073, nll_loss=0.859, ppl=1.81, wps=22629.5, ups=1.61, wpb=14049.6, bsz=507.4, num_updates=9000, lr=2.3094e-05, gnorm=0.686, train_wall=62, wall=6048
2021-01-02 13:34:39 | INFO | train_inner | epoch 022:    259 / 421 symm_mse=0.299, loss=3.096, nll_loss=0.874, ppl=1.83, wps=22485.7, ups=1.62, wpb=13902.5, bsz=489.7, num_updates=9100, lr=2.29668e-05, gnorm=0.702, train_wall=62, wall=6110
2021-01-02 13:35:41 | INFO | train_inner | epoch 022:    359 / 421 symm_mse=0.296, loss=3.086, nll_loss=0.867, ppl=1.82, wps=22864.1, ups=1.63, wpb=14032.6, bsz=487.8, num_updates=9200, lr=2.28416e-05, gnorm=0.693, train_wall=61, wall=6171
2021-01-02 13:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:36:35 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_mse 0 | loss 5.42 | nll_loss 3.858 | ppl 14.5 | bleu 22.9 | wps 6216.9 | wpb 10324.2 | bsz 375 | num_updates 9262 | best_bleu 23.03
2021-01-02 13:36:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:36:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 22 @ 9262 updates, score 22.9) (writing took 2.9391209054738283 seconds)
2021-01-02 13:36:38 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-02 13:36:38 | INFO | train | epoch 022 | symm_mse 0.296 | loss 3.088 | nll_loss 0.868 | ppl 1.83 | wps 20861.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 9262 | lr 2.2765e-05 | gnorm 0.695 | train_wall 259 | wall 6228
2021-01-02 13:36:38 | INFO | fairseq.trainer | begin training epoch 23
2021-01-02 13:36:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:36:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:37:04 | INFO | train_inner | epoch 023:     38 / 421 symm_mse=0.302, loss=3.107, nll_loss=0.879, ppl=1.84, wps=16542, ups=1.2, wpb=13834.9, bsz=470.7, num_updates=9300, lr=2.27185e-05, gnorm=0.709, train_wall=61, wall=6255
2021-01-02 13:38:06 | INFO | train_inner | epoch 023:    138 / 421 symm_mse=0.294, loss=3.082, nll_loss=0.865, ppl=1.82, wps=22594.6, ups=1.62, wpb=13964.3, bsz=496.2, num_updates=9400, lr=2.25973e-05, gnorm=0.69, train_wall=62, wall=6317
2021-01-02 13:39:08 | INFO | train_inner | epoch 023:    238 / 421 symm_mse=0.298, loss=3.095, nll_loss=0.875, ppl=1.83, wps=22497.3, ups=1.61, wpb=13991.9, bsz=478, num_updates=9500, lr=2.24781e-05, gnorm=0.699, train_wall=62, wall=6379
2021-01-02 13:40:10 | INFO | train_inner | epoch 023:    338 / 421 symm_mse=0.293, loss=3.079, nll_loss=0.863, ppl=1.82, wps=22575, ups=1.62, wpb=13972.2, bsz=506.8, num_updates=9600, lr=2.23607e-05, gnorm=0.686, train_wall=62, wall=6441
2021-01-02 13:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:41:18 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_mse 0 | loss 5.419 | nll_loss 3.858 | ppl 14.5 | bleu 22.77 | wps 6160 | wpb 10324.2 | bsz 375 | num_updates 9683 | best_bleu 23.03
2021-01-02 13:41:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:41:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 23 @ 9683 updates, score 22.77) (writing took 2.952138490974903 seconds)
2021-01-02 13:41:21 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-02 13:41:21 | INFO | train | epoch 023 | symm_mse 0.295 | loss 3.084 | nll_loss 0.867 | ppl 1.82 | wps 20802.7 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 9683 | lr 2.22646e-05 | gnorm 0.693 | train_wall 259 | wall 6511
2021-01-02 13:41:21 | INFO | fairseq.trainer | begin training epoch 24
2021-01-02 13:41:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:41:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:41:34 | INFO | train_inner | epoch 024:     17 / 421 symm_mse=0.291, loss=3.071, nll_loss=0.859, ppl=1.81, wps=16621, ups=1.19, wpb=13990.7, bsz=508.2, num_updates=9700, lr=2.22451e-05, gnorm=0.688, train_wall=61, wall=6525
2021-01-02 13:42:36 | INFO | train_inner | epoch 024:    117 / 421 symm_mse=0.295, loss=3.084, nll_loss=0.865, ppl=1.82, wps=22775.8, ups=1.62, wpb=14043.6, bsz=489.9, num_updates=9800, lr=2.21313e-05, gnorm=0.693, train_wall=61, wall=6587
2021-01-02 13:43:38 | INFO | train_inner | epoch 024:    217 / 421 symm_mse=0.292, loss=3.075, nll_loss=0.86, ppl=1.82, wps=22617.6, ups=1.62, wpb=13992.3, bsz=488.8, num_updates=9900, lr=2.20193e-05, gnorm=0.689, train_wall=62, wall=6648
2021-01-02 13:44:40 | INFO | train_inner | epoch 024:    317 / 421 symm_mse=0.296, loss=3.094, nll_loss=0.876, ppl=1.83, wps=22246.3, ups=1.62, wpb=13726.7, bsz=487.9, num_updates=10000, lr=2.19089e-05, gnorm=0.701, train_wall=62, wall=6710
2021-01-02 13:45:41 | INFO | train_inner | epoch 024:    417 / 421 symm_mse=0.291, loss=3.077, nll_loss=0.866, ppl=1.82, wps=22986.2, ups=1.62, wpb=14165.6, bsz=504.5, num_updates=10100, lr=2.18002e-05, gnorm=0.682, train_wall=61, wall=6772
2021-01-02 13:45:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:45:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:45:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:45:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:45:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:45:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:45:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:45:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:45:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:46:00 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_mse 0 | loss 5.417 | nll_loss 3.857 | ppl 14.49 | bleu 22.89 | wps 6024.7 | wpb 10324.2 | bsz 375 | num_updates 10104 | best_bleu 23.03
2021-01-02 13:46:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:46:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:46:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:46:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:46:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:46:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:46:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:46:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 24 @ 10104 updates, score 22.89) (writing took 2.8859697058796883 seconds)
2021-01-02 13:46:03 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-02 13:46:03 | INFO | train | epoch 024 | symm_mse 0.294 | loss 3.082 | nll_loss 0.866 | ppl 1.82 | wps 20815.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10104 | lr 2.17959e-05 | gnorm 0.692 | train_wall 259 | wall 6794
2021-01-02 13:46:03 | INFO | fairseq.trainer | begin training epoch 25
2021-01-02 13:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:47:05 | INFO | train_inner | epoch 025:     96 / 421 symm_mse=0.293, loss=3.071, nll_loss=0.855, ppl=1.81, wps=16593.6, ups=1.19, wpb=13965.8, bsz=477.5, num_updates=10200, lr=2.1693e-05, gnorm=0.694, train_wall=61, wall=6856
2021-01-02 13:48:07 | INFO | train_inner | epoch 025:    196 / 421 symm_mse=0.293, loss=3.084, nll_loss=0.869, ppl=1.83, wps=22472.8, ups=1.61, wpb=13916.1, bsz=500.1, num_updates=10300, lr=2.15875e-05, gnorm=0.692, train_wall=62, wall=6918
2021-01-02 13:49:09 | INFO | train_inner | epoch 025:    296 / 421 symm_mse=0.293, loss=3.085, nll_loss=0.87, ppl=1.83, wps=22610.2, ups=1.61, wpb=14064.9, bsz=487, num_updates=10400, lr=2.14834e-05, gnorm=0.688, train_wall=62, wall=6980
2021-01-02 13:50:11 | INFO | train_inner | epoch 025:    396 / 421 symm_mse=0.29, loss=3.075, nll_loss=0.865, ppl=1.82, wps=22607, ups=1.61, wpb=14008.2, bsz=508.2, num_updates=10500, lr=2.13809e-05, gnorm=0.682, train_wall=62, wall=7042
2021-01-02 13:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:50:43 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_mse 0 | loss 5.417 | nll_loss 3.857 | ppl 14.49 | bleu 22.72 | wps 6131 | wpb 10324.2 | bsz 375 | num_updates 10525 | best_bleu 23.03
2021-01-02 13:50:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:50:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 25 @ 10525 updates, score 22.72) (writing took 2.8934221379458904 seconds)
2021-01-02 13:50:46 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-02 13:50:46 | INFO | train | epoch 025 | symm_mse 0.292 | loss 3.079 | nll_loss 0.865 | ppl 1.82 | wps 20778.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10525 | lr 2.13555e-05 | gnorm 0.69 | train_wall 260 | wall 7077
2021-01-02 13:50:46 | INFO | fairseq.trainer | begin training epoch 26
2021-01-02 13:50:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:51:36 | INFO | train_inner | epoch 026:     75 / 421 symm_mse=0.289, loss=3.063, nll_loss=0.853, ppl=1.81, wps=16302.4, ups=1.18, wpb=13766.8, bsz=507.2, num_updates=10600, lr=2.12798e-05, gnorm=0.691, train_wall=62, wall=7126
2021-01-02 13:52:38 | INFO | train_inner | epoch 026:    175 / 421 symm_mse=0.29, loss=3.07, nll_loss=0.858, ppl=1.81, wps=22779.8, ups=1.62, wpb=14063.1, bsz=499.9, num_updates=10700, lr=2.11801e-05, gnorm=0.682, train_wall=62, wall=7188
2021-01-02 13:53:39 | INFO | train_inner | epoch 026:    275 / 421 symm_mse=0.292, loss=3.081, nll_loss=0.869, ppl=1.83, wps=22861.1, ups=1.62, wpb=14095.9, bsz=494.4, num_updates=10800, lr=2.10819e-05, gnorm=0.68, train_wall=61, wall=7250
2021-01-02 13:54:42 | INFO | train_inner | epoch 026:    375 / 421 symm_mse=0.294, loss=3.087, nll_loss=0.871, ppl=1.83, wps=22331.4, ups=1.6, wpb=13928.6, bsz=473.6, num_updates=10900, lr=2.09849e-05, gnorm=0.694, train_wall=62, wall=7312
2021-01-02 13:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:55:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:55:27 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_mse 0 | loss 5.419 | nll_loss 3.857 | ppl 14.49 | bleu 22.67 | wps 6002 | wpb 10324.2 | bsz 375 | num_updates 10946 | best_bleu 23.03
2021-01-02 13:55:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 13:55:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:55:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 26 @ 10946 updates, score 22.67) (writing took 2.9338768050074577 seconds)
2021-01-02 13:55:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-02 13:55:30 | INFO | train | epoch 026 | symm_mse 0.291 | loss 3.076 | nll_loss 0.864 | ppl 1.82 | wps 20750 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10946 | lr 2.09408e-05 | gnorm 0.687 | train_wall 260 | wall 7360
2021-01-02 13:55:30 | INFO | fairseq.trainer | begin training epoch 27
2021-01-02 13:55:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:55:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:56:06 | INFO | train_inner | epoch 027:     54 / 421 symm_mse=0.293, loss=3.085, nll_loss=0.872, ppl=1.83, wps=16387.3, ups=1.18, wpb=13886.4, bsz=483.5, num_updates=11000, lr=2.08893e-05, gnorm=0.693, train_wall=62, wall=7397
2021-01-02 13:57:09 | INFO | train_inner | epoch 027:    154 / 421 symm_mse=0.292, loss=3.077, nll_loss=0.862, ppl=1.82, wps=22545.5, ups=1.61, wpb=14008.5, bsz=483.7, num_updates=11100, lr=2.0795e-05, gnorm=0.688, train_wall=62, wall=7459
2021-01-02 13:58:10 | INFO | train_inner | epoch 027:    254 / 421 symm_mse=0.288, loss=3.068, nll_loss=0.86, ppl=1.82, wps=22842.6, ups=1.62, wpb=14065.1, bsz=498.8, num_updates=11200, lr=2.0702e-05, gnorm=0.683, train_wall=61, wall=7521
2021-01-02 13:59:12 | INFO | train_inner | epoch 027:    354 / 421 symm_mse=0.289, loss=3.071, nll_loss=0.861, ppl=1.82, wps=22359.8, ups=1.6, wpb=13936.2, bsz=501.4, num_updates=11300, lr=2.06102e-05, gnorm=0.689, train_wall=62, wall=7583
2021-01-02 13:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 13:59:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:59:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:59:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:59:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 13:59:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:59:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:59:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:59:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 13:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 13:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 13:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 13:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:00:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:00:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:00:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:00:10 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_mse 0 | loss 5.413 | nll_loss 3.853 | ppl 14.45 | bleu 22.78 | wps 6176.9 | wpb 10324.2 | bsz 375 | num_updates 11367 | best_bleu 23.03
2021-01-02 14:00:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:00:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:00:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:00:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:00:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 27 @ 11367 updates, score 22.78) (writing took 2.4565811678767204 seconds)
2021-01-02 14:00:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-02 14:00:13 | INFO | train | epoch 027 | symm_mse 0.29 | loss 3.074 | nll_loss 0.863 | ppl 1.82 | wps 20780.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 11367 | lr 2.05493e-05 | gnorm 0.688 | train_wall 260 | wall 7643
2021-01-02 14:00:13 | INFO | fairseq.trainer | begin training epoch 28
2021-01-02 14:00:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:00:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:00:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:00:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:00:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:00:36 | INFO | train_inner | epoch 028:     33 / 421 symm_mse=0.287, loss=3.063, nll_loss=0.856, ppl=1.81, wps=16442.1, ups=1.2, wpb=13736.5, bsz=489.8, num_updates=11400, lr=2.05196e-05, gnorm=0.695, train_wall=61, wall=7667
2021-01-02 14:01:38 | INFO | train_inner | epoch 028:    133 / 421 symm_mse=0.291, loss=3.074, nll_loss=0.861, ppl=1.82, wps=22517.8, ups=1.61, wpb=13970.3, bsz=473.9, num_updates=11500, lr=2.04302e-05, gnorm=0.682, train_wall=62, wall=7729
2021-01-02 14:02:40 | INFO | train_inner | epoch 028:    233 / 421 symm_mse=0.288, loss=3.066, nll_loss=0.857, ppl=1.81, wps=22707.8, ups=1.62, wpb=14051.6, bsz=492.8, num_updates=11600, lr=2.03419e-05, gnorm=0.678, train_wall=62, wall=7790
2021-01-02 14:03:42 | INFO | train_inner | epoch 028:    333 / 421 symm_mse=0.29, loss=3.08, nll_loss=0.872, ppl=1.83, wps=22581.1, ups=1.61, wpb=14062.4, bsz=500.3, num_updates=11700, lr=2.02548e-05, gnorm=0.678, train_wall=62, wall=7853
2021-01-02 14:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:04:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:04:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:04:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:04:53 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_mse 0 | loss 5.414 | nll_loss 3.852 | ppl 14.44 | bleu 22.87 | wps 5916.9 | wpb 10324.2 | bsz 375 | num_updates 11788 | best_bleu 23.03
2021-01-02 14:04:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:04:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 28 @ 11788 updates, score 22.87) (writing took 2.8839785624295473 seconds)
2021-01-02 14:04:56 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-02 14:04:56 | INFO | train | epoch 028 | symm_mse 0.289 | loss 3.071 | nll_loss 0.862 | ppl 1.82 | wps 20736.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 11788 | lr 2.0179e-05 | gnorm 0.682 | train_wall 260 | wall 7927
2021-01-02 14:04:56 | INFO | fairseq.trainer | begin training epoch 29
2021-01-02 14:04:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:04:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:05:07 | INFO | train_inner | epoch 029:     12 / 421 symm_mse=0.289, loss=3.075, nll_loss=0.867, ppl=1.82, wps=16367.6, ups=1.18, wpb=13890.5, bsz=495.7, num_updates=11800, lr=2.01688e-05, gnorm=0.692, train_wall=62, wall=7938
2021-01-02 14:06:09 | INFO | train_inner | epoch 029:    112 / 421 symm_mse=0.286, loss=3.065, nll_loss=0.859, ppl=1.81, wps=22894.3, ups=1.62, wpb=14129.1, bsz=498.7, num_updates=11900, lr=2.00839e-05, gnorm=0.677, train_wall=62, wall=7999
2021-01-02 14:07:11 | INFO | train_inner | epoch 029:    212 / 421 symm_mse=0.29, loss=3.071, nll_loss=0.86, ppl=1.82, wps=22667.2, ups=1.62, wpb=14014.8, bsz=486.4, num_updates=12000, lr=2e-05, gnorm=0.687, train_wall=62, wall=8061
2021-01-02 14:08:12 | INFO | train_inner | epoch 029:    312 / 421 symm_mse=0.286, loss=3.064, nll_loss=0.859, ppl=1.81, wps=22483.6, ups=1.62, wpb=13916.7, bsz=507.4, num_updates=12100, lr=1.99172e-05, gnorm=0.679, train_wall=62, wall=8123
2021-01-02 14:09:14 | INFO | train_inner | epoch 029:    412 / 421 symm_mse=0.29, loss=3.078, nll_loss=0.868, ppl=1.82, wps=22343.5, ups=1.61, wpb=13860.4, bsz=482.7, num_updates=12200, lr=1.98354e-05, gnorm=0.688, train_wall=62, wall=8185
2021-01-02 14:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:09:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:09:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:09:37 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_mse 0 | loss 5.412 | nll_loss 3.852 | ppl 14.44 | bleu 22.77 | wps 6151.7 | wpb 10324.2 | bsz 375 | num_updates 12209 | best_bleu 23.03
2021-01-02 14:09:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:09:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:09:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 29 @ 12209 updates, score 22.77) (writing took 2.9115215726196766 seconds)
2021-01-02 14:09:39 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-02 14:09:39 | INFO | train | epoch 029 | symm_mse 0.288 | loss 3.069 | nll_loss 0.861 | ppl 1.82 | wps 20766.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 12209 | lr 1.98281e-05 | gnorm 0.684 | train_wall 260 | wall 8210
2021-01-02 14:09:39 | INFO | fairseq.trainer | begin training epoch 30
2021-01-02 14:09:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:09:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:10:38 | INFO | train_inner | epoch 030:     91 / 421 symm_mse=0.284, loss=3.055, nll_loss=0.851, ppl=1.8, wps=16685.2, ups=1.19, wpb=14010.2, bsz=498.8, num_updates=12300, lr=1.97546e-05, gnorm=0.677, train_wall=61, wall=8269
2021-01-02 14:11:41 | INFO | train_inner | epoch 030:    191 / 421 symm_mse=0.286, loss=3.062, nll_loss=0.857, ppl=1.81, wps=22382.1, ups=1.61, wpb=13933.2, bsz=483.3, num_updates=12400, lr=1.96748e-05, gnorm=0.681, train_wall=62, wall=8331
2021-01-02 14:12:43 | INFO | train_inner | epoch 030:    291 / 421 symm_mse=0.289, loss=3.075, nll_loss=0.865, ppl=1.82, wps=22539.6, ups=1.62, wpb=13950.6, bsz=497.5, num_updates=12500, lr=1.95959e-05, gnorm=0.687, train_wall=62, wall=8393
2021-01-02 14:13:44 | INFO | train_inner | epoch 030:    391 / 421 symm_mse=0.286, loss=3.067, nll_loss=0.862, ppl=1.82, wps=22706.2, ups=1.62, wpb=14044.2, bsz=493.8, num_updates=12600, lr=1.9518e-05, gnorm=0.676, train_wall=62, wall=8455
2021-01-02 14:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:14:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:14:19 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_mse 0 | loss 5.415 | nll_loss 3.854 | ppl 14.46 | bleu 22.87 | wps 6139.9 | wpb 10324.2 | bsz 375 | num_updates 12630 | best_bleu 23.03
2021-01-02 14:14:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:14:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 30 @ 12630 updates, score 22.87) (writing took 2.975801119580865 seconds)
2021-01-02 14:14:22 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-02 14:14:22 | INFO | train | epoch 030 | symm_mse 0.287 | loss 3.066 | nll_loss 0.86 | ppl 1.82 | wps 20802.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 12630 | lr 1.94948e-05 | gnorm 0.682 | train_wall 259 | wall 8493
2021-01-02 14:14:22 | INFO | fairseq.trainer | begin training epoch 31
2021-01-02 14:14:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:15:09 | INFO | train_inner | epoch 031:     70 / 421 symm_mse=0.29, loss=3.077, nll_loss=0.866, ppl=1.82, wps=16490.5, ups=1.19, wpb=13873.1, bsz=481.6, num_updates=12700, lr=1.9441e-05, gnorm=0.691, train_wall=61, wall=8539
2021-01-02 14:16:11 | INFO | train_inner | epoch 031:    170 / 421 symm_mse=0.283, loss=3.054, nll_loss=0.852, ppl=1.81, wps=22552.4, ups=1.61, wpb=14050.2, bsz=507.7, num_updates=12800, lr=1.93649e-05, gnorm=0.673, train_wall=62, wall=8602
2021-01-02 14:17:13 | INFO | train_inner | epoch 031:    270 / 421 symm_mse=0.289, loss=3.073, nll_loss=0.864, ppl=1.82, wps=22422.5, ups=1.61, wpb=13924.8, bsz=465.5, num_updates=12900, lr=1.92897e-05, gnorm=0.686, train_wall=62, wall=8664
2021-01-02 14:18:15 | INFO | train_inner | epoch 031:    370 / 421 symm_mse=0.284, loss=3.059, nll_loss=0.856, ppl=1.81, wps=22500.2, ups=1.61, wpb=14000.3, bsz=501.4, num_updates=13000, lr=1.92154e-05, gnorm=0.674, train_wall=62, wall=8726
2021-01-02 14:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:19:03 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_mse 0 | loss 5.414 | nll_loss 3.854 | ppl 14.46 | bleu 22.8 | wps 6225.6 | wpb 10324.2 | bsz 375 | num_updates 13051 | best_bleu 23.03
2021-01-02 14:19:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:19:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:19:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 31 @ 13051 updates, score 22.8) (writing took 2.8615864478051662 seconds)
2021-01-02 14:19:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-02 14:19:06 | INFO | train | epoch 031 | symm_mse 0.286 | loss 3.065 | nll_loss 0.86 | ppl 1.81 | wps 20696.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 13051 | lr 1.91778e-05 | gnorm 0.68 | train_wall 261 | wall 8777
2021-01-02 14:19:06 | INFO | fairseq.trainer | begin training epoch 32
2021-01-02 14:19:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:19:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:19:40 | INFO | train_inner | epoch 032:     49 / 421 symm_mse=0.286, loss=3.069, nll_loss=0.864, ppl=1.82, wps=16290.5, ups=1.19, wpb=13736.1, bsz=497.8, num_updates=13100, lr=1.91419e-05, gnorm=0.685, train_wall=62, wall=8810
2021-01-02 14:20:42 | INFO | train_inner | epoch 032:    149 / 421 symm_mse=0.283, loss=3.055, nll_loss=0.854, ppl=1.81, wps=22783.7, ups=1.6, wpb=14205.9, bsz=491.4, num_updates=13200, lr=1.90693e-05, gnorm=0.666, train_wall=62, wall=8872
2021-01-02 14:21:44 | INFO | train_inner | epoch 032:    249 / 421 symm_mse=0.284, loss=3.055, nll_loss=0.852, ppl=1.8, wps=22641.2, ups=1.61, wpb=14086.9, bsz=488.9, num_updates=13300, lr=1.89974e-05, gnorm=0.676, train_wall=62, wall=8935
2021-01-02 14:22:46 | INFO | train_inner | epoch 032:    349 / 421 symm_mse=0.289, loss=3.071, nll_loss=0.862, ppl=1.82, wps=22316.1, ups=1.61, wpb=13848.6, bsz=472.5, num_updates=13400, lr=1.89264e-05, gnorm=0.691, train_wall=62, wall=8997
2021-01-02 14:23:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:23:47 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_mse 0 | loss 5.415 | nll_loss 3.853 | ppl 14.45 | bleu 22.78 | wps 6268.7 | wpb 10324.2 | bsz 375 | num_updates 13472 | best_bleu 23.03
2021-01-02 14:23:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:23:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:23:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 32 @ 13472 updates, score 22.78) (writing took 2.873485654592514 seconds)
2021-01-02 14:23:50 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-02 14:23:50 | INFO | train | epoch 032 | symm_mse 0.285 | loss 3.062 | nll_loss 0.858 | ppl 1.81 | wps 20755 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 13472 | lr 1.88758e-05 | gnorm 0.679 | train_wall 260 | wall 9060
2021-01-02 14:23:50 | INFO | fairseq.trainer | begin training epoch 33
2021-01-02 14:23:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:23:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:24:10 | INFO | train_inner | epoch 033:     28 / 421 symm_mse=0.284, loss=3.065, nll_loss=0.864, ppl=1.82, wps=16564.4, ups=1.19, wpb=13913.9, bsz=525.4, num_updates=13500, lr=1.88562e-05, gnorm=0.677, train_wall=62, wall=9081
2021-01-02 14:25:12 | INFO | train_inner | epoch 033:    128 / 421 symm_mse=0.281, loss=3.05, nll_loss=0.852, ppl=1.8, wps=22493.2, ups=1.61, wpb=14000, bsz=500, num_updates=13600, lr=1.87867e-05, gnorm=0.666, train_wall=62, wall=9143
2021-01-02 14:26:15 | INFO | train_inner | epoch 033:    228 / 421 symm_mse=0.284, loss=3.06, nll_loss=0.858, ppl=1.81, wps=22547.8, ups=1.61, wpb=14017.7, bsz=502.2, num_updates=13700, lr=1.8718e-05, gnorm=0.675, train_wall=62, wall=9205
2021-01-02 14:27:17 | INFO | train_inner | epoch 033:    328 / 421 symm_mse=0.288, loss=3.07, nll_loss=0.861, ppl=1.82, wps=22425.7, ups=1.61, wpb=13934, bsz=474.8, num_updates=13800, lr=1.86501e-05, gnorm=0.685, train_wall=62, wall=9267
2021-01-02 14:28:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:28:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:28:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:28:30 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.849 | ppl 14.41 | bleu 22.87 | wps 6185.9 | wpb 10324.2 | bsz 375 | num_updates 13893 | best_bleu 23.03
2021-01-02 14:28:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:28:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 33 @ 13893 updates, score 22.87) (writing took 2.9215537942945957 seconds)
2021-01-02 14:28:33 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-02 14:28:33 | INFO | train | epoch 033 | symm_mse 0.284 | loss 3.06 | nll_loss 0.858 | ppl 1.81 | wps 20730.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 13893 | lr 1.85876e-05 | gnorm 0.676 | train_wall 260 | wall 9344
2021-01-02 14:28:33 | INFO | fairseq.trainer | begin training epoch 34
2021-01-02 14:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:28:41 | INFO | train_inner | epoch 034:      7 / 421 symm_mse=0.284, loss=3.062, nll_loss=0.86, ppl=1.82, wps=16513.8, ups=1.19, wpb=13912, bsz=487.7, num_updates=13900, lr=1.85829e-05, gnorm=0.682, train_wall=62, wall=9352
2021-01-02 14:29:43 | INFO | train_inner | epoch 034:    107 / 421 symm_mse=0.285, loss=3.062, nll_loss=0.857, ppl=1.81, wps=22640.3, ups=1.62, wpb=13986, bsz=504.2, num_updates=14000, lr=1.85164e-05, gnorm=0.678, train_wall=62, wall=9413
2021-01-02 14:30:45 | INFO | train_inner | epoch 034:    207 / 421 symm_mse=0.284, loss=3.063, nll_loss=0.86, ppl=1.82, wps=22539.8, ups=1.61, wpb=13983.3, bsz=494.8, num_updates=14100, lr=1.84506e-05, gnorm=0.674, train_wall=62, wall=9475
2021-01-02 14:31:47 | INFO | train_inner | epoch 034:    307 / 421 symm_mse=0.281, loss=3.049, nll_loss=0.851, ppl=1.8, wps=22423.6, ups=1.61, wpb=13929, bsz=504.4, num_updates=14200, lr=1.83855e-05, gnorm=0.672, train_wall=62, wall=9537
2021-01-02 14:32:49 | INFO | train_inner | epoch 034:    407 / 421 symm_mse=0.283, loss=3.058, nll_loss=0.857, ppl=1.81, wps=22476.3, ups=1.61, wpb=13997.4, bsz=476.1, num_updates=14300, lr=1.83211e-05, gnorm=0.673, train_wall=62, wall=9600
2021-01-02 14:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:32:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:32:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:32:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:32:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:33:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:33:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:33:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:33:14 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.849 | ppl 14.41 | bleu 22.79 | wps 6123.6 | wpb 10324.2 | bsz 375 | num_updates 14314 | best_bleu 23.03
2021-01-02 14:33:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:33:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:33:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:33:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:33:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 34 @ 14314 updates, score 22.79) (writing took 2.8915160428732634 seconds)
2021-01-02 14:33:17 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-02 14:33:17 | INFO | train | epoch 034 | symm_mse 0.283 | loss 3.058 | nll_loss 0.856 | ppl 1.81 | wps 20731.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14314 | lr 1.83122e-05 | gnorm 0.676 | train_wall 260 | wall 9628
2021-01-02 14:33:17 | INFO | fairseq.trainer | begin training epoch 35
2021-01-02 14:33:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:33:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:34:13 | INFO | train_inner | epoch 035:     86 / 421 symm_mse=0.28, loss=3.047, nll_loss=0.848, ppl=1.8, wps=16563, ups=1.19, wpb=13962.1, bsz=477.3, num_updates=14400, lr=1.82574e-05, gnorm=0.671, train_wall=62, wall=9684
2021-01-02 14:35:16 | INFO | train_inner | epoch 035:    186 / 421 symm_mse=0.28, loss=3.044, nll_loss=0.845, ppl=1.8, wps=22408.1, ups=1.6, wpb=13963.9, bsz=507.5, num_updates=14500, lr=1.81944e-05, gnorm=0.67, train_wall=62, wall=9746
2021-01-02 14:36:18 | INFO | train_inner | epoch 035:    286 / 421 symm_mse=0.284, loss=3.064, nll_loss=0.86, ppl=1.82, wps=22605.8, ups=1.6, wpb=14103.9, bsz=479.2, num_updates=14600, lr=1.81319e-05, gnorm=0.674, train_wall=62, wall=9809
2021-01-02 14:37:21 | INFO | train_inner | epoch 035:    386 / 421 symm_mse=0.285, loss=3.065, nll_loss=0.862, ppl=1.82, wps=22250.3, ups=1.6, wpb=13917.1, bsz=499.5, num_updates=14700, lr=1.80702e-05, gnorm=0.678, train_wall=62, wall=9871
2021-01-02 14:37:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:37:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:37:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:37:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:37:58 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.847 | ppl 14.39 | bleu 22.87 | wps 6246.1 | wpb 10324.2 | bsz 375 | num_updates 14735 | best_bleu 23.03
2021-01-02 14:37:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:37:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:37:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:38:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:38:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:38:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:38:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 35 @ 14735 updates, score 22.87) (writing took 2.89757146127522 seconds)
2021-01-02 14:38:01 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-02 14:38:01 | INFO | train | epoch 035 | symm_mse 0.283 | loss 3.056 | nll_loss 0.856 | ppl 1.81 | wps 20702.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14735 | lr 1.80487e-05 | gnorm 0.673 | train_wall 261 | wall 9912
2021-01-02 14:38:01 | INFO | fairseq.trainer | begin training epoch 36
2021-01-02 14:38:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:38:45 | INFO | train_inner | epoch 036:     65 / 421 symm_mse=0.283, loss=3.06, nll_loss=0.858, ppl=1.81, wps=16380.3, ups=1.19, wpb=13745.1, bsz=484.9, num_updates=14800, lr=1.8009e-05, gnorm=0.683, train_wall=61, wall=9955
2021-01-02 14:39:47 | INFO | train_inner | epoch 036:    165 / 421 symm_mse=0.285, loss=3.065, nll_loss=0.86, ppl=1.81, wps=22422.3, ups=1.61, wpb=13932.1, bsz=482.5, num_updates=14900, lr=1.79485e-05, gnorm=0.681, train_wall=62, wall=10017
2021-01-02 14:40:49 | INFO | train_inner | epoch 036:    265 / 421 symm_mse=0.28, loss=3.048, nll_loss=0.851, ppl=1.8, wps=22614.4, ups=1.61, wpb=14089.7, bsz=509.1, num_updates=15000, lr=1.78885e-05, gnorm=0.668, train_wall=62, wall=10080
2021-01-02 14:41:51 | INFO | train_inner | epoch 036:    365 / 421 symm_mse=0.278, loss=3.044, nll_loss=0.85, ppl=1.8, wps=22885.3, ups=1.61, wpb=14257.4, bsz=501.1, num_updates=15100, lr=1.78292e-05, gnorm=0.659, train_wall=62, wall=10142
2021-01-02 14:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:42:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:42:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:42:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:42:42 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_mse 0 | loss 5.414 | nll_loss 3.852 | ppl 14.44 | bleu 22.83 | wps 6022 | wpb 10324.2 | bsz 375 | num_updates 15156 | best_bleu 23.03
2021-01-02 14:42:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 36 @ 15156 updates, score 22.83) (writing took 2.849095940589905 seconds)
2021-01-02 14:42:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-02 14:42:45 | INFO | train | epoch 036 | symm_mse 0.282 | loss 3.055 | nll_loss 0.855 | ppl 1.81 | wps 20706.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15156 | lr 1.77962e-05 | gnorm 0.674 | train_wall 260 | wall 10196
2021-01-02 14:42:45 | INFO | fairseq.trainer | begin training epoch 37
2021-01-02 14:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:42:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:42:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:43:15 | INFO | train_inner | epoch 037:     44 / 421 symm_mse=0.286, loss=3.067, nll_loss=0.863, ppl=1.82, wps=16383.9, ups=1.19, wpb=13760.8, bsz=468.6, num_updates=15200, lr=1.77705e-05, gnorm=0.684, train_wall=61, wall=10226
2021-01-02 14:44:18 | INFO | train_inner | epoch 037:    144 / 421 symm_mse=0.283, loss=3.058, nll_loss=0.856, ppl=1.81, wps=22208.5, ups=1.6, wpb=13844.6, bsz=487.3, num_updates=15300, lr=1.77123e-05, gnorm=0.68, train_wall=62, wall=10288
2021-01-02 14:45:20 | INFO | train_inner | epoch 037:    244 / 421 symm_mse=0.278, loss=3.043, nll_loss=0.848, ppl=1.8, wps=22495.7, ups=1.6, wpb=14021.7, bsz=500.2, num_updates=15400, lr=1.76547e-05, gnorm=0.664, train_wall=62, wall=10351
2021-01-02 14:46:22 | INFO | train_inner | epoch 037:    344 / 421 symm_mse=0.28, loss=3.051, nll_loss=0.853, ppl=1.81, wps=22631.3, ups=1.6, wpb=14112, bsz=500.6, num_updates=15500, lr=1.75977e-05, gnorm=0.663, train_wall=62, wall=10413
2021-01-02 14:47:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:47:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:47:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:47:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:47:27 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_mse 0 | loss 5.412 | nll_loss 3.85 | ppl 14.42 | bleu 22.77 | wps 6164.3 | wpb 10324.2 | bsz 375 | num_updates 15577 | best_bleu 23.03
2021-01-02 14:47:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 37 @ 15577 updates, score 22.77) (writing took 2.9155148081481457 seconds)
2021-01-02 14:47:30 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-02 14:47:30 | INFO | train | epoch 037 | symm_mse 0.281 | loss 3.053 | nll_loss 0.854 | ppl 1.81 | wps 20681.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15577 | lr 1.75541e-05 | gnorm 0.672 | train_wall 261 | wall 10480
2021-01-02 14:47:30 | INFO | fairseq.trainer | begin training epoch 38
2021-01-02 14:47:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:47:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:47:47 | INFO | train_inner | epoch 038:     23 / 421 symm_mse=0.282, loss=3.061, nll_loss=0.862, ppl=1.82, wps=16281.8, ups=1.18, wpb=13763.3, bsz=486, num_updates=15600, lr=1.75412e-05, gnorm=0.683, train_wall=62, wall=10498
2021-01-02 14:48:49 | INFO | train_inner | epoch 038:    123 / 421 symm_mse=0.279, loss=3.043, nll_loss=0.846, ppl=1.8, wps=22399.1, ups=1.62, wpb=13863.9, bsz=510.3, num_updates=15700, lr=1.74852e-05, gnorm=0.672, train_wall=62, wall=10559
2021-01-02 14:49:51 | INFO | train_inner | epoch 038:    223 / 421 symm_mse=0.28, loss=3.051, nll_loss=0.854, ppl=1.81, wps=22641.5, ups=1.61, wpb=14077.8, bsz=498.1, num_updates=15800, lr=1.74298e-05, gnorm=0.665, train_wall=62, wall=10622
2021-01-02 14:50:53 | INFO | train_inner | epoch 038:    323 / 421 symm_mse=0.283, loss=3.063, nll_loss=0.863, ppl=1.82, wps=22678.1, ups=1.61, wpb=14103.4, bsz=481.4, num_updates=15900, lr=1.73749e-05, gnorm=0.67, train_wall=62, wall=10684
2021-01-02 14:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:51:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:51:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:51:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:51:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:51:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:51:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:51:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:51:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:52:11 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_mse 0 | loss 5.413 | nll_loss 3.852 | ppl 14.44 | bleu 22.96 | wps 6216.1 | wpb 10324.2 | bsz 375 | num_updates 15998 | best_bleu 23.03
2021-01-02 14:52:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:52:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:52:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 38 @ 15998 updates, score 22.96) (writing took 2.873990559950471 seconds)
2021-01-02 14:52:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-02 14:52:14 | INFO | train | epoch 038 | symm_mse 0.28 | loss 3.051 | nll_loss 0.854 | ppl 1.81 | wps 20703.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15998 | lr 1.73216e-05 | gnorm 0.67 | train_wall 261 | wall 10764
2021-01-02 14:52:14 | INFO | fairseq.trainer | begin training epoch 39
2021-01-02 14:52:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:52:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:52:18 | INFO | train_inner | epoch 039:      2 / 421 symm_mse=0.278, loss=3.043, nll_loss=0.848, ppl=1.8, wps=16372.5, ups=1.18, wpb=13913.6, bsz=491.7, num_updates=16000, lr=1.73205e-05, gnorm=0.671, train_wall=62, wall=10769
2021-01-02 14:53:20 | INFO | train_inner | epoch 039:    102 / 421 symm_mse=0.28, loss=3.051, nll_loss=0.853, ppl=1.81, wps=22744.7, ups=1.63, wpb=13976.1, bsz=494.8, num_updates=16100, lr=1.72666e-05, gnorm=0.665, train_wall=61, wall=10830
2021-01-02 14:54:22 | INFO | train_inner | epoch 039:    202 / 421 symm_mse=0.276, loss=3.03, nll_loss=0.838, ppl=1.79, wps=22447.1, ups=1.6, wpb=14067.4, bsz=497.4, num_updates=16200, lr=1.72133e-05, gnorm=0.66, train_wall=62, wall=10893
2021-01-02 14:55:25 | INFO | train_inner | epoch 039:    302 / 421 symm_mse=0.282, loss=3.06, nll_loss=0.861, ppl=1.82, wps=22573.1, ups=1.61, wpb=14046.5, bsz=496.7, num_updates=16300, lr=1.71604e-05, gnorm=0.667, train_wall=62, wall=10955
2021-01-02 14:56:27 | INFO | train_inner | epoch 039:    402 / 421 symm_mse=0.281, loss=3.053, nll_loss=0.855, ppl=1.81, wps=22238.4, ups=1.6, wpb=13881.8, bsz=485.9, num_updates=16400, lr=1.7108e-05, gnorm=0.675, train_wall=62, wall=11018
2021-01-02 14:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 14:56:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 14:56:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 14:56:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 14:56:55 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.85 | ppl 14.42 | bleu 22.87 | wps 6147.4 | wpb 10324.2 | bsz 375 | num_updates 16419 | best_bleu 23.03
2021-01-02 14:56:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 14:56:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:56:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:56:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 39 @ 16419 updates, score 22.87) (writing took 3.04655397310853 seconds)
2021-01-02 14:56:58 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-02 14:56:58 | INFO | train | epoch 039 | symm_mse 0.28 | loss 3.05 | nll_loss 0.853 | ppl 1.81 | wps 20669.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 16419 | lr 1.70981e-05 | gnorm 0.669 | train_wall 261 | wall 11049
2021-01-02 14:56:58 | INFO | fairseq.trainer | begin training epoch 40
2021-01-02 14:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 14:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 14:57:51 | INFO | train_inner | epoch 040:     81 / 421 symm_mse=0.284, loss=3.059, nll_loss=0.856, ppl=1.81, wps=16331.1, ups=1.19, wpb=13775.3, bsz=458.8, num_updates=16500, lr=1.70561e-05, gnorm=0.683, train_wall=61, wall=11102
2021-01-02 14:58:54 | INFO | train_inner | epoch 040:    181 / 421 symm_mse=0.277, loss=3.041, nll_loss=0.849, ppl=1.8, wps=22427.6, ups=1.6, wpb=13989.1, bsz=509.5, num_updates=16600, lr=1.70046e-05, gnorm=0.668, train_wall=62, wall=11164
2021-01-02 14:59:56 | INFO | train_inner | epoch 040:    281 / 421 symm_mse=0.277, loss=3.04, nll_loss=0.846, ppl=1.8, wps=22588.2, ups=1.6, wpb=14114.3, bsz=492.2, num_updates=16700, lr=1.69536e-05, gnorm=0.665, train_wall=62, wall=11227
2021-01-02 15:00:59 | INFO | train_inner | epoch 040:    381 / 421 symm_mse=0.281, loss=3.059, nll_loss=0.862, ppl=1.82, wps=22454, ups=1.6, wpb=14010.3, bsz=512.3, num_updates=16800, lr=1.69031e-05, gnorm=0.669, train_wall=62, wall=11289
2021-01-02 15:01:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:01:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:01:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:01:40 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.849 | ppl 14.41 | bleu 22.94 | wps 5983.2 | wpb 10324.2 | bsz 375 | num_updates 16840 | best_bleu 23.03
2021-01-02 15:01:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:01:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:01:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 40 @ 16840 updates, score 22.94) (writing took 3.0070969928056 seconds)
2021-01-02 15:01:43 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-02 15:01:43 | INFO | train | epoch 040 | symm_mse 0.279 | loss 3.048 | nll_loss 0.852 | ppl 1.8 | wps 20639.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 16840 | lr 1.6883e-05 | gnorm 0.67 | train_wall 261 | wall 11334
2021-01-02 15:01:43 | INFO | fairseq.trainer | begin training epoch 41
2021-01-02 15:01:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:01:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:02:23 | INFO | train_inner | epoch 041:     60 / 421 symm_mse=0.277, loss=3.042, nll_loss=0.848, ppl=1.8, wps=16372.7, ups=1.18, wpb=13852.1, bsz=485.2, num_updates=16900, lr=1.6853e-05, gnorm=0.668, train_wall=62, wall=11374
2021-01-02 15:03:26 | INFO | train_inner | epoch 041:    160 / 421 symm_mse=0.278, loss=3.04, nll_loss=0.845, ppl=1.8, wps=22310.3, ups=1.59, wpb=14070.9, bsz=484.6, num_updates=17000, lr=1.68034e-05, gnorm=0.665, train_wall=63, wall=11437
2021-01-02 15:04:29 | INFO | train_inner | epoch 041:    260 / 421 symm_mse=0.276, loss=3.04, nll_loss=0.85, ppl=1.8, wps=22176.8, ups=1.59, wpb=13910.3, bsz=522.1, num_updates=17100, lr=1.67542e-05, gnorm=0.665, train_wall=63, wall=11500
2021-01-02 15:05:32 | INFO | train_inner | epoch 041:    360 / 421 symm_mse=0.281, loss=3.055, nll_loss=0.857, ppl=1.81, wps=22251.8, ups=1.59, wpb=13988.5, bsz=486.2, num_updates=17200, lr=1.67054e-05, gnorm=0.673, train_wall=63, wall=11562
2021-01-02 15:06:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:06:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:06:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:06:27 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.849 | ppl 14.41 | bleu 22.86 | wps 6137.9 | wpb 10324.2 | bsz 375 | num_updates 17261 | best_bleu 23.03
2021-01-02 15:06:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 41 @ 17261 updates, score 22.86) (writing took 3.039253391325474 seconds)
2021-01-02 15:06:30 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-02 15:06:30 | INFO | train | epoch 041 | symm_mse 0.278 | loss 3.046 | nll_loss 0.851 | ppl 1.8 | wps 20507.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 17261 | lr 1.66758e-05 | gnorm 0.668 | train_wall 263 | wall 11620
2021-01-02 15:06:30 | INFO | fairseq.trainer | begin training epoch 42
2021-01-02 15:06:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:06:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:06:57 | INFO | train_inner | epoch 042:     39 / 421 symm_mse=0.279, loss=3.045, nll_loss=0.849, ppl=1.8, wps=16436.3, ups=1.17, wpb=13992.9, bsz=466.6, num_updates=17300, lr=1.6657e-05, gnorm=0.671, train_wall=62, wall=11648
2021-01-02 15:07:59 | INFO | train_inner | epoch 042:    139 / 421 symm_mse=0.278, loss=3.045, nll_loss=0.849, ppl=1.8, wps=22181.6, ups=1.6, wpb=13864.2, bsz=490.1, num_updates=17400, lr=1.66091e-05, gnorm=0.668, train_wall=62, wall=11710
2021-01-02 15:09:02 | INFO | train_inner | epoch 042:    239 / 421 symm_mse=0.277, loss=3.04, nll_loss=0.846, ppl=1.8, wps=22438.1, ups=1.6, wpb=14054, bsz=497.4, num_updates=17500, lr=1.65616e-05, gnorm=0.664, train_wall=62, wall=11773
2021-01-02 15:10:05 | INFO | train_inner | epoch 042:    339 / 421 symm_mse=0.277, loss=3.043, nll_loss=0.85, ppl=1.8, wps=22507.5, ups=1.6, wpb=14062.4, bsz=506.2, num_updates=17600, lr=1.65145e-05, gnorm=0.665, train_wall=62, wall=11835
2021-01-02 15:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:10:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:10:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:10:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:10:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:10:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:10:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:10:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:10:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:11:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:11:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:11:13 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.852 | ppl 14.44 | bleu 22.83 | wps 6077.2 | wpb 10324.2 | bsz 375 | num_updates 17682 | best_bleu 23.03
2021-01-02 15:11:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:11:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:11:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:11:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:11:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:11:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:11:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:11:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 42 @ 17682 updates, score 22.83) (writing took 3.0552779994904995 seconds)
2021-01-02 15:11:16 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-02 15:11:16 | INFO | train | epoch 042 | symm_mse 0.278 | loss 3.045 | nll_loss 0.85 | ppl 1.8 | wps 20578 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 17682 | lr 1.64761e-05 | gnorm 0.669 | train_wall 262 | wall 11906
2021-01-02 15:11:16 | INFO | fairseq.trainer | begin training epoch 43
2021-01-02 15:11:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:11:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:11:30 | INFO | train_inner | epoch 043:     18 / 421 symm_mse=0.279, loss=3.051, nll_loss=0.856, ppl=1.81, wps=16145.6, ups=1.17, wpb=13796.6, bsz=485.9, num_updates=17700, lr=1.64677e-05, gnorm=0.681, train_wall=62, wall=11921
2021-01-02 15:12:32 | INFO | train_inner | epoch 043:    118 / 421 symm_mse=0.277, loss=3.042, nll_loss=0.849, ppl=1.8, wps=22405.1, ups=1.6, wpb=13998.6, bsz=490.6, num_updates=17800, lr=1.64214e-05, gnorm=0.662, train_wall=62, wall=11983
2021-01-02 15:13:35 | INFO | train_inner | epoch 043:    218 / 421 symm_mse=0.279, loss=3.044, nll_loss=0.848, ppl=1.8, wps=22216.4, ups=1.6, wpb=13921.6, bsz=480.3, num_updates=17900, lr=1.63755e-05, gnorm=0.675, train_wall=62, wall=12046
2021-01-02 15:14:38 | INFO | train_inner | epoch 043:    318 / 421 symm_mse=0.277, loss=3.045, nll_loss=0.852, ppl=1.81, wps=22350.6, ups=1.59, wpb=14013.3, bsz=509.1, num_updates=18000, lr=1.63299e-05, gnorm=0.664, train_wall=63, wall=12108
2021-01-02 15:15:40 | INFO | train_inner | epoch 043:    418 / 421 symm_mse=0.277, loss=3.046, nll_loss=0.853, ppl=1.81, wps=22494.3, ups=1.6, wpb=14072.7, bsz=493.7, num_updates=18100, lr=1.62848e-05, gnorm=0.663, train_wall=62, wall=12171
2021-01-02 15:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:15:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:15:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:15:59 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.846 | ppl 14.38 | bleu 22.9 | wps 6103.8 | wpb 10324.2 | bsz 375 | num_updates 18103 | best_bleu 23.03
2021-01-02 15:15:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:16:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:16:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:16:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:16:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:16:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:16:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:16:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 43 @ 18103 updates, score 22.9) (writing took 2.9637773372232914 seconds)
2021-01-02 15:16:02 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-02 15:16:02 | INFO | train | epoch 043 | symm_mse 0.277 | loss 3.043 | nll_loss 0.85 | ppl 1.8 | wps 20552.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 18103 | lr 1.62834e-05 | gnorm 0.669 | train_wall 262 | wall 12192
2021-01-02 15:16:02 | INFO | fairseq.trainer | begin training epoch 44
2021-01-02 15:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:16:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:17:05 | INFO | train_inner | epoch 044:     97 / 421 symm_mse=0.275, loss=3.032, nll_loss=0.839, ppl=1.79, wps=16350.5, ups=1.18, wpb=13866.6, bsz=492.2, num_updates=18200, lr=1.624e-05, gnorm=0.676, train_wall=62, wall=12256
2021-01-02 15:18:08 | INFO | train_inner | epoch 044:    197 / 421 symm_mse=0.272, loss=3.022, nll_loss=0.834, ppl=1.78, wps=22375.4, ups=1.59, wpb=14058.6, bsz=492.3, num_updates=18300, lr=1.61955e-05, gnorm=0.657, train_wall=63, wall=12319
2021-01-02 15:19:10 | INFO | train_inner | epoch 044:    297 / 421 symm_mse=0.277, loss=3.047, nll_loss=0.857, ppl=1.81, wps=22414.8, ups=1.61, wpb=13940.4, bsz=512, num_updates=18400, lr=1.61515e-05, gnorm=0.662, train_wall=62, wall=12381
2021-01-02 15:20:13 | INFO | train_inner | epoch 044:    397 / 421 symm_mse=0.281, loss=3.059, nll_loss=0.859, ppl=1.81, wps=22486.4, ups=1.6, wpb=14035.6, bsz=473.9, num_updates=18500, lr=1.61077e-05, gnorm=0.676, train_wall=62, wall=12443
2021-01-02 15:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:20:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:20:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:20:44 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.846 | ppl 14.38 | bleu 22.87 | wps 6064.3 | wpb 10324.2 | bsz 375 | num_updates 18524 | best_bleu 23.03
2021-01-02 15:20:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:20:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:20:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 44 @ 18524 updates, score 22.87) (writing took 3.0159305073320866 seconds)
2021-01-02 15:20:47 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-02 15:20:47 | INFO | train | epoch 044 | symm_mse 0.277 | loss 3.042 | nll_loss 0.849 | ppl 1.8 | wps 20616.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18524 | lr 1.60973e-05 | gnorm 0.667 | train_wall 262 | wall 12478
2021-01-02 15:20:47 | INFO | fairseq.trainer | begin training epoch 45
2021-01-02 15:20:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:20:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:21:38 | INFO | train_inner | epoch 045:     76 / 421 symm_mse=0.276, loss=3.044, nll_loss=0.854, ppl=1.81, wps=16206.4, ups=1.18, wpb=13771.3, bsz=483, num_updates=18600, lr=1.60644e-05, gnorm=0.672, train_wall=62, wall=12528
2021-01-02 15:22:40 | INFO | train_inner | epoch 045:    176 / 421 symm_mse=0.277, loss=3.044, nll_loss=0.85, ppl=1.8, wps=22451.6, ups=1.6, wpb=13992.1, bsz=510.8, num_updates=18700, lr=1.60214e-05, gnorm=0.665, train_wall=62, wall=12591
2021-01-02 15:23:42 | INFO | train_inner | epoch 045:    276 / 421 symm_mse=0.28, loss=3.05, nll_loss=0.852, ppl=1.8, wps=22470.5, ups=1.61, wpb=13974.4, bsz=476, num_updates=18800, lr=1.59787e-05, gnorm=0.672, train_wall=62, wall=12653
2021-01-02 15:24:45 | INFO | train_inner | epoch 045:    376 / 421 symm_mse=0.274, loss=3.031, nll_loss=0.842, ppl=1.79, wps=22431.5, ups=1.59, wpb=14092, bsz=499.6, num_updates=18900, lr=1.59364e-05, gnorm=0.658, train_wall=63, wall=12716
2021-01-02 15:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:25:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:25:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:25:29 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.848 | ppl 14.4 | bleu 22.85 | wps 6107 | wpb 10324.2 | bsz 375 | num_updates 18945 | best_bleu 23.03
2021-01-02 15:25:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:25:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 45 @ 18945 updates, score 22.85) (writing took 2.4840624425560236 seconds)
2021-01-02 15:25:32 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-02 15:25:32 | INFO | train | epoch 045 | symm_mse 0.276 | loss 3.04 | nll_loss 0.848 | ppl 1.8 | wps 20655.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18945 | lr 1.59174e-05 | gnorm 0.667 | train_wall 262 | wall 12762
2021-01-02 15:25:32 | INFO | fairseq.trainer | begin training epoch 46
2021-01-02 15:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:26:09 | INFO | train_inner | epoch 046:     55 / 421 symm_mse=0.275, loss=3.032, nll_loss=0.841, ppl=1.79, wps=16595.4, ups=1.19, wpb=13951.3, bsz=487.3, num_updates=19000, lr=1.58944e-05, gnorm=0.673, train_wall=62, wall=12800
2021-01-02 15:27:12 | INFO | train_inner | epoch 046:    155 / 421 symm_mse=0.277, loss=3.045, nll_loss=0.852, ppl=1.81, wps=21937.3, ups=1.58, wpb=13867.7, bsz=496.7, num_updates=19100, lr=1.58527e-05, gnorm=0.668, train_wall=63, wall=12863
2021-01-02 15:28:15 | INFO | train_inner | epoch 046:    255 / 421 symm_mse=0.276, loss=3.039, nll_loss=0.846, ppl=1.8, wps=22404.9, ups=1.59, wpb=14102.5, bsz=494.7, num_updates=19200, lr=1.58114e-05, gnorm=0.659, train_wall=63, wall=12926
2021-01-02 15:29:18 | INFO | train_inner | epoch 046:    355 / 421 symm_mse=0.276, loss=3.04, nll_loss=0.85, ppl=1.8, wps=22375.7, ups=1.6, wpb=13996.6, bsz=487.2, num_updates=19300, lr=1.57704e-05, gnorm=0.664, train_wall=62, wall=12988
2021-01-02 15:29:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:30:16 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.849 | ppl 14.41 | bleu 22.84 | wps 5974.3 | wpb 10324.2 | bsz 375 | num_updates 19366 | best_bleu 23.03
2021-01-02 15:30:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 46 @ 19366 updates, score 22.84) (writing took 3.0300134979188442 seconds)
2021-01-02 15:30:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-02 15:30:19 | INFO | train | epoch 046 | symm_mse 0.276 | loss 3.04 | nll_loss 0.848 | ppl 1.8 | wps 20476.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 19366 | lr 1.57435e-05 | gnorm 0.664 | train_wall 263 | wall 13050
2021-01-02 15:30:19 | INFO | fairseq.trainer | begin training epoch 47
2021-01-02 15:30:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:30:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:30:43 | INFO | train_inner | epoch 047:     34 / 421 symm_mse=0.274, loss=3.036, nll_loss=0.848, ppl=1.8, wps=16307.6, ups=1.17, wpb=13956.4, bsz=499.5, num_updates=19400, lr=1.57297e-05, gnorm=0.663, train_wall=62, wall=13074
2021-01-02 15:31:46 | INFO | train_inner | epoch 047:    134 / 421 symm_mse=0.28, loss=3.051, nll_loss=0.853, ppl=1.81, wps=22166.2, ups=1.59, wpb=13959.6, bsz=462.2, num_updates=19500, lr=1.56893e-05, gnorm=0.671, train_wall=63, wall=13137
2021-01-02 15:32:49 | INFO | train_inner | epoch 047:    234 / 421 symm_mse=0.273, loss=3.034, nll_loss=0.847, ppl=1.8, wps=22223, ups=1.59, wpb=13996.9, bsz=506.4, num_updates=19600, lr=1.56492e-05, gnorm=0.659, train_wall=63, wall=13200
2021-01-02 15:33:52 | INFO | train_inner | epoch 047:    334 / 421 symm_mse=0.274, loss=3.034, nll_loss=0.847, ppl=1.8, wps=22231.1, ups=1.59, wpb=13997.8, bsz=500.8, num_updates=19700, lr=1.56094e-05, gnorm=0.658, train_wall=63, wall=13263
2021-01-02 15:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:34:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:34:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:34:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:34:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:35:04 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.847 | ppl 14.39 | bleu 22.95 | wps 5679.2 | wpb 10324.2 | bsz 375 | num_updates 19787 | best_bleu 23.03
2021-01-02 15:35:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:35:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:35:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:35:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:35:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:35:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:35:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:35:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 47 @ 19787 updates, score 22.95) (writing took 3.039445824921131 seconds)
2021-01-02 15:35:07 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-02 15:35:07 | INFO | train | epoch 047 | symm_mse 0.275 | loss 3.038 | nll_loss 0.848 | ppl 1.8 | wps 20389.1 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 19787 | lr 1.55751e-05 | gnorm 0.665 | train_wall 264 | wall 13338
2021-01-02 15:35:07 | INFO | fairseq.trainer | begin training epoch 48
2021-01-02 15:35:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:35:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:35:19 | INFO | train_inner | epoch 048:     13 / 421 symm_mse=0.274, loss=3.036, nll_loss=0.847, ppl=1.8, wps=16000.4, ups=1.15, wpb=13865.8, bsz=501.2, num_updates=19800, lr=1.557e-05, gnorm=0.674, train_wall=62, wall=13350
2021-01-02 15:36:21 | INFO | train_inner | epoch 048:    113 / 421 symm_mse=0.277, loss=3.042, nll_loss=0.847, ppl=1.8, wps=22473.2, ups=1.61, wpb=13942.5, bsz=492.8, num_updates=19900, lr=1.55308e-05, gnorm=0.671, train_wall=62, wall=13412
2021-01-02 15:37:24 | INFO | train_inner | epoch 048:    213 / 421 symm_mse=0.275, loss=3.04, nll_loss=0.85, ppl=1.8, wps=22318, ups=1.6, wpb=13986.9, bsz=473.4, num_updates=20000, lr=1.54919e-05, gnorm=0.662, train_wall=62, wall=13474
2021-01-02 15:38:26 | INFO | train_inner | epoch 048:    313 / 421 symm_mse=0.275, loss=3.038, nll_loss=0.848, ppl=1.8, wps=22353.2, ups=1.6, wpb=13999.2, bsz=503.7, num_updates=20100, lr=1.54533e-05, gnorm=0.659, train_wall=62, wall=13537
2021-01-02 15:39:29 | INFO | train_inner | epoch 048:    413 / 421 symm_mse=0.273, loss=3.031, nll_loss=0.843, ppl=1.79, wps=22225.8, ups=1.59, wpb=13966.9, bsz=498.5, num_updates=20200, lr=1.5415e-05, gnorm=0.661, train_wall=63, wall=13600
2021-01-02 15:39:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:39:51 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.849 | ppl 14.41 | bleu 22.83 | wps 6096.3 | wpb 10324.2 | bsz 375 | num_updates 20208 | best_bleu 23.03
2021-01-02 15:39:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:39:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:39:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 48 @ 20208 updates, score 22.83) (writing took 2.9931625965982676 seconds)
2021-01-02 15:39:54 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-02 15:39:54 | INFO | train | epoch 048 | symm_mse 0.275 | loss 3.037 | nll_loss 0.847 | ppl 1.8 | wps 20539.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 20208 | lr 1.5412e-05 | gnorm 0.663 | train_wall 262 | wall 13624
2021-01-02 15:39:54 | INFO | fairseq.trainer | begin training epoch 49
2021-01-02 15:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:39:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:40:54 | INFO | train_inner | epoch 049:     92 / 421 symm_mse=0.272, loss=3.026, nll_loss=0.838, ppl=1.79, wps=16338.5, ups=1.17, wpb=13934.4, bsz=503, num_updates=20300, lr=1.5377e-05, gnorm=0.661, train_wall=62, wall=13685
2021-01-02 15:41:57 | INFO | train_inner | epoch 049:    192 / 421 symm_mse=0.28, loss=3.054, nll_loss=0.857, ppl=1.81, wps=22229.1, ups=1.6, wpb=13889.8, bsz=475.2, num_updates=20400, lr=1.53393e-05, gnorm=0.675, train_wall=62, wall=13747
2021-01-02 15:43:00 | INFO | train_inner | epoch 049:    292 / 421 symm_mse=0.272, loss=3.031, nll_loss=0.844, ppl=1.8, wps=22221.2, ups=1.6, wpb=13923.3, bsz=497, num_updates=20500, lr=1.53018e-05, gnorm=0.657, train_wall=62, wall=13810
2021-01-02 15:44:02 | INFO | train_inner | epoch 049:    392 / 421 symm_mse=0.273, loss=3.032, nll_loss=0.845, ppl=1.8, wps=22710.9, ups=1.61, wpb=14132.9, bsz=483.7, num_updates=20600, lr=1.52647e-05, gnorm=0.654, train_wall=62, wall=13872
2021-01-02 15:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:44:37 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.844 | ppl 14.36 | bleu 22.92 | wps 6016 | wpb 10324.2 | bsz 375 | num_updates 20629 | best_bleu 23.03
2021-01-02 15:44:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 49 @ 20629 updates, score 22.92) (writing took 2.472474832087755 seconds)
2021-01-02 15:44:39 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-02 15:44:39 | INFO | train | epoch 049 | symm_mse 0.274 | loss 3.036 | nll_loss 0.846 | ppl 1.8 | wps 20619.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 20629 | lr 1.52539e-05 | gnorm 0.662 | train_wall 262 | wall 13910
2021-01-02 15:44:39 | INFO | fairseq.trainer | begin training epoch 50
2021-01-02 15:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:44:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:44:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:45:27 | INFO | train_inner | epoch 050:     71 / 421 symm_mse=0.273, loss=3.033, nll_loss=0.845, ppl=1.8, wps=16490.8, ups=1.18, wpb=13974.8, bsz=500.3, num_updates=20700, lr=1.52277e-05, gnorm=0.664, train_wall=62, wall=13957
2021-01-02 15:46:29 | INFO | train_inner | epoch 050:    171 / 421 symm_mse=0.275, loss=3.035, nll_loss=0.843, ppl=1.79, wps=22063.8, ups=1.59, wpb=13849.1, bsz=471, num_updates=20800, lr=1.51911e-05, gnorm=0.668, train_wall=63, wall=14020
2021-01-02 15:47:32 | INFO | train_inner | epoch 050:    271 / 421 symm_mse=0.27, loss=3.02, nll_loss=0.837, ppl=1.79, wps=22416.1, ups=1.59, wpb=14131.2, bsz=510.4, num_updates=20900, lr=1.51547e-05, gnorm=0.651, train_wall=63, wall=14083
2021-01-02 15:48:35 | INFO | train_inner | epoch 050:    371 / 421 symm_mse=0.275, loss=3.04, nll_loss=0.851, ppl=1.8, wps=22244.2, ups=1.59, wpb=13993.5, bsz=510.5, num_updates=21000, lr=1.51186e-05, gnorm=0.664, train_wall=63, wall=14146
2021-01-02 15:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:49:23 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.846 | ppl 14.38 | bleu 22.93 | wps 6084.8 | wpb 10324.2 | bsz 375 | num_updates 21050 | best_bleu 23.03
2021-01-02 15:49:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:49:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 50 @ 21050 updates, score 22.93) (writing took 2.811933796852827 seconds)
2021-01-02 15:49:26 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-02 15:49:26 | INFO | train | epoch 050 | symm_mse 0.274 | loss 3.035 | nll_loss 0.846 | ppl 1.8 | wps 20502.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 21050 | lr 1.51006e-05 | gnorm 0.664 | train_wall 263 | wall 14197
2021-01-02 15:49:26 | INFO | fairseq.trainer | begin training epoch 51
2021-01-02 15:49:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:49:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:50:00 | INFO | train_inner | epoch 051:     50 / 421 symm_mse=0.279, loss=3.054, nll_loss=0.858, ppl=1.81, wps=16233.7, ups=1.18, wpb=13798.6, bsz=464.4, num_updates=21100, lr=1.50827e-05, gnorm=0.679, train_wall=62, wall=14231
2021-01-02 15:51:03 | INFO | train_inner | epoch 051:    150 / 421 symm_mse=0.273, loss=3.033, nll_loss=0.846, ppl=1.8, wps=22447.1, ups=1.6, wpb=14049.9, bsz=505.3, num_updates=21200, lr=1.50471e-05, gnorm=0.656, train_wall=62, wall=14293
2021-01-02 15:52:06 | INFO | train_inner | epoch 051:    250 / 421 symm_mse=0.274, loss=3.038, nll_loss=0.849, ppl=1.8, wps=22193, ups=1.59, wpb=13949.7, bsz=501, num_updates=21300, lr=1.50117e-05, gnorm=0.662, train_wall=63, wall=14356
2021-01-02 15:53:09 | INFO | train_inner | epoch 051:    350 / 421 symm_mse=0.272, loss=3.027, nll_loss=0.84, ppl=1.79, wps=22223.4, ups=1.59, wpb=13994.4, bsz=484.5, num_updates=21400, lr=1.49766e-05, gnorm=0.661, train_wall=63, wall=14419
2021-01-02 15:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:54:10 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.845 | ppl 14.37 | bleu 22.89 | wps 6077.7 | wpb 10324.2 | bsz 375 | num_updates 21471 | best_bleu 23.03
2021-01-02 15:54:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:54:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:54:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:54:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:54:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 51 @ 21471 updates, score 22.89) (writing took 2.988527934998274 seconds)
2021-01-02 15:54:13 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-02 15:54:13 | INFO | train | epoch 051 | symm_mse 0.273 | loss 3.034 | nll_loss 0.845 | ppl 1.8 | wps 20515.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 21471 | lr 1.49518e-05 | gnorm 0.661 | train_wall 263 | wall 14483
2021-01-02 15:54:13 | INFO | fairseq.trainer | begin training epoch 52
2021-01-02 15:54:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:54:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:54:34 | INFO | train_inner | epoch 052:     29 / 421 symm_mse=0.271, loss=3.023, nll_loss=0.839, ppl=1.79, wps=16451.3, ups=1.17, wpb=14005.4, bsz=491.4, num_updates=21500, lr=1.49417e-05, gnorm=0.657, train_wall=62, wall=14504
2021-01-02 15:55:37 | INFO | train_inner | epoch 052:    129 / 421 symm_mse=0.273, loss=3.031, nll_loss=0.842, ppl=1.79, wps=22311.6, ups=1.59, wpb=14011.3, bsz=500.1, num_updates=21600, lr=1.49071e-05, gnorm=0.656, train_wall=63, wall=14567
2021-01-02 15:56:39 | INFO | train_inner | epoch 052:    229 / 421 symm_mse=0.279, loss=3.055, nll_loss=0.859, ppl=1.81, wps=22161.6, ups=1.59, wpb=13925.4, bsz=462.5, num_updates=21700, lr=1.48727e-05, gnorm=0.672, train_wall=63, wall=14630
2021-01-02 15:57:42 | INFO | train_inner | epoch 052:    329 / 421 symm_mse=0.271, loss=3.023, nll_loss=0.838, ppl=1.79, wps=22239.7, ups=1.6, wpb=13906.2, bsz=492.3, num_updates=21800, lr=1.48386e-05, gnorm=0.66, train_wall=62, wall=14693
2021-01-02 15:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 15:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 15:58:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 15:58:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 15:58:56 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.846 | ppl 14.38 | bleu 22.98 | wps 6068.1 | wpb 10324.2 | bsz 375 | num_updates 21892 | best_bleu 23.03
2021-01-02 15:58:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 15:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 52 @ 21892 updates, score 22.98) (writing took 3.0636378694325686 seconds)
2021-01-02 15:58:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-02 15:58:59 | INFO | train | epoch 052 | symm_mse 0.273 | loss 3.032 | nll_loss 0.844 | ppl 1.79 | wps 20508.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 21892 | lr 1.48074e-05 | gnorm 0.662 | train_wall 263 | wall 14770
2021-01-02 15:58:59 | INFO | fairseq.trainer | begin training epoch 53
2021-01-02 15:59:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 15:59:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 15:59:08 | INFO | train_inner | epoch 053:      8 / 421 symm_mse=0.267, loss=3.015, nll_loss=0.836, ppl=1.78, wps=16329.3, ups=1.17, wpb=13998.2, bsz=519.4, num_updates=21900, lr=1.48047e-05, gnorm=0.658, train_wall=62, wall=14778
2021-01-02 16:00:10 | INFO | train_inner | epoch 053:    108 / 421 symm_mse=0.275, loss=3.037, nll_loss=0.846, ppl=1.8, wps=22643.2, ups=1.61, wpb=14041.4, bsz=485.9, num_updates=22000, lr=1.4771e-05, gnorm=0.661, train_wall=62, wall=14840
2021-01-02 16:01:12 | INFO | train_inner | epoch 053:    208 / 421 symm_mse=0.272, loss=3.025, nll_loss=0.837, ppl=1.79, wps=22238.6, ups=1.6, wpb=13904.8, bsz=492.6, num_updates=22100, lr=1.47375e-05, gnorm=0.663, train_wall=62, wall=14903
2021-01-02 16:02:15 | INFO | train_inner | epoch 053:    308 / 421 symm_mse=0.269, loss=3.019, nll_loss=0.836, ppl=1.79, wps=22243.3, ups=1.58, wpb=14051.7, bsz=498.5, num_updates=22200, lr=1.47043e-05, gnorm=0.656, train_wall=63, wall=14966
2021-01-02 16:03:18 | INFO | train_inner | epoch 053:    408 / 421 symm_mse=0.276, loss=3.044, nll_loss=0.853, ppl=1.81, wps=22222, ups=1.6, wpb=13918.4, bsz=491.2, num_updates=22300, lr=1.46713e-05, gnorm=0.668, train_wall=62, wall=15029
2021-01-02 16:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:03:43 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.848 | ppl 14.4 | bleu 22.79 | wps 6088.7 | wpb 10324.2 | bsz 375 | num_updates 22313 | best_bleu 23.03
2021-01-02 16:03:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:03:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 53 @ 22313 updates, score 22.79) (writing took 3.059468427672982 seconds)
2021-01-02 16:03:46 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-02 16:03:46 | INFO | train | epoch 053 | symm_mse 0.273 | loss 3.031 | nll_loss 0.844 | ppl 1.79 | wps 20532.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 22313 | lr 1.4667e-05 | gnorm 0.663 | train_wall 263 | wall 15056
2021-01-02 16:03:46 | INFO | fairseq.trainer | begin training epoch 54
2021-01-02 16:03:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:03:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:04:43 | INFO | train_inner | epoch 054:     87 / 421 symm_mse=0.272, loss=3.028, nll_loss=0.84, ppl=1.79, wps=16429.6, ups=1.17, wpb=13991.3, bsz=490.6, num_updates=22400, lr=1.46385e-05, gnorm=0.664, train_wall=62, wall=15114
2021-01-02 16:05:46 | INFO | train_inner | epoch 054:    187 / 421 symm_mse=0.273, loss=3.035, nll_loss=0.848, ppl=1.8, wps=22226.9, ups=1.59, wpb=13999.8, bsz=486.5, num_updates=22500, lr=1.46059e-05, gnorm=0.656, train_wall=63, wall=15177
2021-01-02 16:06:49 | INFO | train_inner | epoch 054:    287 / 421 symm_mse=0.27, loss=3.024, nll_loss=0.841, ppl=1.79, wps=22378.5, ups=1.59, wpb=14070.5, bsz=509, num_updates=22600, lr=1.45736e-05, gnorm=0.652, train_wall=63, wall=15240
2021-01-02 16:07:52 | INFO | train_inner | epoch 054:    387 / 421 symm_mse=0.274, loss=3.035, nll_loss=0.845, ppl=1.8, wps=22085.9, ups=1.59, wpb=13862.2, bsz=486.7, num_updates=22700, lr=1.45414e-05, gnorm=0.664, train_wall=63, wall=15302
2021-01-02 16:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:08:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:08:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:08:30 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.85 | ppl 14.42 | bleu 22.85 | wps 6074.2 | wpb 10324.2 | bsz 375 | num_updates 22734 | best_bleu 23.03
2021-01-02 16:08:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:08:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:08:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 54 @ 22734 updates, score 22.85) (writing took 2.9722713604569435 seconds)
2021-01-02 16:08:33 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-02 16:08:33 | INFO | train | epoch 054 | symm_mse 0.272 | loss 3.029 | nll_loss 0.843 | ppl 1.79 | wps 20494.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 22734 | lr 1.45306e-05 | gnorm 0.659 | train_wall 263 | wall 15343
2021-01-02 16:08:33 | INFO | fairseq.trainer | begin training epoch 55
2021-01-02 16:08:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:08:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:09:17 | INFO | train_inner | epoch 055:     66 / 421 symm_mse=0.274, loss=3.038, nll_loss=0.849, ppl=1.8, wps=16282.8, ups=1.18, wpb=13841.5, bsz=488.1, num_updates=22800, lr=1.45095e-05, gnorm=0.668, train_wall=62, wall=15387
2021-01-02 16:10:20 | INFO | train_inner | epoch 055:    166 / 421 symm_mse=0.268, loss=3.018, nll_loss=0.838, ppl=1.79, wps=22399.1, ups=1.59, wpb=14052.4, bsz=517.9, num_updates=22900, lr=1.44778e-05, gnorm=0.653, train_wall=63, wall=15450
2021-01-02 16:11:22 | INFO | train_inner | epoch 055:    266 / 421 symm_mse=0.272, loss=3.027, nll_loss=0.839, ppl=1.79, wps=22361.9, ups=1.59, wpb=14060.1, bsz=471.7, num_updates=23000, lr=1.44463e-05, gnorm=0.662, train_wall=63, wall=15513
2021-01-02 16:12:25 | INFO | train_inner | epoch 055:    366 / 421 symm_mse=0.272, loss=3.031, nll_loss=0.846, ppl=1.8, wps=22112.4, ups=1.59, wpb=13892.3, bsz=496.7, num_updates=23100, lr=1.4415e-05, gnorm=0.664, train_wall=63, wall=15576
2021-01-02 16:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:13:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:13:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:13:16 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.847 | ppl 14.39 | bleu 22.9 | wps 6111.5 | wpb 10324.2 | bsz 375 | num_updates 23155 | best_bleu 23.03
2021-01-02 16:13:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 55 @ 23155 updates, score 22.9) (writing took 3.0045012701302767 seconds)
2021-01-02 16:13:19 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-02 16:13:19 | INFO | train | epoch 055 | symm_mse 0.272 | loss 3.029 | nll_loss 0.843 | ppl 1.79 | wps 20555.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23155 | lr 1.43979e-05 | gnorm 0.663 | train_wall 263 | wall 15629
2021-01-02 16:13:19 | INFO | fairseq.trainer | begin training epoch 56
2021-01-02 16:13:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:13:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:13:50 | INFO | train_inner | epoch 056:     45 / 421 symm_mse=0.274, loss=3.036, nll_loss=0.846, ppl=1.8, wps=16321.9, ups=1.18, wpb=13788.6, bsz=481.8, num_updates=23200, lr=1.43839e-05, gnorm=0.678, train_wall=62, wall=15660
2021-01-02 16:14:53 | INFO | train_inner | epoch 056:    145 / 421 symm_mse=0.266, loss=3.014, nll_loss=0.836, ppl=1.79, wps=22474.6, ups=1.58, wpb=14199.1, bsz=519.6, num_updates=23300, lr=1.4353e-05, gnorm=0.646, train_wall=63, wall=15724
2021-01-02 16:15:55 | INFO | train_inner | epoch 056:    245 / 421 symm_mse=0.271, loss=3.028, nll_loss=0.842, ppl=1.79, wps=22346.7, ups=1.6, wpb=13964, bsz=488.3, num_updates=23400, lr=1.43223e-05, gnorm=0.661, train_wall=62, wall=15786
2021-01-02 16:16:58 | INFO | train_inner | epoch 056:    345 / 421 symm_mse=0.274, loss=3.03, nll_loss=0.841, ppl=1.79, wps=21943, ups=1.59, wpb=13808, bsz=486.3, num_updates=23500, lr=1.42918e-05, gnorm=0.67, train_wall=63, wall=15849
2021-01-02 16:17:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:17:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:17:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:17:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:18:03 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.845 | ppl 14.37 | bleu 22.91 | wps 5535.7 | wpb 10324.2 | bsz 375 | num_updates 23576 | best_bleu 23.03
2021-01-02 16:18:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:18:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 56 @ 23576 updates, score 22.91) (writing took 3.0142275411635637 seconds)
2021-01-02 16:18:06 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-02 16:18:06 | INFO | train | epoch 056 | symm_mse 0.271 | loss 3.028 | nll_loss 0.842 | ppl 1.79 | wps 20450.4 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 23576 | lr 1.42687e-05 | gnorm 0.661 | train_wall 263 | wall 15917
2021-01-02 16:18:06 | INFO | fairseq.trainer | begin training epoch 57
2021-01-02 16:18:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:18:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:18:24 | INFO | train_inner | epoch 057:     24 / 421 symm_mse=0.272, loss=3.035, nll_loss=0.85, ppl=1.8, wps=16235.4, ups=1.16, wpb=13962, bsz=488.1, num_updates=23600, lr=1.42615e-05, gnorm=0.662, train_wall=62, wall=15935
2021-01-02 16:19:27 | INFO | train_inner | epoch 057:    124 / 421 symm_mse=0.27, loss=3.027, nll_loss=0.843, ppl=1.79, wps=22311.9, ups=1.6, wpb=13953.8, bsz=500.8, num_updates=23700, lr=1.42314e-05, gnorm=0.653, train_wall=62, wall=15997
2021-01-02 16:20:29 | INFO | train_inner | epoch 057:    224 / 421 symm_mse=0.272, loss=3.024, nll_loss=0.836, ppl=1.79, wps=22280.2, ups=1.6, wpb=13941.2, bsz=473.1, num_updates=23800, lr=1.42014e-05, gnorm=0.66, train_wall=62, wall=16060
2021-01-02 16:21:32 | INFO | train_inner | epoch 057:    324 / 421 symm_mse=0.269, loss=3.022, nll_loss=0.839, ppl=1.79, wps=22290.3, ups=1.59, wpb=14044.8, bsz=504.2, num_updates=23900, lr=1.41717e-05, gnorm=0.653, train_wall=63, wall=16123
2021-01-02 16:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:22:50 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.847 | ppl 14.39 | bleu 22.86 | wps 6045.7 | wpb 10324.2 | bsz 375 | num_updates 23997 | best_bleu 23.03
2021-01-02 16:22:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 57 @ 23997 updates, score 22.86) (writing took 2.962519582360983 seconds)
2021-01-02 16:22:53 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-02 16:22:53 | INFO | train | epoch 057 | symm_mse 0.271 | loss 3.027 | nll_loss 0.842 | ppl 1.79 | wps 20519.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23997 | lr 1.4143e-05 | gnorm 0.658 | train_wall 263 | wall 16204
2021-01-02 16:22:53 | INFO | fairseq.trainer | begin training epoch 58
2021-01-02 16:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:22:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:22:58 | INFO | train_inner | epoch 058:      3 / 421 symm_mse=0.271, loss=3.027, nll_loss=0.842, ppl=1.79, wps=16280.3, ups=1.17, wpb=13965.8, bsz=483.9, num_updates=24000, lr=1.41421e-05, gnorm=0.662, train_wall=63, wall=16209
2021-01-02 16:24:00 | INFO | train_inner | epoch 058:    103 / 421 symm_mse=0.266, loss=3.011, nll_loss=0.833, ppl=1.78, wps=22744, ups=1.61, wpb=14125.7, bsz=513.5, num_updates=24100, lr=1.41128e-05, gnorm=0.646, train_wall=62, wall=16271
2021-01-02 16:25:04 | INFO | train_inner | epoch 058:    203 / 421 symm_mse=0.276, loss=3.042, nll_loss=0.85, ppl=1.8, wps=21975.5, ups=1.58, wpb=13886.5, bsz=477.5, num_updates=24200, lr=1.40836e-05, gnorm=0.67, train_wall=63, wall=16334
2021-01-02 16:26:07 | INFO | train_inner | epoch 058:    303 / 421 symm_mse=0.271, loss=3.028, nll_loss=0.842, ppl=1.79, wps=22119.3, ups=1.58, wpb=13979.3, bsz=488.4, num_updates=24300, lr=1.40546e-05, gnorm=0.659, train_wall=63, wall=16397
2021-01-02 16:27:09 | INFO | train_inner | epoch 058:    403 / 421 symm_mse=0.27, loss=3.026, nll_loss=0.842, ppl=1.79, wps=22263.2, ups=1.6, wpb=13927.2, bsz=482.3, num_updates=24400, lr=1.40257e-05, gnorm=0.659, train_wall=62, wall=16460
2021-01-02 16:27:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:27:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:27:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:27:38 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.844 | ppl 14.36 | bleu 22.75 | wps 5841.9 | wpb 10324.2 | bsz 375 | num_updates 24418 | best_bleu 23.03
2021-01-02 16:27:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:27:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 58 @ 24418 updates, score 22.75) (writing took 2.492040239274502 seconds)
2021-01-02 16:27:40 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-02 16:27:40 | INFO | train | epoch 058 | symm_mse 0.27 | loss 3.025 | nll_loss 0.841 | ppl 1.79 | wps 20483.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 24418 | lr 1.40206e-05 | gnorm 0.658 | train_wall 263 | wall 16491
2021-01-02 16:27:40 | INFO | fairseq.trainer | begin training epoch 59
2021-01-02 16:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:27:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:27:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:28:34 | INFO | train_inner | epoch 059:     82 / 421 symm_mse=0.271, loss=3.026, nll_loss=0.841, ppl=1.79, wps=16451.1, ups=1.18, wpb=13975.5, bsz=496.1, num_updates=24500, lr=1.39971e-05, gnorm=0.661, train_wall=62, wall=16545
2021-01-02 16:29:38 | INFO | train_inner | epoch 059:    182 / 421 symm_mse=0.269, loss=3.021, nll_loss=0.838, ppl=1.79, wps=21996.7, ups=1.58, wpb=13932.4, bsz=486.6, num_updates=24600, lr=1.39686e-05, gnorm=0.654, train_wall=63, wall=16608
2021-01-02 16:30:40 | INFO | train_inner | epoch 059:    282 / 421 symm_mse=0.272, loss=3.031, nll_loss=0.843, ppl=1.79, wps=22153.1, ups=1.59, wpb=13891, bsz=476.6, num_updates=24700, lr=1.39403e-05, gnorm=0.666, train_wall=63, wall=16671
2021-01-02 16:31:43 | INFO | train_inner | epoch 059:    382 / 421 symm_mse=0.266, loss=3.015, nll_loss=0.838, ppl=1.79, wps=22525.7, ups=1.59, wpb=14152.9, bsz=520.6, num_updates=24800, lr=1.39122e-05, gnorm=0.643, train_wall=63, wall=16734
2021-01-02 16:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:32:24 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.846 | ppl 14.38 | bleu 22.89 | wps 6075.4 | wpb 10324.2 | bsz 375 | num_updates 24839 | best_bleu 23.03
2021-01-02 16:32:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:32:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:32:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 59 @ 24839 updates, score 22.89) (writing took 2.5847515501081944 seconds)
2021-01-02 16:32:27 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-02 16:32:27 | INFO | train | epoch 059 | symm_mse 0.27 | loss 3.025 | nll_loss 0.841 | ppl 1.79 | wps 20508.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 24839 | lr 1.39012e-05 | gnorm 0.657 | train_wall 264 | wall 16778
2021-01-02 16:32:27 | INFO | fairseq.trainer | begin training epoch 60
2021-01-02 16:32:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:32:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:33:07 | INFO | train_inner | epoch 060:     61 / 421 symm_mse=0.271, loss=3.027, nll_loss=0.842, ppl=1.79, wps=16495.8, ups=1.19, wpb=13915, bsz=494.5, num_updates=24900, lr=1.38842e-05, gnorm=0.656, train_wall=62, wall=16818
2021-01-02 16:34:10 | INFO | train_inner | epoch 060:    161 / 421 symm_mse=0.27, loss=3.024, nll_loss=0.839, ppl=1.79, wps=22246.5, ups=1.59, wpb=13949.2, bsz=495.3, num_updates=25000, lr=1.38564e-05, gnorm=0.656, train_wall=63, wall=16881
2021-01-02 16:35:13 | INFO | train_inner | epoch 060:    261 / 421 symm_mse=0.271, loss=3.025, nll_loss=0.84, ppl=1.79, wps=22249.8, ups=1.59, wpb=13980, bsz=487, num_updates=25100, lr=1.38288e-05, gnorm=0.659, train_wall=63, wall=16944
2021-01-02 16:36:15 | INFO | train_inner | epoch 060:    361 / 421 symm_mse=0.269, loss=3.023, nll_loss=0.841, ppl=1.79, wps=22518.4, ups=1.61, wpb=14002.2, bsz=497.2, num_updates=25200, lr=1.38013e-05, gnorm=0.653, train_wall=62, wall=17006
2021-01-02 16:36:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:36:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:36:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:36:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:37:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:37:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:37:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:37:09 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.97 | wps 6136.6 | wpb 10324.2 | bsz 375 | num_updates 25260 | best_bleu 23.03
2021-01-02 16:37:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:37:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:37:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:37:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:37:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 60 @ 25260 updates, score 22.97) (writing took 3.0089175682514906 seconds)
2021-01-02 16:37:12 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-02 16:37:12 | INFO | train | epoch 060 | symm_mse 0.27 | loss 3.023 | nll_loss 0.839 | ppl 1.79 | wps 20609.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 25260 | lr 1.37849e-05 | gnorm 0.659 | train_wall 262 | wall 17063
2021-01-02 16:37:12 | INFO | fairseq.trainer | begin training epoch 61
2021-01-02 16:37:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:37:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:37:40 | INFO | train_inner | epoch 061:     40 / 421 symm_mse=0.268, loss=3.019, nll_loss=0.838, ppl=1.79, wps=16452.9, ups=1.18, wpb=13960.9, bsz=497.4, num_updates=25300, lr=1.3774e-05, gnorm=0.668, train_wall=62, wall=17091
2021-01-02 16:38:43 | INFO | train_inner | epoch 061:    140 / 421 symm_mse=0.27, loss=3.025, nll_loss=0.841, ppl=1.79, wps=22049.5, ups=1.59, wpb=13876.7, bsz=488.5, num_updates=25400, lr=1.37469e-05, gnorm=0.657, train_wall=63, wall=17154
2021-01-02 16:39:46 | INFO | train_inner | epoch 061:    240 / 421 symm_mse=0.271, loss=3.027, nll_loss=0.842, ppl=1.79, wps=22047.6, ups=1.58, wpb=13922.9, bsz=496.8, num_updates=25500, lr=1.37199e-05, gnorm=0.663, train_wall=63, wall=17217
2021-01-02 16:40:49 | INFO | train_inner | epoch 061:    340 / 421 symm_mse=0.269, loss=3.019, nll_loss=0.836, ppl=1.79, wps=22484.4, ups=1.59, wpb=14164, bsz=486.6, num_updates=25600, lr=1.36931e-05, gnorm=0.654, train_wall=63, wall=17280
2021-01-02 16:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:41:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:41:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:41:56 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.89 | wps 6018.3 | wpb 10324.2 | bsz 375 | num_updates 25681 | best_bleu 23.03
2021-01-02 16:41:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:41:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:41:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:41:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 61 @ 25681 updates, score 22.89) (writing took 3.017274785786867 seconds)
2021-01-02 16:41:59 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-02 16:41:59 | INFO | train | epoch 061 | symm_mse 0.27 | loss 3.023 | nll_loss 0.84 | ppl 1.79 | wps 20485.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 25681 | lr 1.36715e-05 | gnorm 0.658 | train_wall 263 | wall 17350
2021-01-02 16:41:59 | INFO | fairseq.trainer | begin training epoch 62
2021-01-02 16:42:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:42:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:42:14 | INFO | train_inner | epoch 062:     19 / 421 symm_mse=0.27, loss=3.026, nll_loss=0.844, ppl=1.8, wps=16214.8, ups=1.17, wpb=13835, bsz=493.3, num_updates=25700, lr=1.36664e-05, gnorm=0.661, train_wall=62, wall=17365
2021-01-02 16:43:17 | INFO | train_inner | epoch 062:    119 / 421 symm_mse=0.27, loss=3.019, nll_loss=0.833, ppl=1.78, wps=22425.5, ups=1.6, wpb=13986.2, bsz=487.6, num_updates=25800, lr=1.36399e-05, gnorm=0.661, train_wall=62, wall=17427
2021-01-02 16:44:20 | INFO | train_inner | epoch 062:    219 / 421 symm_mse=0.269, loss=3.024, nll_loss=0.841, ppl=1.79, wps=22384.8, ups=1.59, wpb=14050.9, bsz=497.9, num_updates=25900, lr=1.36135e-05, gnorm=0.65, train_wall=63, wall=17490
2021-01-02 16:45:23 | INFO | train_inner | epoch 062:    319 / 421 symm_mse=0.268, loss=3.018, nll_loss=0.838, ppl=1.79, wps=22191.1, ups=1.59, wpb=13962.5, bsz=495.5, num_updates=26000, lr=1.35873e-05, gnorm=0.657, train_wall=63, wall=17553
2021-01-02 16:46:25 | INFO | train_inner | epoch 062:    419 / 421 symm_mse=0.27, loss=3.027, nll_loss=0.844, ppl=1.8, wps=22383, ups=1.6, wpb=13977.3, bsz=489.4, num_updates=26100, lr=1.35613e-05, gnorm=0.656, train_wall=62, wall=17616
2021-01-02 16:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:46:43 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.95 | wps 6038.4 | wpb 10324.2 | bsz 375 | num_updates 26102 | best_bleu 23.03
2021-01-02 16:46:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:46:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 62 @ 26102 updates, score 22.95) (writing took 3.0330426450818777 seconds)
2021-01-02 16:46:46 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-02 16:46:46 | INFO | train | epoch 062 | symm_mse 0.269 | loss 3.022 | nll_loss 0.839 | ppl 1.79 | wps 20535.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 26102 | lr 1.35608e-05 | gnorm 0.658 | train_wall 263 | wall 17636
2021-01-02 16:46:46 | INFO | fairseq.trainer | begin training epoch 63
2021-01-02 16:46:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:46:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:46:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:47:50 | INFO | train_inner | epoch 063:     98 / 421 symm_mse=0.268, loss=3.012, nll_loss=0.831, ppl=1.78, wps=16607.5, ups=1.18, wpb=14090.7, bsz=472.6, num_updates=26200, lr=1.35354e-05, gnorm=0.662, train_wall=62, wall=17700
2021-01-02 16:48:53 | INFO | train_inner | epoch 063:    198 / 421 symm_mse=0.27, loss=3.026, nll_loss=0.843, ppl=1.79, wps=22070.9, ups=1.59, wpb=13897.7, bsz=494.5, num_updates=26300, lr=1.35096e-05, gnorm=0.657, train_wall=63, wall=17763
2021-01-02 16:49:56 | INFO | train_inner | epoch 063:    298 / 421 symm_mse=0.267, loss=3.017, nll_loss=0.838, ppl=1.79, wps=22088.8, ups=1.58, wpb=13936.7, bsz=513.1, num_updates=26400, lr=1.3484e-05, gnorm=0.653, train_wall=63, wall=17826
2021-01-02 16:50:59 | INFO | train_inner | epoch 063:    398 / 421 symm_mse=0.27, loss=3.03, nll_loss=0.846, ppl=1.8, wps=22208.8, ups=1.59, wpb=13957.4, bsz=498.3, num_updates=26500, lr=1.34585e-05, gnorm=0.655, train_wall=63, wall=17889
2021-01-02 16:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:51:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:51:29 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.845 | ppl 14.37 | bleu 22.9 | wps 6143.8 | wpb 10324.2 | bsz 375 | num_updates 26523 | best_bleu 23.03
2021-01-02 16:51:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:51:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 63 @ 26523 updates, score 22.9) (writing took 2.997969314455986 seconds)
2021-01-02 16:51:32 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-02 16:51:32 | INFO | train | epoch 063 | symm_mse 0.269 | loss 3.021 | nll_loss 0.839 | ppl 1.79 | wps 20519.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 26523 | lr 1.34527e-05 | gnorm 0.655 | train_wall 263 | wall 17923
2021-01-02 16:51:32 | INFO | fairseq.trainer | begin training epoch 64
2021-01-02 16:51:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:51:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:52:23 | INFO | train_inner | epoch 064:     77 / 421 symm_mse=0.267, loss=3.01, nll_loss=0.829, ppl=1.78, wps=16486.8, ups=1.18, wpb=13916.8, bsz=482.8, num_updates=26600, lr=1.34332e-05, gnorm=0.663, train_wall=62, wall=17974
2021-01-02 16:53:26 | INFO | train_inner | epoch 064:    177 / 421 symm_mse=0.268, loss=3.022, nll_loss=0.842, ppl=1.79, wps=22304.8, ups=1.6, wpb=13975.7, bsz=487.3, num_updates=26700, lr=1.3408e-05, gnorm=0.656, train_wall=62, wall=18036
2021-01-02 16:54:29 | INFO | train_inner | epoch 064:    277 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.839, ppl=1.79, wps=22307.6, ups=1.58, wpb=14128.6, bsz=497.4, num_updates=26800, lr=1.3383e-05, gnorm=0.647, train_wall=63, wall=18100
2021-01-02 16:55:32 | INFO | train_inner | epoch 064:    377 / 421 symm_mse=0.271, loss=3.027, nll_loss=0.842, ppl=1.79, wps=22158.7, ups=1.6, wpb=13883.7, bsz=493.9, num_updates=26900, lr=1.33581e-05, gnorm=0.66, train_wall=62, wall=18162
2021-01-02 16:55:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 16:56:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 16:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 16:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 16:56:17 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.844 | ppl 14.36 | bleu 22.81 | wps 5582.7 | wpb 10324.2 | bsz 375 | num_updates 26944 | best_bleu 23.03
2021-01-02 16:56:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 16:56:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 64 @ 26944 updates, score 22.81) (writing took 2.9944198094308376 seconds)
2021-01-02 16:56:20 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-02 16:56:20 | INFO | train | epoch 064 | symm_mse 0.268 | loss 3.02 | nll_loss 0.838 | ppl 1.79 | wps 20448.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 26944 | lr 1.33472e-05 | gnorm 0.656 | train_wall 263 | wall 18211
2021-01-02 16:56:20 | INFO | fairseq.trainer | begin training epoch 65
2021-01-02 16:56:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 16:56:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 16:56:57 | INFO | train_inner | epoch 065:     56 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.838, ppl=1.79, wps=16095, ups=1.17, wpb=13770.9, bsz=483.4, num_updates=27000, lr=1.33333e-05, gnorm=0.662, train_wall=62, wall=18248
2021-01-02 16:58:00 | INFO | train_inner | epoch 065:    156 / 421 symm_mse=0.268, loss=3.016, nll_loss=0.835, ppl=1.78, wps=22397, ups=1.6, wpb=14040.4, bsz=480.6, num_updates=27100, lr=1.33087e-05, gnorm=0.652, train_wall=62, wall=18311
2021-01-02 16:59:03 | INFO | train_inner | epoch 065:    256 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.839, ppl=1.79, wps=22286.2, ups=1.6, wpb=13927, bsz=516.4, num_updates=27200, lr=1.32842e-05, gnorm=0.654, train_wall=62, wall=18373
2021-01-02 17:00:05 | INFO | train_inner | epoch 065:    356 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.84, ppl=1.79, wps=22386.9, ups=1.59, wpb=14052, bsz=486.2, num_updates=27300, lr=1.32599e-05, gnorm=0.649, train_wall=63, wall=18436
2021-01-02 17:00:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:01:03 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.848 | ppl 14.4 | bleu 22.81 | wps 6120.4 | wpb 10324.2 | bsz 375 | num_updates 27365 | best_bleu 23.03
2021-01-02 17:01:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:01:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:01:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:01:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:01:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:01:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:01:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:01:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 65 @ 27365 updates, score 22.81) (writing took 2.8882670272141695 seconds)
2021-01-02 17:01:05 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-02 17:01:05 | INFO | train | epoch 065 | symm_mse 0.268 | loss 3.019 | nll_loss 0.838 | ppl 1.79 | wps 20601.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27365 | lr 1.32441e-05 | gnorm 0.654 | train_wall 262 | wall 18496
2021-01-02 17:01:05 | INFO | fairseq.trainer | begin training epoch 66
2021-01-02 17:01:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:01:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:01:30 | INFO | train_inner | epoch 066:     35 / 421 symm_mse=0.268, loss=3.022, nll_loss=0.841, ppl=1.79, wps=16372.3, ups=1.18, wpb=13884.4, bsz=501.7, num_updates=27400, lr=1.32357e-05, gnorm=0.664, train_wall=62, wall=18521
2021-01-02 17:02:33 | INFO | train_inner | epoch 066:    135 / 421 symm_mse=0.265, loss=3.005, nll_loss=0.827, ppl=1.77, wps=22201.5, ups=1.58, wpb=14031.2, bsz=494.3, num_updates=27500, lr=1.32116e-05, gnorm=0.649, train_wall=63, wall=18584
2021-01-02 17:03:37 | INFO | train_inner | epoch 066:    235 / 421 symm_mse=0.266, loss=3.011, nll_loss=0.832, ppl=1.78, wps=22131.3, ups=1.58, wpb=14005.9, bsz=498.6, num_updates=27600, lr=1.31876e-05, gnorm=0.654, train_wall=63, wall=18647
2021-01-02 17:04:40 | INFO | train_inner | epoch 066:    335 / 421 symm_mse=0.268, loss=3.019, nll_loss=0.837, ppl=1.79, wps=22314.9, ups=1.58, wpb=14104.2, bsz=488.2, num_updates=27700, lr=1.31638e-05, gnorm=0.651, train_wall=63, wall=18710
2021-01-02 17:05:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:05:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:05:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:05:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:05:50 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.843 | ppl 14.35 | bleu 22.69 | wps 6015.2 | wpb 10324.2 | bsz 375 | num_updates 27786 | best_bleu 23.03
2021-01-02 17:05:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:05:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:05:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 66 @ 27786 updates, score 22.69) (writing took 2.8681834172457457 seconds)
2021-01-02 17:05:53 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-02 17:05:53 | INFO | train | epoch 066 | symm_mse 0.268 | loss 3.018 | nll_loss 0.837 | ppl 1.79 | wps 20465.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27786 | lr 1.31434e-05 | gnorm 0.656 | train_wall 264 | wall 18783
2021-01-02 17:05:53 | INFO | fairseq.trainer | begin training epoch 67
2021-01-02 17:05:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:05:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:06:05 | INFO | train_inner | epoch 067:     14 / 421 symm_mse=0.269, loss=3.026, nll_loss=0.845, ppl=1.8, wps=16291, ups=1.18, wpb=13844.4, bsz=484.9, num_updates=27800, lr=1.31401e-05, gnorm=0.662, train_wall=62, wall=18795
2021-01-02 17:07:07 | INFO | train_inner | epoch 067:    114 / 421 symm_mse=0.272, loss=3.034, nll_loss=0.846, ppl=1.8, wps=22413.5, ups=1.61, wpb=13962.4, bsz=482.9, num_updates=27900, lr=1.31165e-05, gnorm=0.663, train_wall=62, wall=18858
2021-01-02 17:08:10 | INFO | train_inner | epoch 067:    214 / 421 symm_mse=0.266, loss=3.014, nll_loss=0.835, ppl=1.78, wps=22287.9, ups=1.58, wpb=14065.6, bsz=497.8, num_updates=28000, lr=1.30931e-05, gnorm=0.652, train_wall=63, wall=18921
2021-01-02 17:09:13 | INFO | train_inner | epoch 067:    314 / 421 symm_mse=0.265, loss=3.007, nll_loss=0.83, ppl=1.78, wps=22194.5, ups=1.58, wpb=14011.8, bsz=495.5, num_updates=28100, lr=1.30698e-05, gnorm=0.652, train_wall=63, wall=18984
2021-01-02 17:10:16 | INFO | train_inner | epoch 067:    414 / 421 symm_mse=0.268, loss=3.019, nll_loss=0.839, ppl=1.79, wps=22120.1, ups=1.6, wpb=13845.2, bsz=495.4, num_updates=28200, lr=1.30466e-05, gnorm=0.656, train_wall=62, wall=19047
2021-01-02 17:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:10:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:10:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:10:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:10:37 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.75 | wps 6089.9 | wpb 10324.2 | bsz 375 | num_updates 28207 | best_bleu 23.03
2021-01-02 17:10:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:10:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:10:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 67 @ 28207 updates, score 22.75) (writing took 2.8650418631732464 seconds)
2021-01-02 17:10:40 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-02 17:10:40 | INFO | train | epoch 067 | symm_mse 0.268 | loss 3.017 | nll_loss 0.837 | ppl 1.79 | wps 20493.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 28207 | lr 1.30449e-05 | gnorm 0.656 | train_wall 263 | wall 19070
2021-01-02 17:10:40 | INFO | fairseq.trainer | begin training epoch 68
2021-01-02 17:10:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:10:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:11:41 | INFO | train_inner | epoch 068:     93 / 421 symm_mse=0.264, loss=3.004, nll_loss=0.828, ppl=1.77, wps=16356.9, ups=1.18, wpb=13895.4, bsz=502.8, num_updates=28300, lr=1.30235e-05, gnorm=0.653, train_wall=62, wall=19131
2021-01-02 17:12:44 | INFO | train_inner | epoch 068:    193 / 421 symm_mse=0.267, loss=3.014, nll_loss=0.834, ppl=1.78, wps=22097.6, ups=1.59, wpb=13941.5, bsz=498.7, num_updates=28400, lr=1.30005e-05, gnorm=0.656, train_wall=63, wall=19195
2021-01-02 17:13:47 | INFO | train_inner | epoch 068:    293 / 421 symm_mse=0.269, loss=3.022, nll_loss=0.84, ppl=1.79, wps=22392.9, ups=1.59, wpb=14061.8, bsz=483.7, num_updates=28500, lr=1.29777e-05, gnorm=0.653, train_wall=63, wall=19257
2021-01-02 17:14:49 | INFO | train_inner | epoch 068:    393 / 421 symm_mse=0.268, loss=3.022, nll_loss=0.842, ppl=1.79, wps=22386.2, ups=1.6, wpb=14016.9, bsz=485.7, num_updates=28600, lr=1.2955e-05, gnorm=0.65, train_wall=62, wall=19320
2021-01-02 17:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:15:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:15:23 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.85 | ppl 14.42 | bleu 22.78 | wps 6111.3 | wpb 10324.2 | bsz 375 | num_updates 28628 | best_bleu 23.03
2021-01-02 17:15:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 68 @ 28628 updates, score 22.78) (writing took 2.906485825777054 seconds)
2021-01-02 17:15:26 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-02 17:15:26 | INFO | train | epoch 068 | symm_mse 0.267 | loss 3.017 | nll_loss 0.837 | ppl 1.79 | wps 20551.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 28628 | lr 1.29487e-05 | gnorm 0.654 | train_wall 263 | wall 19357
2021-01-02 17:15:26 | INFO | fairseq.trainer | begin training epoch 69
2021-01-02 17:15:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:15:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:15:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:16:14 | INFO | train_inner | epoch 069:     72 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.839, ppl=1.79, wps=16389.2, ups=1.18, wpb=13856.8, bsz=488.3, num_updates=28700, lr=1.29324e-05, gnorm=0.662, train_wall=62, wall=19405
2021-01-02 17:17:17 | INFO | train_inner | epoch 069:    172 / 421 symm_mse=0.266, loss=3.014, nll_loss=0.836, ppl=1.78, wps=22339.2, ups=1.6, wpb=13998.6, bsz=496.1, num_updates=28800, lr=1.29099e-05, gnorm=0.652, train_wall=62, wall=19467
2021-01-02 17:18:19 | INFO | train_inner | epoch 069:    272 / 421 symm_mse=0.265, loss=3.009, nll_loss=0.832, ppl=1.78, wps=22509.2, ups=1.59, wpb=14161.2, bsz=500.7, num_updates=28900, lr=1.28876e-05, gnorm=0.647, train_wall=63, wall=19530
2021-01-02 17:19:22 | INFO | train_inner | epoch 069:    372 / 421 symm_mse=0.268, loss=3.02, nll_loss=0.839, ppl=1.79, wps=22042.6, ups=1.59, wpb=13844.5, bsz=482.7, num_updates=29000, lr=1.28654e-05, gnorm=0.663, train_wall=63, wall=19593
2021-01-02 17:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:19:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:20:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:20:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:20:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:20:09 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.844 | ppl 14.36 | bleu 22.94 | wps 6072.6 | wpb 10324.2 | bsz 375 | num_updates 29049 | best_bleu 23.03
2021-01-02 17:20:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:20:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:20:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:20:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:20:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 69 @ 29049 updates, score 22.94) (writing took 2.896323861554265 seconds)
2021-01-02 17:20:12 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-02 17:20:12 | INFO | train | epoch 069 | symm_mse 0.267 | loss 3.016 | nll_loss 0.836 | ppl 1.79 | wps 20532.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 29049 | lr 1.28545e-05 | gnorm 0.656 | train_wall 263 | wall 19643
2021-01-02 17:20:12 | INFO | fairseq.trainer | begin training epoch 70
2021-01-02 17:20:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:20:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:20:47 | INFO | train_inner | epoch 070:     51 / 421 symm_mse=0.264, loss=3.006, nll_loss=0.829, ppl=1.78, wps=16457.8, ups=1.18, wpb=13940.7, bsz=497.4, num_updates=29100, lr=1.28432e-05, gnorm=0.655, train_wall=62, wall=19678
2021-01-02 17:21:49 | INFO | train_inner | epoch 070:    151 / 421 symm_mse=0.266, loss=3.011, nll_loss=0.832, ppl=1.78, wps=22545.8, ups=1.61, wpb=14040.9, bsz=491.7, num_updates=29200, lr=1.28212e-05, gnorm=0.655, train_wall=62, wall=19740
2021-01-02 17:22:52 | INFO | train_inner | epoch 070:    251 / 421 symm_mse=0.27, loss=3.028, nll_loss=0.844, ppl=1.8, wps=22385.6, ups=1.6, wpb=13968.4, bsz=492.8, num_updates=29300, lr=1.27993e-05, gnorm=0.659, train_wall=62, wall=19802
2021-01-02 17:23:55 | INFO | train_inner | epoch 070:    351 / 421 symm_mse=0.263, loss=3.004, nll_loss=0.828, ppl=1.78, wps=22298, ups=1.59, wpb=14021.5, bsz=494.1, num_updates=29400, lr=1.27775e-05, gnorm=0.648, train_wall=63, wall=19865
2021-01-02 17:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:24:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:24:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:24:55 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.73 | wps 6069 | wpb 10324.2 | bsz 375 | num_updates 29470 | best_bleu 23.03
2021-01-02 17:24:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:24:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:24:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 70 @ 29470 updates, score 22.73) (writing took 2.9099775329232216 seconds)
2021-01-02 17:24:58 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-02 17:24:58 | INFO | train | epoch 070 | symm_mse 0.266 | loss 3.014 | nll_loss 0.835 | ppl 1.78 | wps 20588.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 29470 | lr 1.27623e-05 | gnorm 0.654 | train_wall 262 | wall 19929
2021-01-02 17:24:58 | INFO | fairseq.trainer | begin training epoch 71
2021-01-02 17:24:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:25:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:25:20 | INFO | train_inner | epoch 071:     30 / 421 symm_mse=0.271, loss=3.032, nll_loss=0.849, ppl=1.8, wps=16191, ups=1.17, wpb=13792.5, bsz=475.6, num_updates=29500, lr=1.27559e-05, gnorm=0.664, train_wall=62, wall=19950
2021-01-02 17:26:22 | INFO | train_inner | epoch 071:    130 / 421 symm_mse=0.266, loss=3.014, nll_loss=0.835, ppl=1.78, wps=22439.3, ups=1.6, wpb=14042.9, bsz=493.1, num_updates=29600, lr=1.27343e-05, gnorm=0.647, train_wall=62, wall=20013
2021-01-02 17:27:25 | INFO | train_inner | epoch 071:    230 / 421 symm_mse=0.266, loss=3.018, nll_loss=0.84, ppl=1.79, wps=22305.1, ups=1.6, wpb=13948, bsz=496.8, num_updates=29700, lr=1.27128e-05, gnorm=0.654, train_wall=62, wall=20075
2021-01-02 17:28:27 | INFO | train_inner | epoch 071:    330 / 421 symm_mse=0.267, loss=3.017, nll_loss=0.837, ppl=1.79, wps=22661.7, ups=1.61, wpb=14070.8, bsz=493.8, num_updates=29800, lr=1.26915e-05, gnorm=0.651, train_wall=62, wall=20138
2021-01-02 17:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:29:42 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.848 | ppl 14.4 | bleu 22.8 | wps 5202.6 | wpb 10324.2 | bsz 375 | num_updates 29891 | best_bleu 23.03
2021-01-02 17:29:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:29:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 71 @ 29891 updates, score 22.8) (writing took 2.969964211806655 seconds)
2021-01-02 17:29:45 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-02 17:29:45 | INFO | train | epoch 071 | symm_mse 0.266 | loss 3.014 | nll_loss 0.835 | ppl 1.78 | wps 20475.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 29891 | lr 1.26722e-05 | gnorm 0.653 | train_wall 262 | wall 20216
2021-01-02 17:29:45 | INFO | fairseq.trainer | begin training epoch 72
2021-01-02 17:29:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:29:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:29:54 | INFO | train_inner | epoch 072:      9 / 421 symm_mse=0.265, loss=3.004, nll_loss=0.826, ppl=1.77, wps=15870.7, ups=1.15, wpb=13812, bsz=497.7, num_updates=29900, lr=1.26702e-05, gnorm=0.661, train_wall=62, wall=20225
2021-01-02 17:30:56 | INFO | train_inner | epoch 072:    109 / 421 symm_mse=0.267, loss=3.015, nll_loss=0.835, ppl=1.78, wps=22358.4, ups=1.61, wpb=13899.2, bsz=500.5, num_updates=30000, lr=1.26491e-05, gnorm=0.65, train_wall=62, wall=20287
2021-01-02 17:31:59 | INFO | train_inner | epoch 072:    209 / 421 symm_mse=0.269, loss=3.027, nll_loss=0.844, ppl=1.79, wps=22388.3, ups=1.59, wpb=14056.5, bsz=479, num_updates=30100, lr=1.26281e-05, gnorm=0.659, train_wall=63, wall=20350
2021-01-02 17:33:02 | INFO | train_inner | epoch 072:    309 / 421 symm_mse=0.261, loss=2.993, nll_loss=0.821, ppl=1.77, wps=22425, ups=1.59, wpb=14090.8, bsz=503.7, num_updates=30200, lr=1.26072e-05, gnorm=0.644, train_wall=63, wall=20412
2021-01-02 17:34:04 | INFO | train_inner | epoch 072:    409 / 421 symm_mse=0.265, loss=3.011, nll_loss=0.832, ppl=1.78, wps=22344.8, ups=1.6, wpb=13971.2, bsz=488.9, num_updates=30300, lr=1.25863e-05, gnorm=0.658, train_wall=62, wall=20475
2021-01-02 17:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:34:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:34:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:34:30 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.88 | wps 5328.9 | wpb 10324.2 | bsz 375 | num_updates 30312 | best_bleu 23.03
2021-01-02 17:34:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:34:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 72 @ 30312 updates, score 22.88) (writing took 3.0480660777539015 seconds)
2021-01-02 17:34:33 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-02 17:34:33 | INFO | train | epoch 072 | symm_mse 0.266 | loss 3.013 | nll_loss 0.834 | ppl 1.78 | wps 20451.5 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 30312 | lr 1.25838e-05 | gnorm 0.654 | train_wall 262 | wall 20503
2021-01-02 17:34:33 | INFO | fairseq.trainer | begin training epoch 73
2021-01-02 17:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:34:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:35:31 | INFO | train_inner | epoch 073:     88 / 421 symm_mse=0.267, loss=3.016, nll_loss=0.836, ppl=1.78, wps=16005, ups=1.16, wpb=13804.3, bsz=486.8, num_updates=30400, lr=1.25656e-05, gnorm=0.655, train_wall=62, wall=20561
2021-01-02 17:36:33 | INFO | train_inner | epoch 073:    188 / 421 symm_mse=0.266, loss=3.013, nll_loss=0.833, ppl=1.78, wps=22272.9, ups=1.6, wpb=13883.7, bsz=482.3, num_updates=30500, lr=1.2545e-05, gnorm=0.655, train_wall=62, wall=20623
2021-01-02 17:37:35 | INFO | train_inner | epoch 073:    288 / 421 symm_mse=0.26, loss=2.991, nll_loss=0.821, ppl=1.77, wps=22571.8, ups=1.6, wpb=14107.1, bsz=514.2, num_updates=30600, lr=1.25245e-05, gnorm=0.64, train_wall=62, wall=20686
2021-01-02 17:38:38 | INFO | train_inner | epoch 073:    388 / 421 symm_mse=0.267, loss=3.021, nll_loss=0.842, ppl=1.79, wps=22516.2, ups=1.61, wpb=14021.4, bsz=489.6, num_updates=30700, lr=1.25041e-05, gnorm=0.652, train_wall=62, wall=20748
2021-01-02 17:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:39:15 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.8 | wps 6062.8 | wpb 10324.2 | bsz 375 | num_updates 30733 | best_bleu 23.03
2021-01-02 17:39:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:39:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 73 @ 30733 updates, score 22.8) (writing took 2.9160318840295076 seconds)
2021-01-02 17:39:17 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-02 17:39:17 | INFO | train | epoch 073 | symm_mse 0.266 | loss 3.012 | nll_loss 0.834 | ppl 1.78 | wps 20665.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 30733 | lr 1.24974e-05 | gnorm 0.651 | train_wall 261 | wall 20788
2021-01-02 17:39:17 | INFO | fairseq.trainer | begin training epoch 74
2021-01-02 17:39:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:39:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:40:02 | INFO | train_inner | epoch 074:     67 / 421 symm_mse=0.267, loss=3.019, nll_loss=0.839, ppl=1.79, wps=16549.4, ups=1.19, wpb=13947.4, bsz=487, num_updates=30800, lr=1.24838e-05, gnorm=0.654, train_wall=61, wall=20833
2021-01-02 17:41:04 | INFO | train_inner | epoch 074:    167 / 421 symm_mse=0.262, loss=2.999, nll_loss=0.826, ppl=1.77, wps=22564.7, ups=1.6, wpb=14112, bsz=503.8, num_updates=30900, lr=1.24635e-05, gnorm=0.643, train_wall=62, wall=20895
2021-01-02 17:42:07 | INFO | train_inner | epoch 074:    267 / 421 symm_mse=0.269, loss=3.025, nll_loss=0.843, ppl=1.79, wps=22449.1, ups=1.61, wpb=13972.7, bsz=481.2, num_updates=31000, lr=1.24434e-05, gnorm=0.657, train_wall=62, wall=20957
2021-01-02 17:43:09 | INFO | train_inner | epoch 074:    367 / 421 symm_mse=0.267, loss=3.016, nll_loss=0.837, ppl=1.79, wps=22187.7, ups=1.6, wpb=13875.6, bsz=488.2, num_updates=31100, lr=1.24234e-05, gnorm=0.655, train_wall=62, wall=21020
2021-01-02 17:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:43:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:43:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:43:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:43:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:43:59 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.847 | ppl 14.39 | bleu 22.81 | wps 5934.4 | wpb 10324.2 | bsz 375 | num_updates 31154 | best_bleu 23.03
2021-01-02 17:43:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:44:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 74 @ 31154 updates, score 22.81) (writing took 2.8932524286210537 seconds)
2021-01-02 17:44:02 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-02 17:44:02 | INFO | train | epoch 074 | symm_mse 0.265 | loss 3.012 | nll_loss 0.834 | ppl 1.78 | wps 20651.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 31154 | lr 1.24126e-05 | gnorm 0.651 | train_wall 261 | wall 21073
2021-01-02 17:44:02 | INFO | fairseq.trainer | begin training epoch 75
2021-01-02 17:44:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:44:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:44:34 | INFO | train_inner | epoch 075:     46 / 421 symm_mse=0.262, loss=3, nll_loss=0.826, ppl=1.77, wps=16411.8, ups=1.18, wpb=13850.8, bsz=498.8, num_updates=31200, lr=1.24035e-05, gnorm=0.651, train_wall=61, wall=21104
2021-01-02 17:45:37 | INFO | train_inner | epoch 075:    146 / 421 symm_mse=0.263, loss=2.999, nll_loss=0.824, ppl=1.77, wps=22441.9, ups=1.59, wpb=14149.4, bsz=504.3, num_updates=31300, lr=1.23836e-05, gnorm=0.645, train_wall=63, wall=21167
2021-01-02 17:46:40 | INFO | train_inner | epoch 075:    246 / 421 symm_mse=0.269, loss=3.021, nll_loss=0.839, ppl=1.79, wps=22255.5, ups=1.59, wpb=13988.6, bsz=487.1, num_updates=31400, lr=1.23639e-05, gnorm=0.659, train_wall=63, wall=21230
2021-01-02 17:47:42 | INFO | train_inner | epoch 075:    346 / 421 symm_mse=0.266, loss=3.013, nll_loss=0.835, ppl=1.78, wps=22176.4, ups=1.6, wpb=13842.7, bsz=481.1, num_updates=31500, lr=1.23443e-05, gnorm=0.655, train_wall=62, wall=21293
2021-01-02 17:48:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:48:45 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.846 | ppl 14.38 | bleu 22.88 | wps 6127.6 | wpb 10324.2 | bsz 375 | num_updates 31575 | best_bleu 23.03
2021-01-02 17:48:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:48:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 75 @ 31575 updates, score 22.88) (writing took 2.964317336678505 seconds)
2021-01-02 17:48:48 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-02 17:48:48 | INFO | train | epoch 075 | symm_mse 0.265 | loss 3.01 | nll_loss 0.833 | ppl 1.78 | wps 20594.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 31575 | lr 1.23296e-05 | gnorm 0.653 | train_wall 262 | wall 21358
2021-01-02 17:48:48 | INFO | fairseq.trainer | begin training epoch 76
2021-01-02 17:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:49:06 | INFO | train_inner | epoch 076:     25 / 421 symm_mse=0.263, loss=3.008, nll_loss=0.834, ppl=1.78, wps=16450.5, ups=1.18, wpb=13898.5, bsz=505.8, num_updates=31600, lr=1.23247e-05, gnorm=0.656, train_wall=62, wall=21377
2021-01-02 17:50:09 | INFO | train_inner | epoch 076:    125 / 421 symm_mse=0.264, loss=3.009, nll_loss=0.834, ppl=1.78, wps=22512.2, ups=1.6, wpb=14096.4, bsz=515.1, num_updates=31700, lr=1.23053e-05, gnorm=0.644, train_wall=62, wall=21440
2021-01-02 17:51:12 | INFO | train_inner | epoch 076:    225 / 421 symm_mse=0.265, loss=3.005, nll_loss=0.827, ppl=1.77, wps=22375.3, ups=1.6, wpb=13973.1, bsz=492.4, num_updates=31800, lr=1.22859e-05, gnorm=0.65, train_wall=62, wall=21502
2021-01-02 17:52:14 | INFO | train_inner | epoch 076:    325 / 421 symm_mse=0.268, loss=3.017, nll_loss=0.835, ppl=1.78, wps=22266.6, ups=1.59, wpb=13963, bsz=461.8, num_updates=31900, lr=1.22666e-05, gnorm=0.66, train_wall=63, wall=21565
2021-01-02 17:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:53:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:53:32 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.843 | ppl 14.35 | bleu 22.9 | wps 5487.2 | wpb 10324.2 | bsz 375 | num_updates 31996 | best_bleu 23.03
2021-01-02 17:53:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:53:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 76 @ 31996 updates, score 22.9) (writing took 2.952674938365817 seconds)
2021-01-02 17:53:35 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-02 17:53:35 | INFO | train | epoch 076 | symm_mse 0.265 | loss 3.01 | nll_loss 0.833 | ppl 1.78 | wps 20482.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 31996 | lr 1.22482e-05 | gnorm 0.651 | train_wall 262 | wall 21646
2021-01-02 17:53:35 | INFO | fairseq.trainer | begin training epoch 77
2021-01-02 17:53:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:53:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:53:41 | INFO | train_inner | epoch 077:      4 / 421 symm_mse=0.266, loss=3.018, nll_loss=0.841, ppl=1.79, wps=16041.2, ups=1.16, wpb=13844.7, bsz=485, num_updates=32000, lr=1.22474e-05, gnorm=0.656, train_wall=62, wall=21651
2021-01-02 17:54:42 | INFO | train_inner | epoch 077:    104 / 421 symm_mse=0.262, loss=2.998, nll_loss=0.825, ppl=1.77, wps=22643.1, ups=1.62, wpb=13985.2, bsz=500.6, num_updates=32100, lr=1.22284e-05, gnorm=0.644, train_wall=62, wall=21713
2021-01-02 17:55:45 | INFO | train_inner | epoch 077:    204 / 421 symm_mse=0.263, loss=3.004, nll_loss=0.829, ppl=1.78, wps=22403, ups=1.6, wpb=14043.8, bsz=486.1, num_updates=32200, lr=1.22094e-05, gnorm=0.645, train_wall=63, wall=21776
2021-01-02 17:56:47 | INFO | train_inner | epoch 077:    304 / 421 symm_mse=0.268, loss=3.019, nll_loss=0.837, ppl=1.79, wps=22265.7, ups=1.6, wpb=13904.1, bsz=480.2, num_updates=32300, lr=1.21904e-05, gnorm=0.659, train_wall=62, wall=21838
2021-01-02 17:57:50 | INFO | train_inner | epoch 077:    404 / 421 symm_mse=0.265, loss=3.017, nll_loss=0.841, ppl=1.79, wps=22440.5, ups=1.6, wpb=14050.1, bsz=507.9, num_updates=32400, lr=1.21716e-05, gnorm=0.648, train_wall=62, wall=21901
2021-01-02 17:58:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 17:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 17:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 17:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 17:58:17 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.842 | ppl 14.34 | bleu 22.83 | wps 6048.7 | wpb 10324.2 | bsz 375 | num_updates 32417 | best_bleu 23.03
2021-01-02 17:58:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 17:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:58:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 77 @ 32417 updates, score 22.83) (writing took 2.85778640024364 seconds)
2021-01-02 17:58:20 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-02 17:58:20 | INFO | train | epoch 077 | symm_mse 0.265 | loss 3.009 | nll_loss 0.832 | ppl 1.78 | wps 20632.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 32417 | lr 1.21684e-05 | gnorm 0.651 | train_wall 262 | wall 21931
2021-01-02 17:58:20 | INFO | fairseq.trainer | begin training epoch 78
2021-01-02 17:58:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 17:58:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 17:59:15 | INFO | train_inner | epoch 078:     83 / 421 symm_mse=0.266, loss=3.013, nll_loss=0.834, ppl=1.78, wps=16270, ups=1.18, wpb=13787.4, bsz=490.6, num_updates=32500, lr=1.21529e-05, gnorm=0.662, train_wall=62, wall=21985
2021-01-02 18:00:17 | INFO | train_inner | epoch 078:    183 / 421 symm_mse=0.266, loss=3.01, nll_loss=0.832, ppl=1.78, wps=22608.4, ups=1.6, wpb=14097.3, bsz=477.3, num_updates=32600, lr=1.21342e-05, gnorm=0.648, train_wall=62, wall=22048
2021-01-02 18:01:20 | INFO | train_inner | epoch 078:    283 / 421 symm_mse=0.261, loss=2.993, nll_loss=0.821, ppl=1.77, wps=22338.9, ups=1.6, wpb=13992.1, bsz=506.1, num_updates=32700, lr=1.21157e-05, gnorm=0.649, train_wall=62, wall=22110
2021-01-02 18:02:22 | INFO | train_inner | epoch 078:    383 / 421 symm_mse=0.264, loss=3.016, nll_loss=0.841, ppl=1.79, wps=22394.5, ups=1.6, wpb=14019.5, bsz=506.6, num_updates=32800, lr=1.20972e-05, gnorm=0.649, train_wall=62, wall=22173
2021-01-02 18:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:03:04 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.847 | ppl 14.39 | bleu 22.94 | wps 5477.8 | wpb 10324.2 | bsz 375 | num_updates 32838 | best_bleu 23.03
2021-01-02 18:03:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:03:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 78 @ 32838 updates, score 22.94) (writing took 2.8664490841329098 seconds)
2021-01-02 18:03:07 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-02 18:03:07 | INFO | train | epoch 078 | symm_mse 0.265 | loss 3.009 | nll_loss 0.832 | ppl 1.78 | wps 20501 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 32838 | lr 1.20902e-05 | gnorm 0.652 | train_wall 262 | wall 22217
2021-01-02 18:03:07 | INFO | fairseq.trainer | begin training epoch 79
2021-01-02 18:03:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:03:48 | INFO | train_inner | epoch 079:     62 / 421 symm_mse=0.266, loss=3.012, nll_loss=0.832, ppl=1.78, wps=16128.9, ups=1.17, wpb=13781.3, bsz=469.8, num_updates=32900, lr=1.20788e-05, gnorm=0.663, train_wall=61, wall=22258
2021-01-02 18:04:51 | INFO | train_inner | epoch 079:    162 / 421 symm_mse=0.263, loss=3.004, nll_loss=0.829, ppl=1.78, wps=22410.1, ups=1.59, wpb=14053.4, bsz=495.8, num_updates=33000, lr=1.20605e-05, gnorm=0.646, train_wall=63, wall=22321
2021-01-02 18:05:53 | INFO | train_inner | epoch 079:    262 / 421 symm_mse=0.263, loss=3.003, nll_loss=0.829, ppl=1.78, wps=22327.6, ups=1.59, wpb=14036.6, bsz=502, num_updates=33100, lr=1.20422e-05, gnorm=0.645, train_wall=63, wall=22384
2021-01-02 18:06:56 | INFO | train_inner | epoch 079:    362 / 421 symm_mse=0.266, loss=3.013, nll_loss=0.835, ppl=1.78, wps=22255.5, ups=1.6, wpb=13914.4, bsz=489, num_updates=33200, lr=1.20241e-05, gnorm=0.656, train_wall=62, wall=22447
2021-01-02 18:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:07:50 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.83 | wps 5853.9 | wpb 10324.2 | bsz 375 | num_updates 33259 | best_bleu 23.03
2021-01-02 18:07:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:07:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 79 @ 33259 updates, score 22.83) (writing took 2.898768501356244 seconds)
2021-01-02 18:07:53 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-02 18:07:53 | INFO | train | epoch 079 | symm_mse 0.264 | loss 3.008 | nll_loss 0.832 | ppl 1.78 | wps 20583.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 33259 | lr 1.20134e-05 | gnorm 0.652 | train_wall 262 | wall 22503
2021-01-02 18:07:53 | INFO | fairseq.trainer | begin training epoch 80
2021-01-02 18:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:08:21 | INFO | train_inner | epoch 080:     41 / 421 symm_mse=0.263, loss=3.006, nll_loss=0.832, ppl=1.78, wps=16363.1, ups=1.18, wpb=13895.2, bsz=496.2, num_updates=33300, lr=1.2006e-05, gnorm=0.654, train_wall=62, wall=22531
2021-01-02 18:09:23 | INFO | train_inner | epoch 080:    141 / 421 symm_mse=0.263, loss=3.004, nll_loss=0.829, ppl=1.78, wps=22416.9, ups=1.6, wpb=14015.3, bsz=502.4, num_updates=33400, lr=1.1988e-05, gnorm=0.646, train_wall=62, wall=22594
2021-01-02 18:10:26 | INFO | train_inner | epoch 080:    241 / 421 symm_mse=0.264, loss=3.011, nll_loss=0.836, ppl=1.78, wps=22528.7, ups=1.6, wpb=14112, bsz=494.1, num_updates=33500, lr=1.19701e-05, gnorm=0.648, train_wall=62, wall=22657
2021-01-02 18:11:29 | INFO | train_inner | epoch 080:    341 / 421 symm_mse=0.264, loss=3.006, nll_loss=0.831, ppl=1.78, wps=22303.6, ups=1.6, wpb=13952.8, bsz=476.2, num_updates=33600, lr=1.19523e-05, gnorm=0.65, train_wall=62, wall=22719
2021-01-02 18:12:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:12:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:12:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:12:35 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.92 | wps 5916.6 | wpb 10324.2 | bsz 375 | num_updates 33680 | best_bleu 23.03
2021-01-02 18:12:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:12:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 80 @ 33680 updates, score 22.92) (writing took 3.038428373634815 seconds)
2021-01-02 18:12:38 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-02 18:12:38 | INFO | train | epoch 080 | symm_mse 0.264 | loss 3.007 | nll_loss 0.832 | ppl 1.78 | wps 20574.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 33680 | lr 1.19381e-05 | gnorm 0.65 | train_wall 262 | wall 22789
2021-01-02 18:12:38 | INFO | fairseq.trainer | begin training epoch 81
2021-01-02 18:12:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:12:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:12:54 | INFO | train_inner | epoch 081:     20 / 421 symm_mse=0.264, loss=3.008, nll_loss=0.832, ppl=1.78, wps=16254.9, ups=1.17, wpb=13861.8, bsz=491.9, num_updates=33700, lr=1.19345e-05, gnorm=0.66, train_wall=62, wall=22804
2021-01-02 18:13:56 | INFO | train_inner | epoch 081:    120 / 421 symm_mse=0.262, loss=2.999, nll_loss=0.825, ppl=1.77, wps=22493.1, ups=1.61, wpb=13969.8, bsz=502, num_updates=33800, lr=1.19169e-05, gnorm=0.646, train_wall=62, wall=22867
2021-01-02 18:14:59 | INFO | train_inner | epoch 081:    220 / 421 symm_mse=0.265, loss=3.011, nll_loss=0.834, ppl=1.78, wps=22063.6, ups=1.59, wpb=13903.9, bsz=499.9, num_updates=33900, lr=1.18993e-05, gnorm=0.652, train_wall=63, wall=22930
2021-01-02 18:16:01 | INFO | train_inner | epoch 081:    320 / 421 symm_mse=0.263, loss=3.005, nll_loss=0.831, ppl=1.78, wps=22596.7, ups=1.6, wpb=14120.4, bsz=487, num_updates=34000, lr=1.18818e-05, gnorm=0.645, train_wall=62, wall=22992
2021-01-02 18:17:04 | INFO | train_inner | epoch 081:    420 / 421 symm_mse=0.264, loss=3.008, nll_loss=0.832, ppl=1.78, wps=22187.5, ups=1.59, wpb=13969.7, bsz=489.8, num_updates=34100, lr=1.18643e-05, gnorm=0.65, train_wall=63, wall=23055
2021-01-02 18:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:17:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:17:22 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.845 | ppl 14.37 | bleu 22.82 | wps 5906.9 | wpb 10324.2 | bsz 375 | num_updates 34101 | best_bleu 23.03
2021-01-02 18:17:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:17:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 81 @ 34101 updates, score 22.82) (writing took 2.9917130190879107 seconds)
2021-01-02 18:17:25 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-02 18:17:25 | INFO | train | epoch 081 | symm_mse 0.264 | loss 3.006 | nll_loss 0.831 | ppl 1.78 | wps 20530.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 34101 | lr 1.18642e-05 | gnorm 0.65 | train_wall 263 | wall 23075
2021-01-02 18:17:25 | INFO | fairseq.trainer | begin training epoch 82
2021-01-02 18:17:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:17:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:18:30 | INFO | train_inner | epoch 082:     99 / 421 symm_mse=0.266, loss=3.011, nll_loss=0.833, ppl=1.78, wps=16325.1, ups=1.17, wpb=13925.6, bsz=481.2, num_updates=34200, lr=1.1847e-05, gnorm=0.654, train_wall=62, wall=23140
2021-01-02 18:19:33 | INFO | train_inner | epoch 082:    199 / 421 symm_mse=0.264, loss=3.005, nll_loss=0.829, ppl=1.78, wps=22255.3, ups=1.59, wpb=13999.5, bsz=488, num_updates=34300, lr=1.18297e-05, gnorm=0.652, train_wall=63, wall=23203
2021-01-02 18:20:35 | INFO | train_inner | epoch 082:    299 / 421 symm_mse=0.263, loss=3.005, nll_loss=0.831, ppl=1.78, wps=22244.3, ups=1.59, wpb=13976.5, bsz=488.2, num_updates=34400, lr=1.18125e-05, gnorm=0.649, train_wall=63, wall=23266
2021-01-02 18:21:38 | INFO | train_inner | epoch 082:    399 / 421 symm_mse=0.26, loss=3, nll_loss=0.83, ppl=1.78, wps=22276.7, ups=1.6, wpb=13959.8, bsz=515.1, num_updates=34500, lr=1.17954e-05, gnorm=0.644, train_wall=62, wall=23329
2021-01-02 18:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:21:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:21:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:21:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:21:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:22:08 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.87 | wps 6067.9 | wpb 10324.2 | bsz 375 | num_updates 34522 | best_bleu 23.03
2021-01-02 18:22:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:22:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:22:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:22:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:22:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:22:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:22:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:22:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 82 @ 34522 updates, score 22.87) (writing took 2.9166038129478693 seconds)
2021-01-02 18:22:11 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-02 18:22:11 | INFO | train | epoch 082 | symm_mse 0.263 | loss 3.005 | nll_loss 0.83 | ppl 1.78 | wps 20549.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 34522 | lr 1.17916e-05 | gnorm 0.649 | train_wall 263 | wall 23362
2021-01-02 18:22:11 | INFO | fairseq.trainer | begin training epoch 83
2021-01-02 18:22:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:22:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:23:02 | INFO | train_inner | epoch 083:     78 / 421 symm_mse=0.265, loss=3.012, nll_loss=0.834, ppl=1.78, wps=16455.7, ups=1.19, wpb=13862, bsz=480.6, num_updates=34600, lr=1.17783e-05, gnorm=0.656, train_wall=61, wall=23413
2021-01-02 18:24:05 | INFO | train_inner | epoch 083:    178 / 421 symm_mse=0.264, loss=3.008, nll_loss=0.833, ppl=1.78, wps=22334.8, ups=1.6, wpb=13916.6, bsz=488.6, num_updates=34700, lr=1.17613e-05, gnorm=0.65, train_wall=62, wall=23475
2021-01-02 18:25:07 | INFO | train_inner | epoch 083:    278 / 421 symm_mse=0.264, loss=3.006, nll_loss=0.83, ppl=1.78, wps=22257.9, ups=1.59, wpb=13982, bsz=495, num_updates=34800, lr=1.17444e-05, gnorm=0.647, train_wall=63, wall=23538
2021-01-02 18:26:10 | INFO | train_inner | epoch 083:    378 / 421 symm_mse=0.262, loss=3, nll_loss=0.827, ppl=1.77, wps=22502.3, ups=1.6, wpb=14049.8, bsz=508, num_updates=34900, lr=1.17276e-05, gnorm=0.647, train_wall=62, wall=23601
2021-01-02 18:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:26:54 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.81 | wps 5895.6 | wpb 10324.2 | bsz 375 | num_updates 34943 | best_bleu 23.03
2021-01-02 18:26:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 83 @ 34943 updates, score 22.81) (writing took 2.902152970433235 seconds)
2021-01-02 18:26:57 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-02 18:26:57 | INFO | train | epoch 083 | symm_mse 0.263 | loss 3.005 | nll_loss 0.83 | ppl 1.78 | wps 20593.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 34943 | lr 1.17203e-05 | gnorm 0.65 | train_wall 262 | wall 23647
2021-01-02 18:26:57 | INFO | fairseq.trainer | begin training epoch 84
2021-01-02 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:26:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:26:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:27:35 | INFO | train_inner | epoch 084:     57 / 421 symm_mse=0.262, loss=2.998, nll_loss=0.823, ppl=1.77, wps=16345, ups=1.17, wpb=13975, bsz=481.6, num_updates=35000, lr=1.17108e-05, gnorm=0.652, train_wall=62, wall=23686
2021-01-02 18:28:38 | INFO | train_inner | epoch 084:    157 / 421 symm_mse=0.263, loss=3.004, nll_loss=0.83, ppl=1.78, wps=22140.9, ups=1.59, wpb=13918.9, bsz=487.9, num_updates=35100, lr=1.16941e-05, gnorm=0.65, train_wall=63, wall=23749
2021-01-02 18:29:41 | INFO | train_inner | epoch 084:    257 / 421 symm_mse=0.263, loss=3.001, nll_loss=0.826, ppl=1.77, wps=22336.2, ups=1.6, wpb=13950.9, bsz=496.1, num_updates=35200, lr=1.16775e-05, gnorm=0.65, train_wall=62, wall=23811
2021-01-02 18:30:43 | INFO | train_inner | epoch 084:    357 / 421 symm_mse=0.264, loss=3.008, nll_loss=0.833, ppl=1.78, wps=22613.2, ups=1.61, wpb=14065.2, bsz=497.5, num_updates=35300, lr=1.16609e-05, gnorm=0.651, train_wall=62, wall=23874
2021-01-02 18:31:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:31:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:31:40 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.75 | wps 6055.7 | wpb 10324.2 | bsz 375 | num_updates 35364 | best_bleu 23.03
2021-01-02 18:31:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:31:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 84 @ 35364 updates, score 22.75) (writing took 2.909515380859375 seconds)
2021-01-02 18:31:43 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-02 18:31:43 | INFO | train | epoch 084 | symm_mse 0.263 | loss 3.003 | nll_loss 0.829 | ppl 1.78 | wps 20546.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 35364 | lr 1.16504e-05 | gnorm 0.651 | train_wall 263 | wall 23934
2021-01-02 18:31:43 | INFO | fairseq.trainer | begin training epoch 85
2021-01-02 18:31:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:31:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:32:08 | INFO | train_inner | epoch 085:     36 / 421 symm_mse=0.257, loss=2.985, nll_loss=0.818, ppl=1.76, wps=16373.8, ups=1.17, wpb=13976.4, bsz=506.2, num_updates=35400, lr=1.16445e-05, gnorm=0.644, train_wall=62, wall=23959
2021-01-02 18:33:10 | INFO | train_inner | epoch 085:    136 / 421 symm_mse=0.26, loss=2.994, nll_loss=0.822, ppl=1.77, wps=22501, ups=1.61, wpb=13980.4, bsz=498.1, num_updates=35500, lr=1.1628e-05, gnorm=0.644, train_wall=62, wall=24021
2021-01-02 18:34:13 | INFO | train_inner | epoch 085:    236 / 421 symm_mse=0.262, loss=3.003, nll_loss=0.83, ppl=1.78, wps=22401.8, ups=1.59, wpb=14089.2, bsz=493.7, num_updates=35600, lr=1.16117e-05, gnorm=0.652, train_wall=63, wall=24084
2021-01-02 18:35:16 | INFO | train_inner | epoch 085:    336 / 421 symm_mse=0.265, loss=3.014, nll_loss=0.838, ppl=1.79, wps=22353.4, ups=1.61, wpb=13923.5, bsz=483.7, num_updates=35700, lr=1.15954e-05, gnorm=0.655, train_wall=62, wall=24146
2021-01-02 18:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:36:27 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.96 | wps 5246.5 | wpb 10324.2 | bsz 375 | num_updates 35785 | best_bleu 23.03
2021-01-02 18:36:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 85 @ 35785 updates, score 22.96) (writing took 2.9509058874100447 seconds)
2021-01-02 18:36:30 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-02 18:36:30 | INFO | train | epoch 085 | symm_mse 0.263 | loss 3.004 | nll_loss 0.829 | ppl 1.78 | wps 20503.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 35785 | lr 1.15816e-05 | gnorm 0.651 | train_wall 261 | wall 24220
2021-01-02 18:36:30 | INFO | fairseq.trainer | begin training epoch 86
2021-01-02 18:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:36:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:36:42 | INFO | train_inner | epoch 086:     15 / 421 symm_mse=0.267, loss=3.017, nll_loss=0.836, ppl=1.78, wps=16009.7, ups=1.15, wpb=13875.6, bsz=480.7, num_updates=35800, lr=1.15792e-05, gnorm=0.663, train_wall=62, wall=24233
2021-01-02 18:37:44 | INFO | train_inner | epoch 086:    115 / 421 symm_mse=0.26, loss=2.991, nll_loss=0.819, ppl=1.76, wps=22627, ups=1.62, wpb=13984.8, bsz=487.7, num_updates=35900, lr=1.15631e-05, gnorm=0.646, train_wall=62, wall=24295
2021-01-02 18:38:47 | INFO | train_inner | epoch 086:    215 / 421 symm_mse=0.261, loss=3.001, nll_loss=0.829, ppl=1.78, wps=22258.2, ups=1.6, wpb=13938.5, bsz=498.3, num_updates=36000, lr=1.1547e-05, gnorm=0.642, train_wall=62, wall=24357
2021-01-02 18:39:49 | INFO | train_inner | epoch 086:    315 / 421 symm_mse=0.266, loss=3.018, nll_loss=0.84, ppl=1.79, wps=22335.7, ups=1.6, wpb=14000.8, bsz=477.6, num_updates=36100, lr=1.1531e-05, gnorm=0.651, train_wall=62, wall=24420
2021-01-02 18:40:52 | INFO | train_inner | epoch 086:    415 / 421 symm_mse=0.261, loss=2.999, nll_loss=0.827, ppl=1.77, wps=22231.8, ups=1.59, wpb=14012.9, bsz=511.5, num_updates=36200, lr=1.15151e-05, gnorm=0.643, train_wall=63, wall=24483
2021-01-02 18:40:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:40:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:40:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:40:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:40:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:40:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:40:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:40:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:40:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:41:13 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.847 | ppl 14.39 | bleu 22.86 | wps 6086.3 | wpb 10324.2 | bsz 375 | num_updates 36206 | best_bleu 23.03
2021-01-02 18:41:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:41:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:41:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:41:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:41:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:41:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:41:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:41:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 86 @ 36206 updates, score 22.86) (writing took 2.9371776171028614 seconds)
2021-01-02 18:41:15 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-02 18:41:15 | INFO | train | epoch 086 | symm_mse 0.262 | loss 3.002 | nll_loss 0.829 | ppl 1.78 | wps 20581 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 36206 | lr 1.15141e-05 | gnorm 0.649 | train_wall 262 | wall 24506
2021-01-02 18:41:15 | INFO | fairseq.trainer | begin training epoch 87
2021-01-02 18:41:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:42:17 | INFO | train_inner | epoch 087:     94 / 421 symm_mse=0.262, loss=2.998, nll_loss=0.824, ppl=1.77, wps=16372, ups=1.19, wpb=13761.8, bsz=487.9, num_updates=36300, lr=1.14992e-05, gnorm=0.663, train_wall=61, wall=24567
2021-01-02 18:43:19 | INFO | train_inner | epoch 087:    194 / 421 symm_mse=0.262, loss=2.995, nll_loss=0.821, ppl=1.77, wps=22456, ups=1.6, wpb=14010.6, bsz=480, num_updates=36400, lr=1.14834e-05, gnorm=0.648, train_wall=62, wall=24630
2021-01-02 18:44:22 | INFO | train_inner | epoch 087:    294 / 421 symm_mse=0.262, loss=3, nll_loss=0.827, ppl=1.77, wps=22497.5, ups=1.59, wpb=14131.5, bsz=494.4, num_updates=36500, lr=1.14676e-05, gnorm=0.647, train_wall=63, wall=24692
2021-01-02 18:45:24 | INFO | train_inner | epoch 087:    394 / 421 symm_mse=0.263, loss=3.012, nll_loss=0.838, ppl=1.79, wps=22309.5, ups=1.6, wpb=13941.3, bsz=509.6, num_updates=36600, lr=1.1452e-05, gnorm=0.65, train_wall=62, wall=24755
2021-01-02 18:45:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:45:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:45:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:45:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:45:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:45:58 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.89 | wps 6026.8 | wpb 10324.2 | bsz 375 | num_updates 36627 | best_bleu 23.03
2021-01-02 18:45:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:45:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:45:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:46:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:46:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:46:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:46:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 87 @ 36627 updates, score 22.89) (writing took 2.8880598321557045 seconds)
2021-01-02 18:46:00 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-02 18:46:00 | INFO | train | epoch 087 | symm_mse 0.262 | loss 3.002 | nll_loss 0.828 | ppl 1.78 | wps 20636.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 36627 | lr 1.14477e-05 | gnorm 0.65 | train_wall 261 | wall 24791
2021-01-02 18:46:00 | INFO | fairseq.trainer | begin training epoch 88
2021-01-02 18:46:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:46:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:46:49 | INFO | train_inner | epoch 088:     73 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.83, ppl=1.78, wps=16485.4, ups=1.18, wpb=13922, bsz=483, num_updates=36700, lr=1.14364e-05, gnorm=0.655, train_wall=62, wall=24839
2021-01-02 18:47:51 | INFO | train_inner | epoch 088:    173 / 421 symm_mse=0.26, loss=2.988, nll_loss=0.817, ppl=1.76, wps=22023.7, ups=1.59, wpb=13837.3, bsz=483.8, num_updates=36800, lr=1.14208e-05, gnorm=0.645, train_wall=63, wall=24902
2021-01-02 18:48:54 | INFO | train_inner | epoch 088:    273 / 421 symm_mse=0.263, loss=3.01, nll_loss=0.836, ppl=1.79, wps=22475.3, ups=1.59, wpb=14095, bsz=506.8, num_updates=36900, lr=1.14053e-05, gnorm=0.649, train_wall=63, wall=24965
2021-01-02 18:49:57 | INFO | train_inner | epoch 088:    373 / 421 symm_mse=0.264, loss=3.007, nll_loss=0.831, ppl=1.78, wps=22191.7, ups=1.59, wpb=13973.8, bsz=490.4, num_updates=37000, lr=1.13899e-05, gnorm=0.652, train_wall=63, wall=25028
2021-01-02 18:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:50:44 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.844 | ppl 14.36 | bleu 22.9 | wps 5983.3 | wpb 10324.2 | bsz 375 | num_updates 37048 | best_bleu 23.03
2021-01-02 18:50:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:50:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:50:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 88 @ 37048 updates, score 22.9) (writing took 2.9687583930790424 seconds)
2021-01-02 18:50:47 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-02 18:50:47 | INFO | train | epoch 088 | symm_mse 0.262 | loss 3.001 | nll_loss 0.828 | ppl 1.78 | wps 20546.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 37048 | lr 1.13825e-05 | gnorm 0.649 | train_wall 263 | wall 25077
2021-01-02 18:50:47 | INFO | fairseq.trainer | begin training epoch 89
2021-01-02 18:50:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:51:22 | INFO | train_inner | epoch 089:     52 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.824, ppl=1.77, wps=16508.1, ups=1.18, wpb=13986.3, bsz=500.3, num_updates=37100, lr=1.13745e-05, gnorm=0.645, train_wall=62, wall=25113
2021-01-02 18:52:25 | INFO | train_inner | epoch 089:    152 / 421 symm_mse=0.259, loss=2.989, nll_loss=0.818, ppl=1.76, wps=22232.2, ups=1.6, wpb=13927.9, bsz=491.8, num_updates=37200, lr=1.13592e-05, gnorm=0.644, train_wall=62, wall=25175
2021-01-02 18:53:27 | INFO | train_inner | epoch 089:    252 / 421 symm_mse=0.264, loss=3.012, nll_loss=0.836, ppl=1.79, wps=22501, ups=1.61, wpb=13999.4, bsz=495.4, num_updates=37300, lr=1.1344e-05, gnorm=0.652, train_wall=62, wall=25237
2021-01-02 18:54:29 | INFO | train_inner | epoch 089:    352 / 421 symm_mse=0.264, loss=3.005, nll_loss=0.827, ppl=1.77, wps=22506, ups=1.6, wpb=14106.2, bsz=471, num_updates=37400, lr=1.13288e-05, gnorm=0.649, train_wall=62, wall=25300
2021-01-02 18:55:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:55:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 18:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 18:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 18:55:29 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.846 | ppl 14.38 | bleu 22.94 | wps 6043.5 | wpb 10324.2 | bsz 375 | num_updates 37469 | best_bleu 23.03
2021-01-02 18:55:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 18:55:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 89 @ 37469 updates, score 22.94) (writing took 2.8963324166834354 seconds)
2021-01-02 18:55:31 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-02 18:55:31 | INFO | train | epoch 089 | symm_mse 0.262 | loss 3.001 | nll_loss 0.828 | ppl 1.77 | wps 20657.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 37469 | lr 1.13184e-05 | gnorm 0.649 | train_wall 261 | wall 25362
2021-01-02 18:55:31 | INFO | fairseq.trainer | begin training epoch 90
2021-01-02 18:55:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 18:55:54 | INFO | train_inner | epoch 090:     31 / 421 symm_mse=0.262, loss=3, nll_loss=0.827, ppl=1.77, wps=16416.3, ups=1.19, wpb=13829.1, bsz=499.7, num_updates=37500, lr=1.13137e-05, gnorm=0.656, train_wall=61, wall=25384
2021-01-02 18:56:56 | INFO | train_inner | epoch 090:    131 / 421 symm_mse=0.262, loss=2.996, nll_loss=0.822, ppl=1.77, wps=22537.2, ups=1.61, wpb=13971.9, bsz=468, num_updates=37600, lr=1.12987e-05, gnorm=0.649, train_wall=62, wall=25446
2021-01-02 18:57:58 | INFO | train_inner | epoch 090:    231 / 421 symm_mse=0.26, loss=2.997, nll_loss=0.827, ppl=1.77, wps=22531.5, ups=1.6, wpb=14079.9, bsz=510.4, num_updates=37700, lr=1.12837e-05, gnorm=0.639, train_wall=62, wall=25509
2021-01-02 18:59:01 | INFO | train_inner | epoch 090:    331 / 421 symm_mse=0.264, loss=3.009, nll_loss=0.833, ppl=1.78, wps=22044.6, ups=1.6, wpb=13773.4, bsz=492.6, num_updates=37800, lr=1.12687e-05, gnorm=0.658, train_wall=62, wall=25571
2021-01-02 18:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 18:59:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:59:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:59:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 18:59:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:00:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:00:14 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.98 | wps 6064.5 | wpb 10324.2 | bsz 375 | num_updates 37890 | best_bleu 23.03
2021-01-02 19:00:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 90 @ 37890 updates, score 22.98) (writing took 2.885457795113325 seconds)
2021-01-02 19:00:16 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-02 19:00:16 | INFO | train | epoch 090 | symm_mse 0.262 | loss 3 | nll_loss 0.827 | ppl 1.77 | wps 20631.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 37890 | lr 1.12553e-05 | gnorm 0.647 | train_wall 261 | wall 25647
2021-01-02 19:00:16 | INFO | fairseq.trainer | begin training epoch 91
2021-01-02 19:00:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:00:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:00:26 | INFO | train_inner | epoch 091:     10 / 421 symm_mse=0.26, loss=2.998, nll_loss=0.829, ppl=1.78, wps=16462.1, ups=1.17, wpb=14051.5, bsz=501.5, num_updates=37900, lr=1.12538e-05, gnorm=0.642, train_wall=62, wall=25657
2021-01-02 19:01:28 | INFO | train_inner | epoch 091:    110 / 421 symm_mse=0.261, loss=3, nll_loss=0.828, ppl=1.78, wps=22559.6, ups=1.62, wpb=13964.4, bsz=498.9, num_updates=38000, lr=1.1239e-05, gnorm=0.648, train_wall=62, wall=25719
2021-01-02 19:02:30 | INFO | train_inner | epoch 091:    210 / 421 symm_mse=0.263, loss=3.009, nll_loss=0.834, ppl=1.78, wps=22307.1, ups=1.61, wpb=13883.8, bsz=503.3, num_updates=38100, lr=1.12243e-05, gnorm=0.656, train_wall=62, wall=25781
2021-01-02 19:03:33 | INFO | train_inner | epoch 091:    310 / 421 symm_mse=0.26, loss=2.99, nll_loss=0.818, ppl=1.76, wps=22448.5, ups=1.59, wpb=14130.5, bsz=481.8, num_updates=38200, lr=1.12096e-05, gnorm=0.645, train_wall=63, wall=25844
2021-01-02 19:04:35 | INFO | train_inner | epoch 091:    410 / 421 symm_mse=0.262, loss=3.002, nll_loss=0.83, ppl=1.78, wps=22530.3, ups=1.61, wpb=13954.4, bsz=488, num_updates=38300, lr=1.11949e-05, gnorm=0.65, train_wall=62, wall=25906
2021-01-02 19:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:04:58 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.845 | ppl 14.37 | bleu 22.98 | wps 6010.6 | wpb 10324.2 | bsz 375 | num_updates 38311 | best_bleu 23.03
2021-01-02 19:04:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:04:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:04:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:05:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:05:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:05:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:05:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 91 @ 38311 updates, score 22.98) (writing took 2.951747788116336 seconds)
2021-01-02 19:05:01 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-02 19:05:01 | INFO | train | epoch 091 | symm_mse 0.262 | loss 3 | nll_loss 0.828 | ppl 1.77 | wps 20644.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 38311 | lr 1.11933e-05 | gnorm 0.65 | train_wall 261 | wall 25932
2021-01-02 19:05:01 | INFO | fairseq.trainer | begin training epoch 92
2021-01-02 19:05:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:05:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:05:59 | INFO | train_inner | epoch 092:     89 / 421 symm_mse=0.261, loss=2.996, nll_loss=0.823, ppl=1.77, wps=16421.6, ups=1.18, wpb=13872.5, bsz=487.1, num_updates=38400, lr=1.11803e-05, gnorm=0.653, train_wall=61, wall=25990
2021-01-02 19:07:01 | INFO | train_inner | epoch 092:    189 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.83, ppl=1.78, wps=22529.1, ups=1.61, wpb=13956.8, bsz=481.9, num_updates=38500, lr=1.11658e-05, gnorm=0.646, train_wall=62, wall=26052
2021-01-02 19:08:04 | INFO | train_inner | epoch 092:    289 / 421 symm_mse=0.26, loss=2.998, nll_loss=0.826, ppl=1.77, wps=22417.1, ups=1.59, wpb=14079.7, bsz=508.1, num_updates=38600, lr=1.11513e-05, gnorm=0.643, train_wall=63, wall=26115
2021-01-02 19:09:06 | INFO | train_inner | epoch 092:    389 / 421 symm_mse=0.261, loss=3, nll_loss=0.829, ppl=1.78, wps=22448.9, ups=1.61, wpb=13919.4, bsz=495.5, num_updates=38700, lr=1.11369e-05, gnorm=0.649, train_wall=62, wall=26177
2021-01-02 19:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:09:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:09:43 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.849 | ppl 14.41 | bleu 22.83 | wps 6122.5 | wpb 10324.2 | bsz 375 | num_updates 38732 | best_bleu 23.03
2021-01-02 19:09:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:09:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:09:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 92 @ 38732 updates, score 22.83) (writing took 2.944689478725195 seconds)
2021-01-02 19:09:46 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-02 19:09:46 | INFO | train | epoch 092 | symm_mse 0.261 | loss 2.999 | nll_loss 0.827 | ppl 1.77 | wps 20687.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 38732 | lr 1.11323e-05 | gnorm 0.648 | train_wall 261 | wall 26216
2021-01-02 19:09:46 | INFO | fairseq.trainer | begin training epoch 93
2021-01-02 19:09:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:09:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:10:31 | INFO | train_inner | epoch 093:     68 / 421 symm_mse=0.259, loss=2.989, nll_loss=0.819, ppl=1.76, wps=16437.4, ups=1.18, wpb=13931.8, bsz=499.8, num_updates=38800, lr=1.11226e-05, gnorm=0.648, train_wall=62, wall=26262
2021-01-02 19:11:33 | INFO | train_inner | epoch 093:    168 / 421 symm_mse=0.26, loss=2.994, nll_loss=0.822, ppl=1.77, wps=22234.2, ups=1.6, wpb=13886.1, bsz=487.4, num_updates=38900, lr=1.11083e-05, gnorm=0.653, train_wall=62, wall=26324
2021-01-02 19:12:36 | INFO | train_inner | epoch 093:    268 / 421 symm_mse=0.264, loss=3.01, nll_loss=0.833, ppl=1.78, wps=22239.1, ups=1.61, wpb=13850.6, bsz=485.8, num_updates=39000, lr=1.1094e-05, gnorm=0.656, train_wall=62, wall=26386
2021-01-02 19:13:38 | INFO | train_inner | epoch 093:    368 / 421 symm_mse=0.261, loss=2.996, nll_loss=0.824, ppl=1.77, wps=22918.6, ups=1.61, wpb=14222.5, bsz=489.6, num_updates=39100, lr=1.10798e-05, gnorm=0.64, train_wall=62, wall=26448
2021-01-02 19:14:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:14:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:14:27 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.9 | wps 6045.3 | wpb 10324.2 | bsz 375 | num_updates 39153 | best_bleu 23.03
2021-01-02 19:14:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:14:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 93 @ 39153 updates, score 22.9) (writing took 2.9355667158961296 seconds)
2021-01-02 19:14:30 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-02 19:14:30 | INFO | train | epoch 093 | symm_mse 0.261 | loss 2.998 | nll_loss 0.826 | ppl 1.77 | wps 20654.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 39153 | lr 1.10723e-05 | gnorm 0.649 | train_wall 261 | wall 26501
2021-01-02 19:14:30 | INFO | fairseq.trainer | begin training epoch 94
2021-01-02 19:14:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:14:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:15:02 | INFO | train_inner | epoch 094:     47 / 421 symm_mse=0.259, loss=2.991, nll_loss=0.823, ppl=1.77, wps=16549.8, ups=1.18, wpb=13992.9, bsz=499.8, num_updates=39200, lr=1.10657e-05, gnorm=0.649, train_wall=62, wall=26533
2021-01-02 19:16:05 | INFO | train_inner | epoch 094:    147 / 421 symm_mse=0.26, loss=2.993, nll_loss=0.822, ppl=1.77, wps=22405.3, ups=1.6, wpb=14008.8, bsz=483.8, num_updates=39300, lr=1.10516e-05, gnorm=0.641, train_wall=62, wall=26596
2021-01-02 19:17:07 | INFO | train_inner | epoch 094:    247 / 421 symm_mse=0.262, loss=3.001, nll_loss=0.828, ppl=1.78, wps=22619.8, ups=1.61, wpb=14073.1, bsz=497.6, num_updates=39400, lr=1.10375e-05, gnorm=0.646, train_wall=62, wall=26658
2021-01-02 19:18:09 | INFO | train_inner | epoch 094:    347 / 421 symm_mse=0.265, loss=3.016, nll_loss=0.841, ppl=1.79, wps=22294.1, ups=1.62, wpb=13782.1, bsz=478.6, num_updates=39500, lr=1.10236e-05, gnorm=0.655, train_wall=62, wall=26720
2021-01-02 19:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:18:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:18:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:18:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:18:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:18:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:18:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:18:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:18:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:19:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:19:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:19:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:19:12 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.94 | wps 6016.8 | wpb 10324.2 | bsz 375 | num_updates 39574 | best_bleu 23.03
2021-01-02 19:19:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:19:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:19:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:19:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:19:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:19:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 94 @ 39574 updates, score 22.94) (writing took 2.959486696869135 seconds)
2021-01-02 19:19:15 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-02 19:19:15 | INFO | train | epoch 094 | symm_mse 0.261 | loss 2.998 | nll_loss 0.826 | ppl 1.77 | wps 20679.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 39574 | lr 1.10133e-05 | gnorm 0.648 | train_wall 261 | wall 26785
2021-01-02 19:19:15 | INFO | fairseq.trainer | begin training epoch 95
2021-01-02 19:19:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:19:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:19:34 | INFO | train_inner | epoch 095:     26 / 421 symm_mse=0.258, loss=2.989, nll_loss=0.82, ppl=1.77, wps=16454.1, ups=1.18, wpb=13986.8, bsz=521.4, num_updates=39600, lr=1.10096e-05, gnorm=0.647, train_wall=62, wall=26805
2021-01-02 19:20:36 | INFO | train_inner | epoch 095:    126 / 421 symm_mse=0.264, loss=3.005, nll_loss=0.829, ppl=1.78, wps=22394.3, ups=1.62, wpb=13861.6, bsz=476.5, num_updates=39700, lr=1.09958e-05, gnorm=0.652, train_wall=62, wall=26866
2021-01-02 19:21:38 | INFO | train_inner | epoch 095:    226 / 421 symm_mse=0.26, loss=2.99, nll_loss=0.818, ppl=1.76, wps=22276.6, ups=1.6, wpb=13904.9, bsz=482.6, num_updates=39800, lr=1.09819e-05, gnorm=0.647, train_wall=62, wall=26929
2021-01-02 19:22:40 | INFO | train_inner | epoch 095:    326 / 421 symm_mse=0.258, loss=2.994, nll_loss=0.827, ppl=1.77, wps=22783.3, ups=1.61, wpb=14181.4, bsz=509.4, num_updates=39900, lr=1.09682e-05, gnorm=0.639, train_wall=62, wall=26991
2021-01-02 19:23:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:23:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:23:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:23:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:23:58 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.91 | wps 5504.2 | wpb 10324.2 | bsz 375 | num_updates 39995 | best_bleu 23.03
2021-01-02 19:23:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:23:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:24:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 95 @ 39995 updates, score 22.91) (writing took 2.959510412067175 seconds)
2021-01-02 19:24:01 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-02 19:24:01 | INFO | train | epoch 095 | symm_mse 0.261 | loss 2.997 | nll_loss 0.826 | ppl 1.77 | wps 20576.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 39995 | lr 1.09551e-05 | gnorm 0.648 | train_wall 261 | wall 27071
2021-01-02 19:24:01 | INFO | fairseq.trainer | begin training epoch 96
2021-01-02 19:24:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:24:07 | INFO | train_inner | epoch 096:      5 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.831, ppl=1.78, wps=16054.7, ups=1.16, wpb=13880.1, bsz=488.7, num_updates=40000, lr=1.09545e-05, gnorm=0.655, train_wall=62, wall=27078
2021-01-02 19:25:09 | INFO | train_inner | epoch 096:    105 / 421 symm_mse=0.258, loss=2.987, nll_loss=0.818, ppl=1.76, wps=22919.7, ups=1.62, wpb=14175.6, bsz=499, num_updates=40100, lr=1.09408e-05, gnorm=0.633, train_wall=62, wall=27139
2021-01-02 19:26:11 | INFO | train_inner | epoch 096:    205 / 421 symm_mse=0.261, loss=3, nll_loss=0.828, ppl=1.78, wps=22148.1, ups=1.6, wpb=13801.8, bsz=502.2, num_updates=40200, lr=1.09272e-05, gnorm=0.65, train_wall=62, wall=27202
2021-01-02 19:27:13 | INFO | train_inner | epoch 096:    305 / 421 symm_mse=0.263, loss=3.006, nll_loss=0.832, ppl=1.78, wps=22561.5, ups=1.61, wpb=14033.4, bsz=490.9, num_updates=40300, lr=1.09136e-05, gnorm=0.649, train_wall=62, wall=27264
2021-01-02 19:28:16 | INFO | train_inner | epoch 096:    405 / 421 symm_mse=0.261, loss=2.997, nll_loss=0.825, ppl=1.77, wps=22386.3, ups=1.61, wpb=13939.7, bsz=479.9, num_updates=40400, lr=1.09001e-05, gnorm=0.653, train_wall=62, wall=27326
2021-01-02 19:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:28:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:28:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:28:42 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.847 | ppl 14.39 | bleu 22.81 | wps 5885.5 | wpb 10324.2 | bsz 375 | num_updates 40416 | best_bleu 23.03
2021-01-02 19:28:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:28:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 96 @ 40416 updates, score 22.81) (writing took 3.0240187495946884 seconds)
2021-01-02 19:28:45 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-02 19:28:45 | INFO | train | epoch 096 | symm_mse 0.261 | loss 2.997 | nll_loss 0.825 | ppl 1.77 | wps 20652.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 40416 | lr 1.08979e-05 | gnorm 0.647 | train_wall 261 | wall 27356
2021-01-02 19:28:45 | INFO | fairseq.trainer | begin training epoch 97
2021-01-02 19:28:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:28:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:29:40 | INFO | train_inner | epoch 097:     84 / 421 symm_mse=0.259, loss=2.997, nll_loss=0.828, ppl=1.77, wps=16610.8, ups=1.18, wpb=14091.7, bsz=512.2, num_updates=40500, lr=1.08866e-05, gnorm=0.637, train_wall=61, wall=27411
2021-01-02 19:30:42 | INFO | train_inner | epoch 097:    184 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.83, ppl=1.78, wps=22267.8, ups=1.61, wpb=13802.1, bsz=472.5, num_updates=40600, lr=1.08732e-05, gnorm=0.656, train_wall=62, wall=27473
2021-01-02 19:31:45 | INFO | train_inner | epoch 097:    284 / 421 symm_mse=0.259, loss=2.992, nll_loss=0.823, ppl=1.77, wps=22582.1, ups=1.61, wpb=14048.6, bsz=500.6, num_updates=40700, lr=1.08598e-05, gnorm=0.643, train_wall=62, wall=27535
2021-01-02 19:32:47 | INFO | train_inner | epoch 097:    384 / 421 symm_mse=0.26, loss=2.992, nll_loss=0.822, ppl=1.77, wps=22438, ups=1.6, wpb=13995.7, bsz=497.8, num_updates=40800, lr=1.08465e-05, gnorm=0.648, train_wall=62, wall=27598
2021-01-02 19:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:33:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:33:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:33:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:33:28 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.87 | wps 5796.3 | wpb 10324.2 | bsz 375 | num_updates 40837 | best_bleu 23.03
2021-01-02 19:33:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:33:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 97 @ 40837 updates, score 22.87) (writing took 2.9584032632410526 seconds)
2021-01-02 19:33:31 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-02 19:33:31 | INFO | train | epoch 097 | symm_mse 0.26 | loss 2.996 | nll_loss 0.825 | ppl 1.77 | wps 20625.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 40837 | lr 1.08416e-05 | gnorm 0.647 | train_wall 261 | wall 27641
2021-01-02 19:33:31 | INFO | fairseq.trainer | begin training epoch 98
2021-01-02 19:33:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:33:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:34:13 | INFO | train_inner | epoch 098:     63 / 421 symm_mse=0.257, loss=2.982, nll_loss=0.814, ppl=1.76, wps=16113.5, ups=1.16, wpb=13843.1, bsz=482.2, num_updates=40900, lr=1.08333e-05, gnorm=0.648, train_wall=62, wall=27684
2021-01-02 19:35:15 | INFO | train_inner | epoch 098:    163 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.823, ppl=1.77, wps=22409.4, ups=1.61, wpb=13929.2, bsz=503.2, num_updates=41000, lr=1.082e-05, gnorm=0.652, train_wall=62, wall=27746
2021-01-02 19:36:17 | INFO | train_inner | epoch 098:    263 / 421 symm_mse=0.264, loss=3.013, nll_loss=0.838, ppl=1.79, wps=22361.3, ups=1.6, wpb=13939.5, bsz=483, num_updates=41100, lr=1.08069e-05, gnorm=0.648, train_wall=62, wall=27808
2021-01-02 19:37:20 | INFO | train_inner | epoch 098:    363 / 421 symm_mse=0.259, loss=2.988, nll_loss=0.819, ppl=1.76, wps=22325.6, ups=1.59, wpb=14068.5, bsz=492.2, num_updates=41200, lr=1.07937e-05, gnorm=0.645, train_wall=63, wall=27871
2021-01-02 19:37:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:38:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:38:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:38:13 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.85 | wps 5953.5 | wpb 10324.2 | bsz 375 | num_updates 41258 | best_bleu 23.03
2021-01-02 19:38:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:38:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:38:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:38:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:38:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:38:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:38:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:38:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 98 @ 41258 updates, score 22.85) (writing took 3.0369631983339787 seconds)
2021-01-02 19:38:16 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-02 19:38:16 | INFO | train | epoch 098 | symm_mse 0.26 | loss 2.996 | nll_loss 0.825 | ppl 1.77 | wps 20575.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 41258 | lr 1.07862e-05 | gnorm 0.647 | train_wall 262 | wall 27927
2021-01-02 19:38:16 | INFO | fairseq.trainer | begin training epoch 99
2021-01-02 19:38:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:38:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:38:46 | INFO | train_inner | epoch 099:     42 / 421 symm_mse=0.26, loss=2.993, nll_loss=0.823, ppl=1.77, wps=16299.9, ups=1.18, wpb=13872, bsz=489.7, num_updates=41300, lr=1.07807e-05, gnorm=0.648, train_wall=62, wall=27956
2021-01-02 19:39:48 | INFO | train_inner | epoch 099:    142 / 421 symm_mse=0.26, loss=2.995, nll_loss=0.824, ppl=1.77, wps=22484.8, ups=1.6, wpb=14015.1, bsz=501.8, num_updates=41400, lr=1.07676e-05, gnorm=0.647, train_wall=62, wall=28018
2021-01-02 19:40:50 | INFO | train_inner | epoch 099:    242 / 421 symm_mse=0.261, loss=2.996, nll_loss=0.824, ppl=1.77, wps=22500.2, ups=1.6, wpb=14021.7, bsz=480, num_updates=41500, lr=1.07547e-05, gnorm=0.649, train_wall=62, wall=28081
2021-01-02 19:41:53 | INFO | train_inner | epoch 099:    342 / 421 symm_mse=0.263, loss=3.007, nll_loss=0.832, ppl=1.78, wps=22203.8, ups=1.6, wpb=13915.6, bsz=489.7, num_updates=41600, lr=1.07417e-05, gnorm=0.654, train_wall=62, wall=28143
2021-01-02 19:42:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:42:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:42:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:42:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:42:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:42:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:42:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:42:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:42:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:43:00 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.86 | wps 5498 | wpb 10324.2 | bsz 375 | num_updates 41679 | best_bleu 23.03
2021-01-02 19:43:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:43:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:43:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:43:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:43:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 99 @ 41679 updates, score 22.86) (writing took 2.9316245391964912 seconds)
2021-01-02 19:43:03 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-02 19:43:03 | INFO | train | epoch 099 | symm_mse 0.26 | loss 2.994 | nll_loss 0.824 | ppl 1.77 | wps 20543.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 41679 | lr 1.07315e-05 | gnorm 0.648 | train_wall 261 | wall 28213
2021-01-02 19:43:03 | INFO | fairseq.trainer | begin training epoch 100
2021-01-02 19:43:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:43:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:43:19 | INFO | train_inner | epoch 100:     21 / 421 symm_mse=0.255, loss=2.982, nll_loss=0.818, ppl=1.76, wps=16275.9, ups=1.16, wpb=13985.8, bsz=496.4, num_updates=41700, lr=1.07288e-05, gnorm=0.642, train_wall=62, wall=28229
2021-01-02 19:44:21 | INFO | train_inner | epoch 100:    121 / 421 symm_mse=0.255, loss=2.976, nll_loss=0.811, ppl=1.75, wps=22816.9, ups=1.62, wpb=14118.8, bsz=510.7, num_updates=41800, lr=1.0716e-05, gnorm=0.634, train_wall=62, wall=28291
2021-01-02 19:45:23 | INFO | train_inner | epoch 100:    221 / 421 symm_mse=0.259, loss=2.991, nll_loss=0.821, ppl=1.77, wps=22669.6, ups=1.62, wpb=14026.1, bsz=489.2, num_updates=41900, lr=1.07032e-05, gnorm=0.645, train_wall=62, wall=28353
2021-01-02 19:46:25 | INFO | train_inner | epoch 100:    321 / 421 symm_mse=0.261, loss=3.003, nll_loss=0.831, ppl=1.78, wps=22272.9, ups=1.59, wpb=13989.4, bsz=484.4, num_updates=42000, lr=1.06904e-05, gnorm=0.649, train_wall=63, wall=28416
2021-01-02 19:47:28 | INFO | train_inner | epoch 100:    421 / 421 symm_mse=0.263, loss=3.011, nll_loss=0.837, ppl=1.79, wps=22017.8, ups=1.6, wpb=13759.9, bsz=487.8, num_updates=42100, lr=1.06777e-05, gnorm=0.661, train_wall=62, wall=28478
2021-01-02 19:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:47:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:47:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:47:45 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.81 | wps 5925 | wpb 10324.2 | bsz 375 | num_updates 42100 | best_bleu 23.03
2021-01-02 19:47:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:47:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:47:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 100 @ 42100 updates, score 22.81) (writing took 2.9706143587827682 seconds)
2021-01-02 19:47:48 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-02 19:47:48 | INFO | train | epoch 100 | symm_mse 0.259 | loss 2.994 | nll_loss 0.824 | ppl 1.77 | wps 20632.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 42100 | lr 1.06777e-05 | gnorm 0.647 | train_wall 261 | wall 28498
2021-01-02 19:47:48 | INFO | fairseq.trainer | begin training epoch 101
2021-01-02 19:47:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:47:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:48:53 | INFO | train_inner | epoch 101:    100 / 421 symm_mse=0.256, loss=2.976, nll_loss=0.811, ppl=1.76, wps=16346.8, ups=1.18, wpb=13883.2, bsz=499.1, num_updates=42200, lr=1.06651e-05, gnorm=0.646, train_wall=62, wall=28563
2021-01-02 19:49:55 | INFO | train_inner | epoch 101:    200 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.831, ppl=1.78, wps=22576.5, ups=1.6, wpb=14079.7, bsz=491, num_updates=42300, lr=1.06525e-05, gnorm=0.648, train_wall=62, wall=28626
2021-01-02 19:50:57 | INFO | train_inner | epoch 101:    300 / 421 symm_mse=0.259, loss=2.991, nll_loss=0.821, ppl=1.77, wps=22614, ups=1.61, wpb=14068.5, bsz=485.1, num_updates=42400, lr=1.06399e-05, gnorm=0.644, train_wall=62, wall=28688
2021-01-02 19:51:59 | INFO | train_inner | epoch 101:    400 / 421 symm_mse=0.261, loss=3.001, nll_loss=0.829, ppl=1.78, wps=22376.1, ups=1.61, wpb=13890.9, bsz=496, num_updates=42500, lr=1.06274e-05, gnorm=0.648, train_wall=62, wall=28750
2021-01-02 19:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:52:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:52:29 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.81 | wps 5763.6 | wpb 10324.2 | bsz 375 | num_updates 42521 | best_bleu 23.03
2021-01-02 19:52:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:52:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 101 @ 42521 updates, score 22.81) (writing took 2.9339358750730753 seconds)
2021-01-02 19:52:32 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-02 19:52:32 | INFO | train | epoch 101 | symm_mse 0.259 | loss 2.993 | nll_loss 0.823 | ppl 1.77 | wps 20651.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 42521 | lr 1.06248e-05 | gnorm 0.647 | train_wall 261 | wall 28783
2021-01-02 19:52:32 | INFO | fairseq.trainer | begin training epoch 102
2021-01-02 19:52:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:52:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:53:25 | INFO | train_inner | epoch 102:     79 / 421 symm_mse=0.263, loss=3.002, nll_loss=0.827, ppl=1.77, wps=16407.6, ups=1.17, wpb=13969.1, bsz=481.5, num_updates=42600, lr=1.06149e-05, gnorm=0.658, train_wall=62, wall=28835
2021-01-02 19:54:27 | INFO | train_inner | epoch 102:    179 / 421 symm_mse=0.258, loss=2.985, nll_loss=0.817, ppl=1.76, wps=22294.5, ups=1.6, wpb=13930.9, bsz=507, num_updates=42700, lr=1.06025e-05, gnorm=0.643, train_wall=62, wall=28898
2021-01-02 19:55:29 | INFO | train_inner | epoch 102:    279 / 421 symm_mse=0.258, loss=2.99, nll_loss=0.822, ppl=1.77, wps=22352, ups=1.61, wpb=13897.1, bsz=483, num_updates=42800, lr=1.05901e-05, gnorm=0.646, train_wall=62, wall=28960
2021-01-02 19:56:32 | INFO | train_inner | epoch 102:    379 / 421 symm_mse=0.257, loss=2.989, nll_loss=0.823, ppl=1.77, wps=22414.3, ups=1.6, wpb=14020.9, bsz=498.7, num_updates=42900, lr=1.05777e-05, gnorm=0.644, train_wall=62, wall=29022
2021-01-02 19:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 19:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 19:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 19:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 19:57:15 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.845 | ppl 14.37 | bleu 22.89 | wps 6017.6 | wpb 10324.2 | bsz 375 | num_updates 42942 | best_bleu 23.03
2021-01-02 19:57:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 19:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 102 @ 42942 updates, score 22.89) (writing took 2.962819989770651 seconds)
2021-01-02 19:57:18 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-02 19:57:18 | INFO | train | epoch 102 | symm_mse 0.259 | loss 2.993 | nll_loss 0.823 | ppl 1.77 | wps 20627.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 42942 | lr 1.05725e-05 | gnorm 0.648 | train_wall 261 | wall 29068
2021-01-02 19:57:18 | INFO | fairseq.trainer | begin training epoch 103
2021-01-02 19:57:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 19:57:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 19:57:57 | INFO | train_inner | epoch 103:     58 / 421 symm_mse=0.261, loss=2.999, nll_loss=0.826, ppl=1.77, wps=16461.2, ups=1.18, wpb=14003.3, bsz=502, num_updates=43000, lr=1.05654e-05, gnorm=0.65, train_wall=62, wall=29107
2021-01-02 19:58:59 | INFO | train_inner | epoch 103:    158 / 421 symm_mse=0.257, loss=2.986, nll_loss=0.819, ppl=1.76, wps=22400.9, ups=1.61, wpb=13884.4, bsz=487.8, num_updates=43100, lr=1.05531e-05, gnorm=0.647, train_wall=62, wall=29169
2021-01-02 20:00:01 | INFO | train_inner | epoch 103:    258 / 421 symm_mse=0.259, loss=2.99, nll_loss=0.82, ppl=1.77, wps=22729.8, ups=1.61, wpb=14078.5, bsz=477.8, num_updates=43200, lr=1.05409e-05, gnorm=0.645, train_wall=62, wall=29231
2021-01-02 20:01:03 | INFO | train_inner | epoch 103:    358 / 421 symm_mse=0.26, loss=2.999, nll_loss=0.83, ppl=1.78, wps=22683.2, ups=1.61, wpb=14069.8, bsz=494.8, num_updates=43300, lr=1.05287e-05, gnorm=0.645, train_wall=62, wall=29293
2021-01-02 20:01:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:01:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:01:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:01:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:01:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:01:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:01:59 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.96 | wps 5989.4 | wpb 10324.2 | bsz 375 | num_updates 43363 | best_bleu 23.03
2021-01-02 20:01:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:02:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 103 @ 43363 updates, score 22.96) (writing took 2.9604819640517235 seconds)
2021-01-02 20:02:02 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-02 20:02:02 | INFO | train | epoch 103 | symm_mse 0.259 | loss 2.993 | nll_loss 0.824 | ppl 1.77 | wps 20670.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 43363 | lr 1.05211e-05 | gnorm 0.647 | train_wall 261 | wall 29353
2021-01-02 20:02:02 | INFO | fairseq.trainer | begin training epoch 104
2021-01-02 20:02:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:02:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:02:28 | INFO | train_inner | epoch 104:     37 / 421 symm_mse=0.261, loss=3, nll_loss=0.828, ppl=1.78, wps=16255.1, ups=1.17, wpb=13858.5, bsz=500.9, num_updates=43400, lr=1.05166e-05, gnorm=0.652, train_wall=62, wall=29379
2021-01-02 20:03:30 | INFO | train_inner | epoch 104:    137 / 421 symm_mse=0.258, loss=2.984, nll_loss=0.815, ppl=1.76, wps=22504.6, ups=1.61, wpb=13991.8, bsz=485.7, num_updates=43500, lr=1.05045e-05, gnorm=0.648, train_wall=62, wall=29441
2021-01-02 20:04:32 | INFO | train_inner | epoch 104:    237 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.824, ppl=1.77, wps=22557.9, ups=1.61, wpb=13997.3, bsz=499.1, num_updates=43600, lr=1.04925e-05, gnorm=0.649, train_wall=62, wall=29503
2021-01-02 20:05:34 | INFO | train_inner | epoch 104:    337 / 421 symm_mse=0.26, loss=2.996, nll_loss=0.826, ppl=1.77, wps=22509.2, ups=1.61, wpb=13981.3, bsz=497, num_updates=43700, lr=1.04804e-05, gnorm=0.643, train_wall=62, wall=29565
2021-01-02 20:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:06:44 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.92 | wps 6027.2 | wpb 10324.2 | bsz 375 | num_updates 43784 | best_bleu 23.03
2021-01-02 20:06:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:06:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 104 @ 43784 updates, score 22.92) (writing took 2.8870499935001135 seconds)
2021-01-02 20:06:47 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-02 20:06:47 | INFO | train | epoch 104 | symm_mse 0.259 | loss 2.991 | nll_loss 0.822 | ppl 1.77 | wps 20668.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 43784 | lr 1.04704e-05 | gnorm 0.647 | train_wall 261 | wall 29637
2021-01-02 20:06:47 | INFO | fairseq.trainer | begin training epoch 105
2021-01-02 20:06:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:06:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:07:00 | INFO | train_inner | epoch 105:     16 / 421 symm_mse=0.258, loss=2.988, nll_loss=0.821, ppl=1.77, wps=16241.2, ups=1.17, wpb=13856.2, bsz=485.6, num_updates=43800, lr=1.04685e-05, gnorm=0.646, train_wall=62, wall=29650
2021-01-02 20:08:01 | INFO | train_inner | epoch 105:    116 / 421 symm_mse=0.257, loss=2.988, nll_loss=0.82, ppl=1.77, wps=22656.6, ups=1.62, wpb=13988.2, bsz=507, num_updates=43900, lr=1.04565e-05, gnorm=0.642, train_wall=62, wall=29712
2021-01-02 20:09:04 | INFO | train_inner | epoch 105:    216 / 421 symm_mse=0.259, loss=2.991, nll_loss=0.821, ppl=1.77, wps=22535.2, ups=1.61, wpb=13991.7, bsz=483.1, num_updates=44000, lr=1.04447e-05, gnorm=0.644, train_wall=62, wall=29774
2021-01-02 20:10:06 | INFO | train_inner | epoch 105:    316 / 421 symm_mse=0.26, loss=2.998, nll_loss=0.828, ppl=1.78, wps=22615, ups=1.61, wpb=14043.8, bsz=485.8, num_updates=44100, lr=1.04328e-05, gnorm=0.646, train_wall=62, wall=29836
2021-01-02 20:11:08 | INFO | train_inner | epoch 105:    416 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.824, ppl=1.77, wps=22303.5, ups=1.59, wpb=13989, bsz=497.6, num_updates=44200, lr=1.0421e-05, gnorm=0.639, train_wall=63, wall=29899
2021-01-02 20:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:11:28 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.91 | wps 6024.5 | wpb 10324.2 | bsz 375 | num_updates 44205 | best_bleu 23.03
2021-01-02 20:11:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:11:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 105 @ 44205 updates, score 22.91) (writing took 2.845981478691101 seconds)
2021-01-02 20:11:31 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-02 20:11:31 | INFO | train | epoch 105 | symm_mse 0.259 | loss 2.992 | nll_loss 0.823 | ppl 1.77 | wps 20691.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 44205 | lr 1.04204e-05 | gnorm 0.644 | train_wall 261 | wall 29921
2021-01-02 20:11:31 | INFO | fairseq.trainer | begin training epoch 106
2021-01-02 20:11:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:12:33 | INFO | train_inner | epoch 106:     95 / 421 symm_mse=0.259, loss=2.991, nll_loss=0.821, ppl=1.77, wps=16409.8, ups=1.19, wpb=13822.1, bsz=478.9, num_updates=44300, lr=1.04092e-05, gnorm=0.652, train_wall=61, wall=29983
2021-01-02 20:13:35 | INFO | train_inner | epoch 106:    195 / 421 symm_mse=0.255, loss=2.977, nll_loss=0.813, ppl=1.76, wps=22426.7, ups=1.6, wpb=14001.9, bsz=513.3, num_updates=44400, lr=1.03975e-05, gnorm=0.636, train_wall=62, wall=30046
2021-01-02 20:14:38 | INFO | train_inner | epoch 106:    295 / 421 symm_mse=0.263, loss=3.009, nll_loss=0.834, ppl=1.78, wps=22195.8, ups=1.59, wpb=13964, bsz=483.5, num_updates=44500, lr=1.03858e-05, gnorm=0.649, train_wall=63, wall=30109
2021-01-02 20:15:41 | INFO | train_inner | epoch 106:    395 / 421 symm_mse=0.257, loss=2.983, nll_loss=0.817, ppl=1.76, wps=22383.4, ups=1.6, wpb=14026.1, bsz=500.2, num_updates=44600, lr=1.03742e-05, gnorm=0.641, train_wall=62, wall=30171
2021-01-02 20:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:16:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:16:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:16:14 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.849 | ppl 14.41 | bleu 22.9 | wps 5840 | wpb 10324.2 | bsz 375 | num_updates 44626 | best_bleu 23.03
2021-01-02 20:16:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:16:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:16:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:16:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:16:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:16:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 106 @ 44626 updates, score 22.9) (writing took 2.8592196200042963 seconds)
2021-01-02 20:16:17 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-02 20:16:17 | INFO | train | epoch 106 | symm_mse 0.259 | loss 2.991 | nll_loss 0.822 | ppl 1.77 | wps 20587.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 44626 | lr 1.03711e-05 | gnorm 0.644 | train_wall 262 | wall 30207
2021-01-02 20:16:17 | INFO | fairseq.trainer | begin training epoch 107
2021-01-02 20:16:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:16:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:17:06 | INFO | train_inner | epoch 107:     74 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.823, ppl=1.77, wps=16477.8, ups=1.18, wpb=14000, bsz=476.7, num_updates=44700, lr=1.03626e-05, gnorm=0.644, train_wall=61, wall=30256
2021-01-02 20:18:08 | INFO | train_inner | epoch 107:    174 / 421 symm_mse=0.258, loss=2.986, nll_loss=0.817, ppl=1.76, wps=22552.2, ups=1.61, wpb=14039.2, bsz=499.8, num_updates=44800, lr=1.0351e-05, gnorm=0.642, train_wall=62, wall=30318
2021-01-02 20:19:10 | INFO | train_inner | epoch 107:    274 / 421 symm_mse=0.259, loss=2.994, nll_loss=0.824, ppl=1.77, wps=22580.7, ups=1.62, wpb=13956.4, bsz=498.3, num_updates=44900, lr=1.03395e-05, gnorm=0.648, train_wall=62, wall=30380
2021-01-02 20:20:12 | INFO | train_inner | epoch 107:    374 / 421 symm_mse=0.259, loss=2.996, nll_loss=0.829, ppl=1.78, wps=22220.2, ups=1.61, wpb=13812.2, bsz=496, num_updates=45000, lr=1.0328e-05, gnorm=0.647, train_wall=62, wall=30442
2021-01-02 20:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:20:57 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.844 | ppl 14.36 | bleu 22.99 | wps 6472.6 | wpb 10324.2 | bsz 375 | num_updates 45047 | best_bleu 23.03
2021-01-02 20:20:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:20:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:20:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:21:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 107 @ 45047 updates, score 22.99) (writing took 3.1266290824860334 seconds)
2021-01-02 20:21:00 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-02 20:21:00 | INFO | train | epoch 107 | symm_mse 0.258 | loss 2.991 | nll_loss 0.822 | ppl 1.77 | wps 20759.7 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 45047 | lr 1.03226e-05 | gnorm 0.645 | train_wall 260 | wall 30490
2021-01-02 20:21:00 | INFO | fairseq.trainer | begin training epoch 108
2021-01-02 20:21:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:21:35 | INFO | train_inner | epoch 108:     53 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.811, ppl=1.75, wps=16870.6, ups=1.19, wpb=14121.7, bsz=497.7, num_updates=45100, lr=1.03165e-05, gnorm=0.638, train_wall=61, wall=30526
2021-01-02 20:22:37 | INFO | train_inner | epoch 108:    153 / 421 symm_mse=0.26, loss=2.993, nll_loss=0.822, ppl=1.77, wps=22514, ups=1.62, wpb=13887.2, bsz=471.7, num_updates=45200, lr=1.03051e-05, gnorm=0.647, train_wall=62, wall=30588
2021-01-02 20:23:39 | INFO | train_inner | epoch 108:    253 / 421 symm_mse=0.26, loss=2.992, nll_loss=0.821, ppl=1.77, wps=22463.2, ups=1.61, wpb=13926.7, bsz=491.3, num_updates=45300, lr=1.02937e-05, gnorm=0.652, train_wall=62, wall=30650
2021-01-02 20:24:41 | INFO | train_inner | epoch 108:    353 / 421 symm_mse=0.259, loss=2.993, nll_loss=0.823, ppl=1.77, wps=22566.7, ups=1.62, wpb=13971.7, bsz=485, num_updates=45400, lr=1.02824e-05, gnorm=0.647, train_wall=62, wall=30712
2021-01-02 20:25:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:25:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:25:40 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.844 | ppl 14.36 | bleu 22.83 | wps 6010.4 | wpb 10324.2 | bsz 375 | num_updates 45468 | best_bleu 23.03
2021-01-02 20:25:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:25:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:25:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 108 @ 45468 updates, score 22.83) (writing took 2.9397384244948626 seconds)
2021-01-02 20:25:43 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-02 20:25:43 | INFO | train | epoch 108 | symm_mse 0.258 | loss 2.99 | nll_loss 0.822 | ppl 1.77 | wps 20760.4 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 45468 | lr 1.02747e-05 | gnorm 0.647 | train_wall 260 | wall 30774
2021-01-02 20:25:43 | INFO | fairseq.trainer | begin training epoch 109
2021-01-02 20:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:26:06 | INFO | train_inner | epoch 109:     32 / 421 symm_mse=0.258, loss=2.996, nll_loss=0.83, ppl=1.78, wps=16405, ups=1.18, wpb=13911, bsz=510.6, num_updates=45500, lr=1.02711e-05, gnorm=0.651, train_wall=62, wall=30796
2021-01-02 20:27:08 | INFO | train_inner | epoch 109:    132 / 421 symm_mse=0.259, loss=2.996, nll_loss=0.826, ppl=1.77, wps=22566, ups=1.61, wpb=13987.2, bsz=497.9, num_updates=45600, lr=1.02598e-05, gnorm=0.64, train_wall=62, wall=30858
2021-01-02 20:28:10 | INFO | train_inner | epoch 109:    232 / 421 symm_mse=0.258, loss=2.989, nll_loss=0.82, ppl=1.77, wps=22570.3, ups=1.62, wpb=13961.6, bsz=489.7, num_updates=45700, lr=1.02486e-05, gnorm=0.647, train_wall=62, wall=30920
2021-01-02 20:29:12 | INFO | train_inner | epoch 109:    332 / 421 symm_mse=0.258, loss=2.984, nll_loss=0.816, ppl=1.76, wps=22553.2, ups=1.61, wpb=13998.7, bsz=486.6, num_updates=45800, lr=1.02374e-05, gnorm=0.644, train_wall=62, wall=30982
2021-01-02 20:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:30:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:30:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:30:24 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.78 | wps 6030.6 | wpb 10324.2 | bsz 375 | num_updates 45889 | best_bleu 23.03
2021-01-02 20:30:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 109 @ 45889 updates, score 22.78) (writing took 2.875930605456233 seconds)
2021-01-02 20:30:27 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-02 20:30:27 | INFO | train | epoch 109 | symm_mse 0.258 | loss 2.99 | nll_loss 0.822 | ppl 1.77 | wps 20746.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 45889 | lr 1.02274e-05 | gnorm 0.644 | train_wall 260 | wall 31057
2021-01-02 20:30:27 | INFO | fairseq.trainer | begin training epoch 110
2021-01-02 20:30:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:30:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:30:37 | INFO | train_inner | epoch 110:     11 / 421 symm_mse=0.256, loss=2.988, nll_loss=0.822, ppl=1.77, wps=16460.2, ups=1.18, wpb=13957.2, bsz=495.4, num_updates=45900, lr=1.02262e-05, gnorm=0.645, train_wall=62, wall=31067
2021-01-02 20:31:38 | INFO | train_inner | epoch 110:    111 / 421 symm_mse=0.258, loss=2.988, nll_loss=0.819, ppl=1.76, wps=22658.2, ups=1.63, wpb=13930.8, bsz=481.6, num_updates=46000, lr=1.02151e-05, gnorm=0.646, train_wall=61, wall=31129
2021-01-02 20:32:40 | INFO | train_inner | epoch 110:    211 / 421 symm_mse=0.257, loss=2.985, nll_loss=0.817, ppl=1.76, wps=22524, ups=1.6, wpb=14039.4, bsz=478.6, num_updates=46100, lr=1.0204e-05, gnorm=0.649, train_wall=62, wall=31191
2021-01-02 20:33:42 | INFO | train_inner | epoch 110:    311 / 421 symm_mse=0.259, loss=2.999, nll_loss=0.83, ppl=1.78, wps=22359.2, ups=1.61, wpb=13881.6, bsz=507.3, num_updates=46200, lr=1.01929e-05, gnorm=0.646, train_wall=62, wall=31253
2021-01-02 20:34:45 | INFO | train_inner | epoch 110:    411 / 421 symm_mse=0.257, loss=2.983, nll_loss=0.818, ppl=1.76, wps=22599.7, ups=1.6, wpb=14092.8, bsz=505.5, num_updates=46300, lr=1.01819e-05, gnorm=0.641, train_wall=62, wall=31315
2021-01-02 20:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:34:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:34:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:34:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:34:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:34:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:34:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:34:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:34:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:34:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:35:08 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.88 | wps 6030.6 | wpb 10324.2 | bsz 375 | num_updates 46310 | best_bleu 23.03
2021-01-02 20:35:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:35:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:35:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:35:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:35:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:35:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:35:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:35:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 110 @ 46310 updates, score 22.88) (writing took 2.91851082444191 seconds)
2021-01-02 20:35:11 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-02 20:35:11 | INFO | train | epoch 110 | symm_mse 0.258 | loss 2.989 | nll_loss 0.821 | ppl 1.77 | wps 20710.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 46310 | lr 1.01808e-05 | gnorm 0.647 | train_wall 260 | wall 31341
2021-01-02 20:35:11 | INFO | fairseq.trainer | begin training epoch 111
2021-01-02 20:35:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:35:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:36:09 | INFO | train_inner | epoch 111:     90 / 421 symm_mse=0.257, loss=2.986, nll_loss=0.819, ppl=1.76, wps=16728.5, ups=1.19, wpb=14116, bsz=495.6, num_updates=46400, lr=1.0171e-05, gnorm=0.64, train_wall=61, wall=31400
2021-01-02 20:37:11 | INFO | train_inner | epoch 111:    190 / 421 symm_mse=0.257, loss=2.989, nll_loss=0.823, ppl=1.77, wps=22489.4, ups=1.61, wpb=13991.1, bsz=489.8, num_updates=46500, lr=1.016e-05, gnorm=0.642, train_wall=62, wall=31462
2021-01-02 20:38:14 | INFO | train_inner | epoch 111:    290 / 421 symm_mse=0.258, loss=2.992, nll_loss=0.824, ppl=1.77, wps=22247.6, ups=1.6, wpb=13902.4, bsz=483.1, num_updates=46600, lr=1.01491e-05, gnorm=0.645, train_wall=62, wall=31525
2021-01-02 20:39:16 | INFO | train_inner | epoch 111:    390 / 421 symm_mse=0.258, loss=2.988, nll_loss=0.821, ppl=1.77, wps=22316.6, ups=1.6, wpb=13943.7, bsz=496.4, num_updates=46700, lr=1.01382e-05, gnorm=0.643, train_wall=62, wall=31587
2021-01-02 20:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:39:52 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.849 | ppl 14.41 | bleu 22.98 | wps 6095.6 | wpb 10324.2 | bsz 375 | num_updates 46731 | best_bleu 23.03
2021-01-02 20:39:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:39:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:39:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 111 @ 46731 updates, score 22.98) (writing took 3.162457572296262 seconds)
2021-01-02 20:39:55 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-02 20:39:55 | INFO | train | epoch 111 | symm_mse 0.258 | loss 2.988 | nll_loss 0.821 | ppl 1.77 | wps 20658.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 46731 | lr 1.01349e-05 | gnorm 0.644 | train_wall 261 | wall 31626
2021-01-02 20:39:55 | INFO | fairseq.trainer | begin training epoch 112
2021-01-02 20:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:39:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:40:41 | INFO | train_inner | epoch 112:     69 / 421 symm_mse=0.258, loss=2.985, nll_loss=0.816, ppl=1.76, wps=16423.7, ups=1.18, wpb=13896.2, bsz=496.3, num_updates=46800, lr=1.01274e-05, gnorm=0.65, train_wall=62, wall=31672
2021-01-02 20:41:43 | INFO | train_inner | epoch 112:    169 / 421 symm_mse=0.257, loss=2.984, nll_loss=0.817, ppl=1.76, wps=22440.8, ups=1.61, wpb=13971.6, bsz=489.3, num_updates=46900, lr=1.01166e-05, gnorm=0.644, train_wall=62, wall=31734
2021-01-02 20:42:46 | INFO | train_inner | epoch 112:    269 / 421 symm_mse=0.26, loss=2.998, nll_loss=0.827, ppl=1.77, wps=22177.7, ups=1.6, wpb=13883.7, bsz=472.8, num_updates=47000, lr=1.01058e-05, gnorm=0.654, train_wall=62, wall=31796
2021-01-02 20:43:48 | INFO | train_inner | epoch 112:    369 / 421 symm_mse=0.256, loss=2.983, nll_loss=0.818, ppl=1.76, wps=22535, ups=1.6, wpb=14044.4, bsz=503.4, num_updates=47100, lr=1.00951e-05, gnorm=0.638, train_wall=62, wall=31859
2021-01-02 20:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:44:37 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.845 | ppl 14.37 | bleu 22.85 | wps 6049.1 | wpb 10324.2 | bsz 375 | num_updates 47152 | best_bleu 23.03
2021-01-02 20:44:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:44:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:44:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 112 @ 47152 updates, score 22.85) (writing took 2.8835133090615273 seconds)
2021-01-02 20:44:40 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-02 20:44:40 | INFO | train | epoch 112 | symm_mse 0.258 | loss 2.988 | nll_loss 0.82 | ppl 1.77 | wps 20647.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 47152 | lr 1.00895e-05 | gnorm 0.644 | train_wall 261 | wall 31911
2021-01-02 20:44:40 | INFO | fairseq.trainer | begin training epoch 113
2021-01-02 20:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:45:13 | INFO | train_inner | epoch 113:     48 / 421 symm_mse=0.259, loss=2.994, nll_loss=0.825, ppl=1.77, wps=16509.1, ups=1.18, wpb=13952, bsz=495.1, num_updates=47200, lr=1.00844e-05, gnorm=0.643, train_wall=62, wall=31943
2021-01-02 20:46:15 | INFO | train_inner | epoch 113:    148 / 421 symm_mse=0.255, loss=2.981, nll_loss=0.817, ppl=1.76, wps=22500.2, ups=1.61, wpb=14018, bsz=501.3, num_updates=47300, lr=1.00737e-05, gnorm=0.637, train_wall=62, wall=32006
2021-01-02 20:47:17 | INFO | train_inner | epoch 113:    248 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.81, ppl=1.75, wps=22661.7, ups=1.61, wpb=14084.4, bsz=507.6, num_updates=47400, lr=1.00631e-05, gnorm=0.635, train_wall=62, wall=32068
2021-01-02 20:48:20 | INFO | train_inner | epoch 113:    348 / 421 symm_mse=0.257, loss=2.988, nll_loss=0.822, ppl=1.77, wps=22046.9, ups=1.58, wpb=13916.6, bsz=498.7, num_updates=47500, lr=1.00525e-05, gnorm=0.645, train_wall=63, wall=32131
2021-01-02 20:49:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:49:23 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.97 | wps 5984 | wpb 10324.2 | bsz 375 | num_updates 47573 | best_bleu 23.03
2021-01-02 20:49:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 113 @ 47573 updates, score 22.97) (writing took 2.929814487695694 seconds)
2021-01-02 20:49:26 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-02 20:49:26 | INFO | train | epoch 113 | symm_mse 0.257 | loss 2.987 | nll_loss 0.82 | ppl 1.77 | wps 20590.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 47573 | lr 1.00448e-05 | gnorm 0.644 | train_wall 262 | wall 32196
2021-01-02 20:49:26 | INFO | fairseq.trainer | begin training epoch 114
2021-01-02 20:49:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:49:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:49:45 | INFO | train_inner | epoch 114:     27 / 421 symm_mse=0.262, loss=3.002, nll_loss=0.829, ppl=1.78, wps=16322.6, ups=1.17, wpb=13894.5, bsz=464.8, num_updates=47600, lr=1.00419e-05, gnorm=0.66, train_wall=62, wall=32216
2021-01-02 20:50:47 | INFO | train_inner | epoch 114:    127 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.831, ppl=1.78, wps=22497.5, ups=1.62, wpb=13900.1, bsz=483, num_updates=47700, lr=1.00314e-05, gnorm=0.654, train_wall=62, wall=32278
2021-01-02 20:51:50 | INFO | train_inner | epoch 114:    227 / 421 symm_mse=0.259, loss=2.99, nll_loss=0.82, ppl=1.77, wps=22461.8, ups=1.6, wpb=14030.9, bsz=489.1, num_updates=47800, lr=1.00209e-05, gnorm=0.647, train_wall=62, wall=32340
2021-01-02 20:52:52 | INFO | train_inner | epoch 114:    327 / 421 symm_mse=0.258, loss=2.985, nll_loss=0.817, ppl=1.76, wps=22224.5, ups=1.6, wpb=13927.4, bsz=480.9, num_updates=47900, lr=1.00104e-05, gnorm=0.648, train_wall=62, wall=32403
2021-01-02 20:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:53:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:53:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:53:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:53:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:53:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:53:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:53:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:53:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:54:08 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.95 | wps 5961.9 | wpb 10324.2 | bsz 375 | num_updates 47994 | best_bleu 23.03
2021-01-02 20:54:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:54:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:54:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:54:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:54:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:54:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:54:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:54:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 114 @ 47994 updates, score 22.95) (writing took 2.885149773210287 seconds)
2021-01-02 20:54:11 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-02 20:54:11 | INFO | train | epoch 114 | symm_mse 0.257 | loss 2.987 | nll_loss 0.82 | ppl 1.76 | wps 20592.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 47994 | lr 1.00006e-05 | gnorm 0.646 | train_wall 262 | wall 32482
2021-01-02 20:54:11 | INFO | fairseq.trainer | begin training epoch 115
2021-01-02 20:54:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:54:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:54:18 | INFO | train_inner | epoch 115:      6 / 421 symm_mse=0.25, loss=2.966, nll_loss=0.809, ppl=1.75, wps=16234.3, ups=1.16, wpb=13966.9, bsz=528.5, num_updates=48000, lr=1e-05, gnorm=0.636, train_wall=63, wall=32489
2021-01-02 20:55:20 | INFO | train_inner | epoch 115:    106 / 421 symm_mse=0.258, loss=2.989, nll_loss=0.822, ppl=1.77, wps=22749.5, ups=1.63, wpb=13974.8, bsz=486.3, num_updates=48100, lr=9.9896e-06, gnorm=0.642, train_wall=61, wall=32550
2021-01-02 20:56:22 | INFO | train_inner | epoch 115:    206 / 421 symm_mse=0.258, loss=2.987, nll_loss=0.819, ppl=1.76, wps=22470.8, ups=1.61, wpb=13948.4, bsz=491.4, num_updates=48200, lr=9.97923e-06, gnorm=0.642, train_wall=62, wall=32612
2021-01-02 20:57:25 | INFO | train_inner | epoch 115:    306 / 421 symm_mse=0.256, loss=2.98, nll_loss=0.814, ppl=1.76, wps=22211.4, ups=1.6, wpb=13922.8, bsz=493, num_updates=48300, lr=9.9689e-06, gnorm=0.645, train_wall=62, wall=32675
2021-01-02 20:58:27 | INFO | train_inner | epoch 115:    406 / 421 symm_mse=0.256, loss=2.985, nll_loss=0.82, ppl=1.76, wps=22485.8, ups=1.59, wpb=14107, bsz=498.6, num_updates=48400, lr=9.95859e-06, gnorm=0.638, train_wall=63, wall=32738
2021-01-02 20:58:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 20:58:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 20:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 20:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 20:58:53 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.9 | wps 6112.4 | wpb 10324.2 | bsz 375 | num_updates 48415 | best_bleu 23.03
2021-01-02 20:58:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 20:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:58:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 115 @ 48415 updates, score 22.9) (writing took 2.828756606206298 seconds)
2021-01-02 20:58:56 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-02 20:58:56 | INFO | train | epoch 115 | symm_mse 0.257 | loss 2.986 | nll_loss 0.819 | ppl 1.76 | wps 20659.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48415 | lr 9.95705e-06 | gnorm 0.644 | train_wall 261 | wall 32767
2021-01-02 20:58:56 | INFO | fairseq.trainer | begin training epoch 116
2021-01-02 20:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 20:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 20:59:52 | INFO | train_inner | epoch 116:     85 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.817, ppl=1.76, wps=16520, ups=1.18, wpb=13963.5, bsz=489.8, num_updates=48500, lr=9.94832e-06, gnorm=0.643, train_wall=62, wall=32822
2021-01-02 21:00:54 | INFO | train_inner | epoch 116:    185 / 421 symm_mse=0.254, loss=2.971, nll_loss=0.808, ppl=1.75, wps=22702.1, ups=1.6, wpb=14169.6, bsz=503.2, num_updates=48600, lr=9.93808e-06, gnorm=0.634, train_wall=62, wall=32885
2021-01-02 21:01:57 | INFO | train_inner | epoch 116:    285 / 421 symm_mse=0.261, loss=2.999, nll_loss=0.827, ppl=1.77, wps=21980.6, ups=1.6, wpb=13708, bsz=476.4, num_updates=48700, lr=9.92787e-06, gnorm=0.658, train_wall=62, wall=32947
2021-01-02 21:02:59 | INFO | train_inner | epoch 116:    385 / 421 symm_mse=0.257, loss=2.99, nll_loss=0.822, ppl=1.77, wps=22366.2, ups=1.59, wpb=14043.3, bsz=500.8, num_updates=48800, lr=9.91769e-06, gnorm=0.64, train_wall=63, wall=33010
2021-01-02 21:03:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:03:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:03:38 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.847 | ppl 14.39 | bleu 22.92 | wps 6078.3 | wpb 10324.2 | bsz 375 | num_updates 48836 | best_bleu 23.03
2021-01-02 21:03:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:03:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:03:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 116 @ 48836 updates, score 22.92) (writing took 2.8469141349196434 seconds)
2021-01-02 21:03:41 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-02 21:03:41 | INFO | train | epoch 116 | symm_mse 0.257 | loss 2.986 | nll_loss 0.819 | ppl 1.76 | wps 20634.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48836 | lr 9.91404e-06 | gnorm 0.644 | train_wall 262 | wall 33052
2021-01-02 21:03:41 | INFO | fairseq.trainer | begin training epoch 117
2021-01-02 21:03:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:03:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:04:24 | INFO | train_inner | epoch 117:     64 / 421 symm_mse=0.262, loss=3.004, nll_loss=0.832, ppl=1.78, wps=16484.5, ups=1.19, wpb=13880.7, bsz=472.5, num_updates=48900, lr=9.90755e-06, gnorm=0.658, train_wall=61, wall=33094
2021-01-02 21:05:26 | INFO | train_inner | epoch 117:    164 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.811, ppl=1.75, wps=22479.5, ups=1.6, wpb=14082.2, bsz=504.9, num_updates=49000, lr=9.89743e-06, gnorm=0.634, train_wall=62, wall=33157
2021-01-02 21:06:29 | INFO | train_inner | epoch 117:    264 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.816, ppl=1.76, wps=22335.2, ups=1.59, wpb=14012, bsz=506.2, num_updates=49100, lr=9.88735e-06, gnorm=0.641, train_wall=63, wall=33220
2021-01-02 21:07:31 | INFO | train_inner | epoch 117:    364 / 421 symm_mse=0.256, loss=2.984, nll_loss=0.818, ppl=1.76, wps=22240.5, ups=1.6, wpb=13879.7, bsz=483.9, num_updates=49200, lr=9.8773e-06, gnorm=0.647, train_wall=62, wall=33282
2021-01-02 21:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:08:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:08:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:08:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:08:24 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.845 | ppl 14.37 | bleu 22.85 | wps 6068.3 | wpb 10324.2 | bsz 375 | num_updates 49257 | best_bleu 23.03
2021-01-02 21:08:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 117 @ 49257 updates, score 22.85) (writing took 2.8910834155976772 seconds)
2021-01-02 21:08:26 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-02 21:08:26 | INFO | train | epoch 117 | symm_mse 0.257 | loss 2.985 | nll_loss 0.819 | ppl 1.76 | wps 20599.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 49257 | lr 9.87158e-06 | gnorm 0.644 | train_wall 262 | wall 33337
2021-01-02 21:08:26 | INFO | fairseq.trainer | begin training epoch 118
2021-01-02 21:08:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:08:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:08:56 | INFO | train_inner | epoch 118:     43 / 421 symm_mse=0.257, loss=2.991, nll_loss=0.825, ppl=1.77, wps=16504.4, ups=1.18, wpb=13980.3, bsz=496.7, num_updates=49300, lr=9.86727e-06, gnorm=0.647, train_wall=62, wall=33367
2021-01-02 21:09:59 | INFO | train_inner | epoch 118:    143 / 421 symm_mse=0.257, loss=2.984, nll_loss=0.817, ppl=1.76, wps=22320.7, ups=1.6, wpb=13971.2, bsz=478.3, num_updates=49400, lr=9.85728e-06, gnorm=0.642, train_wall=62, wall=33429
2021-01-02 21:11:01 | INFO | train_inner | epoch 118:    243 / 421 symm_mse=0.259, loss=2.99, nll_loss=0.819, ppl=1.76, wps=22346.8, ups=1.6, wpb=13927.7, bsz=482.2, num_updates=49500, lr=9.84732e-06, gnorm=0.651, train_wall=62, wall=33492
2021-01-02 21:12:04 | INFO | train_inner | epoch 118:    343 / 421 symm_mse=0.256, loss=2.986, nll_loss=0.82, ppl=1.77, wps=22077.6, ups=1.59, wpb=13889.3, bsz=503.8, num_updates=49600, lr=9.83739e-06, gnorm=0.644, train_wall=63, wall=33555
2021-01-02 21:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:12:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:12:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:12:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:12:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:13:10 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.849 | ppl 14.41 | bleu 22.88 | wps 6071.2 | wpb 10324.2 | bsz 375 | num_updates 49678 | best_bleu 23.03
2021-01-02 21:13:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:13:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:13:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:13:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:13:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:13:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:13:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:13:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 118 @ 49678 updates, score 22.88) (writing took 2.9057694990187883 seconds)
2021-01-02 21:13:13 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-02 21:13:13 | INFO | train | epoch 118 | symm_mse 0.257 | loss 2.985 | nll_loss 0.819 | ppl 1.76 | wps 20562.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 49678 | lr 9.82966e-06 | gnorm 0.644 | train_wall 263 | wall 33623
2021-01-02 21:13:13 | INFO | fairseq.trainer | begin training epoch 119
2021-01-02 21:13:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:13:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:13:29 | INFO | train_inner | epoch 119:     22 / 421 symm_mse=0.255, loss=2.984, nll_loss=0.82, ppl=1.77, wps=16300.8, ups=1.17, wpb=13910.5, bsz=498.5, num_updates=49700, lr=9.82749e-06, gnorm=0.644, train_wall=63, wall=33640
2021-01-02 21:14:31 | INFO | train_inner | epoch 119:    122 / 421 symm_mse=0.255, loss=2.978, nll_loss=0.813, ppl=1.76, wps=22572.4, ups=1.61, wpb=14040.1, bsz=488.4, num_updates=49800, lr=9.81761e-06, gnorm=0.639, train_wall=62, wall=33702
2021-01-02 21:15:35 | INFO | train_inner | epoch 119:    222 / 421 symm_mse=0.259, loss=2.989, nll_loss=0.818, ppl=1.76, wps=22184.5, ups=1.58, wpb=14013, bsz=472.1, num_updates=49900, lr=9.80777e-06, gnorm=0.649, train_wall=63, wall=33765
2021-01-02 21:16:37 | INFO | train_inner | epoch 119:    322 / 421 symm_mse=0.257, loss=2.993, nll_loss=0.827, ppl=1.77, wps=22246.4, ups=1.59, wpb=13969, bsz=510.7, num_updates=50000, lr=9.79796e-06, gnorm=0.645, train_wall=63, wall=33828
2021-01-02 21:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:17:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:17:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:17:56 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.844 | ppl 14.36 | bleu 22.83 | wps 6060.1 | wpb 10324.2 | bsz 375 | num_updates 50099 | best_bleu 23.03
2021-01-02 21:17:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 119 @ 50099 updates, score 22.83) (writing took 2.9139106646180153 seconds)
2021-01-02 21:17:59 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-02 21:17:59 | INFO | train | epoch 119 | symm_mse 0.256 | loss 2.984 | nll_loss 0.818 | ppl 1.76 | wps 20552.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50099 | lr 9.78827e-06 | gnorm 0.645 | train_wall 263 | wall 33909
2021-01-02 21:17:59 | INFO | fairseq.trainer | begin training epoch 120
2021-01-02 21:18:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:18:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:18:03 | INFO | train_inner | epoch 120:      1 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.811, ppl=1.75, wps=16389.7, ups=1.17, wpb=13969.4, bsz=502.6, num_updates=50100, lr=9.78818e-06, gnorm=0.646, train_wall=62, wall=33913
2021-01-02 21:19:05 | INFO | train_inner | epoch 120:    101 / 421 symm_mse=0.255, loss=2.978, nll_loss=0.813, ppl=1.76, wps=22605.5, ups=1.61, wpb=14059.1, bsz=491.9, num_updates=50200, lr=9.77842e-06, gnorm=0.64, train_wall=62, wall=33975
2021-01-02 21:20:08 | INFO | train_inner | epoch 120:    201 / 421 symm_mse=0.255, loss=2.979, nll_loss=0.815, ppl=1.76, wps=22078.3, ups=1.59, wpb=13908.9, bsz=492.2, num_updates=50300, lr=9.7687e-06, gnorm=0.647, train_wall=63, wall=34038
2021-01-02 21:21:11 | INFO | train_inner | epoch 120:    301 / 421 symm_mse=0.257, loss=2.98, nll_loss=0.813, ppl=1.76, wps=22296.3, ups=1.59, wpb=13996.1, bsz=476, num_updates=50400, lr=9.759e-06, gnorm=0.646, train_wall=63, wall=34101
2021-01-02 21:22:13 | INFO | train_inner | epoch 120:    401 / 421 symm_mse=0.259, loss=2.999, nll_loss=0.831, ppl=1.78, wps=22301.2, ups=1.59, wpb=14006, bsz=494.6, num_updates=50500, lr=9.74933e-06, gnorm=0.639, train_wall=63, wall=34164
2021-01-02 21:22:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:22:43 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.845 | ppl 14.37 | bleu 22.87 | wps 5990.7 | wpb 10324.2 | bsz 375 | num_updates 50520 | best_bleu 23.03
2021-01-02 21:22:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:22:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:22:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 120 @ 50520 updates, score 22.87) (writing took 3.0864285696297884 seconds)
2021-01-02 21:22:46 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-02 21:22:46 | INFO | train | epoch 120 | symm_mse 0.256 | loss 2.984 | nll_loss 0.818 | ppl 1.76 | wps 20498.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50520 | lr 9.7474e-06 | gnorm 0.644 | train_wall 263 | wall 34196
2021-01-02 21:22:46 | INFO | fairseq.trainer | begin training epoch 121
2021-01-02 21:22:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:22:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:23:38 | INFO | train_inner | epoch 121:     80 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.81, ppl=1.75, wps=16317.2, ups=1.18, wpb=13853.9, bsz=502, num_updates=50600, lr=9.7397e-06, gnorm=0.649, train_wall=62, wall=34249
2021-01-02 21:24:41 | INFO | train_inner | epoch 121:    180 / 421 symm_mse=0.258, loss=2.986, nll_loss=0.818, ppl=1.76, wps=22247.5, ups=1.6, wpb=13886.3, bsz=493, num_updates=50700, lr=9.73009e-06, gnorm=0.65, train_wall=62, wall=34311
2021-01-02 21:25:43 | INFO | train_inner | epoch 121:    280 / 421 symm_mse=0.254, loss=2.979, nll_loss=0.816, ppl=1.76, wps=22577.6, ups=1.6, wpb=14115.8, bsz=502.8, num_updates=50800, lr=9.7205e-06, gnorm=0.632, train_wall=62, wall=34374
2021-01-02 21:26:46 | INFO | train_inner | epoch 121:    380 / 421 symm_mse=0.259, loss=2.994, nll_loss=0.825, ppl=1.77, wps=22185.7, ups=1.59, wpb=13937.7, bsz=495.9, num_updates=50900, lr=9.71095e-06, gnorm=0.65, train_wall=63, wall=34437
2021-01-02 21:27:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:27:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:27:28 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.83 | wps 6114.9 | wpb 10324.2 | bsz 375 | num_updates 50941 | best_bleu 23.03
2021-01-02 21:27:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:27:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 121 @ 50941 updates, score 22.83) (writing took 3.034189037978649 seconds)
2021-01-02 21:27:31 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-02 21:27:31 | INFO | train | epoch 121 | symm_mse 0.256 | loss 2.983 | nll_loss 0.817 | ppl 1.76 | wps 20577.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50941 | lr 9.70704e-06 | gnorm 0.644 | train_wall 262 | wall 34482
2021-01-02 21:27:31 | INFO | fairseq.trainer | begin training epoch 122
2021-01-02 21:27:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:27:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:28:11 | INFO | train_inner | epoch 122:     59 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.816, ppl=1.76, wps=16406.9, ups=1.18, wpb=13933, bsz=477.6, num_updates=51000, lr=9.70143e-06, gnorm=0.646, train_wall=62, wall=34522
2021-01-02 21:29:14 | INFO | train_inner | epoch 122:    159 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.817, ppl=1.76, wps=22424.8, ups=1.58, wpb=14155.5, bsz=499.6, num_updates=51100, lr=9.69193e-06, gnorm=0.635, train_wall=63, wall=34585
2021-01-02 21:30:17 | INFO | train_inner | epoch 122:    259 / 421 symm_mse=0.254, loss=2.981, nll_loss=0.818, ppl=1.76, wps=22144.8, ups=1.58, wpb=14008.1, bsz=513.3, num_updates=51200, lr=9.68246e-06, gnorm=0.638, train_wall=63, wall=34648
2021-01-02 21:31:20 | INFO | train_inner | epoch 122:    359 / 421 symm_mse=0.255, loss=2.976, nll_loss=0.812, ppl=1.76, wps=22301.2, ups=1.59, wpb=14063.1, bsz=482.8, num_updates=51300, lr=9.67302e-06, gnorm=0.639, train_wall=63, wall=34711
2021-01-02 21:31:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:32:16 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.848 | ppl 14.4 | bleu 22.88 | wps 6001.6 | wpb 10324.2 | bsz 375 | num_updates 51362 | best_bleu 23.03
2021-01-02 21:32:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 122 @ 51362 updates, score 22.88) (writing took 3.0536505430936813 seconds)
2021-01-02 21:32:19 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2021-01-02 21:32:19 | INFO | train | epoch 122 | symm_mse 0.256 | loss 2.982 | nll_loss 0.817 | ppl 1.76 | wps 20466.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 51362 | lr 9.66718e-06 | gnorm 0.644 | train_wall 264 | wall 34769
2021-01-02 21:32:19 | INFO | fairseq.trainer | begin training epoch 123
2021-01-02 21:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:32:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:32:45 | INFO | train_inner | epoch 123:     38 / 421 symm_mse=0.26, loss=2.995, nll_loss=0.825, ppl=1.77, wps=16104.4, ups=1.18, wpb=13655.5, bsz=474.9, num_updates=51400, lr=9.6636e-06, gnorm=0.665, train_wall=62, wall=34796
2021-01-02 21:33:48 | INFO | train_inner | epoch 123:    138 / 421 symm_mse=0.258, loss=2.988, nll_loss=0.819, ppl=1.76, wps=22223.4, ups=1.59, wpb=13976.4, bsz=481.8, num_updates=51500, lr=9.65422e-06, gnorm=0.645, train_wall=63, wall=34859
2021-01-02 21:34:51 | INFO | train_inner | epoch 123:    238 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.816, ppl=1.76, wps=22416.1, ups=1.6, wpb=14039.3, bsz=500.6, num_updates=51600, lr=9.64486e-06, gnorm=0.641, train_wall=62, wall=34921
2021-01-02 21:35:54 | INFO | train_inner | epoch 123:    338 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.815, ppl=1.76, wps=22240.5, ups=1.59, wpb=13996.7, bsz=503.3, num_updates=51700, lr=9.63552e-06, gnorm=0.637, train_wall=63, wall=34984
2021-01-02 21:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:36:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:36:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:36:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:36:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:36:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:36:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:36:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:37:02 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.88 | wps 6090.7 | wpb 10324.2 | bsz 375 | num_updates 51783 | best_bleu 23.03
2021-01-02 21:37:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:37:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 123 @ 51783 updates, score 22.88) (writing took 3.01218194141984 seconds)
2021-01-02 21:37:05 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2021-01-02 21:37:05 | INFO | train | epoch 123 | symm_mse 0.256 | loss 2.982 | nll_loss 0.817 | ppl 1.76 | wps 20536 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 51783 | lr 9.6278e-06 | gnorm 0.643 | train_wall 263 | wall 35056
2021-01-02 21:37:05 | INFO | fairseq.trainer | begin training epoch 124
2021-01-02 21:37:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:37:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:37:19 | INFO | train_inner | epoch 124:     17 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.815, ppl=1.76, wps=16303.6, ups=1.17, wpb=13896, bsz=488.3, num_updates=51800, lr=9.62622e-06, gnorm=0.647, train_wall=62, wall=35070
2021-01-02 21:38:21 | INFO | train_inner | epoch 124:    117 / 421 symm_mse=0.255, loss=2.974, nll_loss=0.81, ppl=1.75, wps=22682.3, ups=1.61, wpb=14064.5, bsz=493.2, num_updates=51900, lr=9.61694e-06, gnorm=0.642, train_wall=62, wall=35132
2021-01-02 21:39:24 | INFO | train_inner | epoch 124:    217 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.809, ppl=1.75, wps=22285.7, ups=1.59, wpb=14007.4, bsz=492.8, num_updates=52000, lr=9.60769e-06, gnorm=0.636, train_wall=63, wall=35194
2021-01-02 21:40:27 | INFO | train_inner | epoch 124:    317 / 421 symm_mse=0.256, loss=2.985, nll_loss=0.82, ppl=1.77, wps=22223.5, ups=1.59, wpb=13972, bsz=515, num_updates=52100, lr=9.59846e-06, gnorm=0.638, train_wall=63, wall=35257
2021-01-02 21:41:29 | INFO | train_inner | epoch 124:    417 / 421 symm_mse=0.258, loss=2.995, nll_loss=0.827, ppl=1.77, wps=22199.6, ups=1.59, wpb=13928.4, bsz=475.2, num_updates=52200, lr=9.58927e-06, gnorm=0.647, train_wall=63, wall=35320
2021-01-02 21:41:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:41:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:41:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:41:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:41:49 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.848 | ppl 14.4 | bleu 22.78 | wps 5599.8 | wpb 10324.2 | bsz 375 | num_updates 52204 | best_bleu 23.03
2021-01-02 21:41:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:41:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 124 @ 52204 updates, score 22.78) (writing took 2.990903738886118 seconds)
2021-01-02 21:41:52 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2021-01-02 21:41:52 | INFO | train | epoch 124 | symm_mse 0.256 | loss 2.982 | nll_loss 0.817 | ppl 1.76 | wps 20468.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 52204 | lr 9.5889e-06 | gnorm 0.643 | train_wall 263 | wall 35343
2021-01-02 21:41:52 | INFO | fairseq.trainer | begin training epoch 125
2021-01-02 21:41:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:42:55 | INFO | train_inner | epoch 125:     96 / 421 symm_mse=0.256, loss=2.979, nll_loss=0.813, ppl=1.76, wps=16156.9, ups=1.17, wpb=13835, bsz=484.3, num_updates=52300, lr=9.58009e-06, gnorm=0.649, train_wall=62, wall=35406
2021-01-02 21:43:58 | INFO | train_inner | epoch 125:    196 / 421 symm_mse=0.256, loss=2.983, nll_loss=0.818, ppl=1.76, wps=22224, ups=1.59, wpb=13957.3, bsz=502.6, num_updates=52400, lr=9.57095e-06, gnorm=0.638, train_wall=63, wall=35468
2021-01-02 21:45:01 | INFO | train_inner | epoch 125:    296 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.814, ppl=1.76, wps=22463, ups=1.6, wpb=14079, bsz=480.6, num_updates=52500, lr=9.56183e-06, gnorm=0.643, train_wall=62, wall=35531
2021-01-02 21:46:03 | INFO | train_inner | epoch 125:    396 / 421 symm_mse=0.255, loss=2.982, nll_loss=0.819, ppl=1.76, wps=22412.3, ups=1.6, wpb=14031.2, bsz=496.6, num_updates=52600, lr=9.55274e-06, gnorm=0.64, train_wall=62, wall=35594
2021-01-02 21:46:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:46:35 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.845 | ppl 14.37 | bleu 22.98 | wps 6024 | wpb 10324.2 | bsz 375 | num_updates 52625 | best_bleu 23.03
2021-01-02 21:46:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:46:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 125 @ 52625 updates, score 22.98) (writing took 3.064945427700877 seconds)
2021-01-02 21:46:38 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2021-01-02 21:46:38 | INFO | train | epoch 125 | symm_mse 0.256 | loss 2.981 | nll_loss 0.817 | ppl 1.76 | wps 20571.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 52625 | lr 9.55047e-06 | gnorm 0.643 | train_wall 262 | wall 35629
2021-01-02 21:46:38 | INFO | fairseq.trainer | begin training epoch 126
2021-01-02 21:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:47:28 | INFO | train_inner | epoch 126:     75 / 421 symm_mse=0.253, loss=2.975, nll_loss=0.814, ppl=1.76, wps=16537, ups=1.18, wpb=13991, bsz=505.5, num_updates=52700, lr=9.54367e-06, gnorm=0.648, train_wall=62, wall=35678
2021-01-02 21:48:31 | INFO | train_inner | epoch 126:    175 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.812, ppl=1.76, wps=22315.5, ups=1.59, wpb=14009.5, bsz=506.2, num_updates=52800, lr=9.53463e-06, gnorm=0.638, train_wall=63, wall=35741
2021-01-02 21:49:33 | INFO | train_inner | epoch 126:    275 / 421 symm_mse=0.26, loss=3, nll_loss=0.829, ppl=1.78, wps=22155.8, ups=1.6, wpb=13824.1, bsz=477.8, num_updates=52900, lr=9.52561e-06, gnorm=0.654, train_wall=62, wall=35804
2021-01-02 21:50:36 | INFO | train_inner | epoch 126:    375 / 421 symm_mse=0.256, loss=2.98, nll_loss=0.815, ppl=1.76, wps=22206.2, ups=1.6, wpb=13908.7, bsz=493.2, num_updates=53000, lr=9.51662e-06, gnorm=0.645, train_wall=62, wall=35866
2021-01-02 21:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:51:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:51:21 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.89 | wps 6107.8 | wpb 10324.2 | bsz 375 | num_updates 53046 | best_bleu 23.03
2021-01-02 21:51:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:51:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 126 @ 53046 updates, score 22.89) (writing took 3.088775672018528 seconds)
2021-01-02 21:51:24 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2021-01-02 21:51:24 | INFO | train | epoch 126 | symm_mse 0.256 | loss 2.981 | nll_loss 0.816 | ppl 1.76 | wps 20587.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53046 | lr 9.51249e-06 | gnorm 0.644 | train_wall 262 | wall 35915
2021-01-02 21:51:24 | INFO | fairseq.trainer | begin training epoch 127
2021-01-02 21:51:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:51:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:52:00 | INFO | train_inner | epoch 127:     54 / 421 symm_mse=0.255, loss=2.979, nll_loss=0.814, ppl=1.76, wps=16438.6, ups=1.18, wpb=13942.5, bsz=473, num_updates=53100, lr=9.50765e-06, gnorm=0.642, train_wall=62, wall=35951
2021-01-02 21:53:04 | INFO | train_inner | epoch 127:    154 / 421 symm_mse=0.255, loss=2.979, nll_loss=0.813, ppl=1.76, wps=22083.9, ups=1.58, wpb=13969.6, bsz=493.5, num_updates=53200, lr=9.49871e-06, gnorm=0.638, train_wall=63, wall=36014
2021-01-02 21:54:07 | INFO | train_inner | epoch 127:    254 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.812, ppl=1.76, wps=22262.8, ups=1.59, wpb=14016.7, bsz=501.6, num_updates=53300, lr=9.4898e-06, gnorm=0.642, train_wall=63, wall=36077
2021-01-02 21:55:09 | INFO | train_inner | epoch 127:    354 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.817, ppl=1.76, wps=22354.2, ups=1.59, wpb=14029.2, bsz=490.6, num_updates=53400, lr=9.48091e-06, gnorm=0.64, train_wall=63, wall=36140
2021-01-02 21:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 21:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:55:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:55:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:55:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 21:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 21:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 21:56:08 | INFO | valid | epoch 127 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.849 | ppl 14.41 | bleu 22.86 | wps 6052.5 | wpb 10324.2 | bsz 375 | num_updates 53467 | best_bleu 23.03
2021-01-02 21:56:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 21:56:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:56:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:56:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:56:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:56:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:56:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:56:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 127 @ 53467 updates, score 22.86) (writing took 3.0495405681431293 seconds)
2021-01-02 21:56:11 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2021-01-02 21:56:11 | INFO | train | epoch 127 | symm_mse 0.255 | loss 2.981 | nll_loss 0.816 | ppl 1.76 | wps 20494.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53467 | lr 9.47497e-06 | gnorm 0.642 | train_wall 263 | wall 36202
2021-01-02 21:56:11 | INFO | fairseq.trainer | begin training epoch 128
2021-01-02 21:56:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 21:56:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 21:56:34 | INFO | train_inner | epoch 128:     33 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.817, ppl=1.76, wps=16448.8, ups=1.18, wpb=13966.9, bsz=496.8, num_updates=53500, lr=9.47204e-06, gnorm=0.646, train_wall=62, wall=36225
2021-01-02 21:57:37 | INFO | train_inner | epoch 128:    133 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.817, ppl=1.76, wps=22264.8, ups=1.6, wpb=13921.2, bsz=488, num_updates=53600, lr=9.4632e-06, gnorm=0.644, train_wall=62, wall=36287
2021-01-02 21:58:40 | INFO | train_inner | epoch 128:    233 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.816, ppl=1.76, wps=22272.2, ups=1.58, wpb=14080.6, bsz=490.6, num_updates=53700, lr=9.45439e-06, gnorm=0.636, train_wall=63, wall=36351
2021-01-02 21:59:43 | INFO | train_inner | epoch 128:    333 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.815, ppl=1.76, wps=22144, ups=1.59, wpb=13957.7, bsz=489, num_updates=53800, lr=9.4456e-06, gnorm=0.644, train_wall=63, wall=36414
2021-01-02 22:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:00:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:00:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:00:55 | INFO | valid | epoch 128 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.847 | ppl 14.39 | bleu 22.77 | wps 6034 | wpb 10324.2 | bsz 375 | num_updates 53888 | best_bleu 23.03
2021-01-02 22:00:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:00:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:00:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:00:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 128 @ 53888 updates, score 22.77) (writing took 2.981378396973014 seconds)
2021-01-02 22:00:58 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2021-01-02 22:00:58 | INFO | train | epoch 128 | symm_mse 0.255 | loss 2.98 | nll_loss 0.816 | ppl 1.76 | wps 20516.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53888 | lr 9.43788e-06 | gnorm 0.643 | train_wall 263 | wall 36488
2021-01-02 22:00:58 | INFO | fairseq.trainer | begin training epoch 129
2021-01-02 22:00:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:01:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:01:08 | INFO | train_inner | epoch 129:     12 / 421 symm_mse=0.256, loss=2.986, nll_loss=0.82, ppl=1.77, wps=16229.8, ups=1.17, wpb=13849.5, bsz=502.1, num_updates=53900, lr=9.43683e-06, gnorm=0.653, train_wall=62, wall=36499
2021-01-02 22:02:10 | INFO | train_inner | epoch 129:    112 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.811, ppl=1.75, wps=22420.8, ups=1.61, wpb=13914.1, bsz=501.1, num_updates=54000, lr=9.42809e-06, gnorm=0.639, train_wall=62, wall=36561
2021-01-02 22:03:14 | INFO | train_inner | epoch 129:    212 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.811, ppl=1.75, wps=22120.7, ups=1.58, wpb=13959, bsz=522, num_updates=54100, lr=9.41937e-06, gnorm=0.64, train_wall=63, wall=36624
2021-01-02 22:04:16 | INFO | train_inner | epoch 129:    312 / 421 symm_mse=0.255, loss=2.978, nll_loss=0.815, ppl=1.76, wps=22565, ups=1.59, wpb=14206.1, bsz=474.5, num_updates=54200, lr=9.41068e-06, gnorm=0.634, train_wall=63, wall=36687
2021-01-02 22:05:19 | INFO | train_inner | epoch 129:    412 / 421 symm_mse=0.257, loss=2.991, nll_loss=0.825, ppl=1.77, wps=22141.9, ups=1.59, wpb=13899.2, bsz=478.6, num_updates=54300, lr=9.40201e-06, gnorm=0.656, train_wall=63, wall=36750
2021-01-02 22:05:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:05:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:05:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:05:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:05:43 | INFO | valid | epoch 129 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.8 | wps 5334 | wpb 10324.2 | bsz 375 | num_updates 54309 | best_bleu 23.03
2021-01-02 22:05:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:05:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:05:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 129 @ 54309 updates, score 22.8) (writing took 2.9962871465831995 seconds)
2021-01-02 22:05:46 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2021-01-02 22:05:46 | INFO | train | epoch 129 | symm_mse 0.255 | loss 2.98 | nll_loss 0.816 | ppl 1.76 | wps 20393.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 54309 | lr 9.40123e-06 | gnorm 0.644 | train_wall 263 | wall 36777
2021-01-02 22:05:46 | INFO | fairseq.trainer | begin training epoch 130
2021-01-02 22:05:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:05:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:06:45 | INFO | train_inner | epoch 130:     91 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.81, ppl=1.75, wps=16314, ups=1.16, wpb=14042.1, bsz=491.2, num_updates=54400, lr=9.39336e-06, gnorm=0.636, train_wall=61, wall=36836
2021-01-02 22:07:48 | INFO | train_inner | epoch 130:    191 / 421 symm_mse=0.255, loss=2.981, nll_loss=0.817, ppl=1.76, wps=22071.3, ups=1.59, wpb=13866.6, bsz=486.7, num_updates=54500, lr=9.38474e-06, gnorm=0.64, train_wall=63, wall=36899
2021-01-02 22:08:51 | INFO | train_inner | epoch 130:    291 / 421 symm_mse=0.257, loss=2.987, nll_loss=0.82, ppl=1.77, wps=22168.9, ups=1.59, wpb=13919.2, bsz=479.9, num_updates=54600, lr=9.37614e-06, gnorm=0.649, train_wall=63, wall=36962
2021-01-02 22:09:54 | INFO | train_inner | epoch 130:    391 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.812, ppl=1.76, wps=22419, ups=1.6, wpb=14053.1, bsz=508.8, num_updates=54700, lr=9.36757e-06, gnorm=0.635, train_wall=62, wall=37024
2021-01-02 22:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:10:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:10:29 | INFO | valid | epoch 130 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.847 | ppl 14.39 | bleu 22.89 | wps 6081.5 | wpb 10324.2 | bsz 375 | num_updates 54730 | best_bleu 23.03
2021-01-02 22:10:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:10:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:10:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 130 @ 54730 updates, score 22.89) (writing took 3.00898171402514 seconds)
2021-01-02 22:10:32 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2021-01-02 22:10:32 | INFO | train | epoch 130 | symm_mse 0.255 | loss 2.979 | nll_loss 0.815 | ppl 1.76 | wps 20579.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 54730 | lr 9.365e-06 | gnorm 0.641 | train_wall 262 | wall 37062
2021-01-02 22:10:32 | INFO | fairseq.trainer | begin training epoch 131
2021-01-02 22:10:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:10:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:11:18 | INFO | train_inner | epoch 131:     70 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.811, ppl=1.75, wps=16538.3, ups=1.18, wpb=13975.1, bsz=498, num_updates=54800, lr=9.35902e-06, gnorm=0.648, train_wall=62, wall=37109
2021-01-02 22:12:21 | INFO | train_inner | epoch 131:    170 / 421 symm_mse=0.257, loss=2.985, nll_loss=0.818, ppl=1.76, wps=22094, ups=1.59, wpb=13871.8, bsz=491, num_updates=54900, lr=9.35049e-06, gnorm=0.65, train_wall=63, wall=37172
2021-01-02 22:13:23 | INFO | train_inner | epoch 131:    270 / 421 symm_mse=0.256, loss=2.985, nll_loss=0.82, ppl=1.77, wps=22431.2, ups=1.6, wpb=13997.9, bsz=492.2, num_updates=55000, lr=9.34199e-06, gnorm=0.646, train_wall=62, wall=37234
2021-01-02 22:14:27 | INFO | train_inner | epoch 131:    370 / 421 symm_mse=0.253, loss=2.973, nll_loss=0.811, ppl=1.75, wps=22343.4, ups=1.58, wpb=14128.9, bsz=490.2, num_updates=55100, lr=9.33351e-06, gnorm=0.637, train_wall=63, wall=37297
2021-01-02 22:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:15:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:15:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:15:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:15:15 | INFO | valid | epoch 131 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.89 | wps 6027.7 | wpb 10324.2 | bsz 375 | num_updates 55151 | best_bleu 23.03
2021-01-02 22:15:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:15:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 131 @ 55151 updates, score 22.89) (writing took 3.005972918123007 seconds)
2021-01-02 22:15:18 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2021-01-02 22:15:18 | INFO | train | epoch 131 | symm_mse 0.255 | loss 2.979 | nll_loss 0.815 | ppl 1.76 | wps 20523.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 55151 | lr 9.32919e-06 | gnorm 0.645 | train_wall 263 | wall 37349
2021-01-02 22:15:18 | INFO | fairseq.trainer | begin training epoch 132
2021-01-02 22:15:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:15:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:15:52 | INFO | train_inner | epoch 132:     49 / 421 symm_mse=0.257, loss=2.986, nll_loss=0.82, ppl=1.76, wps=16275.2, ups=1.17, wpb=13873.5, bsz=484.8, num_updates=55200, lr=9.32505e-06, gnorm=0.647, train_wall=62, wall=37382
2021-01-02 22:16:55 | INFO | train_inner | epoch 132:    149 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.816, ppl=1.76, wps=22301.6, ups=1.59, wpb=14065.3, bsz=496.2, num_updates=55300, lr=9.31661e-06, gnorm=0.641, train_wall=63, wall=37445
2021-01-02 22:17:58 | INFO | train_inner | epoch 132:    249 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.804, ppl=1.75, wps=22427.5, ups=1.58, wpb=14214.4, bsz=496.5, num_updates=55400, lr=9.3082e-06, gnorm=0.63, train_wall=63, wall=37509
2021-01-02 22:19:01 | INFO | train_inner | epoch 132:    349 / 421 symm_mse=0.256, loss=2.984, nll_loss=0.817, ppl=1.76, wps=22033.1, ups=1.59, wpb=13841.7, bsz=502.2, num_updates=55500, lr=9.29981e-06, gnorm=0.647, train_wall=63, wall=37572
2021-01-02 22:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:19:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:19:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:19:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:19:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:19:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:19:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:19:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:19:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:19:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:19:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:19:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:20:03 | INFO | valid | epoch 132 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.88 | wps 5979.4 | wpb 10324.2 | bsz 375 | num_updates 55572 | best_bleu 23.03
2021-01-02 22:20:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:20:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:20:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:20:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:20:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 132 @ 55572 updates, score 22.88) (writing took 3.1068395916372538 seconds)
2021-01-02 22:20:06 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2021-01-02 22:20:06 | INFO | train | epoch 132 | symm_mse 0.255 | loss 2.979 | nll_loss 0.814 | ppl 1.76 | wps 20455 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 55572 | lr 9.29378e-06 | gnorm 0.643 | train_wall 264 | wall 37636
2021-01-02 22:20:06 | INFO | fairseq.trainer | begin training epoch 133
2021-01-02 22:20:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:20:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:20:26 | INFO | train_inner | epoch 133:     28 / 421 symm_mse=0.255, loss=2.977, nll_loss=0.811, ppl=1.76, wps=16070.6, ups=1.17, wpb=13721.1, bsz=484.5, num_updates=55600, lr=9.29144e-06, gnorm=0.656, train_wall=62, wall=37657
2021-01-02 22:21:29 | INFO | train_inner | epoch 133:    128 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.817, ppl=1.76, wps=22534.4, ups=1.61, wpb=14001, bsz=506.6, num_updates=55700, lr=9.2831e-06, gnorm=0.638, train_wall=62, wall=37719
2021-01-02 22:22:32 | INFO | train_inner | epoch 133:    228 / 421 symm_mse=0.258, loss=2.986, nll_loss=0.818, ppl=1.76, wps=21982.4, ups=1.58, wpb=13881.1, bsz=469.8, num_updates=55800, lr=9.27478e-06, gnorm=0.652, train_wall=63, wall=37782
2021-01-02 22:23:34 | INFO | train_inner | epoch 133:    328 / 421 symm_mse=0.254, loss=2.983, nll_loss=0.821, ppl=1.77, wps=22402.7, ups=1.6, wpb=14043.3, bsz=512.9, num_updates=55900, lr=9.26648e-06, gnorm=0.641, train_wall=62, wall=37845
2021-01-02 22:24:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:24:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:24:51 | INFO | valid | epoch 133 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.848 | ppl 14.4 | bleu 22.82 | wps 5316.8 | wpb 10324.2 | bsz 375 | num_updates 55993 | best_bleu 23.03
2021-01-02 22:24:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:24:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 133 @ 55993 updates, score 22.82) (writing took 3.0637007504701614 seconds)
2021-01-02 22:24:54 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2021-01-02 22:24:54 | INFO | train | epoch 133 | symm_mse 0.255 | loss 2.978 | nll_loss 0.815 | ppl 1.76 | wps 20386.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 55993 | lr 9.25878e-06 | gnorm 0.643 | train_wall 263 | wall 37925
2021-01-02 22:24:54 | INFO | fairseq.trainer | begin training epoch 134
2021-01-02 22:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:25:02 | INFO | train_inner | epoch 134:      7 / 421 symm_mse=0.253, loss=2.973, nll_loss=0.81, ppl=1.75, wps=15857.9, ups=1.14, wpb=13889.3, bsz=476, num_updates=56000, lr=9.2582e-06, gnorm=0.645, train_wall=63, wall=37933
2021-01-02 22:26:04 | INFO | train_inner | epoch 134:    107 / 421 symm_mse=0.251, loss=2.963, nll_loss=0.804, ppl=1.75, wps=22539.9, ups=1.61, wpb=13987.8, bsz=506.9, num_updates=56100, lr=9.24995e-06, gnorm=0.637, train_wall=62, wall=37995
2021-01-02 22:27:07 | INFO | train_inner | epoch 134:    207 / 421 symm_mse=0.258, loss=2.99, nll_loss=0.822, ppl=1.77, wps=21905.2, ups=1.58, wpb=13824.7, bsz=481.8, num_updates=56200, lr=9.24171e-06, gnorm=0.65, train_wall=63, wall=38058
2021-01-02 22:28:10 | INFO | train_inner | epoch 134:    307 / 421 symm_mse=0.257, loss=2.981, nll_loss=0.814, ppl=1.76, wps=22308, ups=1.59, wpb=14007.4, bsz=477.7, num_updates=56300, lr=9.2335e-06, gnorm=0.645, train_wall=63, wall=38121
2021-01-02 22:29:13 | INFO | train_inner | epoch 134:    407 / 421 symm_mse=0.252, loss=2.974, nll_loss=0.814, ppl=1.76, wps=22425.7, ups=1.58, wpb=14152.7, bsz=501.6, num_updates=56400, lr=9.22531e-06, gnorm=0.634, train_wall=63, wall=38184
2021-01-02 22:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:29:38 | INFO | valid | epoch 134 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 23 | wps 6020.4 | wpb 10324.2 | bsz 375 | num_updates 56414 | best_bleu 23.03
2021-01-02 22:29:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:29:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 134 @ 56414 updates, score 23.0) (writing took 2.9070545583963394 seconds)
2021-01-02 22:29:41 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2021-01-02 22:29:41 | INFO | train | epoch 134 | symm_mse 0.254 | loss 2.977 | nll_loss 0.814 | ppl 1.76 | wps 20504.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 56414 | lr 9.22417e-06 | gnorm 0.644 | train_wall 263 | wall 38212
2021-01-02 22:29:41 | INFO | fairseq.trainer | begin training epoch 135
2021-01-02 22:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:30:38 | INFO | train_inner | epoch 135:     86 / 421 symm_mse=0.257, loss=2.992, nll_loss=0.825, ppl=1.77, wps=16335.2, ups=1.18, wpb=13853.9, bsz=490.2, num_updates=56500, lr=9.21714e-06, gnorm=0.656, train_wall=62, wall=38269
2021-01-02 22:31:41 | INFO | train_inner | epoch 135:    186 / 421 symm_mse=0.254, loss=2.979, nll_loss=0.817, ppl=1.76, wps=22320.1, ups=1.58, wpb=14083.1, bsz=505, num_updates=56600, lr=9.209e-06, gnorm=0.634, train_wall=63, wall=38332
2021-01-02 22:32:44 | INFO | train_inner | epoch 135:    286 / 421 symm_mse=0.255, loss=2.977, nll_loss=0.813, ppl=1.76, wps=22344.9, ups=1.59, wpb=14089, bsz=481.4, num_updates=56700, lr=9.20087e-06, gnorm=0.64, train_wall=63, wall=38395
2021-01-02 22:33:47 | INFO | train_inner | epoch 135:    386 / 421 symm_mse=0.253, loss=2.969, nll_loss=0.809, ppl=1.75, wps=22107.2, ups=1.6, wpb=13823.2, bsz=497.7, num_updates=56800, lr=9.19277e-06, gnorm=0.643, train_wall=62, wall=38457
2021-01-02 22:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:34:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:34:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:34:25 | INFO | valid | epoch 135 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.96 | wps 5965.1 | wpb 10324.2 | bsz 375 | num_updates 56835 | best_bleu 23.03
2021-01-02 22:34:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:34:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 135 @ 56835 updates, score 22.96) (writing took 3.0155263282358646 seconds)
2021-01-02 22:34:28 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2021-01-02 22:34:28 | INFO | train | epoch 135 | symm_mse 0.254 | loss 2.977 | nll_loss 0.814 | ppl 1.76 | wps 20485.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 56835 | lr 9.18994e-06 | gnorm 0.641 | train_wall 263 | wall 38499
2021-01-02 22:34:28 | INFO | fairseq.trainer | begin training epoch 136
2021-01-02 22:34:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:34:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:35:12 | INFO | train_inner | epoch 136:     65 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.814, ppl=1.76, wps=16189.1, ups=1.18, wpb=13757.8, bsz=476.9, num_updates=56900, lr=9.18469e-06, gnorm=0.654, train_wall=62, wall=38542
2021-01-02 22:36:14 | INFO | train_inner | epoch 136:    165 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.817, ppl=1.76, wps=22388.4, ups=1.6, wpb=13990.1, bsz=480.3, num_updates=57000, lr=9.17663e-06, gnorm=0.645, train_wall=62, wall=38605
2021-01-02 22:37:17 | INFO | train_inner | epoch 136:    265 / 421 symm_mse=0.25, loss=2.965, nll_loss=0.807, ppl=1.75, wps=22445.7, ups=1.59, wpb=14083, bsz=516.5, num_updates=57100, lr=9.16859e-06, gnorm=0.633, train_wall=63, wall=38667
2021-01-02 22:38:20 | INFO | train_inner | epoch 136:    365 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.808, ppl=1.75, wps=22251, ups=1.59, wpb=14023.1, bsz=493, num_updates=57200, lr=9.16057e-06, gnorm=0.644, train_wall=63, wall=38730
2021-01-02 22:38:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:38:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:38:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:38:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:38:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:38:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:38:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:38:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:38:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:39:11 | INFO | valid | epoch 136 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.91 | wps 5930.5 | wpb 10324.2 | bsz 375 | num_updates 57256 | best_bleu 23.03
2021-01-02 22:39:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:39:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 136 @ 57256 updates, score 22.91) (writing took 3.038022642955184 seconds)
2021-01-02 22:39:14 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2021-01-02 22:39:14 | INFO | train | epoch 136 | symm_mse 0.254 | loss 2.976 | nll_loss 0.813 | ppl 1.76 | wps 20551.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 57256 | lr 9.15609e-06 | gnorm 0.645 | train_wall 262 | wall 38785
2021-01-02 22:39:14 | INFO | fairseq.trainer | begin training epoch 137
2021-01-02 22:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:39:45 | INFO | train_inner | epoch 137:     44 / 421 symm_mse=0.255, loss=2.981, nll_loss=0.817, ppl=1.76, wps=16353.3, ups=1.18, wpb=13902.7, bsz=482.4, num_updates=57300, lr=9.15258e-06, gnorm=0.651, train_wall=62, wall=38815
2021-01-02 22:40:48 | INFO | train_inner | epoch 137:    144 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.81, ppl=1.75, wps=22275.4, ups=1.59, wpb=14012.8, bsz=500.2, num_updates=57400, lr=9.1446e-06, gnorm=0.64, train_wall=63, wall=38878
2021-01-02 22:41:51 | INFO | train_inner | epoch 137:    244 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.805, ppl=1.75, wps=22420.6, ups=1.59, wpb=14122.8, bsz=507.5, num_updates=57500, lr=9.13664e-06, gnorm=0.633, train_wall=63, wall=38941
2021-01-02 22:42:53 | INFO | train_inner | epoch 137:    344 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.816, ppl=1.76, wps=22378.2, ups=1.59, wpb=14032.3, bsz=505, num_updates=57600, lr=9.12871e-06, gnorm=0.636, train_wall=63, wall=39004
2021-01-02 22:43:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:43:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:43:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:43:58 | INFO | valid | epoch 137 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.846 | ppl 14.38 | bleu 23.03 | wps 5804.4 | wpb 10324.2 | bsz 375 | num_updates 57677 | best_bleu 23.03
2021-01-02 22:43:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:44:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 137 @ 57677 updates, score 23.03) (writing took 4.881385359913111 seconds)
2021-01-02 22:44:03 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2021-01-02 22:44:03 | INFO | train | epoch 137 | symm_mse 0.254 | loss 2.976 | nll_loss 0.813 | ppl 1.76 | wps 20354.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 57677 | lr 9.12261e-06 | gnorm 0.644 | train_wall 263 | wall 39074
2021-01-02 22:44:03 | INFO | fairseq.trainer | begin training epoch 138
2021-01-02 22:44:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:44:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:44:21 | INFO | train_inner | epoch 138:     23 / 421 symm_mse=0.256, loss=2.981, nll_loss=0.815, ppl=1.76, wps=15778.1, ups=1.15, wpb=13779.6, bsz=461.5, num_updates=57700, lr=9.1208e-06, gnorm=0.664, train_wall=62, wall=39091
2021-01-02 22:45:23 | INFO | train_inner | epoch 138:    123 / 421 symm_mse=0.253, loss=2.976, nll_loss=0.814, ppl=1.76, wps=22433, ups=1.61, wpb=13936.9, bsz=493.2, num_updates=57800, lr=9.1129e-06, gnorm=0.637, train_wall=62, wall=39154
2021-01-02 22:46:26 | INFO | train_inner | epoch 138:    223 / 421 symm_mse=0.251, loss=2.962, nll_loss=0.803, ppl=1.75, wps=22286.2, ups=1.59, wpb=14018.1, bsz=501.2, num_updates=57900, lr=9.10503e-06, gnorm=0.642, train_wall=63, wall=39216
2021-01-02 22:47:29 | INFO | train_inner | epoch 138:    323 / 421 symm_mse=0.257, loss=2.991, nll_loss=0.824, ppl=1.77, wps=22236.8, ups=1.59, wpb=13958.9, bsz=491.7, num_updates=58000, lr=9.09718e-06, gnorm=0.644, train_wall=63, wall=39279
2021-01-02 22:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:48:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:48:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:48:46 | INFO | valid | epoch 138 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.848 | ppl 14.41 | bleu 22.86 | wps 6086.4 | wpb 10324.2 | bsz 375 | num_updates 58098 | best_bleu 23.03
2021-01-02 22:48:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:48:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 138 @ 58098 updates, score 22.86) (writing took 3.051560604944825 seconds)
2021-01-02 22:48:49 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2021-01-02 22:48:49 | INFO | train | epoch 138 | symm_mse 0.254 | loss 2.976 | nll_loss 0.813 | ppl 1.76 | wps 20562.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 58098 | lr 9.0895e-06 | gnorm 0.643 | train_wall 262 | wall 39360
2021-01-02 22:48:49 | INFO | fairseq.trainer | begin training epoch 139
2021-01-02 22:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:48:54 | INFO | train_inner | epoch 139:      2 / 421 symm_mse=0.254, loss=2.977, nll_loss=0.814, ppl=1.76, wps=16325.9, ups=1.17, wpb=13931.4, bsz=490.7, num_updates=58100, lr=9.08934e-06, gnorm=0.649, train_wall=62, wall=39365
2021-01-02 22:49:56 | INFO | train_inner | epoch 139:    102 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.806, ppl=1.75, wps=22628, ups=1.62, wpb=13998, bsz=503, num_updates=58200, lr=9.08153e-06, gnorm=0.638, train_wall=62, wall=39426
2021-01-02 22:50:59 | INFO | train_inner | epoch 139:    202 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.811, ppl=1.76, wps=22501, ups=1.59, wpb=14152.5, bsz=481.4, num_updates=58300, lr=9.07374e-06, gnorm=0.638, train_wall=63, wall=39489
2021-01-02 22:52:02 | INFO | train_inner | epoch 139:    302 / 421 symm_mse=0.255, loss=2.986, nll_loss=0.822, ppl=1.77, wps=22166.3, ups=1.59, wpb=13937.9, bsz=507.2, num_updates=58400, lr=9.06597e-06, gnorm=0.644, train_wall=63, wall=39552
2021-01-02 22:53:04 | INFO | train_inner | epoch 139:    402 / 421 symm_mse=0.255, loss=2.983, nll_loss=0.819, ppl=1.76, wps=22128.7, ups=1.59, wpb=13928.6, bsz=484.1, num_updates=58500, lr=9.05822e-06, gnorm=0.644, train_wall=63, wall=39615
2021-01-02 22:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:53:34 | INFO | valid | epoch 139 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.847 | ppl 14.39 | bleu 22.92 | wps 5418.5 | wpb 10324.2 | bsz 375 | num_updates 58519 | best_bleu 23.03
2021-01-02 22:53:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:53:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:53:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 139 @ 58519 updates, score 22.92) (writing took 3.0064127687364817 seconds)
2021-01-02 22:53:37 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2021-01-02 22:53:37 | INFO | train | epoch 139 | symm_mse 0.254 | loss 2.975 | nll_loss 0.813 | ppl 1.76 | wps 20429.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 58519 | lr 9.05675e-06 | gnorm 0.642 | train_wall 263 | wall 39648
2021-01-02 22:53:37 | INFO | fairseq.trainer | begin training epoch 140
2021-01-02 22:53:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:53:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:54:30 | INFO | train_inner | epoch 140:     81 / 421 symm_mse=0.254, loss=2.97, nll_loss=0.807, ppl=1.75, wps=16130.3, ups=1.16, wpb=13850.3, bsz=481.8, num_updates=58600, lr=9.05048e-06, gnorm=0.649, train_wall=61, wall=39701
2021-01-02 22:55:33 | INFO | train_inner | epoch 140:    181 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22197.4, ups=1.59, wpb=13955.5, bsz=503.3, num_updates=58700, lr=9.04277e-06, gnorm=0.64, train_wall=63, wall=39764
2021-01-02 22:56:36 | INFO | train_inner | epoch 140:    281 / 421 symm_mse=0.253, loss=2.976, nll_loss=0.816, ppl=1.76, wps=22354.7, ups=1.59, wpb=14042.3, bsz=499.2, num_updates=58800, lr=9.03508e-06, gnorm=0.637, train_wall=63, wall=39827
2021-01-02 22:57:39 | INFO | train_inner | epoch 140:    381 / 421 symm_mse=0.255, loss=2.981, nll_loss=0.816, ppl=1.76, wps=22182.2, ups=1.59, wpb=13915.2, bsz=478.8, num_updates=58900, lr=9.02741e-06, gnorm=0.643, train_wall=63, wall=39889
2021-01-02 22:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 22:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 22:58:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 22:58:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 22:58:21 | INFO | valid | epoch 140 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.849 | ppl 14.41 | bleu 22.94 | wps 5950.5 | wpb 10324.2 | bsz 375 | num_updates 58940 | best_bleu 23.03
2021-01-02 22:58:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 22:58:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:58:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 140 @ 58940 updates, score 22.94) (writing took 3.0080283731222153 seconds)
2021-01-02 22:58:24 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2021-01-02 22:58:24 | INFO | train | epoch 140 | symm_mse 0.253 | loss 2.974 | nll_loss 0.812 | ppl 1.76 | wps 20533 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 58940 | lr 9.02434e-06 | gnorm 0.642 | train_wall 263 | wall 39934
2021-01-02 22:58:24 | INFO | fairseq.trainer | begin training epoch 141
2021-01-02 22:58:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 22:58:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 22:59:04 | INFO | train_inner | epoch 141:     60 / 421 symm_mse=0.254, loss=2.979, nll_loss=0.817, ppl=1.76, wps=16331.1, ups=1.18, wpb=13880.8, bsz=494.8, num_updates=59000, lr=9.01975e-06, gnorm=0.651, train_wall=62, wall=39974
2021-01-02 23:00:06 | INFO | train_inner | epoch 141:    160 / 421 symm_mse=0.252, loss=2.967, nll_loss=0.807, ppl=1.75, wps=22484.8, ups=1.6, wpb=14011, bsz=500.8, num_updates=59100, lr=9.01212e-06, gnorm=0.638, train_wall=62, wall=40037
2021-01-02 23:01:09 | INFO | train_inner | epoch 141:    260 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.81, ppl=1.75, wps=22473.1, ups=1.6, wpb=14051.8, bsz=484.6, num_updates=59200, lr=9.0045e-06, gnorm=0.641, train_wall=62, wall=40099
2021-01-02 23:02:11 | INFO | train_inner | epoch 141:    360 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.811, ppl=1.75, wps=22378.8, ups=1.59, wpb=14045.7, bsz=505.7, num_updates=59300, lr=8.99691e-06, gnorm=0.635, train_wall=63, wall=40162
2021-01-02 23:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:03:06 | INFO | valid | epoch 141 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.846 | ppl 14.38 | bleu 22.91 | wps 6004.6 | wpb 10324.2 | bsz 375 | num_updates 59361 | best_bleu 23.03
2021-01-02 23:03:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 141 @ 59361 updates, score 22.91) (writing took 2.9697986226528883 seconds)
2021-01-02 23:03:09 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2021-01-02 23:03:09 | INFO | train | epoch 141 | symm_mse 0.253 | loss 2.974 | nll_loss 0.812 | ppl 1.76 | wps 20602.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 59361 | lr 8.99228e-06 | gnorm 0.643 | train_wall 262 | wall 40220
2021-01-02 23:03:09 | INFO | fairseq.trainer | begin training epoch 142
2021-01-02 23:03:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:03:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:03:37 | INFO | train_inner | epoch 142:     39 / 421 symm_mse=0.257, loss=2.98, nll_loss=0.812, ppl=1.76, wps=16328.9, ups=1.17, wpb=13913.4, bsz=478.8, num_updates=59400, lr=8.98933e-06, gnorm=0.658, train_wall=62, wall=40247
2021-01-02 23:04:39 | INFO | train_inner | epoch 142:    139 / 421 symm_mse=0.252, loss=2.974, nll_loss=0.814, ppl=1.76, wps=22489.4, ups=1.6, wpb=14087.6, bsz=504.5, num_updates=59500, lr=8.98177e-06, gnorm=0.633, train_wall=62, wall=40310
2021-01-02 23:05:43 | INFO | train_inner | epoch 142:    239 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.809, ppl=1.75, wps=22048.1, ups=1.58, wpb=13961.6, bsz=495.8, num_updates=59600, lr=8.97424e-06, gnorm=0.639, train_wall=63, wall=40373
2021-01-02 23:06:45 | INFO | train_inner | epoch 142:    339 / 421 symm_mse=0.254, loss=2.972, nll_loss=0.809, ppl=1.75, wps=22307.4, ups=1.59, wpb=14038.2, bsz=476.4, num_updates=59700, lr=8.96672e-06, gnorm=0.639, train_wall=63, wall=40436
2021-01-02 23:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:07:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:07:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:07:55 | INFO | valid | epoch 142 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.847 | ppl 14.39 | bleu 22.91 | wps 5339 | wpb 10324.2 | bsz 375 | num_updates 59782 | best_bleu 23.03
2021-01-02 23:07:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:07:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:07:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:07:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 142 @ 59782 updates, score 22.91) (writing took 2.8566748667508364 seconds)
2021-01-02 23:07:58 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2021-01-02 23:07:58 | INFO | train | epoch 142 | symm_mse 0.253 | loss 2.975 | nll_loss 0.812 | ppl 1.76 | wps 20375.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 59782 | lr 8.96057e-06 | gnorm 0.641 | train_wall 264 | wall 40508
2021-01-02 23:07:58 | INFO | fairseq.trainer | begin training epoch 143
2021-01-02 23:07:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:08:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:08:12 | INFO | train_inner | epoch 143:     18 / 421 symm_mse=0.252, loss=2.972, nll_loss=0.813, ppl=1.76, wps=15863.3, ups=1.15, wpb=13735.8, bsz=495.1, num_updates=59800, lr=8.95922e-06, gnorm=0.648, train_wall=62, wall=40523
2021-01-02 23:09:14 | INFO | train_inner | epoch 143:    118 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.809, ppl=1.75, wps=22580.8, ups=1.62, wpb=13974.8, bsz=490.6, num_updates=59900, lr=8.95173e-06, gnorm=0.641, train_wall=62, wall=40585
2021-01-02 23:10:17 | INFO | train_inner | epoch 143:    218 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.809, ppl=1.75, wps=22365.3, ups=1.58, wpb=14124.9, bsz=486.2, num_updates=60000, lr=8.94427e-06, gnorm=0.638, train_wall=63, wall=40648
2021-01-02 23:11:20 | INFO | train_inner | epoch 143:    318 / 421 symm_mse=0.253, loss=2.977, nll_loss=0.815, ppl=1.76, wps=22355.8, ups=1.59, wpb=14031.6, bsz=502.4, num_updates=60100, lr=8.93683e-06, gnorm=0.637, train_wall=63, wall=40710
2021-01-02 23:12:22 | INFO | train_inner | epoch 143:    418 / 421 symm_mse=0.255, loss=2.983, nll_loss=0.819, ppl=1.76, wps=22082.8, ups=1.6, wpb=13815, bsz=497, num_updates=60200, lr=8.9294e-06, gnorm=0.652, train_wall=62, wall=40773
2021-01-02 23:12:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:12:41 | INFO | valid | epoch 143 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.845 | ppl 14.37 | bleu 22.93 | wps 5953.5 | wpb 10324.2 | bsz 375 | num_updates 60203 | best_bleu 23.03
2021-01-02 23:12:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:12:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 143 @ 60203 updates, score 22.93) (writing took 2.8914445731788874 seconds)
2021-01-02 23:12:44 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2021-01-02 23:12:44 | INFO | train | epoch 143 | symm_mse 0.253 | loss 2.974 | nll_loss 0.812 | ppl 1.76 | wps 20555 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 60203 | lr 8.92918e-06 | gnorm 0.643 | train_wall 262 | wall 40795
2021-01-02 23:12:44 | INFO | fairseq.trainer | begin training epoch 144
2021-01-02 23:12:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:12:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:12:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:13:47 | INFO | train_inner | epoch 144:     97 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.805, ppl=1.75, wps=16556, ups=1.18, wpb=14045.9, bsz=505.2, num_updates=60300, lr=8.92199e-06, gnorm=0.642, train_wall=62, wall=40858
2021-01-02 23:14:50 | INFO | train_inner | epoch 144:    197 / 421 symm_mse=0.253, loss=2.975, nll_loss=0.814, ppl=1.76, wps=22315.3, ups=1.59, wpb=14002.8, bsz=487.9, num_updates=60400, lr=8.91461e-06, gnorm=0.638, train_wall=63, wall=40921
2021-01-02 23:15:52 | INFO | train_inner | epoch 144:    297 / 421 symm_mse=0.255, loss=2.98, nll_loss=0.815, ppl=1.76, wps=22264.1, ups=1.6, wpb=13873.5, bsz=498.7, num_updates=60500, lr=8.90724e-06, gnorm=0.644, train_wall=62, wall=40983
2021-01-02 23:16:55 | INFO | train_inner | epoch 144:    397 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.809, ppl=1.75, wps=22047.2, ups=1.59, wpb=13908.2, bsz=475.1, num_updates=60600, lr=8.89988e-06, gnorm=0.645, train_wall=63, wall=41046
2021-01-02 23:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:17:27 | INFO | valid | epoch 144 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.849 | ppl 14.41 | bleu 22.82 | wps 5926.5 | wpb 10324.2 | bsz 375 | num_updates 60624 | best_bleu 23.03
2021-01-02 23:17:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:17:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 144 @ 60624 updates, score 22.82) (writing took 2.8784081675112247 seconds)
2021-01-02 23:17:30 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2021-01-02 23:17:30 | INFO | train | epoch 144 | symm_mse 0.253 | loss 2.973 | nll_loss 0.811 | ppl 1.75 | wps 20564.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 60624 | lr 8.89812e-06 | gnorm 0.641 | train_wall 262 | wall 41081
2021-01-02 23:17:30 | INFO | fairseq.trainer | begin training epoch 145
2021-01-02 23:17:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:18:20 | INFO | train_inner | epoch 145:     76 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.805, ppl=1.75, wps=16393.2, ups=1.18, wpb=13908.5, bsz=486.6, num_updates=60700, lr=8.89255e-06, gnorm=0.639, train_wall=62, wall=41131
2021-01-02 23:19:23 | INFO | train_inner | epoch 145:    176 / 421 symm_mse=0.252, loss=2.973, nll_loss=0.813, ppl=1.76, wps=22541, ups=1.61, wpb=14034.8, bsz=514.8, num_updates=60800, lr=8.88523e-06, gnorm=0.634, train_wall=62, wall=41193
2021-01-02 23:20:25 | INFO | train_inner | epoch 145:    276 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.81, ppl=1.75, wps=22399.8, ups=1.6, wpb=14031.2, bsz=484.8, num_updates=60900, lr=8.87794e-06, gnorm=0.641, train_wall=62, wall=41256
2021-01-02 23:21:28 | INFO | train_inner | epoch 145:    376 / 421 symm_mse=0.256, loss=2.982, nll_loss=0.816, ppl=1.76, wps=22414.6, ups=1.6, wpb=13973.1, bsz=478.3, num_updates=61000, lr=8.87066e-06, gnorm=0.644, train_wall=62, wall=41318
2021-01-02 23:21:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:21:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:21:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:21:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:21:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:22:13 | INFO | valid | epoch 145 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 22.82 | wps 5851.4 | wpb 10324.2 | bsz 375 | num_updates 61045 | best_bleu 23.03
2021-01-02 23:22:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:22:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:22:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:22:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:22:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:22:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:22:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:22:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 145 @ 61045 updates, score 22.82) (writing took 2.9154057539999485 seconds)
2021-01-02 23:22:16 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2021-01-02 23:22:16 | INFO | train | epoch 145 | symm_mse 0.253 | loss 2.973 | nll_loss 0.811 | ppl 1.75 | wps 20588.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 61045 | lr 8.86739e-06 | gnorm 0.641 | train_wall 262 | wall 41366
2021-01-02 23:22:16 | INFO | fairseq.trainer | begin training epoch 146
2021-01-02 23:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:22:52 | INFO | train_inner | epoch 146:     55 / 421 symm_mse=0.254, loss=2.977, nll_loss=0.814, ppl=1.76, wps=16213.9, ups=1.18, wpb=13767.2, bsz=492.8, num_updates=61100, lr=8.86339e-06, gnorm=0.651, train_wall=61, wall=41403
2021-01-02 23:23:55 | INFO | train_inner | epoch 146:    155 / 421 symm_mse=0.252, loss=2.97, nll_loss=0.809, ppl=1.75, wps=22338.8, ups=1.6, wpb=13968.5, bsz=490, num_updates=61200, lr=8.85615e-06, gnorm=0.644, train_wall=62, wall=41466
2021-01-02 23:24:58 | INFO | train_inner | epoch 146:    255 / 421 symm_mse=0.254, loss=2.977, nll_loss=0.815, ppl=1.76, wps=22485.3, ups=1.59, wpb=14101.8, bsz=501.4, num_updates=61300, lr=8.84892e-06, gnorm=0.637, train_wall=63, wall=41528
2021-01-02 23:26:00 | INFO | train_inner | epoch 146:    355 / 421 symm_mse=0.252, loss=2.967, nll_loss=0.807, ppl=1.75, wps=22638.4, ups=1.61, wpb=14100, bsz=490.3, num_updates=61400, lr=8.84171e-06, gnorm=0.637, train_wall=62, wall=41591
2021-01-02 23:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:26:58 | INFO | valid | epoch 146 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.849 | ppl 14.41 | bleu 22.87 | wps 5996.9 | wpb 10324.2 | bsz 375 | num_updates 61466 | best_bleu 23.03
2021-01-02 23:26:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:27:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 146 @ 61466 updates, score 22.87) (writing took 3.0477679278701544 seconds)
2021-01-02 23:27:01 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2021-01-02 23:27:01 | INFO | train | epoch 146 | symm_mse 0.253 | loss 2.972 | nll_loss 0.81 | ppl 1.75 | wps 20596.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 61466 | lr 8.83697e-06 | gnorm 0.643 | train_wall 262 | wall 41652
2021-01-02 23:27:01 | INFO | fairseq.trainer | begin training epoch 147
2021-01-02 23:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:27:25 | INFO | train_inner | epoch 147:     34 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.806, ppl=1.75, wps=16155.3, ups=1.17, wpb=13778.5, bsz=496.5, num_updates=61500, lr=8.83452e-06, gnorm=0.65, train_wall=62, wall=41676
2021-01-02 23:28:28 | INFO | train_inner | epoch 147:    134 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.81, ppl=1.75, wps=22462.9, ups=1.59, wpb=14087.1, bsz=486.1, num_updates=61600, lr=8.82735e-06, gnorm=0.639, train_wall=63, wall=41739
2021-01-02 23:29:31 | INFO | train_inner | epoch 147:    234 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.809, ppl=1.75, wps=22223.6, ups=1.59, wpb=13988.4, bsz=484, num_updates=61700, lr=8.82019e-06, gnorm=0.646, train_wall=63, wall=41802
2021-01-02 23:30:34 | INFO | train_inner | epoch 147:    334 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.815, ppl=1.76, wps=22371.5, ups=1.59, wpb=14045.6, bsz=484.5, num_updates=61800, lr=8.81305e-06, gnorm=0.642, train_wall=63, wall=41864
2021-01-02 23:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:31:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:31:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:31:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:31:46 | INFO | valid | epoch 147 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.849 | ppl 14.41 | bleu 22.96 | wps 5347.6 | wpb 10324.2 | bsz 375 | num_updates 61887 | best_bleu 23.03
2021-01-02 23:31:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:31:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 147 @ 61887 updates, score 22.96) (writing took 3.0363300554454327 seconds)
2021-01-02 23:31:49 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2021-01-02 23:31:49 | INFO | train | epoch 147 | symm_mse 0.253 | loss 2.973 | nll_loss 0.811 | ppl 1.75 | wps 20399.1 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 61887 | lr 8.80686e-06 | gnorm 0.644 | train_wall 263 | wall 41940
2021-01-02 23:31:49 | INFO | fairseq.trainer | begin training epoch 148
2021-01-02 23:31:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:31:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:32:01 | INFO | train_inner | epoch 148:     13 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.814, ppl=1.76, wps=15837.4, ups=1.15, wpb=13761.7, bsz=507.4, num_updates=61900, lr=8.80593e-06, gnorm=0.653, train_wall=62, wall=41951
2021-01-02 23:33:03 | INFO | train_inner | epoch 148:    113 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.81, ppl=1.75, wps=22364.4, ups=1.61, wpb=13855.9, bsz=500.1, num_updates=62000, lr=8.79883e-06, gnorm=0.646, train_wall=62, wall=42013
2021-01-02 23:34:06 | INFO | train_inner | epoch 148:    213 / 421 symm_mse=0.252, loss=2.967, nll_loss=0.805, ppl=1.75, wps=22249.9, ups=1.58, wpb=14087.8, bsz=498.6, num_updates=62100, lr=8.79174e-06, gnorm=0.642, train_wall=63, wall=42076
2021-01-02 23:35:09 | INFO | train_inner | epoch 148:    313 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.813, ppl=1.76, wps=22381.8, ups=1.6, wpb=14030.2, bsz=488.8, num_updates=62200, lr=8.78467e-06, gnorm=0.638, train_wall=62, wall=42139
2021-01-02 23:36:11 | INFO | train_inner | epoch 148:    413 / 421 symm_mse=0.253, loss=2.973, nll_loss=0.812, ppl=1.76, wps=22430.5, ups=1.6, wpb=14000.2, bsz=485.4, num_updates=62300, lr=8.77762e-06, gnorm=0.642, train_wall=62, wall=42202
2021-01-02 23:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:36:32 | INFO | valid | epoch 148 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 23 | wps 6023.6 | wpb 10324.2 | bsz 375 | num_updates 62308 | best_bleu 23.03
2021-01-02 23:36:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:36:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:36:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 148 @ 62308 updates, score 23.0) (writing took 3.0702916625887156 seconds)
2021-01-02 23:36:36 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2021-01-02 23:36:36 | INFO | train | epoch 148 | symm_mse 0.253 | loss 2.972 | nll_loss 0.81 | ppl 1.75 | wps 20552.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 62308 | lr 8.77705e-06 | gnorm 0.644 | train_wall 262 | wall 42226
2021-01-02 23:36:36 | INFO | fairseq.trainer | begin training epoch 149
2021-01-02 23:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:37:36 | INFO | train_inner | epoch 149:     92 / 421 symm_mse=0.257, loss=2.987, nll_loss=0.819, ppl=1.76, wps=16238.3, ups=1.18, wpb=13733.1, bsz=479.8, num_updates=62400, lr=8.77058e-06, gnorm=0.655, train_wall=61, wall=42286
2021-01-02 23:38:38 | INFO | train_inner | epoch 149:    192 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.807, ppl=1.75, wps=22291.1, ups=1.6, wpb=13972.5, bsz=492.8, num_updates=62500, lr=8.76356e-06, gnorm=0.638, train_wall=62, wall=42349
2021-01-02 23:39:41 | INFO | train_inner | epoch 149:    292 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.806, ppl=1.75, wps=22531.9, ups=1.59, wpb=14177.3, bsz=489.4, num_updates=62600, lr=8.75656e-06, gnorm=0.633, train_wall=63, wall=42412
2021-01-02 23:40:44 | INFO | train_inner | epoch 149:    392 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.811, ppl=1.75, wps=22218.2, ups=1.59, wpb=13963.7, bsz=504.2, num_updates=62700, lr=8.74957e-06, gnorm=0.644, train_wall=63, wall=42475
2021-01-02 23:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:41:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:41:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:41:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:41:19 | INFO | valid | epoch 149 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.85 | ppl 14.42 | bleu 22.87 | wps 5972.2 | wpb 10324.2 | bsz 375 | num_updates 62729 | best_bleu 23.03
2021-01-02 23:41:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:41:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 149 @ 62729 updates, score 22.87) (writing took 3.0112412087619305 seconds)
2021-01-02 23:41:22 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2021-01-02 23:41:22 | INFO | train | epoch 149 | symm_mse 0.253 | loss 2.972 | nll_loss 0.81 | ppl 1.75 | wps 20556 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 62729 | lr 8.74755e-06 | gnorm 0.642 | train_wall 262 | wall 42512
2021-01-02 23:41:22 | INFO | fairseq.trainer | begin training epoch 150
2021-01-02 23:41:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:41:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:42:09 | INFO | train_inner | epoch 150:     71 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.811, ppl=1.75, wps=16456.8, ups=1.18, wpb=13925.5, bsz=493.4, num_updates=62800, lr=8.7426e-06, gnorm=0.643, train_wall=62, wall=42559
2021-01-02 23:43:11 | INFO | train_inner | epoch 150:    171 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.813, ppl=1.76, wps=22345.4, ups=1.61, wpb=13890.2, bsz=489.2, num_updates=62900, lr=8.73565e-06, gnorm=0.645, train_wall=62, wall=42621
2021-01-02 23:44:13 | INFO | train_inner | epoch 150:    271 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.807, ppl=1.75, wps=22217.2, ups=1.59, wpb=13938, bsz=494.2, num_updates=63000, lr=8.72872e-06, gnorm=0.64, train_wall=63, wall=42684
2021-01-02 23:45:16 | INFO | train_inner | epoch 150:    371 / 421 symm_mse=0.252, loss=2.972, nll_loss=0.813, ppl=1.76, wps=22711.3, ups=1.59, wpb=14243.7, bsz=514.6, num_updates=63100, lr=8.7218e-06, gnorm=0.635, train_wall=63, wall=42747
2021-01-02 23:45:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:46:04 | INFO | valid | epoch 150 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.847 | ppl 14.39 | bleu 22.97 | wps 5915.7 | wpb 10324.2 | bsz 375 | num_updates 63150 | best_bleu 23.03
2021-01-02 23:46:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:46:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:46:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:46:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:46:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:46:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:46:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:46:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 150 @ 63150 updates, score 22.97) (writing took 3.072959328070283 seconds)
2021-01-02 23:46:07 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2021-01-02 23:46:07 | INFO | train | epoch 150 | symm_mse 0.253 | loss 2.971 | nll_loss 0.81 | ppl 1.75 | wps 20600.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 63150 | lr 8.71834e-06 | gnorm 0.643 | train_wall 262 | wall 42798
2021-01-02 23:46:07 | INFO | fairseq.trainer | begin training epoch 151
2021-01-02 23:46:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:46:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:46:41 | INFO | train_inner | epoch 151:     50 / 421 symm_mse=0.252, loss=2.961, nll_loss=0.8, ppl=1.74, wps=16251.7, ups=1.17, wpb=13831.3, bsz=477, num_updates=63200, lr=8.71489e-06, gnorm=0.651, train_wall=62, wall=42832
2021-01-02 23:47:44 | INFO | train_inner | epoch 151:    150 / 421 symm_mse=0.255, loss=2.977, nll_loss=0.813, ppl=1.76, wps=22267.4, ups=1.6, wpb=13920.5, bsz=482.2, num_updates=63300, lr=8.70801e-06, gnorm=0.646, train_wall=62, wall=42894
2021-01-02 23:48:47 | INFO | train_inner | epoch 151:    250 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.809, ppl=1.75, wps=21979.1, ups=1.59, wpb=13831.6, bsz=486.8, num_updates=63400, lr=8.70114e-06, gnorm=0.648, train_wall=63, wall=42957
2021-01-02 23:49:49 | INFO | train_inner | epoch 151:    350 / 421 symm_mse=0.251, loss=2.972, nll_loss=0.814, ppl=1.76, wps=22693.5, ups=1.6, wpb=14158.8, bsz=499.7, num_updates=63500, lr=8.69428e-06, gnorm=0.641, train_wall=62, wall=43020
2021-01-02 23:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:50:51 | INFO | valid | epoch 151 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.848 | ppl 14.4 | bleu 22.97 | wps 5906.7 | wpb 10324.2 | bsz 375 | num_updates 63571 | best_bleu 23.03
2021-01-02 23:50:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:50:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:50:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 151 @ 63571 updates, score 22.97) (writing took 3.0622274819761515 seconds)
2021-01-02 23:50:54 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2021-01-02 23:50:54 | INFO | train | epoch 151 | symm_mse 0.252 | loss 2.971 | nll_loss 0.81 | ppl 1.75 | wps 20533.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 63571 | lr 8.68943e-06 | gnorm 0.644 | train_wall 262 | wall 43084
2021-01-02 23:50:54 | INFO | fairseq.trainer | begin training epoch 152
2021-01-02 23:50:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:50:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:51:15 | INFO | train_inner | epoch 152:     29 / 421 symm_mse=0.252, loss=2.969, nll_loss=0.81, ppl=1.75, wps=16340.2, ups=1.17, wpb=13958.1, bsz=492.3, num_updates=63600, lr=8.68744e-06, gnorm=0.646, train_wall=62, wall=43105
2021-01-02 23:52:17 | INFO | train_inner | epoch 152:    129 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.801, ppl=1.74, wps=22267.3, ups=1.59, wpb=13973.5, bsz=496.2, num_updates=63700, lr=8.68062e-06, gnorm=0.638, train_wall=63, wall=43168
2021-01-02 23:53:20 | INFO | train_inner | epoch 152:    229 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.811, ppl=1.75, wps=22448.3, ups=1.6, wpb=14007.1, bsz=494.6, num_updates=63800, lr=8.67382e-06, gnorm=0.642, train_wall=62, wall=43230
2021-01-02 23:54:22 | INFO | train_inner | epoch 152:    329 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.813, ppl=1.76, wps=22417.6, ups=1.6, wpb=14050.2, bsz=488.2, num_updates=63900, lr=8.66703e-06, gnorm=0.638, train_wall=62, wall=43293
2021-01-02 23:55:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-02 23:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-02 23:55:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-02 23:55:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-02 23:55:36 | INFO | valid | epoch 152 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.85 | ppl 14.42 | bleu 23.01 | wps 6082.3 | wpb 10324.2 | bsz 375 | num_updates 63992 | best_bleu 23.03
2021-01-02 23:55:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-02 23:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 152 @ 63992 updates, score 23.01) (writing took 3.0587228313088417 seconds)
2021-01-02 23:55:39 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2021-01-02 23:55:39 | INFO | train | epoch 152 | symm_mse 0.252 | loss 2.97 | nll_loss 0.809 | ppl 1.75 | wps 20586.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 63992 | lr 8.6608e-06 | gnorm 0.643 | train_wall 262 | wall 43370
2021-01-02 23:55:39 | INFO | fairseq.trainer | begin training epoch 153
2021-01-02 23:55:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-02 23:55:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-02 23:55:47 | INFO | train_inner | epoch 153:      8 / 421 symm_mse=0.254, loss=2.976, nll_loss=0.814, ppl=1.76, wps=16160.1, ups=1.18, wpb=13748.4, bsz=498.8, num_updates=64000, lr=8.66025e-06, gnorm=0.654, train_wall=62, wall=43378
2021-01-02 23:56:50 | INFO | train_inner | epoch 153:    108 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.804, ppl=1.75, wps=22446.9, ups=1.61, wpb=13934.4, bsz=493.4, num_updates=64100, lr=8.6535e-06, gnorm=0.641, train_wall=62, wall=43440
2021-01-02 23:57:52 | INFO | train_inner | epoch 153:    208 / 421 symm_mse=0.253, loss=2.976, nll_loss=0.814, ppl=1.76, wps=22306.3, ups=1.59, wpb=13992.3, bsz=498.9, num_updates=64200, lr=8.64675e-06, gnorm=0.642, train_wall=63, wall=43503
2021-01-02 23:58:55 | INFO | train_inner | epoch 153:    308 / 421 symm_mse=0.254, loss=2.981, nll_loss=0.818, ppl=1.76, wps=22296.4, ups=1.6, wpb=13947.1, bsz=484.6, num_updates=64300, lr=8.64003e-06, gnorm=0.643, train_wall=62, wall=43565
2021-01-02 23:59:57 | INFO | train_inner | epoch 153:    408 / 421 symm_mse=0.251, loss=2.963, nll_loss=0.804, ppl=1.75, wps=22522.4, ups=1.6, wpb=14111, bsz=491.5, num_updates=64400, lr=8.63332e-06, gnorm=0.638, train_wall=62, wall=43628
2021-01-03 00:00:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:00:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:00:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:00:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:00:23 | INFO | valid | epoch 153 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.85 | ppl 14.42 | bleu 22.94 | wps 5876.7 | wpb 10324.2 | bsz 375 | num_updates 64413 | best_bleu 23.03
2021-01-03 00:00:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:00:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:00:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 153 @ 64413 updates, score 22.94) (writing took 3.106086065992713 seconds)
2021-01-03 00:00:26 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2021-01-03 00:00:26 | INFO | train | epoch 153 | symm_mse 0.252 | loss 2.97 | nll_loss 0.809 | ppl 1.75 | wps 20531.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 64413 | lr 8.63245e-06 | gnorm 0.642 | train_wall 262 | wall 43656
2021-01-03 00:00:26 | INFO | fairseq.trainer | begin training epoch 154
2021-01-03 00:00:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:00:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:01:23 | INFO | train_inner | epoch 154:     87 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.805, ppl=1.75, wps=16423.6, ups=1.18, wpb=13972.4, bsz=496.9, num_updates=64500, lr=8.62662e-06, gnorm=0.636, train_wall=61, wall=43713
2021-01-03 00:02:25 | INFO | train_inner | epoch 154:    187 / 421 symm_mse=0.254, loss=2.974, nll_loss=0.811, ppl=1.75, wps=22188, ups=1.59, wpb=13917.9, bsz=477.1, num_updates=64600, lr=8.61994e-06, gnorm=0.642, train_wall=63, wall=43776
2021-01-03 00:03:28 | INFO | train_inner | epoch 154:    287 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.805, ppl=1.75, wps=22754.5, ups=1.59, wpb=14268, bsz=501.1, num_updates=64700, lr=8.61328e-06, gnorm=0.631, train_wall=63, wall=43839
2021-01-03 00:04:30 | INFO | train_inner | epoch 154:    387 / 421 symm_mse=0.252, loss=2.972, nll_loss=0.811, ppl=1.75, wps=22315.1, ups=1.62, wpb=13797, bsz=493.4, num_updates=64800, lr=8.60663e-06, gnorm=0.643, train_wall=62, wall=43900
2021-01-03 00:04:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:04:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:04:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:04:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:04:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:04:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:04:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:04:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:05:08 | INFO | valid | epoch 154 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.849 | ppl 14.41 | bleu 22.98 | wps 5899.5 | wpb 10324.2 | bsz 375 | num_updates 64834 | best_bleu 23.03
2021-01-03 00:05:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:05:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:05:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:05:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:05:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 154 @ 64834 updates, score 22.98) (writing took 2.9873617943376303 seconds)
2021-01-03 00:05:11 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2021-01-03 00:05:11 | INFO | train | epoch 154 | symm_mse 0.252 | loss 2.97 | nll_loss 0.809 | ppl 1.75 | wps 20629 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 64834 | lr 8.60437e-06 | gnorm 0.639 | train_wall 261 | wall 43941
2021-01-03 00:05:11 | INFO | fairseq.trainer | begin training epoch 155
2021-01-03 00:05:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:05:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:05:55 | INFO | train_inner | epoch 155:     66 / 421 symm_mse=0.25, loss=2.959, nll_loss=0.801, ppl=1.74, wps=16374.3, ups=1.18, wpb=13886.5, bsz=497.1, num_updates=64900, lr=8.6e-06, gnorm=0.643, train_wall=61, wall=43985
2021-01-03 00:06:57 | INFO | train_inner | epoch 155:    166 / 421 symm_mse=0.252, loss=2.972, nll_loss=0.813, ppl=1.76, wps=22572.9, ups=1.61, wpb=14014.4, bsz=510, num_updates=65000, lr=8.59338e-06, gnorm=0.633, train_wall=62, wall=44047
2021-01-03 00:07:59 | INFO | train_inner | epoch 155:    266 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.812, ppl=1.76, wps=22234.1, ups=1.6, wpb=13876.6, bsz=484, num_updates=65100, lr=8.58678e-06, gnorm=0.639, train_wall=62, wall=44110
2021-01-03 00:09:01 | INFO | train_inner | epoch 155:    366 / 421 symm_mse=0.25, loss=2.956, nll_loss=0.798, ppl=1.74, wps=22648.6, ups=1.61, wpb=14055.2, bsz=498.1, num_updates=65200, lr=8.58019e-06, gnorm=0.638, train_wall=62, wall=44172
2021-01-03 00:09:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:09:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:09:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:09:53 | INFO | valid | epoch 155 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.849 | ppl 14.41 | bleu 22.91 | wps 5774.8 | wpb 10324.2 | bsz 375 | num_updates 65255 | best_bleu 23.03
2021-01-03 00:09:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:09:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:09:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 155 @ 65255 updates, score 22.91) (writing took 3.0898699555546045 seconds)
2021-01-03 00:09:56 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2021-01-03 00:09:56 | INFO | train | epoch 155 | symm_mse 0.252 | loss 2.969 | nll_loss 0.809 | ppl 1.75 | wps 20642.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 65255 | lr 8.57657e-06 | gnorm 0.64 | train_wall 260 | wall 44226
2021-01-03 00:09:56 | INFO | fairseq.trainer | begin training epoch 156
2021-01-03 00:09:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:09:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:10:27 | INFO | train_inner | epoch 156:     45 / 421 symm_mse=0.254, loss=2.98, nll_loss=0.818, ppl=1.76, wps=16409.5, ups=1.17, wpb=14053.9, bsz=489.1, num_updates=65300, lr=8.57362e-06, gnorm=0.643, train_wall=62, wall=44257
2021-01-03 00:11:29 | INFO | train_inner | epoch 156:    145 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.804, ppl=1.75, wps=22478.8, ups=1.61, wpb=13935.1, bsz=480.3, num_updates=65400, lr=8.56706e-06, gnorm=0.642, train_wall=62, wall=44319
2021-01-03 00:12:31 | INFO | train_inner | epoch 156:    245 / 421 symm_mse=0.253, loss=2.974, nll_loss=0.813, ppl=1.76, wps=22466.1, ups=1.61, wpb=13952.4, bsz=501.6, num_updates=65500, lr=8.56052e-06, gnorm=0.641, train_wall=62, wall=44382
2021-01-03 00:13:33 | INFO | train_inner | epoch 156:    345 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22398.6, ups=1.6, wpb=13986.1, bsz=488.1, num_updates=65600, lr=8.55399e-06, gnorm=0.641, train_wall=62, wall=44444
2021-01-03 00:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:14:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:14:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:14:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:14:37 | INFO | valid | epoch 156 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.96 | wps 6009.1 | wpb 10324.2 | bsz 375 | num_updates 65676 | best_bleu 23.03
2021-01-03 00:14:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 156 @ 65676 updates, score 22.96) (writing took 3.0620112270116806 seconds)
2021-01-03 00:14:40 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2021-01-03 00:14:40 | INFO | train | epoch 156 | symm_mse 0.252 | loss 2.968 | nll_loss 0.808 | ppl 1.75 | wps 20673.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 65676 | lr 8.54904e-06 | gnorm 0.642 | train_wall 260 | wall 44511
2021-01-03 00:14:40 | INFO | fairseq.trainer | begin training epoch 157
2021-01-03 00:14:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:14:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:14:58 | INFO | train_inner | epoch 157:     24 / 421 symm_mse=0.253, loss=2.971, nll_loss=0.809, ppl=1.75, wps=16280.3, ups=1.18, wpb=13777.8, bsz=481.7, num_updates=65700, lr=8.54748e-06, gnorm=0.652, train_wall=61, wall=44529
2021-01-03 00:16:00 | INFO | train_inner | epoch 157:    124 / 421 symm_mse=0.252, loss=2.965, nll_loss=0.803, ppl=1.75, wps=22498.6, ups=1.61, wpb=13963.6, bsz=485.1, num_updates=65800, lr=8.54098e-06, gnorm=0.642, train_wall=62, wall=44591
2021-01-03 00:17:03 | INFO | train_inner | epoch 157:    224 / 421 symm_mse=0.251, loss=2.968, nll_loss=0.81, ppl=1.75, wps=22322.6, ups=1.6, wpb=13994.5, bsz=507.4, num_updates=65900, lr=8.5345e-06, gnorm=0.635, train_wall=62, wall=44653
2021-01-03 00:18:05 | INFO | train_inner | epoch 157:    324 / 421 symm_mse=0.253, loss=2.97, nll_loss=0.808, ppl=1.75, wps=22402.2, ups=1.6, wpb=14031.7, bsz=487.4, num_updates=66000, lr=8.52803e-06, gnorm=0.641, train_wall=62, wall=44716
2021-01-03 00:19:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:19:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:19:22 | INFO | valid | epoch 157 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.848 | ppl 14.4 | bleu 22.89 | wps 5980.2 | wpb 10324.2 | bsz 375 | num_updates 66097 | best_bleu 23.03
2021-01-03 00:19:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:19:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 157 @ 66097 updates, score 22.89) (writing took 2.9873932041227818 seconds)
2021-01-03 00:19:25 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2021-01-03 00:19:25 | INFO | train | epoch 157 | symm_mse 0.252 | loss 2.968 | nll_loss 0.808 | ppl 1.75 | wps 20626.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 66097 | lr 8.52177e-06 | gnorm 0.64 | train_wall 261 | wall 44796
2021-01-03 00:19:25 | INFO | fairseq.trainer | begin training epoch 158
2021-01-03 00:19:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:19:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:19:30 | INFO | train_inner | epoch 158:      3 / 421 symm_mse=0.251, loss=2.969, nll_loss=0.81, ppl=1.75, wps=16393.2, ups=1.18, wpb=13946.9, bsz=493.5, num_updates=66100, lr=8.52158e-06, gnorm=0.644, train_wall=62, wall=44801
2021-01-03 00:20:32 | INFO | train_inner | epoch 158:    103 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.813, ppl=1.76, wps=22586.7, ups=1.61, wpb=13991.6, bsz=486.3, num_updates=66200, lr=8.51514e-06, gnorm=0.645, train_wall=62, wall=44863
2021-01-03 00:21:35 | INFO | train_inner | epoch 158:    203 / 421 symm_mse=0.246, loss=2.947, nll_loss=0.792, ppl=1.73, wps=22619.8, ups=1.6, wpb=14129.6, bsz=509.4, num_updates=66300, lr=8.50871e-06, gnorm=0.63, train_wall=62, wall=44926
2021-01-03 00:22:37 | INFO | train_inner | epoch 158:    303 / 421 symm_mse=0.254, loss=2.98, nll_loss=0.818, ppl=1.76, wps=22433.4, ups=1.61, wpb=13927.7, bsz=477.6, num_updates=66400, lr=8.5023e-06, gnorm=0.646, train_wall=62, wall=44988
2021-01-03 00:23:40 | INFO | train_inner | epoch 158:    403 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.808, ppl=1.75, wps=22282.6, ups=1.6, wpb=13965.3, bsz=500.1, num_updates=66500, lr=8.49591e-06, gnorm=0.638, train_wall=62, wall=45050
2021-01-03 00:23:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:23:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:23:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:23:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:23:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:24:08 | INFO | valid | epoch 158 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.851 | ppl 14.43 | bleu 22.86 | wps 6019.4 | wpb 10324.2 | bsz 375 | num_updates 66518 | best_bleu 23.03
2021-01-03 00:24:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:24:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:24:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:24:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:24:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:24:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:24:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:24:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 158 @ 66518 updates, score 22.86) (writing took 3.0131518840789795 seconds)
2021-01-03 00:24:11 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2021-01-03 00:24:11 | INFO | train | epoch 158 | symm_mse 0.252 | loss 2.968 | nll_loss 0.808 | ppl 1.75 | wps 20618.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 66518 | lr 8.49476e-06 | gnorm 0.642 | train_wall 261 | wall 45081
2021-01-03 00:24:11 | INFO | fairseq.trainer | begin training epoch 159
2021-01-03 00:24:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:24:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:25:05 | INFO | train_inner | epoch 159:     82 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.806, ppl=1.75, wps=16482.9, ups=1.18, wpb=14000.2, bsz=473, num_updates=66600, lr=8.48953e-06, gnorm=0.647, train_wall=62, wall=45135
2021-01-03 00:26:07 | INFO | train_inner | epoch 159:    182 / 421 symm_mse=0.254, loss=2.973, nll_loss=0.81, ppl=1.75, wps=22365.6, ups=1.6, wpb=13978.4, bsz=491.2, num_updates=66700, lr=8.48316e-06, gnorm=0.644, train_wall=62, wall=45198
2021-01-03 00:27:09 | INFO | train_inner | epoch 159:    282 / 421 symm_mse=0.25, loss=2.961, nll_loss=0.805, ppl=1.75, wps=22278, ups=1.61, wpb=13858.3, bsz=500.2, num_updates=66800, lr=8.47681e-06, gnorm=0.64, train_wall=62, wall=45260
2021-01-03 00:28:12 | INFO | train_inner | epoch 159:    382 / 421 symm_mse=0.249, loss=2.96, nll_loss=0.805, ppl=1.75, wps=22481.7, ups=1.6, wpb=14045.1, bsz=505.5, num_updates=66900, lr=8.47047e-06, gnorm=0.636, train_wall=62, wall=45322
2021-01-03 00:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:28:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:28:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:28:53 | INFO | valid | epoch 159 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.851 | ppl 14.43 | bleu 22.96 | wps 6008.6 | wpb 10324.2 | bsz 375 | num_updates 66939 | best_bleu 23.03
2021-01-03 00:28:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:28:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:28:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 159 @ 66939 updates, score 22.96) (writing took 3.0721536241471767 seconds)
2021-01-03 00:28:56 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2021-01-03 00:28:56 | INFO | train | epoch 159 | symm_mse 0.251 | loss 2.967 | nll_loss 0.807 | ppl 1.75 | wps 20609.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 66939 | lr 8.468e-06 | gnorm 0.641 | train_wall 261 | wall 45367
2021-01-03 00:28:56 | INFO | fairseq.trainer | begin training epoch 160
2021-01-03 00:28:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:28:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:29:37 | INFO | train_inner | epoch 160:     61 / 421 symm_mse=0.253, loss=2.977, nll_loss=0.815, ppl=1.76, wps=16224.9, ups=1.18, wpb=13806.8, bsz=505.4, num_updates=67000, lr=8.46415e-06, gnorm=0.648, train_wall=62, wall=45407
2021-01-03 00:30:40 | INFO | train_inner | epoch 160:    161 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.805, ppl=1.75, wps=22139.8, ups=1.6, wpb=13868, bsz=480.2, num_updates=67100, lr=8.45784e-06, gnorm=0.641, train_wall=62, wall=45470
2021-01-03 00:31:41 | INFO | train_inner | epoch 160:    261 / 421 symm_mse=0.253, loss=2.976, nll_loss=0.815, ppl=1.76, wps=22689.9, ups=1.62, wpb=14028.5, bsz=487.8, num_updates=67200, lr=8.45154e-06, gnorm=0.638, train_wall=62, wall=45532
2021-01-03 00:32:44 | INFO | train_inner | epoch 160:    361 / 421 symm_mse=0.25, loss=2.961, nll_loss=0.803, ppl=1.74, wps=22533.9, ups=1.6, wpb=14046.4, bsz=487.5, num_updates=67300, lr=8.44526e-06, gnorm=0.635, train_wall=62, wall=45594
2021-01-03 00:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:33:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:33:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:33:38 | INFO | valid | epoch 160 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.849 | ppl 14.41 | bleu 22.82 | wps 6008.1 | wpb 10324.2 | bsz 375 | num_updates 67360 | best_bleu 23.03
2021-01-03 00:33:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:33:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:33:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 160 @ 67360 updates, score 22.82) (writing took 3.0455970596522093 seconds)
2021-01-03 00:33:41 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2021-01-03 00:33:41 | INFO | train | epoch 160 | symm_mse 0.251 | loss 2.968 | nll_loss 0.808 | ppl 1.75 | wps 20652.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 67360 | lr 8.4415e-06 | gnorm 0.639 | train_wall 261 | wall 45651
2021-01-03 00:33:41 | INFO | fairseq.trainer | begin training epoch 161
2021-01-03 00:33:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:33:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:34:08 | INFO | train_inner | epoch 161:     40 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.803, ppl=1.74, wps=16512.3, ups=1.18, wpb=13980.6, bsz=506.4, num_updates=67400, lr=8.43899e-06, gnorm=0.641, train_wall=61, wall=45679
2021-01-03 00:35:10 | INFO | train_inner | epoch 161:    140 / 421 symm_mse=0.252, loss=2.967, nll_loss=0.805, ppl=1.75, wps=22344.7, ups=1.62, wpb=13831.6, bsz=489.1, num_updates=67500, lr=8.43274e-06, gnorm=0.654, train_wall=62, wall=45741
2021-01-03 00:36:13 | INFO | train_inner | epoch 161:    240 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.804, ppl=1.75, wps=22369.9, ups=1.6, wpb=13986.9, bsz=505.6, num_updates=67600, lr=8.4265e-06, gnorm=0.641, train_wall=62, wall=45803
2021-01-03 00:37:16 | INFO | train_inner | epoch 161:    340 / 421 symm_mse=0.252, loss=2.973, nll_loss=0.813, ppl=1.76, wps=22318.2, ups=1.59, wpb=14017.4, bsz=485.5, num_updates=67700, lr=8.42028e-06, gnorm=0.638, train_wall=63, wall=45866
2021-01-03 00:38:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:38:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:38:23 | INFO | valid | epoch 161 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.849 | ppl 14.41 | bleu 22.76 | wps 6062.2 | wpb 10324.2 | bsz 375 | num_updates 67781 | best_bleu 23.03
2021-01-03 00:38:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 161 @ 67781 updates, score 22.76) (writing took 3.0269639026373625 seconds)
2021-01-03 00:38:26 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2021-01-03 00:38:26 | INFO | train | epoch 161 | symm_mse 0.252 | loss 2.967 | nll_loss 0.808 | ppl 1.75 | wps 20629.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 67781 | lr 8.41524e-06 | gnorm 0.643 | train_wall 261 | wall 45936
2021-01-03 00:38:26 | INFO | fairseq.trainer | begin training epoch 162
2021-01-03 00:38:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:38:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:38:41 | INFO | train_inner | epoch 162:     19 / 421 symm_mse=0.253, loss=2.975, nll_loss=0.815, ppl=1.76, wps=16471.4, ups=1.17, wpb=14029.2, bsz=491.1, num_updates=67800, lr=8.41406e-06, gnorm=0.645, train_wall=62, wall=45951
2021-01-03 00:39:42 | INFO | train_inner | epoch 162:    119 / 421 symm_mse=0.251, loss=2.969, nll_loss=0.81, ppl=1.75, wps=22641, ups=1.62, wpb=13948.6, bsz=497.5, num_updates=67900, lr=8.40787e-06, gnorm=0.637, train_wall=61, wall=46013
2021-01-03 00:40:44 | INFO | train_inner | epoch 162:    219 / 421 symm_mse=0.252, loss=2.966, nll_loss=0.806, ppl=1.75, wps=22594.3, ups=1.62, wpb=13976.5, bsz=484, num_updates=68000, lr=8.40168e-06, gnorm=0.645, train_wall=62, wall=46075
2021-01-03 00:41:47 | INFO | train_inner | epoch 162:    319 / 421 symm_mse=0.249, loss=2.955, nll_loss=0.799, ppl=1.74, wps=22373.4, ups=1.6, wpb=14021.5, bsz=510.2, num_updates=68100, lr=8.39551e-06, gnorm=0.631, train_wall=62, wall=46138
2021-01-03 00:42:50 | INFO | train_inner | epoch 162:    419 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.814, ppl=1.76, wps=22332.8, ups=1.59, wpb=14018.9, bsz=474, num_updates=68200, lr=8.38935e-06, gnorm=0.645, train_wall=63, wall=46200
2021-01-03 00:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:42:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:42:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:42:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:42:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:42:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:42:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:42:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:42:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:43:08 | INFO | valid | epoch 162 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.852 | ppl 14.44 | bleu 22.88 | wps 5915.2 | wpb 10324.2 | bsz 375 | num_updates 68202 | best_bleu 23.03
2021-01-03 00:43:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:43:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:43:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:43:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:43:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 162 @ 68202 updates, score 22.88) (writing took 3.0187783874571323 seconds)
2021-01-03 00:43:11 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2021-01-03 00:43:11 | INFO | train | epoch 162 | symm_mse 0.251 | loss 2.967 | nll_loss 0.807 | ppl 1.75 | wps 20620.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 68202 | lr 8.38923e-06 | gnorm 0.639 | train_wall 261 | wall 46222
2021-01-03 00:43:11 | INFO | fairseq.trainer | begin training epoch 163
2021-01-03 00:43:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:43:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:43:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:43:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:44:15 | INFO | train_inner | epoch 163:     98 / 421 symm_mse=0.247, loss=2.953, nll_loss=0.798, ppl=1.74, wps=16227.8, ups=1.17, wpb=13883.1, bsz=488.5, num_updates=68300, lr=8.38321e-06, gnorm=0.629, train_wall=62, wall=46286
2021-01-03 00:45:17 | INFO | train_inner | epoch 163:    198 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.803, ppl=1.74, wps=22433.1, ups=1.61, wpb=13963.1, bsz=488.4, num_updates=68400, lr=8.37708e-06, gnorm=0.641, train_wall=62, wall=46348
2021-01-03 00:46:20 | INFO | train_inner | epoch 163:    298 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.808, ppl=1.75, wps=22401.4, ups=1.61, wpb=13911.9, bsz=489, num_updates=68500, lr=8.37096e-06, gnorm=0.642, train_wall=62, wall=46410
2021-01-03 00:47:22 | INFO | train_inner | epoch 163:    398 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.815, ppl=1.76, wps=22484.8, ups=1.59, wpb=14128.7, bsz=495.4, num_updates=68600, lr=8.36486e-06, gnorm=0.642, train_wall=63, wall=46473
2021-01-03 00:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:47:56 | INFO | valid | epoch 163 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.85 | ppl 14.42 | bleu 23.01 | wps 5027.8 | wpb 10324.2 | bsz 375 | num_updates 68623 | best_bleu 23.03
2021-01-03 00:47:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:47:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:47:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:47:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 163 @ 68623 updates, score 23.01) (writing took 3.0309844594448805 seconds)
2021-01-03 00:47:59 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2021-01-03 00:47:59 | INFO | train | epoch 163 | symm_mse 0.251 | loss 2.966 | nll_loss 0.807 | ppl 1.75 | wps 20431.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 68623 | lr 8.36346e-06 | gnorm 0.641 | train_wall 261 | wall 46509
2021-01-03 00:47:59 | INFO | fairseq.trainer | begin training epoch 164
2021-01-03 00:48:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:48:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:48:50 | INFO | train_inner | epoch 164:     77 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.803, ppl=1.74, wps=16075.3, ups=1.15, wpb=14026.4, bsz=506.6, num_updates=68700, lr=8.35877e-06, gnorm=0.638, train_wall=62, wall=46560
2021-01-03 00:49:52 | INFO | train_inner | epoch 164:    177 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.804, ppl=1.75, wps=22613.7, ups=1.61, wpb=14042.8, bsz=493.2, num_updates=68800, lr=8.35269e-06, gnorm=0.637, train_wall=62, wall=46622
2021-01-03 00:50:54 | INFO | train_inner | epoch 164:    277 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22487.8, ups=1.61, wpb=13956.4, bsz=483, num_updates=68900, lr=8.34663e-06, gnorm=0.641, train_wall=62, wall=46684
2021-01-03 00:51:56 | INFO | train_inner | epoch 164:    377 / 421 symm_mse=0.253, loss=2.977, nll_loss=0.816, ppl=1.76, wps=22374.7, ups=1.61, wpb=13920.9, bsz=490.8, num_updates=69000, lr=8.34058e-06, gnorm=0.642, train_wall=62, wall=46747
2021-01-03 00:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:52:40 | INFO | valid | epoch 164 | valid on 'valid' subset | symm_mse 0 | loss 5.403 | nll_loss 3.847 | ppl 14.39 | bleu 22.97 | wps 5953.9 | wpb 10324.2 | bsz 375 | num_updates 69044 | best_bleu 23.03
2021-01-03 00:52:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:52:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 164 @ 69044 updates, score 22.97) (writing took 3.108105083927512 seconds)
2021-01-03 00:52:43 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2021-01-03 00:52:43 | INFO | train | epoch 164 | symm_mse 0.251 | loss 2.966 | nll_loss 0.807 | ppl 1.75 | wps 20696.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 69044 | lr 8.33792e-06 | gnorm 0.64 | train_wall 260 | wall 46794
2021-01-03 00:52:43 | INFO | fairseq.trainer | begin training epoch 165
2021-01-03 00:52:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:52:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:53:21 | INFO | train_inner | epoch 165:     56 / 421 symm_mse=0.251, loss=2.968, nll_loss=0.81, ppl=1.75, wps=16508.6, ups=1.18, wpb=13964.9, bsz=490.1, num_updates=69100, lr=8.33454e-06, gnorm=0.641, train_wall=61, wall=46831
2021-01-03 00:54:23 | INFO | train_inner | epoch 165:    156 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.805, ppl=1.75, wps=22595.4, ups=1.62, wpb=13980.9, bsz=494.4, num_updates=69200, lr=8.32851e-06, gnorm=0.643, train_wall=62, wall=46893
2021-01-03 00:55:25 | INFO | train_inner | epoch 165:    256 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.805, ppl=1.75, wps=22106.9, ups=1.6, wpb=13843.7, bsz=499.8, num_updates=69300, lr=8.3225e-06, gnorm=0.648, train_wall=62, wall=46956
2021-01-03 00:56:28 | INFO | train_inner | epoch 165:    356 / 421 symm_mse=0.251, loss=2.967, nll_loss=0.809, ppl=1.75, wps=22309.4, ups=1.59, wpb=14028, bsz=499.3, num_updates=69400, lr=8.31651e-06, gnorm=0.64, train_wall=63, wall=47019
2021-01-03 00:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 00:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 00:57:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 00:57:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 00:57:27 | INFO | valid | epoch 165 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.851 | ppl 14.43 | bleu 22.99 | wps 5305.1 | wpb 10324.2 | bsz 375 | num_updates 69465 | best_bleu 23.03
2021-01-03 00:57:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 00:57:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 165 @ 69465 updates, score 22.99) (writing took 3.0900184866040945 seconds)
2021-01-03 00:57:30 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2021-01-03 00:57:30 | INFO | train | epoch 165 | symm_mse 0.251 | loss 2.966 | nll_loss 0.807 | ppl 1.75 | wps 20483.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 69465 | lr 8.31261e-06 | gnorm 0.646 | train_wall 261 | wall 47081
2021-01-03 00:57:30 | INFO | fairseq.trainer | begin training epoch 166
2021-01-03 00:57:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 00:57:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 00:57:55 | INFO | train_inner | epoch 166:     35 / 421 symm_mse=0.25, loss=2.959, nll_loss=0.801, ppl=1.74, wps=15896, ups=1.15, wpb=13840.1, bsz=494.2, num_updates=69500, lr=8.31052e-06, gnorm=0.656, train_wall=62, wall=47106
2021-01-03 00:58:58 | INFO | train_inner | epoch 166:    135 / 421 symm_mse=0.251, loss=2.962, nll_loss=0.802, ppl=1.74, wps=22241.5, ups=1.59, wpb=14027.8, bsz=481.8, num_updates=69600, lr=8.30455e-06, gnorm=0.637, train_wall=63, wall=47169
2021-01-03 01:00:00 | INFO | train_inner | epoch 166:    235 / 421 symm_mse=0.252, loss=2.973, nll_loss=0.813, ppl=1.76, wps=22340.7, ups=1.61, wpb=13901.9, bsz=502.8, num_updates=69700, lr=8.29859e-06, gnorm=0.645, train_wall=62, wall=47231
2021-01-03 01:01:03 | INFO | train_inner | epoch 166:    335 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.811, ppl=1.75, wps=22515.9, ups=1.6, wpb=14037.1, bsz=497.1, num_updates=69800, lr=8.29264e-06, gnorm=0.644, train_wall=62, wall=47293
2021-01-03 01:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:01:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:01:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:01:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:01:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:01:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:01:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:01:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:01:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:02:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:02:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:02:13 | INFO | valid | epoch 166 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.849 | ppl 14.41 | bleu 23.01 | wps 6012.3 | wpb 10324.2 | bsz 375 | num_updates 69886 | best_bleu 23.03
2021-01-03 01:02:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:02:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 166 @ 69886 updates, score 23.01) (writing took 3.1261847633868456 seconds)
2021-01-03 01:02:16 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2021-01-03 01:02:16 | INFO | train | epoch 166 | symm_mse 0.251 | loss 2.965 | nll_loss 0.806 | ppl 1.75 | wps 20589.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 69886 | lr 8.28754e-06 | gnorm 0.641 | train_wall 262 | wall 47366
2021-01-03 01:02:16 | INFO | fairseq.trainer | begin training epoch 167
2021-01-03 01:02:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:02:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:02:28 | INFO | train_inner | epoch 167:     14 / 421 symm_mse=0.247, loss=2.953, nll_loss=0.799, ppl=1.74, wps=16419.5, ups=1.18, wpb=13966.2, bsz=479.5, num_updates=69900, lr=8.28671e-06, gnorm=0.636, train_wall=62, wall=47378
2021-01-03 01:03:30 | INFO | train_inner | epoch 167:    114 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.81, ppl=1.75, wps=22572.3, ups=1.61, wpb=13994.7, bsz=486.7, num_updates=70000, lr=8.28079e-06, gnorm=0.643, train_wall=62, wall=47440
2021-01-03 01:04:32 | INFO | train_inner | epoch 167:    214 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.806, ppl=1.75, wps=22369.7, ups=1.6, wpb=14001.9, bsz=488.5, num_updates=70100, lr=8.27488e-06, gnorm=0.639, train_wall=62, wall=47503
2021-01-03 01:05:35 | INFO | train_inner | epoch 167:    314 / 421 symm_mse=0.25, loss=2.965, nll_loss=0.808, ppl=1.75, wps=22601.5, ups=1.6, wpb=14085.7, bsz=503.1, num_updates=70200, lr=8.26898e-06, gnorm=0.632, train_wall=62, wall=47565
2021-01-03 01:06:37 | INFO | train_inner | epoch 167:    414 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.802, ppl=1.74, wps=22209.7, ups=1.6, wpb=13900, bsz=495.6, num_updates=70300, lr=8.2631e-06, gnorm=0.644, train_wall=62, wall=47628
2021-01-03 01:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:06:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:06:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:06:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:06:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:06:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:06:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:06:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:06:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:06:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:06:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:06:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:07:00 | INFO | valid | epoch 167 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.848 | ppl 14.4 | bleu 23.01 | wps 5214.1 | wpb 10324.2 | bsz 375 | num_updates 70307 | best_bleu 23.03
2021-01-03 01:07:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:07:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:07:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:07:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:07:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:07:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:07:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:07:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 167 @ 70307 updates, score 23.01) (writing took 3.0650144070386887 seconds)
2021-01-03 01:07:03 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2021-01-03 01:07:03 | INFO | train | epoch 167 | symm_mse 0.251 | loss 2.965 | nll_loss 0.806 | ppl 1.75 | wps 20456.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 70307 | lr 8.26269e-06 | gnorm 0.641 | train_wall 262 | wall 47654
2021-01-03 01:07:03 | INFO | fairseq.trainer | begin training epoch 168
2021-01-03 01:07:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:07:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:08:04 | INFO | train_inner | epoch 168:     93 / 421 symm_mse=0.255, loss=2.982, nll_loss=0.817, ppl=1.76, wps=16052.3, ups=1.16, wpb=13879.9, bsz=490.7, num_updates=70400, lr=8.25723e-06, gnorm=0.65, train_wall=61, wall=47714
2021-01-03 01:09:06 | INFO | train_inner | epoch 168:    193 / 421 symm_mse=0.251, loss=2.962, nll_loss=0.802, ppl=1.74, wps=22401.9, ups=1.6, wpb=14026.1, bsz=486.1, num_updates=70500, lr=8.25137e-06, gnorm=0.636, train_wall=62, wall=47777
2021-01-03 01:10:09 | INFO | train_inner | epoch 168:    293 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.805, ppl=1.75, wps=22199.6, ups=1.6, wpb=13875.3, bsz=490.4, num_updates=70600, lr=8.24552e-06, gnorm=0.642, train_wall=62, wall=47839
2021-01-03 01:11:11 | INFO | train_inner | epoch 168:    393 / 421 symm_mse=0.246, loss=2.952, nll_loss=0.799, ppl=1.74, wps=22474.8, ups=1.6, wpb=14035.8, bsz=503.1, num_updates=70700, lr=8.23969e-06, gnorm=0.629, train_wall=62, wall=47902
2021-01-03 01:11:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:11:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:11:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:11:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:11:47 | INFO | valid | epoch 168 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.85 | ppl 14.42 | bleu 22.87 | wps 5411.3 | wpb 10324.2 | bsz 375 | num_updates 70728 | best_bleu 23.03
2021-01-03 01:11:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 168 @ 70728 updates, score 22.87) (writing took 3.102277960628271 seconds)
2021-01-03 01:11:50 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2021-01-03 01:11:50 | INFO | train | epoch 168 | symm_mse 0.251 | loss 2.965 | nll_loss 0.806 | ppl 1.75 | wps 20512.9 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 70728 | lr 8.23806e-06 | gnorm 0.638 | train_wall 261 | wall 47941
2021-01-03 01:11:50 | INFO | fairseq.trainer | begin training epoch 169
2021-01-03 01:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:11:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:11:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:12:38 | INFO | train_inner | epoch 169:     72 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.805, ppl=1.75, wps=16100.3, ups=1.15, wpb=13944.5, bsz=493.4, num_updates=70800, lr=8.23387e-06, gnorm=0.644, train_wall=62, wall=47989
2021-01-03 01:13:41 | INFO | train_inner | epoch 169:    172 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.797, ppl=1.74, wps=22601.1, ups=1.6, wpb=14148.5, bsz=492.2, num_updates=70900, lr=8.22806e-06, gnorm=0.629, train_wall=62, wall=48051
2021-01-03 01:14:43 | INFO | train_inner | epoch 169:    272 / 421 symm_mse=0.251, loss=2.968, nll_loss=0.808, ppl=1.75, wps=22476.4, ups=1.6, wpb=14038.6, bsz=490.2, num_updates=71000, lr=8.22226e-06, gnorm=0.64, train_wall=62, wall=48114
2021-01-03 01:15:45 | INFO | train_inner | epoch 169:    372 / 421 symm_mse=0.254, loss=2.978, nll_loss=0.817, ppl=1.76, wps=22184.4, ups=1.6, wpb=13835.5, bsz=486.9, num_updates=71100, lr=8.21648e-06, gnorm=0.649, train_wall=62, wall=48176
2021-01-03 01:16:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:16:32 | INFO | valid | epoch 169 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 22.91 | wps 6036.9 | wpb 10324.2 | bsz 375 | num_updates 71149 | best_bleu 23.03
2021-01-03 01:16:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:16:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 169 @ 71149 updates, score 22.91) (writing took 3.0882657803595066 seconds)
2021-01-03 01:16:35 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2021-01-03 01:16:35 | INFO | train | epoch 169 | symm_mse 0.251 | loss 2.965 | nll_loss 0.806 | ppl 1.75 | wps 20611.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 71149 | lr 8.21365e-06 | gnorm 0.641 | train_wall 261 | wall 48226
2021-01-03 01:16:35 | INFO | fairseq.trainer | begin training epoch 170
2021-01-03 01:16:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:16:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:16:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:17:10 | INFO | train_inner | epoch 170:     51 / 421 symm_mse=0.249, loss=2.961, nll_loss=0.806, ppl=1.75, wps=16338.8, ups=1.18, wpb=13801.9, bsz=502.9, num_updates=71200, lr=8.21071e-06, gnorm=0.647, train_wall=61, wall=48260
2021-01-03 01:18:12 | INFO | train_inner | epoch 170:    151 / 421 symm_mse=0.254, loss=2.975, nll_loss=0.812, ppl=1.76, wps=22619.4, ups=1.62, wpb=13975.6, bsz=482.7, num_updates=71300, lr=8.20495e-06, gnorm=0.643, train_wall=62, wall=48322
2021-01-03 01:19:14 | INFO | train_inner | epoch 170:    251 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.805, ppl=1.75, wps=22510, ups=1.61, wpb=13980.4, bsz=490.9, num_updates=71400, lr=8.1992e-06, gnorm=0.641, train_wall=62, wall=48384
2021-01-03 01:20:16 | INFO | train_inner | epoch 170:    351 / 421 symm_mse=0.248, loss=2.955, nll_loss=0.799, ppl=1.74, wps=22374.7, ups=1.61, wpb=13916.7, bsz=501.2, num_updates=71500, lr=8.19346e-06, gnorm=0.641, train_wall=62, wall=48447
2021-01-03 01:20:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:21:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:21:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:21:16 | INFO | valid | epoch 170 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.851 | ppl 14.43 | bleu 22.93 | wps 5980.8 | wpb 10324.2 | bsz 375 | num_updates 71570 | best_bleu 23.03
2021-01-03 01:21:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 170 @ 71570 updates, score 22.93) (writing took 3.082493716850877 seconds)
2021-01-03 01:21:19 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2021-01-03 01:21:19 | INFO | train | epoch 170 | symm_mse 0.251 | loss 2.964 | nll_loss 0.805 | ppl 1.75 | wps 20723.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 71570 | lr 8.18946e-06 | gnorm 0.642 | train_wall 260 | wall 48510
2021-01-03 01:21:19 | INFO | fairseq.trainer | begin training epoch 171
2021-01-03 01:21:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:21:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:21:41 | INFO | train_inner | epoch 171:     30 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.803, ppl=1.74, wps=16557.2, ups=1.18, wpb=14090.7, bsz=485, num_updates=71600, lr=8.18774e-06, gnorm=0.642, train_wall=62, wall=48532
2021-01-03 01:22:43 | INFO | train_inner | epoch 171:    130 / 421 symm_mse=0.25, loss=2.964, nll_loss=0.807, ppl=1.75, wps=22704.4, ups=1.62, wpb=14057.1, bsz=502.6, num_updates=71700, lr=8.18203e-06, gnorm=0.635, train_wall=62, wall=48594
2021-01-03 01:23:45 | INFO | train_inner | epoch 171:    230 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22298.9, ups=1.61, wpb=13844, bsz=488.8, num_updates=71800, lr=8.17633e-06, gnorm=0.648, train_wall=62, wall=48656
2021-01-03 01:24:47 | INFO | train_inner | epoch 171:    330 / 421 symm_mse=0.25, loss=2.959, nll_loss=0.801, ppl=1.74, wps=22496.7, ups=1.61, wpb=14006.2, bsz=499.4, num_updates=71900, lr=8.17064e-06, gnorm=0.639, train_wall=62, wall=48718
2021-01-03 01:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:25:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:25:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:25:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:26:01 | INFO | valid | epoch 171 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.851 | ppl 14.43 | bleu 22.87 | wps 6010.6 | wpb 10324.2 | bsz 375 | num_updates 71991 | best_bleu 23.03
2021-01-03 01:26:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:26:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:26:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:26:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:26:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:26:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:26:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:26:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 171 @ 71991 updates, score 22.87) (writing took 3.1375350952148438 seconds)
2021-01-03 01:26:04 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2021-01-03 01:26:04 | INFO | train | epoch 171 | symm_mse 0.251 | loss 2.964 | nll_loss 0.805 | ppl 1.75 | wps 20650.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 71991 | lr 8.16548e-06 | gnorm 0.641 | train_wall 261 | wall 48794
2021-01-03 01:26:04 | INFO | fairseq.trainer | begin training epoch 172
2021-01-03 01:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:26:13 | INFO | train_inner | epoch 172:      9 / 421 symm_mse=0.251, loss=2.97, nll_loss=0.811, ppl=1.75, wps=16217.3, ups=1.17, wpb=13852.4, bsz=479.4, num_updates=72000, lr=8.16497e-06, gnorm=0.647, train_wall=62, wall=48803
2021-01-03 01:27:14 | INFO | train_inner | epoch 172:    109 / 421 symm_mse=0.251, loss=2.967, nll_loss=0.808, ppl=1.75, wps=22507.3, ups=1.62, wpb=13907.1, bsz=483.4, num_updates=72100, lr=8.1593e-06, gnorm=0.638, train_wall=62, wall=48865
2021-01-03 01:28:17 | INFO | train_inner | epoch 172:    209 / 421 symm_mse=0.25, loss=2.961, nll_loss=0.803, ppl=1.74, wps=22372.9, ups=1.6, wpb=14003.7, bsz=496.9, num_updates=72200, lr=8.15365e-06, gnorm=0.634, train_wall=62, wall=48928
2021-01-03 01:29:19 | INFO | train_inner | epoch 172:    309 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.802, ppl=1.74, wps=22492.3, ups=1.6, wpb=14031.9, bsz=502.3, num_updates=72300, lr=8.14801e-06, gnorm=0.636, train_wall=62, wall=48990
2021-01-03 01:30:21 | INFO | train_inner | epoch 172:    409 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.806, ppl=1.75, wps=22704.7, ups=1.61, wpb=14074.2, bsz=493.2, num_updates=72400, lr=8.14238e-06, gnorm=0.637, train_wall=62, wall=49052
2021-01-03 01:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:30:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:30:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:30:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:30:46 | INFO | valid | epoch 172 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.851 | ppl 14.43 | bleu 22.95 | wps 5936.2 | wpb 10324.2 | bsz 375 | num_updates 72412 | best_bleu 23.03
2021-01-03 01:30:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:30:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:30:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 172 @ 72412 updates, score 22.95) (writing took 3.124206282198429 seconds)
2021-01-03 01:30:49 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2021-01-03 01:30:49 | INFO | train | epoch 172 | symm_mse 0.25 | loss 2.964 | nll_loss 0.805 | ppl 1.75 | wps 20636.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 72412 | lr 8.1417e-06 | gnorm 0.638 | train_wall 261 | wall 49079
2021-01-03 01:30:49 | INFO | fairseq.trainer | begin training epoch 173
2021-01-03 01:30:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:30:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:31:47 | INFO | train_inner | epoch 173:     88 / 421 symm_mse=0.25, loss=2.958, nll_loss=0.799, ppl=1.74, wps=16281.4, ups=1.17, wpb=13887.6, bsz=469.8, num_updates=72500, lr=8.13676e-06, gnorm=0.642, train_wall=62, wall=49137
2021-01-03 01:32:49 | INFO | train_inner | epoch 173:    188 / 421 symm_mse=0.252, loss=2.974, nll_loss=0.813, ppl=1.76, wps=22458.3, ups=1.61, wpb=13983.5, bsz=492.5, num_updates=72600, lr=8.13116e-06, gnorm=0.637, train_wall=62, wall=49200
2021-01-03 01:33:51 | INFO | train_inner | epoch 173:    288 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.805, ppl=1.75, wps=22429.2, ups=1.62, wpb=13885.1, bsz=508.1, num_updates=72700, lr=8.12556e-06, gnorm=0.641, train_wall=62, wall=49262
2021-01-03 01:34:53 | INFO | train_inner | epoch 173:    388 / 421 symm_mse=0.249, loss=2.959, nll_loss=0.802, ppl=1.74, wps=22898.3, ups=1.62, wpb=14143.7, bsz=497.7, num_updates=72800, lr=8.11998e-06, gnorm=0.634, train_wall=62, wall=49323
2021-01-03 01:35:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:35:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:35:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:35:30 | INFO | valid | epoch 173 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.852 | ppl 14.44 | bleu 22.98 | wps 6021.9 | wpb 10324.2 | bsz 375 | num_updates 72833 | best_bleu 23.03
2021-01-03 01:35:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:35:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:35:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 173 @ 72833 updates, score 22.98) (writing took 3.093272015452385 seconds)
2021-01-03 01:35:33 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2021-01-03 01:35:33 | INFO | train | epoch 173 | symm_mse 0.25 | loss 2.963 | nll_loss 0.805 | ppl 1.75 | wps 20721.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 72833 | lr 8.11814e-06 | gnorm 0.638 | train_wall 260 | wall 49363
2021-01-03 01:35:33 | INFO | fairseq.trainer | begin training epoch 174
2021-01-03 01:35:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:35:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:36:17 | INFO | train_inner | epoch 174:     67 / 421 symm_mse=0.25, loss=2.959, nll_loss=0.801, ppl=1.74, wps=16320.8, ups=1.18, wpb=13800.7, bsz=493, num_updates=72900, lr=8.11441e-06, gnorm=0.646, train_wall=61, wall=49408
2021-01-03 01:37:19 | INFO | train_inner | epoch 174:    167 / 421 symm_mse=0.25, loss=2.967, nll_loss=0.811, ppl=1.75, wps=22456.5, ups=1.61, wpb=13942.9, bsz=519.8, num_updates=73000, lr=8.10885e-06, gnorm=0.636, train_wall=62, wall=49470
2021-01-03 01:38:22 | INFO | train_inner | epoch 174:    267 / 421 symm_mse=0.248, loss=2.952, nll_loss=0.796, ppl=1.74, wps=22609.6, ups=1.6, wpb=14121.8, bsz=490.1, num_updates=73100, lr=8.1033e-06, gnorm=0.631, train_wall=62, wall=49532
2021-01-03 01:39:24 | INFO | train_inner | epoch 174:    367 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.805, ppl=1.75, wps=22455.2, ups=1.61, wpb=13985.4, bsz=488.1, num_updates=73200, lr=8.09776e-06, gnorm=0.641, train_wall=62, wall=49595
2021-01-03 01:39:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:40:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:40:14 | INFO | valid | epoch 174 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 22.86 | wps 5971.4 | wpb 10324.2 | bsz 375 | num_updates 73254 | best_bleu 23.03
2021-01-03 01:40:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 174 @ 73254 updates, score 22.86) (writing took 3.1060362830758095 seconds)
2021-01-03 01:40:17 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2021-01-03 01:40:17 | INFO | train | epoch 174 | symm_mse 0.25 | loss 2.963 | nll_loss 0.805 | ppl 1.75 | wps 20657.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 73254 | lr 8.09478e-06 | gnorm 0.639 | train_wall 261 | wall 49648
2021-01-03 01:40:17 | INFO | fairseq.trainer | begin training epoch 175
2021-01-03 01:40:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:40:49 | INFO | train_inner | epoch 175:     46 / 421 symm_mse=0.252, loss=2.966, nll_loss=0.805, ppl=1.75, wps=16349.5, ups=1.18, wpb=13862.1, bsz=470.2, num_updates=73300, lr=8.09224e-06, gnorm=0.645, train_wall=61, wall=49679
2021-01-03 01:41:51 | INFO | train_inner | epoch 175:    146 / 421 symm_mse=0.249, loss=2.953, nll_loss=0.795, ppl=1.74, wps=22506.6, ups=1.61, wpb=14008.7, bsz=491.7, num_updates=73400, lr=8.08672e-06, gnorm=0.639, train_wall=62, wall=49742
2021-01-03 01:42:54 | INFO | train_inner | epoch 175:    246 / 421 symm_mse=0.249, loss=2.966, nll_loss=0.809, ppl=1.75, wps=22427.7, ups=1.6, wpb=14015.2, bsz=521.2, num_updates=73500, lr=8.08122e-06, gnorm=0.633, train_wall=62, wall=49804
2021-01-03 01:43:56 | INFO | train_inner | epoch 175:    346 / 421 symm_mse=0.255, loss=2.979, nll_loss=0.815, ppl=1.76, wps=22227.2, ups=1.6, wpb=13921.3, bsz=472, num_updates=73600, lr=8.07573e-06, gnorm=0.65, train_wall=62, wall=49867
2021-01-03 01:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:45:00 | INFO | valid | epoch 175 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.853 | ppl 14.45 | bleu 22.91 | wps 5968.4 | wpb 10324.2 | bsz 375 | num_updates 73675 | best_bleu 23.03
2021-01-03 01:45:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:45:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:45:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:45:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:45:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:45:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:45:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:45:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 175 @ 73675 updates, score 22.91) (writing took 3.0733226351439953 seconds)
2021-01-03 01:45:03 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2021-01-03 01:45:03 | INFO | train | epoch 175 | symm_mse 0.25 | loss 2.962 | nll_loss 0.804 | ppl 1.75 | wps 20587.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 73675 | lr 8.07162e-06 | gnorm 0.64 | train_wall 262 | wall 49934
2021-01-03 01:45:03 | INFO | fairseq.trainer | begin training epoch 176
2021-01-03 01:45:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:45:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:45:22 | INFO | train_inner | epoch 176:     25 / 421 symm_mse=0.247, loss=2.954, nll_loss=0.8, ppl=1.74, wps=16328.9, ups=1.17, wpb=13979.8, bsz=486.9, num_updates=73700, lr=8.07025e-06, gnorm=0.637, train_wall=62, wall=49952
2021-01-03 01:46:24 | INFO | train_inner | epoch 176:    125 / 421 symm_mse=0.253, loss=2.973, nll_loss=0.811, ppl=1.75, wps=22562.1, ups=1.62, wpb=13949.2, bsz=476.7, num_updates=73800, lr=8.06478e-06, gnorm=0.647, train_wall=62, wall=50014
2021-01-03 01:47:26 | INFO | train_inner | epoch 176:    225 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.802, ppl=1.74, wps=22464.4, ups=1.61, wpb=13969.7, bsz=511, num_updates=73900, lr=8.05932e-06, gnorm=0.635, train_wall=62, wall=50076
2021-01-03 01:48:28 | INFO | train_inner | epoch 176:    325 / 421 symm_mse=0.249, loss=2.957, nll_loss=0.801, ppl=1.74, wps=22377, ups=1.6, wpb=14016.2, bsz=507, num_updates=74000, lr=8.05387e-06, gnorm=0.639, train_wall=62, wall=50139
2021-01-03 01:49:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:49:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:49:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:49:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:49:47 | INFO | valid | epoch 176 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.851 | ppl 14.43 | bleu 22.89 | wps 5356.6 | wpb 10324.2 | bsz 375 | num_updates 74096 | best_bleu 23.03
2021-01-03 01:49:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 176 @ 74096 updates, score 22.89) (writing took 3.1072796527296305 seconds)
2021-01-03 01:49:50 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2021-01-03 01:49:50 | INFO | train | epoch 176 | symm_mse 0.25 | loss 2.962 | nll_loss 0.804 | ppl 1.75 | wps 20478.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 74096 | lr 8.04865e-06 | gnorm 0.64 | train_wall 261 | wall 50221
2021-01-03 01:49:50 | INFO | fairseq.trainer | begin training epoch 177
2021-01-03 01:49:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:49:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:49:56 | INFO | train_inner | epoch 177:      4 / 421 symm_mse=0.249, loss=2.96, nll_loss=0.802, ppl=1.74, wps=15842.6, ups=1.14, wpb=13879, bsz=475.4, num_updates=74100, lr=8.04844e-06, gnorm=0.641, train_wall=62, wall=50227
2021-01-03 01:50:59 | INFO | train_inner | epoch 177:    104 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.804, ppl=1.75, wps=22500.1, ups=1.6, wpb=14045.3, bsz=497.1, num_updates=74200, lr=8.04301e-06, gnorm=0.637, train_wall=62, wall=50289
2021-01-03 01:52:00 | INFO | train_inner | epoch 177:    204 / 421 symm_mse=0.249, loss=2.961, nll_loss=0.805, ppl=1.75, wps=22286.7, ups=1.62, wpb=13795.2, bsz=505.8, num_updates=74300, lr=8.0376e-06, gnorm=0.646, train_wall=62, wall=50351
2021-01-03 01:53:03 | INFO | train_inner | epoch 177:    304 / 421 symm_mse=0.248, loss=2.956, nll_loss=0.8, ppl=1.74, wps=22567.1, ups=1.61, wpb=14034.8, bsz=483.3, num_updates=74400, lr=8.03219e-06, gnorm=0.633, train_wall=62, wall=50413
2021-01-03 01:54:05 | INFO | train_inner | epoch 177:    404 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22615.7, ups=1.6, wpb=14143, bsz=476.6, num_updates=74500, lr=8.0268e-06, gnorm=0.642, train_wall=62, wall=50476
2021-01-03 01:54:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:54:33 | INFO | valid | epoch 177 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.849 | ppl 14.41 | bleu 22.99 | wps 5911 | wpb 10324.2 | bsz 375 | num_updates 74517 | best_bleu 23.03
2021-01-03 01:54:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:54:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:54:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 177 @ 74517 updates, score 22.99) (writing took 3.0946519002318382 seconds)
2021-01-03 01:54:36 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2021-01-03 01:54:36 | INFO | train | epoch 177 | symm_mse 0.25 | loss 2.962 | nll_loss 0.804 | ppl 1.75 | wps 20605.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 74517 | lr 8.02589e-06 | gnorm 0.641 | train_wall 261 | wall 50506
2021-01-03 01:54:36 | INFO | fairseq.trainer | begin training epoch 178
2021-01-03 01:54:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:54:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:55:30 | INFO | train_inner | epoch 178:     83 / 421 symm_mse=0.25, loss=2.965, nll_loss=0.809, ppl=1.75, wps=16321.7, ups=1.17, wpb=13894.5, bsz=514, num_updates=74600, lr=8.02142e-06, gnorm=0.641, train_wall=62, wall=50561
2021-01-03 01:56:33 | INFO | train_inner | epoch 178:    183 / 421 symm_mse=0.251, loss=2.968, nll_loss=0.808, ppl=1.75, wps=22404.4, ups=1.59, wpb=14046.9, bsz=486.2, num_updates=74700, lr=8.01605e-06, gnorm=0.64, train_wall=62, wall=50624
2021-01-03 01:57:36 | INFO | train_inner | epoch 178:    283 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.805, ppl=1.75, wps=22401, ups=1.6, wpb=14038.7, bsz=499.8, num_updates=74800, lr=8.01069e-06, gnorm=0.637, train_wall=62, wall=50686
2021-01-03 01:58:38 | INFO | train_inner | epoch 178:    383 / 421 symm_mse=0.249, loss=2.952, nll_loss=0.795, ppl=1.73, wps=22317, ups=1.6, wpb=13912.6, bsz=484.2, num_updates=74900, lr=8.00534e-06, gnorm=0.635, train_wall=62, wall=50749
2021-01-03 01:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 01:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 01:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 01:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 01:59:19 | INFO | valid | epoch 178 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 22.94 | wps 5569.4 | wpb 10324.2 | bsz 375 | num_updates 74938 | best_bleu 23.03
2021-01-03 01:59:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 01:59:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 178 @ 74938 updates, score 22.94) (writing took 3.0449472162872553 seconds)
2021-01-03 01:59:22 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2021-01-03 01:59:22 | INFO | train | epoch 178 | symm_mse 0.25 | loss 2.961 | nll_loss 0.804 | ppl 1.75 | wps 20524.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 74938 | lr 8.00331e-06 | gnorm 0.638 | train_wall 262 | wall 50793
2021-01-03 01:59:22 | INFO | fairseq.trainer | begin training epoch 179
2021-01-03 01:59:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 01:59:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 01:59:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:00:04 | INFO | train_inner | epoch 179:     62 / 421 symm_mse=0.251, loss=2.963, nll_loss=0.804, ppl=1.75, wps=16167.3, ups=1.16, wpb=13904.9, bsz=474.1, num_updates=75000, lr=8e-06, gnorm=0.645, train_wall=62, wall=50835
2021-01-03 02:01:07 | INFO | train_inner | epoch 179:    162 / 421 symm_mse=0.251, loss=2.967, nll_loss=0.808, ppl=1.75, wps=22418.3, ups=1.6, wpb=14047.6, bsz=487.3, num_updates=75100, lr=7.99467e-06, gnorm=0.637, train_wall=62, wall=50897
2021-01-03 02:02:09 | INFO | train_inner | epoch 179:    262 / 421 symm_mse=0.245, loss=2.946, nll_loss=0.794, ppl=1.73, wps=22392, ups=1.6, wpb=13964, bsz=514.8, num_updates=75200, lr=7.98935e-06, gnorm=0.631, train_wall=62, wall=50960
2021-01-03 02:03:11 | INFO | train_inner | epoch 179:    362 / 421 symm_mse=0.25, loss=2.969, nll_loss=0.812, ppl=1.76, wps=22486.5, ups=1.61, wpb=13982, bsz=498.6, num_updates=75300, lr=7.98405e-06, gnorm=0.639, train_wall=62, wall=51022
2021-01-03 02:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:03:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:03:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:03:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:03:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:03:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:03:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:03:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:03:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:04:05 | INFO | valid | epoch 179 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.851 | ppl 14.43 | bleu 23.04 | wps 5872.8 | wpb 10324.2 | bsz 375 | num_updates 75359 | best_bleu 23.04
2021-01-03 02:04:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:04:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:04:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:04:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:04:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 179 @ 75359 updates, score 23.04) (writing took 4.91876301728189 seconds)
2021-01-03 02:04:10 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2021-01-03 02:04:10 | INFO | train | epoch 179 | symm_mse 0.25 | loss 2.961 | nll_loss 0.804 | ppl 1.75 | wps 20424.3 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 75359 | lr 7.98092e-06 | gnorm 0.639 | train_wall 262 | wall 51081
2021-01-03 02:04:10 | INFO | fairseq.trainer | begin training epoch 180
2021-01-03 02:04:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:04:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:04:39 | INFO | train_inner | epoch 180:     41 / 421 symm_mse=0.248, loss=2.949, nll_loss=0.792, ppl=1.73, wps=15921.3, ups=1.14, wpb=13952.2, bsz=476.7, num_updates=75400, lr=7.97875e-06, gnorm=0.643, train_wall=62, wall=51109
2021-01-03 02:05:41 | INFO | train_inner | epoch 180:    141 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.802, ppl=1.74, wps=22619.2, ups=1.61, wpb=14015, bsz=491.8, num_updates=75500, lr=7.97347e-06, gnorm=0.634, train_wall=62, wall=51171
2021-01-03 02:06:43 | INFO | train_inner | epoch 180:    241 / 421 symm_mse=0.249, loss=2.963, nll_loss=0.807, ppl=1.75, wps=22508.3, ups=1.6, wpb=14070.5, bsz=504.2, num_updates=75600, lr=7.96819e-06, gnorm=0.634, train_wall=62, wall=51234
2021-01-03 02:07:46 | INFO | train_inner | epoch 180:    341 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.797, ppl=1.74, wps=22140, ups=1.6, wpb=13835.6, bsz=498.9, num_updates=75700, lr=7.96293e-06, gnorm=0.639, train_wall=62, wall=51296
2021-01-03 02:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:08:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:08:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:08:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:08:52 | INFO | valid | epoch 180 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.85 | ppl 14.42 | bleu 22.89 | wps 6024.8 | wpb 10324.2 | bsz 375 | num_updates 75780 | best_bleu 23.04
2021-01-03 02:08:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:08:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 180 @ 75780 updates, score 22.89) (writing took 3.0419627763330936 seconds)
2021-01-03 02:08:55 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2021-01-03 02:08:55 | INFO | train | epoch 180 | symm_mse 0.249 | loss 2.96 | nll_loss 0.803 | ppl 1.74 | wps 20624.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 75780 | lr 7.95872e-06 | gnorm 0.638 | train_wall 261 | wall 51366
2021-01-03 02:08:55 | INFO | fairseq.trainer | begin training epoch 181
2021-01-03 02:08:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:08:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:09:11 | INFO | train_inner | epoch 181:     20 / 421 symm_mse=0.254, loss=2.979, nll_loss=0.815, ppl=1.76, wps=16277.9, ups=1.18, wpb=13826.3, bsz=481.7, num_updates=75800, lr=7.95767e-06, gnorm=0.654, train_wall=62, wall=51381
2021-01-03 02:10:13 | INFO | train_inner | epoch 181:    120 / 421 symm_mse=0.25, loss=2.958, nll_loss=0.8, ppl=1.74, wps=22366.8, ups=1.6, wpb=13940, bsz=496.3, num_updates=75900, lr=7.95243e-06, gnorm=0.641, train_wall=62, wall=51444
2021-01-03 02:11:15 | INFO | train_inner | epoch 181:    220 / 421 symm_mse=0.252, loss=2.968, nll_loss=0.807, ppl=1.75, wps=22754.6, ups=1.62, wpb=14065.5, bsz=478.8, num_updates=76000, lr=7.94719e-06, gnorm=0.64, train_wall=62, wall=51506
2021-01-03 02:12:17 | INFO | train_inner | epoch 181:    320 / 421 symm_mse=0.246, loss=2.947, nll_loss=0.794, ppl=1.73, wps=22848.9, ups=1.61, wpb=14196, bsz=509, num_updates=76100, lr=7.94197e-06, gnorm=0.629, train_wall=62, wall=51568
2021-01-03 02:13:19 | INFO | train_inner | epoch 181:    420 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.808, ppl=1.75, wps=22148.5, ups=1.61, wpb=13783.8, bsz=489, num_updates=76200, lr=7.93676e-06, gnorm=0.646, train_wall=62, wall=51630
2021-01-03 02:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:13:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:13:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:13:37 | INFO | valid | epoch 181 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.852 | ppl 14.44 | bleu 23 | wps 5958.2 | wpb 10324.2 | bsz 375 | num_updates 76201 | best_bleu 23.04
2021-01-03 02:13:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:13:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 181 @ 76201 updates, score 23.0) (writing took 3.208999641239643 seconds)
2021-01-03 02:13:40 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2021-01-03 02:13:40 | INFO | train | epoch 181 | symm_mse 0.25 | loss 2.961 | nll_loss 0.803 | ppl 1.74 | wps 20675.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 76201 | lr 7.93671e-06 | gnorm 0.641 | train_wall 260 | wall 51650
2021-01-03 02:13:40 | INFO | fairseq.trainer | begin training epoch 182
2021-01-03 02:13:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:14:44 | INFO | train_inner | epoch 182:     99 / 421 symm_mse=0.249, loss=2.959, nll_loss=0.803, ppl=1.75, wps=16173.2, ups=1.17, wpb=13777.6, bsz=487.1, num_updates=76300, lr=7.93156e-06, gnorm=0.644, train_wall=62, wall=51715
2021-01-03 02:15:47 | INFO | train_inner | epoch 182:    199 / 421 symm_mse=0.247, loss=2.946, nll_loss=0.791, ppl=1.73, wps=22583.5, ups=1.61, wpb=14023.2, bsz=499, num_updates=76400, lr=7.92636e-06, gnorm=0.64, train_wall=62, wall=51777
2021-01-03 02:16:49 | INFO | train_inner | epoch 182:    299 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.81, ppl=1.75, wps=22493.7, ups=1.61, wpb=13961.4, bsz=484.1, num_updates=76500, lr=7.92118e-06, gnorm=0.642, train_wall=62, wall=51839
2021-01-03 02:17:51 | INFO | train_inner | epoch 182:    399 / 421 symm_mse=0.25, loss=2.962, nll_loss=0.805, ppl=1.75, wps=22776.2, ups=1.61, wpb=14177.1, bsz=491.4, num_updates=76600, lr=7.91601e-06, gnorm=0.635, train_wall=62, wall=51901
2021-01-03 02:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:18:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:18:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:18:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:18:21 | INFO | valid | epoch 182 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 23.02 | wps 5981.2 | wpb 10324.2 | bsz 375 | num_updates 76622 | best_bleu 23.04
2021-01-03 02:18:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:18:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 182 @ 76622 updates, score 23.02) (writing took 2.5860903430730104 seconds)
2021-01-03 02:18:24 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2021-01-03 02:18:24 | INFO | train | epoch 182 | symm_mse 0.249 | loss 2.96 | nll_loss 0.802 | ppl 1.74 | wps 20706 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 76622 | lr 7.91487e-06 | gnorm 0.641 | train_wall 260 | wall 51934
2021-01-03 02:18:24 | INFO | fairseq.trainer | begin training epoch 183
2021-01-03 02:18:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:18:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:19:15 | INFO | train_inner | epoch 183:     78 / 421 symm_mse=0.249, loss=2.962, nll_loss=0.806, ppl=1.75, wps=16532.7, ups=1.19, wpb=13947.5, bsz=505.6, num_updates=76700, lr=7.91085e-06, gnorm=0.644, train_wall=61, wall=51986
2021-01-03 02:20:18 | INFO | train_inner | epoch 183:    178 / 421 symm_mse=0.249, loss=2.96, nll_loss=0.803, ppl=1.74, wps=22329.5, ups=1.6, wpb=13993.4, bsz=492.9, num_updates=76800, lr=7.90569e-06, gnorm=0.635, train_wall=62, wall=52048
2021-01-03 02:21:20 | INFO | train_inner | epoch 183:    278 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.805, ppl=1.75, wps=22385.7, ups=1.61, wpb=13891.2, bsz=503, num_updates=76900, lr=7.90055e-06, gnorm=0.643, train_wall=62, wall=52111
2021-01-03 02:22:22 | INFO | train_inner | epoch 183:    378 / 421 symm_mse=0.249, loss=2.949, nll_loss=0.793, ppl=1.73, wps=22682.8, ups=1.61, wpb=14069.9, bsz=477.7, num_updates=77000, lr=7.89542e-06, gnorm=0.637, train_wall=62, wall=52173
2021-01-03 02:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:22:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:22:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:22:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:22:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:22:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:22:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:23:06 | INFO | valid | epoch 183 | valid on 'valid' subset | symm_mse 0 | loss 5.404 | nll_loss 3.848 | ppl 14.4 | bleu 23 | wps 5859.6 | wpb 10324.2 | bsz 375 | num_updates 77043 | best_bleu 23.04
2021-01-03 02:23:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:23:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 183 @ 77043 updates, score 23.0) (writing took 3.049854416400194 seconds)
2021-01-03 02:23:09 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2021-01-03 02:23:09 | INFO | train | epoch 183 | symm_mse 0.25 | loss 2.961 | nll_loss 0.803 | ppl 1.75 | wps 20648.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 77043 | lr 7.89322e-06 | gnorm 0.641 | train_wall 261 | wall 52219
2021-01-03 02:23:09 | INFO | fairseq.trainer | begin training epoch 184
2021-01-03 02:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:23:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:23:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:23:47 | INFO | train_inner | epoch 184:     57 / 421 symm_mse=0.253, loss=2.972, nll_loss=0.811, ppl=1.75, wps=16150.2, ups=1.18, wpb=13710.2, bsz=487.4, num_updates=77100, lr=7.8903e-06, gnorm=0.659, train_wall=61, wall=52257
2021-01-03 02:24:49 | INFO | train_inner | epoch 184:    157 / 421 symm_mse=0.248, loss=2.96, nll_loss=0.806, ppl=1.75, wps=22609.9, ups=1.61, wpb=14027.9, bsz=507.3, num_updates=77200, lr=7.88519e-06, gnorm=0.632, train_wall=62, wall=52320
2021-01-03 02:25:51 | INFO | train_inner | epoch 184:    257 / 421 symm_mse=0.251, loss=2.958, nll_loss=0.798, ppl=1.74, wps=22471.5, ups=1.61, wpb=13924.9, bsz=479, num_updates=77300, lr=7.88008e-06, gnorm=0.647, train_wall=62, wall=52381
2021-01-03 02:26:53 | INFO | train_inner | epoch 184:    357 / 421 symm_mse=0.248, loss=2.958, nll_loss=0.802, ppl=1.74, wps=22735.4, ups=1.62, wpb=14054.9, bsz=511.9, num_updates=77400, lr=7.87499e-06, gnorm=0.635, train_wall=62, wall=52443
2021-01-03 02:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:27:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:27:49 | INFO | valid | epoch 184 | valid on 'valid' subset | symm_mse 0 | loss 5.406 | nll_loss 3.85 | ppl 14.42 | bleu 22.88 | wps 5974.4 | wpb 10324.2 | bsz 375 | num_updates 77464 | best_bleu 23.04
2021-01-03 02:27:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:27:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 184 @ 77464 updates, score 22.88) (writing took 3.066144036129117 seconds)
2021-01-03 02:27:52 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2021-01-03 02:27:52 | INFO | train | epoch 184 | symm_mse 0.249 | loss 2.959 | nll_loss 0.802 | ppl 1.74 | wps 20720.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 77464 | lr 7.87174e-06 | gnorm 0.64 | train_wall 260 | wall 52503
2021-01-03 02:27:52 | INFO | fairseq.trainer | begin training epoch 185
2021-01-03 02:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:27:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:28:18 | INFO | train_inner | epoch 185:     36 / 421 symm_mse=0.248, loss=2.959, nll_loss=0.804, ppl=1.75, wps=16241.8, ups=1.18, wpb=13792.4, bsz=478.7, num_updates=77500, lr=7.86991e-06, gnorm=0.642, train_wall=61, wall=52528
2021-01-03 02:29:20 | INFO | train_inner | epoch 185:    136 / 421 symm_mse=0.245, loss=2.942, nll_loss=0.789, ppl=1.73, wps=22637.4, ups=1.6, wpb=14123.7, bsz=507.7, num_updates=77600, lr=7.86484e-06, gnorm=0.627, train_wall=62, wall=52591
2021-01-03 02:30:22 | INFO | train_inner | epoch 185:    236 / 421 symm_mse=0.249, loss=2.959, nll_loss=0.803, ppl=1.75, wps=22543, ups=1.62, wpb=13953, bsz=496.4, num_updates=77700, lr=7.85977e-06, gnorm=0.635, train_wall=62, wall=52653
2021-01-03 02:31:24 | INFO | train_inner | epoch 185:    336 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.805, ppl=1.75, wps=22602.3, ups=1.61, wpb=14071.3, bsz=470.5, num_updates=77800, lr=7.85472e-06, gnorm=0.641, train_wall=62, wall=52715
2021-01-03 02:32:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:32:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:32:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:32:33 | INFO | valid | epoch 185 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.853 | ppl 14.45 | bleu 22.94 | wps 6003.7 | wpb 10324.2 | bsz 375 | num_updates 77885 | best_bleu 23.04
2021-01-03 02:32:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:32:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 185 @ 77885 updates, score 22.94) (writing took 3.072116555646062 seconds)
2021-01-03 02:32:36 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2021-01-03 02:32:36 | INFO | train | epoch 185 | symm_mse 0.249 | loss 2.959 | nll_loss 0.802 | ppl 1.74 | wps 20714.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 77885 | lr 7.85043e-06 | gnorm 0.638 | train_wall 260 | wall 52787
2021-01-03 02:32:36 | INFO | fairseq.trainer | begin training epoch 186
2021-01-03 02:32:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:32:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:32:49 | INFO | train_inner | epoch 186:     15 / 421 symm_mse=0.252, loss=2.971, nll_loss=0.81, ppl=1.75, wps=16331.7, ups=1.18, wpb=13839.3, bsz=488.8, num_updates=77900, lr=7.84968e-06, gnorm=0.649, train_wall=61, wall=52800
2021-01-03 02:33:50 | INFO | train_inner | epoch 186:    115 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.796, ppl=1.74, wps=22958.7, ups=1.63, wpb=14101.5, bsz=477.3, num_updates=78000, lr=7.84465e-06, gnorm=0.632, train_wall=61, wall=52861
2021-01-03 02:34:52 | INFO | train_inner | epoch 186:    215 / 421 symm_mse=0.251, loss=2.967, nll_loss=0.808, ppl=1.75, wps=22502.1, ups=1.62, wpb=13923.8, bsz=489.2, num_updates=78100, lr=7.83962e-06, gnorm=0.645, train_wall=62, wall=52923
2021-01-03 02:35:55 | INFO | train_inner | epoch 186:    315 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.798, ppl=1.74, wps=22388.3, ups=1.6, wpb=14001, bsz=512.4, num_updates=78200, lr=7.83461e-06, gnorm=0.636, train_wall=62, wall=52985
2021-01-03 02:36:57 | INFO | train_inner | epoch 186:    415 / 421 symm_mse=0.248, loss=2.958, nll_loss=0.803, ppl=1.75, wps=22371.8, ups=1.6, wpb=13989.7, bsz=497.7, num_updates=78300, lr=7.8296e-06, gnorm=0.638, train_wall=62, wall=53048
2021-01-03 02:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:37:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:37:17 | INFO | valid | epoch 186 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.853 | ppl 14.45 | bleu 23.01 | wps 6118.6 | wpb 10324.2 | bsz 375 | num_updates 78306 | best_bleu 23.04
2021-01-03 02:37:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:37:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 186 @ 78306 updates, score 23.01) (writing took 2.9788588602095842 seconds)
2021-01-03 02:37:20 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2021-01-03 02:37:20 | INFO | train | epoch 186 | symm_mse 0.249 | loss 2.959 | nll_loss 0.802 | ppl 1.74 | wps 20704.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 78306 | lr 7.8293e-06 | gnorm 0.641 | train_wall 260 | wall 53071
2021-01-03 02:37:20 | INFO | fairseq.trainer | begin training epoch 187
2021-01-03 02:37:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:37:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:38:22 | INFO | train_inner | epoch 187:     94 / 421 symm_mse=0.249, loss=2.953, nll_loss=0.796, ppl=1.74, wps=16581, ups=1.18, wpb=14025, bsz=480.5, num_updates=78400, lr=7.82461e-06, gnorm=0.647, train_wall=62, wall=53132
2021-01-03 02:39:24 | INFO | train_inner | epoch 187:    194 / 421 symm_mse=0.25, loss=2.959, nll_loss=0.801, ppl=1.74, wps=22381.5, ups=1.61, wpb=13909.4, bsz=497, num_updates=78500, lr=7.81962e-06, gnorm=0.645, train_wall=62, wall=53195
2021-01-03 02:40:26 | INFO | train_inner | epoch 187:    294 / 421 symm_mse=0.25, loss=2.963, nll_loss=0.806, ppl=1.75, wps=22360.2, ups=1.61, wpb=13916.4, bsz=498.4, num_updates=78600, lr=7.81465e-06, gnorm=0.639, train_wall=62, wall=53257
2021-01-03 02:41:29 | INFO | train_inner | epoch 187:    394 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.806, ppl=1.75, wps=22487.6, ups=1.6, wpb=14027.3, bsz=494.5, num_updates=78700, lr=7.80968e-06, gnorm=0.64, train_wall=62, wall=53319
2021-01-03 02:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:41:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:41:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:41:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:41:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:41:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:41:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:41:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:41:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:42:02 | INFO | valid | epoch 187 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.853 | ppl 14.45 | bleu 22.99 | wps 6096.9 | wpb 10324.2 | bsz 375 | num_updates 78727 | best_bleu 23.04
2021-01-03 02:42:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:42:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:42:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:42:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:42:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:42:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:42:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:42:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 187 @ 78727 updates, score 22.99) (writing took 2.9734633080661297 seconds)
2021-01-03 02:42:05 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2021-01-03 02:42:05 | INFO | train | epoch 187 | symm_mse 0.249 | loss 2.959 | nll_loss 0.802 | ppl 1.74 | wps 20686.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 78727 | lr 7.80834e-06 | gnorm 0.642 | train_wall 261 | wall 53355
2021-01-03 02:42:05 | INFO | fairseq.trainer | begin training epoch 188
2021-01-03 02:42:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:42:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:42:53 | INFO | train_inner | epoch 188:     73 / 421 symm_mse=0.248, loss=2.95, nll_loss=0.793, ppl=1.73, wps=16369.3, ups=1.18, wpb=13816.3, bsz=477.4, num_updates=78800, lr=7.80472e-06, gnorm=0.654, train_wall=61, wall=53404
2021-01-03 02:43:55 | INFO | train_inner | epoch 188:    173 / 421 symm_mse=0.251, loss=2.971, nll_loss=0.811, ppl=1.75, wps=22495.3, ups=1.6, wpb=14017.7, bsz=490.9, num_updates=78900, lr=7.79978e-06, gnorm=0.638, train_wall=62, wall=53466
2021-01-03 02:44:58 | INFO | train_inner | epoch 188:    273 / 421 symm_mse=0.25, loss=2.964, nll_loss=0.806, ppl=1.75, wps=22266.5, ups=1.6, wpb=13894.8, bsz=484.2, num_updates=79000, lr=7.79484e-06, gnorm=0.643, train_wall=62, wall=53528
2021-01-03 02:46:00 | INFO | train_inner | epoch 188:    373 / 421 symm_mse=0.244, loss=2.942, nll_loss=0.791, ppl=1.73, wps=22639, ups=1.6, wpb=14106.7, bsz=516, num_updates=79100, lr=7.78991e-06, gnorm=0.629, train_wall=62, wall=53591
2021-01-03 02:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:46:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:46:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:46:47 | INFO | valid | epoch 188 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.853 | ppl 14.46 | bleu 22.99 | wps 6073.5 | wpb 10324.2 | bsz 375 | num_updates 79148 | best_bleu 23.04
2021-01-03 02:46:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:46:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:46:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 188 @ 79148 updates, score 22.99) (writing took 3.070826107636094 seconds)
2021-01-03 02:46:50 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2021-01-03 02:46:50 | INFO | train | epoch 188 | symm_mse 0.249 | loss 2.958 | nll_loss 0.802 | ppl 1.74 | wps 20636.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 79148 | lr 7.78755e-06 | gnorm 0.639 | train_wall 261 | wall 53640
2021-01-03 02:46:50 | INFO | fairseq.trainer | begin training epoch 189
2021-01-03 02:46:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:46:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:47:25 | INFO | train_inner | epoch 189:     52 / 421 symm_mse=0.248, loss=2.959, nll_loss=0.805, ppl=1.75, wps=16445.2, ups=1.18, wpb=13963.8, bsz=506.2, num_updates=79200, lr=7.78499e-06, gnorm=0.641, train_wall=62, wall=53676
2021-01-03 02:48:27 | INFO | train_inner | epoch 189:    152 / 421 symm_mse=0.249, loss=2.961, nll_loss=0.805, ppl=1.75, wps=22447.4, ups=1.61, wpb=13939.5, bsz=498.8, num_updates=79300, lr=7.78008e-06, gnorm=0.635, train_wall=62, wall=53738
2021-01-03 02:49:30 | INFO | train_inner | epoch 189:    252 / 421 symm_mse=0.25, loss=2.958, nll_loss=0.799, ppl=1.74, wps=22275.7, ups=1.6, wpb=13954.9, bsz=474.6, num_updates=79400, lr=7.77518e-06, gnorm=0.645, train_wall=62, wall=53800
2021-01-03 02:50:33 | INFO | train_inner | epoch 189:    352 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.799, ppl=1.74, wps=22514.9, ups=1.59, wpb=14150.2, bsz=498.6, num_updates=79500, lr=7.77029e-06, gnorm=0.632, train_wall=63, wall=53863
2021-01-03 02:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:51:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:51:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:51:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:51:32 | INFO | valid | epoch 189 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.854 | ppl 14.46 | bleu 23.05 | wps 5956 | wpb 10324.2 | bsz 375 | num_updates 79569 | best_bleu 23.05
2021-01-03 02:51:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:51:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 189 @ 79569 updates, score 23.05) (writing took 4.984196616336703 seconds)
2021-01-03 02:51:37 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2021-01-03 02:51:37 | INFO | train | epoch 189 | symm_mse 0.249 | loss 2.958 | nll_loss 0.801 | ppl 1.74 | wps 20469.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 79569 | lr 7.76692e-06 | gnorm 0.64 | train_wall 261 | wall 53928
2021-01-03 02:51:37 | INFO | fairseq.trainer | begin training epoch 190
2021-01-03 02:51:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:51:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:51:59 | INFO | train_inner | epoch 190:     31 / 421 symm_mse=0.252, loss=2.963, nll_loss=0.802, ppl=1.74, wps=15941, ups=1.15, wpb=13842.8, bsz=469.9, num_updates=79600, lr=7.7654e-06, gnorm=0.654, train_wall=62, wall=53950
2021-01-03 02:53:02 | INFO | train_inner | epoch 190:    131 / 421 symm_mse=0.243, loss=2.935, nll_loss=0.785, ppl=1.72, wps=22368.1, ups=1.6, wpb=13947.4, bsz=520.1, num_updates=79700, lr=7.76053e-06, gnorm=0.628, train_wall=62, wall=54012
2021-01-03 02:54:04 | INFO | train_inner | epoch 190:    231 / 421 symm_mse=0.247, loss=2.951, nll_loss=0.797, ppl=1.74, wps=22546.4, ups=1.61, wpb=14036.5, bsz=509, num_updates=79800, lr=7.75567e-06, gnorm=0.635, train_wall=62, wall=54075
2021-01-03 02:55:06 | INFO | train_inner | epoch 190:    331 / 421 symm_mse=0.251, loss=2.972, nll_loss=0.814, ppl=1.76, wps=22569.9, ups=1.6, wpb=14082.9, bsz=479.4, num_updates=79900, lr=7.75081e-06, gnorm=0.637, train_wall=62, wall=54137
2021-01-03 02:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 02:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 02:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 02:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 02:56:19 | INFO | valid | epoch 190 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.854 | ppl 14.46 | bleu 22.93 | wps 6026 | wpb 10324.2 | bsz 375 | num_updates 79990 | best_bleu 23.05
2021-01-03 02:56:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 02:56:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 190 @ 79990 updates, score 22.93) (writing took 3.0432867128401995 seconds)
2021-01-03 02:56:22 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2021-01-03 02:56:22 | INFO | train | epoch 190 | symm_mse 0.249 | loss 2.957 | nll_loss 0.801 | ppl 1.74 | wps 20652.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 79990 | lr 7.74645e-06 | gnorm 0.641 | train_wall 261 | wall 54212
2021-01-03 02:56:22 | INFO | fairseq.trainer | begin training epoch 191
2021-01-03 02:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 02:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 02:56:31 | INFO | train_inner | epoch 191:     10 / 421 symm_mse=0.25, loss=2.964, nll_loss=0.805, ppl=1.75, wps=16204.2, ups=1.18, wpb=13768.7, bsz=479.2, num_updates=80000, lr=7.74597e-06, gnorm=0.661, train_wall=62, wall=54222
2021-01-03 02:57:33 | INFO | train_inner | epoch 191:    110 / 421 symm_mse=0.25, loss=2.966, nll_loss=0.808, ppl=1.75, wps=22545.3, ups=1.62, wpb=13952.8, bsz=511.2, num_updates=80100, lr=7.74113e-06, gnorm=0.639, train_wall=62, wall=54284
2021-01-03 02:58:35 | INFO | train_inner | epoch 191:    210 / 421 symm_mse=0.248, loss=2.951, nll_loss=0.795, ppl=1.74, wps=22552.7, ups=1.61, wpb=14013.1, bsz=487.9, num_updates=80200, lr=7.7363e-06, gnorm=0.634, train_wall=62, wall=54346
2021-01-03 02:59:38 | INFO | train_inner | epoch 191:    310 / 421 symm_mse=0.246, loss=2.948, nll_loss=0.795, ppl=1.74, wps=22499.8, ups=1.6, wpb=14099.4, bsz=482.3, num_updates=80300, lr=7.73148e-06, gnorm=0.634, train_wall=62, wall=54409
2021-01-03 03:00:41 | INFO | train_inner | epoch 191:    410 / 421 symm_mse=0.251, loss=2.964, nll_loss=0.805, ppl=1.75, wps=22255, ups=1.6, wpb=13908.6, bsz=492, num_updates=80400, lr=7.72667e-06, gnorm=0.646, train_wall=62, wall=54471
2021-01-03 03:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:00:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:00:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:00:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:00:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:01:04 | INFO | valid | epoch 191 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.854 | ppl 14.46 | bleu 22.95 | wps 6049.7 | wpb 10324.2 | bsz 375 | num_updates 80411 | best_bleu 23.05
2021-01-03 03:01:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:01:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:01:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:01:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:01:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:01:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:01:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:01:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 191 @ 80411 updates, score 22.95) (writing took 3.0465041883289814 seconds)
2021-01-03 03:01:07 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2021-01-03 03:01:07 | INFO | train | epoch 191 | symm_mse 0.249 | loss 2.957 | nll_loss 0.801 | ppl 1.74 | wps 20620.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 80411 | lr 7.72615e-06 | gnorm 0.64 | train_wall 261 | wall 54498
2021-01-03 03:01:07 | INFO | fairseq.trainer | begin training epoch 192
2021-01-03 03:01:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:01:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:02:05 | INFO | train_inner | epoch 192:     89 / 421 symm_mse=0.249, loss=2.955, nll_loss=0.799, ppl=1.74, wps=16498.7, ups=1.18, wpb=13936.3, bsz=487.1, num_updates=80500, lr=7.72187e-06, gnorm=0.637, train_wall=61, wall=54556
2021-01-03 03:03:08 | INFO | train_inner | epoch 192:    189 / 421 symm_mse=0.25, loss=2.964, nll_loss=0.806, ppl=1.75, wps=22454.8, ups=1.6, wpb=14033.4, bsz=490.9, num_updates=80600, lr=7.71708e-06, gnorm=0.638, train_wall=62, wall=54618
2021-01-03 03:04:10 | INFO | train_inner | epoch 192:    289 / 421 symm_mse=0.251, loss=2.965, nll_loss=0.807, ppl=1.75, wps=22292.8, ups=1.6, wpb=13924.8, bsz=486.5, num_updates=80700, lr=7.7123e-06, gnorm=0.645, train_wall=62, wall=54681
2021-01-03 03:05:13 | INFO | train_inner | epoch 192:    389 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.8, ppl=1.74, wps=22266.1, ups=1.6, wpb=13920.1, bsz=489.2, num_updates=80800, lr=7.70752e-06, gnorm=0.645, train_wall=62, wall=54743
2021-01-03 03:05:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:05:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:05:49 | INFO | valid | epoch 192 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.854 | ppl 14.46 | bleu 22.95 | wps 6054.8 | wpb 10324.2 | bsz 375 | num_updates 80832 | best_bleu 23.05
2021-01-03 03:05:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:05:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:05:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 192 @ 80832 updates, score 22.95) (writing took 2.99893506616354 seconds)
2021-01-03 03:05:52 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2021-01-03 03:05:52 | INFO | train | epoch 192 | symm_mse 0.249 | loss 2.958 | nll_loss 0.801 | ppl 1.74 | wps 20642.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 80832 | lr 7.706e-06 | gnorm 0.639 | train_wall 261 | wall 54783
2021-01-03 03:05:52 | INFO | fairseq.trainer | begin training epoch 193
2021-01-03 03:05:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:05:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:06:37 | INFO | train_inner | epoch 193:     68 / 421 symm_mse=0.245, loss=2.942, nll_loss=0.79, ppl=1.73, wps=16544.1, ups=1.18, wpb=13979.6, bsz=507, num_updates=80900, lr=7.70276e-06, gnorm=0.633, train_wall=61, wall=54828
2021-01-03 03:07:39 | INFO | train_inner | epoch 193:    168 / 421 symm_mse=0.252, loss=2.97, nll_loss=0.81, ppl=1.75, wps=22218.8, ups=1.6, wpb=13844.1, bsz=492, num_updates=81000, lr=7.698e-06, gnorm=0.653, train_wall=62, wall=54890
2021-01-03 03:08:42 | INFO | train_inner | epoch 193:    268 / 421 symm_mse=0.246, loss=2.946, nll_loss=0.794, ppl=1.73, wps=22494.4, ups=1.6, wpb=14052.2, bsz=498.1, num_updates=81100, lr=7.69326e-06, gnorm=0.633, train_wall=62, wall=54952
2021-01-03 03:09:44 | INFO | train_inner | epoch 193:    368 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.802, ppl=1.74, wps=22583.6, ups=1.62, wpb=13939.2, bsz=487.3, num_updates=81200, lr=7.68852e-06, gnorm=0.642, train_wall=62, wall=55014
2021-01-03 03:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:10:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:10:33 | INFO | valid | epoch 193 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.854 | ppl 14.46 | bleu 22.94 | wps 5996.2 | wpb 10324.2 | bsz 375 | num_updates 81253 | best_bleu 23.05
2021-01-03 03:10:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:10:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:10:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 193 @ 81253 updates, score 22.94) (writing took 3.0740242917090654 seconds)
2021-01-03 03:10:36 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2021-01-03 03:10:36 | INFO | train | epoch 193 | symm_mse 0.249 | loss 2.957 | nll_loss 0.801 | ppl 1.74 | wps 20681.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 81253 | lr 7.68601e-06 | gnorm 0.641 | train_wall 261 | wall 55067
2021-01-03 03:10:36 | INFO | fairseq.trainer | begin training epoch 194
2021-01-03 03:10:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:10:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:11:08 | INFO | train_inner | epoch 194:     47 / 421 symm_mse=0.247, loss=2.955, nll_loss=0.801, ppl=1.74, wps=16588.4, ups=1.18, wpb=14075.4, bsz=500.2, num_updates=81300, lr=7.68379e-06, gnorm=0.635, train_wall=62, wall=55099
2021-01-03 03:12:11 | INFO | train_inner | epoch 194:    147 / 421 symm_mse=0.249, loss=2.96, nll_loss=0.803, ppl=1.74, wps=22383.6, ups=1.61, wpb=13942.5, bsz=500.2, num_updates=81400, lr=7.67907e-06, gnorm=0.639, train_wall=62, wall=55161
2021-01-03 03:13:13 | INFO | train_inner | epoch 194:    247 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.801, ppl=1.74, wps=22432, ups=1.61, wpb=13952, bsz=475.3, num_updates=81500, lr=7.67435e-06, gnorm=0.644, train_wall=62, wall=55223
2021-01-03 03:14:15 | INFO | train_inner | epoch 194:    347 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.806, ppl=1.75, wps=22620.3, ups=1.62, wpb=13985.6, bsz=484.2, num_updates=81600, lr=7.66965e-06, gnorm=0.643, train_wall=62, wall=55285
2021-01-03 03:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:15:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:15:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:15:17 | INFO | valid | epoch 194 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.854 | ppl 14.46 | bleu 22.93 | wps 6008.8 | wpb 10324.2 | bsz 375 | num_updates 81674 | best_bleu 23.05
2021-01-03 03:15:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:15:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 194 @ 81674 updates, score 22.93) (writing took 3.097842797636986 seconds)
2021-01-03 03:15:21 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2021-01-03 03:15:21 | INFO | train | epoch 194 | symm_mse 0.248 | loss 2.956 | nll_loss 0.8 | ppl 1.74 | wps 20688.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 81674 | lr 7.66617e-06 | gnorm 0.64 | train_wall 260 | wall 55351
2021-01-03 03:15:21 | INFO | fairseq.trainer | begin training epoch 195
2021-01-03 03:15:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:15:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:15:40 | INFO | train_inner | epoch 195:     26 / 421 symm_mse=0.245, loss=2.942, nll_loss=0.791, ppl=1.73, wps=16396.9, ups=1.17, wpb=13970.7, bsz=500.7, num_updates=81700, lr=7.66495e-06, gnorm=0.64, train_wall=62, wall=55370
2021-01-03 03:16:42 | INFO | train_inner | epoch 195:    126 / 421 symm_mse=0.245, loss=2.944, nll_loss=0.791, ppl=1.73, wps=22599.7, ups=1.62, wpb=13954.2, bsz=473.7, num_updates=81800, lr=7.66027e-06, gnorm=0.634, train_wall=62, wall=55432
2021-01-03 03:17:44 | INFO | train_inner | epoch 195:    226 / 421 symm_mse=0.249, loss=2.96, nll_loss=0.804, ppl=1.75, wps=22470.7, ups=1.61, wpb=13999.4, bsz=493.2, num_updates=81900, lr=7.65559e-06, gnorm=0.636, train_wall=62, wall=55495
2021-01-03 03:18:46 | INFO | train_inner | epoch 195:    326 / 421 symm_mse=0.25, loss=2.961, nll_loss=0.804, ppl=1.75, wps=22358.7, ups=1.6, wpb=13979.4, bsz=512.6, num_updates=82000, lr=7.65092e-06, gnorm=0.644, train_wall=62, wall=55557
2021-01-03 03:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:19:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:19:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:19:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:19:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:19:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:19:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:19:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:19:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:19:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:19:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:19:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:20:02 | INFO | valid | epoch 195 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.854 | ppl 14.46 | bleu 22.91 | wps 5956.1 | wpb 10324.2 | bsz 375 | num_updates 82095 | best_bleu 23.05
2021-01-03 03:20:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:20:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:20:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:20:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:20:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:20:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 195 @ 82095 updates, score 22.91) (writing took 3.0954319443553686 seconds)
2021-01-03 03:20:05 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2021-01-03 03:20:05 | INFO | train | epoch 195 | symm_mse 0.248 | loss 2.956 | nll_loss 0.8 | ppl 1.74 | wps 20651.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 82095 | lr 7.64649e-06 | gnorm 0.643 | train_wall 261 | wall 55636
2021-01-03 03:20:05 | INFO | fairseq.trainer | begin training epoch 196
2021-01-03 03:20:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:20:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:20:12 | INFO | train_inner | epoch 196:      5 / 421 symm_mse=0.251, loss=2.966, nll_loss=0.806, ppl=1.75, wps=16291.4, ups=1.17, wpb=13902.2, bsz=487.6, num_updates=82100, lr=7.64626e-06, gnorm=0.659, train_wall=62, wall=55642
2021-01-03 03:21:14 | INFO | train_inner | epoch 196:    105 / 421 symm_mse=0.248, loss=2.961, nll_loss=0.806, ppl=1.75, wps=22835.8, ups=1.61, wpb=14158.8, bsz=503.9, num_updates=82200, lr=7.64161e-06, gnorm=0.63, train_wall=62, wall=55704
2021-01-03 03:22:16 | INFO | train_inner | epoch 196:    205 / 421 symm_mse=0.249, loss=2.955, nll_loss=0.797, ppl=1.74, wps=22219.3, ups=1.61, wpb=13789.9, bsz=491.5, num_updates=82300, lr=7.63696e-06, gnorm=0.644, train_wall=62, wall=55766
2021-01-03 03:23:18 | INFO | train_inner | epoch 196:    305 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.802, ppl=1.74, wps=22409.7, ups=1.62, wpb=13873.3, bsz=487, num_updates=82400, lr=7.63233e-06, gnorm=0.644, train_wall=62, wall=55828
2021-01-03 03:24:21 | INFO | train_inner | epoch 196:    405 / 421 symm_mse=0.248, loss=2.951, nll_loss=0.794, ppl=1.73, wps=22453.9, ups=1.59, wpb=14111.5, bsz=487.8, num_updates=82500, lr=7.6277e-06, gnorm=0.635, train_wall=63, wall=55891
2021-01-03 03:24:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:24:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:24:47 | INFO | valid | epoch 196 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.853 | ppl 14.45 | bleu 23 | wps 5978.9 | wpb 10324.2 | bsz 375 | num_updates 82516 | best_bleu 23.05
2021-01-03 03:24:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:24:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:24:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 196 @ 82516 updates, score 23.0) (writing took 3.1875229757279158 seconds)
2021-01-03 03:24:51 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2021-01-03 03:24:51 | INFO | train | epoch 196 | symm_mse 0.249 | loss 2.956 | nll_loss 0.8 | ppl 1.74 | wps 20622.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 82516 | lr 7.62696e-06 | gnorm 0.64 | train_wall 261 | wall 55921
2021-01-03 03:24:51 | INFO | fairseq.trainer | begin training epoch 197
2021-01-03 03:24:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:24:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:25:46 | INFO | train_inner | epoch 197:     84 / 421 symm_mse=0.242, loss=2.936, nll_loss=0.789, ppl=1.73, wps=16370.3, ups=1.17, wpb=13945.9, bsz=521.2, num_updates=82600, lr=7.62308e-06, gnorm=0.633, train_wall=62, wall=55976
2021-01-03 03:26:49 | INFO | train_inner | epoch 197:    184 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.8, ppl=1.74, wps=22237.2, ups=1.59, wpb=13946.3, bsz=480.9, num_updates=82700, lr=7.61847e-06, gnorm=0.642, train_wall=63, wall=56039
2021-01-03 03:27:50 | INFO | train_inner | epoch 197:    284 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.801, ppl=1.74, wps=22538.6, ups=1.62, wpb=13945.8, bsz=481.9, num_updates=82800, lr=7.61387e-06, gnorm=0.64, train_wall=62, wall=56101
2021-01-03 03:28:53 | INFO | train_inner | epoch 197:    384 / 421 symm_mse=0.248, loss=2.958, nll_loss=0.802, ppl=1.74, wps=22426.7, ups=1.6, wpb=14039.7, bsz=492.8, num_updates=82900, lr=7.60928e-06, gnorm=0.638, train_wall=62, wall=56164
2021-01-03 03:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:29:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:29:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:29:33 | INFO | valid | epoch 197 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.855 | ppl 14.47 | bleu 22.84 | wps 5964.5 | wpb 10324.2 | bsz 375 | num_updates 82937 | best_bleu 23.05
2021-01-03 03:29:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:29:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:29:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 197 @ 82937 updates, score 22.84) (writing took 3.0398103557527065 seconds)
2021-01-03 03:29:36 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2021-01-03 03:29:36 | INFO | train | epoch 197 | symm_mse 0.248 | loss 2.955 | nll_loss 0.8 | ppl 1.74 | wps 20620.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 82937 | lr 7.60758e-06 | gnorm 0.639 | train_wall 261 | wall 56206
2021-01-03 03:29:36 | INFO | fairseq.trainer | begin training epoch 198
2021-01-03 03:29:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:29:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:30:18 | INFO | train_inner | epoch 198:     63 / 421 symm_mse=0.25, loss=2.958, nll_loss=0.8, ppl=1.74, wps=16423.5, ups=1.18, wpb=13884.3, bsz=480.2, num_updates=83000, lr=7.60469e-06, gnorm=0.646, train_wall=61, wall=56248
2021-01-03 03:31:20 | INFO | train_inner | epoch 198:    163 / 421 symm_mse=0.249, loss=2.958, nll_loss=0.801, ppl=1.74, wps=22394.9, ups=1.6, wpb=14005.4, bsz=496.3, num_updates=83100, lr=7.60011e-06, gnorm=0.641, train_wall=62, wall=56311
2021-01-03 03:32:22 | INFO | train_inner | epoch 198:    263 / 421 symm_mse=0.248, loss=2.954, nll_loss=0.799, ppl=1.74, wps=22597.8, ups=1.6, wpb=14106.3, bsz=493.8, num_updates=83200, lr=7.59555e-06, gnorm=0.631, train_wall=62, wall=56373
2021-01-03 03:33:25 | INFO | train_inner | epoch 198:    363 / 421 symm_mse=0.247, loss=2.954, nll_loss=0.801, ppl=1.74, wps=22554.6, ups=1.61, wpb=13991.4, bsz=494.3, num_updates=83300, lr=7.59098e-06, gnorm=0.64, train_wall=62, wall=56435
2021-01-03 03:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:34:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:34:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:34:18 | INFO | valid | epoch 198 | valid on 'valid' subset | symm_mse 0 | loss 5.409 | nll_loss 3.852 | ppl 14.44 | bleu 23.06 | wps 5706.4 | wpb 10324.2 | bsz 375 | num_updates 83358 | best_bleu 23.06
2021-01-03 03:34:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:34:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_best.pt (epoch 198 @ 83358 updates, score 23.06) (writing took 5.154620956629515 seconds)
2021-01-03 03:34:23 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2021-01-03 03:34:23 | INFO | train | epoch 198 | symm_mse 0.248 | loss 2.956 | nll_loss 0.8 | ppl 1.74 | wps 20465.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 83358 | lr 7.58834e-06 | gnorm 0.64 | train_wall 261 | wall 56494
2021-01-03 03:34:23 | INFO | fairseq.trainer | begin training epoch 199
2021-01-03 03:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:34:52 | INFO | train_inner | epoch 199:     42 / 421 symm_mse=0.249, loss=2.957, nll_loss=0.8, ppl=1.74, wps=15752.7, ups=1.14, wpb=13789.7, bsz=479, num_updates=83400, lr=7.58643e-06, gnorm=0.65, train_wall=61, wall=56523
2021-01-03 03:35:54 | INFO | train_inner | epoch 199:    142 / 421 symm_mse=0.249, loss=2.962, nll_loss=0.806, ppl=1.75, wps=22287.8, ups=1.6, wpb=13902.6, bsz=503.1, num_updates=83500, lr=7.58189e-06, gnorm=0.642, train_wall=62, wall=56585
2021-01-03 03:36:57 | INFO | train_inner | epoch 199:    242 / 421 symm_mse=0.25, loss=2.96, nll_loss=0.803, ppl=1.74, wps=22178.3, ups=1.59, wpb=13944.7, bsz=500.2, num_updates=83600, lr=7.57735e-06, gnorm=0.64, train_wall=63, wall=56648
2021-01-03 03:38:00 | INFO | train_inner | epoch 199:    342 / 421 symm_mse=0.245, loss=2.942, nll_loss=0.79, ppl=1.73, wps=22426.5, ups=1.58, wpb=14156.4, bsz=485, num_updates=83700, lr=7.57282e-06, gnorm=0.629, train_wall=63, wall=56711
2021-01-03 03:38:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:38:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:38:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:38:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:39:06 | INFO | valid | epoch 199 | valid on 'valid' subset | symm_mse 0 | loss 5.408 | nll_loss 3.853 | ppl 14.45 | bleu 22.87 | wps 6039.3 | wpb 10324.2 | bsz 375 | num_updates 83779 | best_bleu 23.06
2021-01-03 03:39:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:39:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 199 @ 83779 updates, score 22.87) (writing took 3.0924828220158815 seconds)
2021-01-03 03:39:10 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2021-01-03 03:39:10 | INFO | train | epoch 199 | symm_mse 0.248 | loss 2.955 | nll_loss 0.799 | ppl 1.74 | wps 20531.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 83779 | lr 7.56925e-06 | gnorm 0.639 | train_wall 263 | wall 56780
2021-01-03 03:39:10 | INFO | fairseq.trainer | begin training epoch 200
2021-01-03 03:39:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:39:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:39:26 | INFO | train_inner | epoch 200:     21 / 421 symm_mse=0.248, loss=2.953, nll_loss=0.798, ppl=1.74, wps=16297.9, ups=1.17, wpb=13904.4, bsz=485.8, num_updates=83800, lr=7.5683e-06, gnorm=0.643, train_wall=62, wall=56796
2021-01-03 03:40:28 | INFO | train_inner | epoch 200:    121 / 421 symm_mse=0.246, loss=2.948, nll_loss=0.794, ppl=1.73, wps=22561.9, ups=1.6, wpb=14100, bsz=520.4, num_updates=83900, lr=7.56379e-06, gnorm=0.633, train_wall=62, wall=56859
2021-01-03 03:41:31 | INFO | train_inner | epoch 200:    221 / 421 symm_mse=0.249, loss=2.957, nll_loss=0.801, ppl=1.74, wps=22086.9, ups=1.59, wpb=13930.3, bsz=490.3, num_updates=84000, lr=7.55929e-06, gnorm=0.639, train_wall=63, wall=56922
2021-01-03 03:42:33 | INFO | train_inner | epoch 200:    321 / 421 symm_mse=0.247, loss=2.953, nll_loss=0.799, ppl=1.74, wps=22360.4, ups=1.61, wpb=13881.7, bsz=489.8, num_updates=84100, lr=7.55479e-06, gnorm=0.636, train_wall=62, wall=56984
2021-01-03 03:43:36 | INFO | train_inner | epoch 200:    421 / 421 symm_mse=0.25, loss=2.965, nll_loss=0.806, ppl=1.75, wps=22340.2, ups=1.6, wpb=13928.5, bsz=477.7, num_updates=84200, lr=7.55031e-06, gnorm=0.642, train_wall=62, wall=57046
2021-01-03 03:43:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-03 03:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-03 03:43:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:43:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:43:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:43:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-03 03:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-03 03:43:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-03 03:43:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-03 03:43:52 | INFO | valid | epoch 200 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.855 | ppl 14.47 | bleu 22.99 | wps 6077.5 | wpb 10324.2 | bsz 375 | num_updates 84200 | best_bleu 23.06
2021-01-03 03:43:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-03 03:43:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all2/checkpoint_last.pt (epoch 200 @ 84200 updates, score 22.99) (writing took 3.134848916903138 seconds)
2021-01-03 03:43:55 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2021-01-03 03:43:55 | INFO | train | epoch 200 | symm_mse 0.248 | loss 2.955 | nll_loss 0.799 | ppl 1.74 | wps 20576.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 84200 | lr 7.55031e-06 | gnorm 0.637 | train_wall 262 | wall 57066
2021-01-03 03:43:55 | INFO | fairseq_cli.train | done training in 57065.1 seconds
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1600 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
