nohup: ignoring input
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.0005
warmup_updates=4000
max_epoch=200
r3f_lambda=0.08
layer_choice=normal
save_dir=./examples/entr/bash/../checkpoints/closer-all
2020-12-08 15:56:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:49 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:13899
2020-12-08 15:56:49 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:13899
2020-12-08 15:56:49 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:13899
2020-12-08 15:56:50 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-08 15:56:50 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-08 15:56:50 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-08 15:56:54 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:13899', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.08, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-08 15:56:54 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-08 15:56:54 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-08 15:56:54 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-08 15:56:54 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-08 15:56:54 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-08 15:56:55 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-08 15:56:55 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-08 15:56:55 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-08 15:56:55 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2020-12-08 15:56:55 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-08 15:56:55 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-08 15:56:55 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-08 15:56:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 15:56:55 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 15:56:55 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 15:56:55 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 15:56:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 15:56:55 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-08 15:56:55 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-08 15:56:55 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-08 15:56:56 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-08 15:56:56 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-08 15:56:56 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-08 15:56:56 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-08 15:56:56 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-08 15:56:56 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-08 15:56:56 | INFO | fairseq.trainer | begin training epoch 1
2020-12-08 15:56:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 15:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 15:58:01 | INFO | train_inner | epoch 001:    100 / 561 symm_mse=20.45, loss=5.029, nll_loss=0.869, ppl=1.83, wps=17511.2, ups=1.65, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.25975e-05, gnorm=3.73, train_wall=61, wall=66
2020-12-08 15:59:02 | INFO | train_inner | epoch 001:    200 / 561 symm_mse=16.501, loss=4.635, nll_loss=0.974, ppl=1.96, wps=17263.7, ups=1.63, wpb=10583.4, bsz=369.8, num_updates=200, lr=2.5095e-05, gnorm=2.632, train_wall=61, wall=127
2020-12-08 16:00:04 | INFO | train_inner | epoch 001:    300 / 561 symm_mse=13.697, loss=4.381, nll_loss=1.08, ppl=2.11, wps=16707.6, ups=1.62, wpb=10335, bsz=373, num_updates=300, lr=3.75925e-05, gnorm=2.095, train_wall=62, wall=189
2020-12-08 16:01:06 | INFO | train_inner | epoch 001:    400 / 561 symm_mse=12.022, loss=4.226, nll_loss=1.129, ppl=2.19, wps=16992.6, ups=1.61, wpb=10571.8, bsz=388.4, num_updates=400, lr=5.009e-05, gnorm=1.766, train_wall=62, wall=251
2020-12-08 16:02:08 | INFO | train_inner | epoch 001:    500 / 561 symm_mse=11.453, loss=4.194, nll_loss=1.168, ppl=2.25, wps=16787.6, ups=1.61, wpb=10411.2, bsz=371.8, num_updates=500, lr=6.25875e-05, gnorm=1.704, train_wall=62, wall=313
2020-12-08 16:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:02:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:02:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:02:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:03:07 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_mse 0 | loss 5.42 | nll_loss 3.958 | ppl 15.54 | bleu 22.68 | wps 4886 | wpb 7508.5 | bsz 272.7 | num_updates 561
2020-12-08 16:03:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.68) (writing took 2.3818820416927338 seconds)
2020-12-08 16:03:09 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-08 16:03:09 | INFO | train | epoch 001 | symm_mse 14.554 | loss 4.479 | nll_loss 1.067 | ppl 2.09 | wps 15948.3 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 7.0211e-05 | gnorm 2.318 | train_wall 346 | wall 374
2020-12-08 16:03:09 | INFO | fairseq.trainer | begin training epoch 2
2020-12-08 16:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:03:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:03:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:03:36 | INFO | train_inner | epoch 002:     39 / 561 symm_mse=11.447, loss=4.242, nll_loss=1.231, ppl=2.35, wps=11779.6, ups=1.14, wpb=10345.6, bsz=358.4, num_updates=600, lr=7.5085e-05, gnorm=1.695, train_wall=62, wall=401
2020-12-08 16:04:39 | INFO | train_inner | epoch 002:    139 / 561 symm_mse=10.846, loss=4.187, nll_loss=1.242, ppl=2.37, wps=16825.4, ups=1.6, wpb=10532.9, bsz=366.4, num_updates=700, lr=8.75825e-05, gnorm=1.64, train_wall=62, wall=464
2020-12-08 16:05:42 | INFO | train_inner | epoch 002:    239 / 561 symm_mse=9.885, loss=4.065, nll_loss=1.222, ppl=2.33, wps=16711.6, ups=1.59, wpb=10499.4, bsz=369.4, num_updates=800, lr=0.00010008, gnorm=1.506, train_wall=63, wall=526
2020-12-08 16:06:44 | INFO | train_inner | epoch 002:    339 / 561 symm_mse=9.709, loss=4.071, nll_loss=1.254, ppl=2.38, wps=16949.6, ups=1.61, wpb=10541.2, bsz=377.1, num_updates=900, lr=0.000112578, gnorm=1.499, train_wall=62, wall=589
2020-12-08 16:07:46 | INFO | train_inner | epoch 002:    439 / 561 symm_mse=9.447, loss=4.065, nll_loss=1.279, ppl=2.43, wps=16748.4, ups=1.6, wpb=10472.1, bsz=362.4, num_updates=1000, lr=0.000125075, gnorm=1.469, train_wall=62, wall=651
2020-12-08 16:08:48 | INFO | train_inner | epoch 002:    539 / 561 symm_mse=9.277, loss=4.074, nll_loss=1.314, ppl=2.49, wps=16832.7, ups=1.61, wpb=10444.3, bsz=375.2, num_updates=1100, lr=0.000137573, gnorm=1.482, train_wall=62, wall=713
2020-12-08 16:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:09:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:09:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:09:25 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_mse 0 | loss 5.39 | nll_loss 3.937 | ppl 15.31 | bleu 21.95 | wps 3963.8 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.68
2020-12-08 16:09:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:09:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:09:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:09:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 2 @ 1122 updates, score 21.95) (writing took 2.998280106112361 seconds)
2020-12-08 16:09:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-08 16:09:28 | INFO | train | epoch 002 | symm_mse 9.87 | loss 4.095 | nll_loss 1.26 | ppl 2.4 | wps 15509.5 | ups 1.48 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 0.000140322 | gnorm 1.529 | train_wall 348 | wall 753
2020-12-08 16:09:28 | INFO | fairseq.trainer | begin training epoch 3
2020-12-08 16:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:10:19 | INFO | train_inner | epoch 003:     78 / 561 symm_mse=9.091, loss=4.045, nll_loss=1.305, ppl=2.47, wps=11486.5, ups=1.1, wpb=10441.4, bsz=356.7, num_updates=1200, lr=0.00015007, gnorm=1.477, train_wall=61, wall=804
2020-12-08 16:11:22 | INFO | train_inner | epoch 003:    178 / 561 symm_mse=8.652, loss=3.995, nll_loss=1.299, ppl=2.46, wps=16599.8, ups=1.59, wpb=10420.6, bsz=376, num_updates=1300, lr=0.000162568, gnorm=1.417, train_wall=63, wall=867
2020-12-08 16:12:24 | INFO | train_inner | epoch 003:    278 / 561 symm_mse=8.541, loss=3.997, nll_loss=1.318, ppl=2.49, wps=16817.1, ups=1.6, wpb=10478.6, bsz=371.6, num_updates=1400, lr=0.000175065, gnorm=1.422, train_wall=62, wall=929
2020-12-08 16:13:27 | INFO | train_inner | epoch 003:    378 / 561 symm_mse=8.447, loss=4.036, nll_loss=1.378, ppl=2.6, wps=16839.9, ups=1.61, wpb=10472.3, bsz=374.7, num_updates=1500, lr=0.000187563, gnorm=1.396, train_wall=62, wall=991
2020-12-08 16:14:30 | INFO | train_inner | epoch 003:    478 / 561 symm_mse=8.227, loss=4, nll_loss=1.365, ppl=2.58, wps=16898.6, ups=1.59, wpb=10650.7, bsz=373.4, num_updates=1600, lr=0.00020006, gnorm=1.351, train_wall=63, wall=1054
2020-12-08 16:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:15:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:15:42 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_mse 0 | loss 5.358 | nll_loss 3.884 | ppl 14.76 | bleu 21.91 | wps 4535 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.68
2020-12-08 16:15:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 3 @ 1683 updates, score 21.91) (writing took 3.0170682705938816 seconds)
2020-12-08 16:15:45 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-08 16:15:45 | INFO | train | epoch 003 | symm_mse 8.519 | loss 4.016 | nll_loss 1.344 | ppl 2.54 | wps 15608.9 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 0.000210433 | gnorm 1.404 | train_wall 348 | wall 1130
2020-12-08 16:15:45 | INFO | fairseq.trainer | begin training epoch 4
2020-12-08 16:15:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:15:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:15:58 | INFO | train_inner | epoch 004:     17 / 561 symm_mse=8.372, loss=4.047, nll_loss=1.403, ppl=2.64, wps=11767.6, ups=1.13, wpb=10447.8, bsz=352, num_updates=1700, lr=0.000212558, gnorm=1.409, train_wall=61, wall=1143
2020-12-08 16:17:00 | INFO | train_inner | epoch 004:    117 / 561 symm_mse=8.099, loss=4.012, nll_loss=1.394, ppl=2.63, wps=16895.2, ups=1.61, wpb=10469.1, bsz=365.6, num_updates=1800, lr=0.000225055, gnorm=1.342, train_wall=62, wall=1205
2020-12-08 16:18:02 | INFO | train_inner | epoch 004:    217 / 561 symm_mse=7.955, loss=4.021, nll_loss=1.423, ppl=2.68, wps=16604.4, ups=1.62, wpb=10271.1, bsz=367.4, num_updates=1900, lr=0.000237553, gnorm=1.348, train_wall=62, wall=1267
2020-12-08 16:19:05 | INFO | train_inner | epoch 004:    317 / 561 symm_mse=7.877, loss=4.021, nll_loss=1.435, ppl=2.7, wps=16860.7, ups=1.59, wpb=10571.4, bsz=356.9, num_updates=2000, lr=0.00025005, gnorm=1.324, train_wall=63, wall=1330
2020-12-08 16:20:07 | INFO | train_inner | epoch 004:    417 / 561 symm_mse=7.642, loss=4.014, nll_loss=1.46, ppl=2.75, wps=16957.6, ups=1.61, wpb=10532.7, bsz=370.6, num_updates=2100, lr=0.000262548, gnorm=1.289, train_wall=62, wall=1392
2020-12-08 16:21:09 | INFO | train_inner | epoch 004:    517 / 561 symm_mse=7.366, loss=3.991, nll_loss=1.471, ppl=2.77, wps=17098, ups=1.61, wpb=10614.4, bsz=387.6, num_updates=2200, lr=0.000275045, gnorm=1.268, train_wall=62, wall=1454
2020-12-08 16:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:21:58 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_mse 0 | loss 5.309 | nll_loss 3.847 | ppl 14.39 | bleu 21.45 | wps 4354.9 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.68
2020-12-08 16:21:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:21:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:21:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:22:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:22:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:22:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 4 @ 2244 updates, score 21.45) (writing took 2.9074574895203114 seconds)
2020-12-08 16:22:01 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-08 16:22:01 | INFO | train | epoch 004 | symm_mse 7.739 | loss 4.005 | nll_loss 1.435 | ppl 2.7 | wps 15626.4 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 0.000280544 | gnorm 1.309 | train_wall 348 | wall 1506
2020-12-08 16:22:01 | INFO | fairseq.trainer | begin training epoch 5
2020-12-08 16:22:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:22:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:22:39 | INFO | train_inner | epoch 005:     56 / 561 symm_mse=7.28, loss=3.964, nll_loss=1.449, ppl=2.73, wps=11649.2, ups=1.12, wpb=10433.5, bsz=373.3, num_updates=2300, lr=0.000287543, gnorm=1.267, train_wall=62, wall=1543
2020-12-08 16:23:41 | INFO | train_inner | epoch 005:    156 / 561 symm_mse=7.318, loss=3.994, nll_loss=1.477, ppl=2.78, wps=16784.5, ups=1.61, wpb=10447.6, bsz=372.4, num_updates=2400, lr=0.00030004, gnorm=1.291, train_wall=62, wall=1606
2020-12-08 16:24:44 | INFO | train_inner | epoch 005:    256 / 561 symm_mse=7.301, loss=4.016, nll_loss=1.506, ppl=2.84, wps=16794.6, ups=1.6, wpb=10524.3, bsz=363.8, num_updates=2500, lr=0.000312538, gnorm=1.247, train_wall=62, wall=1668
2020-12-08 16:25:46 | INFO | train_inner | epoch 005:    356 / 561 symm_mse=7.518, loss=4.105, nll_loss=1.585, ppl=3, wps=16782, ups=1.61, wpb=10415.7, bsz=357.5, num_updates=2600, lr=0.000325035, gnorm=1.336, train_wall=62, wall=1730
2020-12-08 16:26:48 | INFO | train_inner | epoch 005:    456 / 561 symm_mse=6.953, loss=4.011, nll_loss=1.547, ppl=2.92, wps=16868.7, ups=1.6, wpb=10565, bsz=383, num_updates=2700, lr=0.000337533, gnorm=1.219, train_wall=62, wall=1793
2020-12-08 16:27:51 | INFO | train_inner | epoch 005:    556 / 561 symm_mse=7.082, loss=4.061, nll_loss=1.591, ppl=3.01, wps=16807.6, ups=1.6, wpb=10479.6, bsz=374.7, num_updates=2800, lr=0.00035003, gnorm=1.229, train_wall=62, wall=1855
2020-12-08 16:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:28:18 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_mse 0 | loss 5.299 | nll_loss 3.839 | ppl 14.31 | bleu 20.94 | wps 3840 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.68
2020-12-08 16:28:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:28:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:28:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:28:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 5 @ 2805 updates, score 20.94) (writing took 3.210761057212949 seconds)
2020-12-08 16:28:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-08 16:28:21 | INFO | train | epoch 005 | symm_mse 7.259 | loss 4.032 | nll_loss 1.532 | ppl 2.89 | wps 15490.6 | ups 1.48 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 0.000350655 | gnorm 1.267 | train_wall 348 | wall 1886
2020-12-08 16:28:21 | INFO | fairseq.trainer | begin training epoch 6
2020-12-08 16:28:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:28:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:29:23 | INFO | train_inner | epoch 006:     95 / 561 symm_mse=6.914, loss=4.005, nll_loss=1.543, ppl=2.91, wps=11200.6, ups=1.09, wpb=10318.1, bsz=377.4, num_updates=2900, lr=0.000362528, gnorm=1.216, train_wall=61, wall=1948
2020-12-08 16:30:25 | INFO | train_inner | epoch 006:    195 / 561 symm_mse=6.928, loss=4.041, nll_loss=1.584, ppl=3, wps=17028.1, ups=1.59, wpb=10679.3, bsz=372.1, num_updates=3000, lr=0.000375025, gnorm=1.201, train_wall=63, wall=2010
2020-12-08 16:31:28 | INFO | train_inner | epoch 006:    295 / 561 symm_mse=6.937, loss=4.063, nll_loss=1.61, ppl=3.05, wps=16679.3, ups=1.59, wpb=10477.8, bsz=365.4, num_updates=3100, lr=0.000387523, gnorm=1.22, train_wall=63, wall=2073
2020-12-08 16:32:30 | INFO | train_inner | epoch 006:    395 / 561 symm_mse=6.964, loss=4.123, nll_loss=1.675, ppl=3.19, wps=16939.3, ups=1.61, wpb=10517.4, bsz=358, num_updates=3200, lr=0.00040002, gnorm=1.217, train_wall=62, wall=2135
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 66 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
nohup: ignoring input
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.0005
warmup_updates=4000
max_epoch=200
r3f_lambda=0.08
layer_choice=normal
save_dir=./examples/entr/bash/../checkpoints/closer-all
2020-12-08 16:33:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:47 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14610
2020-12-08 16:33:47 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:14610
2020-12-08 16:33:47 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-08 16:33:48 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14610
2020-12-08 16:33:48 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-08 16:33:48 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-08 16:33:51 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14610', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.08, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-08 16:33:51 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-08 16:33:51 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-08 16:33:51 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-08 16:33:51 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-08 16:33:51 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-08 16:33:52 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-08 16:33:52 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-08 16:33:52 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-08 16:33:52 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2020-12-08 16:33:52 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-08 16:33:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-08 16:33:52 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-08 16:33:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 16:33:52 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:33:52 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:33:52 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:33:52 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 16:33:52 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-08 16:33:52 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-08 16:33:53 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-08 16:33:54 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 6 @ 2805 updates)
2020-12-08 16:33:54 | INFO | fairseq.trainer | loading train data for epoch 6
2020-12-08 16:33:54 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-08 16:33:54 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-08 16:33:54 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-08 16:33:54 | INFO | fairseq.trainer | begin training epoch 6
2020-12-08 16:33:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:33:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:33:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
2020-12-08 16:34:57 | INFO | train_inner | epoch 006:     95 / 561 symm_mse=6.911, loss=4.004, nll_loss=1.542, ppl=2.91, wps=11453.2, ups=1.11, wpb=10318.1, bsz=377.4, num_updates=2900, lr=0.000362528, gnorm=1.209, train_wall=62, wall=0
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
normal
2020-12-08 16:35:59 | INFO | train_inner | epoch 006:    195 / 561 symm_mse=6.927, loss=4.04, nll_loss=1.583, ppl=3, wps=17094.9, ups=1.6, wpb=10679.3, bsz=372.1, num_updates=3000, lr=0.000375025, gnorm=1.198, train_wall=62, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 36 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
nohup: ignoring input
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.0005
warmup_updates=4000
max_epoch=200
r3f_lambda=0.08
layer_choice=normal
save_dir=./examples/entr/bash/../checkpoints/closer-all
2020-12-08 16:37:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:20 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17597
2020-12-08 16:37:20 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17597
2020-12-08 16:37:20 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17597
2020-12-08 16:37:20 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-08 16:37:21 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-08 16:37:21 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-08 16:37:25 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:17597', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.08, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-08 16:37:25 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-08 16:37:25 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-08 16:37:25 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-08 16:37:25 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-08 16:37:25 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-08 16:37:26 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-08 16:37:26 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-08 16:37:26 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-08 16:37:26 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2020-12-08 16:37:26 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-08 16:37:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-08 16:37:26 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-08 16:37:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 16:37:26 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:37:26 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:37:26 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 16:37:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-08 16:37:26 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-08 16:37:26 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-08 16:37:27 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-08 16:37:27 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 6 @ 2805 updates)
2020-12-08 16:37:27 | INFO | fairseq.trainer | loading train data for epoch 6
2020-12-08 16:37:27 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-08 16:37:27 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-08 16:37:27 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-08 16:37:28 | INFO | fairseq.trainer | begin training epoch 6
2020-12-08 16:37:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:37:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:38:30 | INFO | train_inner | epoch 006:     95 / 561 symm_mse=6.911, loss=4.004, nll_loss=1.542, ppl=2.91, wps=11481.9, ups=1.11, wpb=10318.1, bsz=377.4, num_updates=2900, lr=0.000362528, gnorm=1.209, train_wall=62, wall=0
2020-12-08 16:39:33 | INFO | train_inner | epoch 006:    195 / 561 symm_mse=6.927, loss=4.04, nll_loss=1.583, ppl=3, wps=16965.2, ups=1.59, wpb=10679.3, bsz=372.1, num_updates=3000, lr=0.000375025, gnorm=1.198, train_wall=63, wall=0
2020-12-08 16:40:35 | INFO | train_inner | epoch 006:    295 / 561 symm_mse=6.939, loss=4.066, nll_loss=1.612, ppl=3.06, wps=16804, ups=1.6, wpb=10477.8, bsz=365.4, num_updates=3100, lr=0.000387523, gnorm=1.22, train_wall=62, wall=0
2020-12-08 16:41:38 | INFO | train_inner | epoch 006:    395 / 561 symm_mse=6.97, loss=4.124, nll_loss=1.676, ppl=3.19, wps=16892.8, ups=1.61, wpb=10517.4, bsz=358, num_updates=3200, lr=0.00040002, gnorm=1.24, train_wall=62, wall=0
2020-12-08 16:42:40 | INFO | train_inner | epoch 006:    495 / 561 symm_mse=6.721, loss=4.07, nll_loss=1.648, ppl=3.13, wps=16802.8, ups=1.59, wpb=10534.9, bsz=372, num_updates=3300, lr=0.000412518, gnorm=1.21, train_wall=62, wall=0
2020-12-08 16:43:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:43:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:43:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:43:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:43:44 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_mse 0 | loss 5.304 | nll_loss 3.852 | ppl 14.44 | bleu 21.54 | wps 4370.4 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.68
2020-12-08 16:43:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:43:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 6 @ 3366 updates, score 21.54) (writing took 2.924091190099716 seconds)
2020-12-08 16:43:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-08 16:43:47 | INFO | train | epoch 006 | symm_mse 7.058 | loss 4.05 | nll_loss 1.578 | ppl 2.99 | wps 15560.9 | ups 1.48 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 0.000420766 | gnorm 1.24 | train_wall 698 | wall 0
2020-12-08 16:43:47 | INFO | fairseq.trainer | begin training epoch 7
2020-12-08 16:43:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:43:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:44:11 | INFO | train_inner | epoch 007:     34 / 561 symm_mse=6.473, loss=4.075, nll_loss=1.684, ppl=3.21, wps=11307.3, ups=1.1, wpb=10237, bsz=369, num_updates=3400, lr=0.000425015, gnorm=1.153, train_wall=62, wall=0
2020-12-08 16:45:14 | INFO | train_inner | epoch 007:    134 / 561 symm_mse=6.743, loss=4.089, nll_loss=1.664, ppl=3.17, wps=16798, ups=1.6, wpb=10508.4, bsz=371.6, num_updates=3500, lr=0.000437513, gnorm=1.214, train_wall=62, wall=0
2020-12-08 16:46:16 | INFO | train_inner | epoch 007:    234 / 561 symm_mse=6.605, loss=4.094, nll_loss=1.687, ppl=3.22, wps=16667.5, ups=1.6, wpb=10404.4, bsz=363.4, num_updates=3600, lr=0.00045001, gnorm=1.193, train_wall=62, wall=0
2020-12-08 16:47:18 | INFO | train_inner | epoch 007:    334 / 561 symm_mse=6.486, loss=4.111, nll_loss=1.724, ppl=3.3, wps=16791.9, ups=1.61, wpb=10456.6, bsz=375.4, num_updates=3700, lr=0.000462508, gnorm=1.16, train_wall=62, wall=0
2020-12-08 16:48:21 | INFO | train_inner | epoch 007:    434 / 561 symm_mse=6.734, loss=4.177, nll_loss=1.77, ppl=3.41, wps=16760, ups=1.6, wpb=10467.8, bsz=366.5, num_updates=3800, lr=0.000475005, gnorm=1.243, train_wall=62, wall=0
2020-12-08 16:49:23 | INFO | train_inner | epoch 007:    534 / 561 symm_mse=6.487, loss=4.152, nll_loss=1.773, ppl=3.42, wps=17036.1, ups=1.59, wpb=10688.9, bsz=373.3, num_updates=3900, lr=0.000487503, gnorm=1.137, train_wall=63, wall=0
2020-12-08 16:49:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:49:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:49:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:49:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:50:02 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_mse 0 | loss 5.292 | nll_loss 3.83 | ppl 14.22 | bleu 21.89 | wps 4438.8 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.68
2020-12-08 16:50:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:50:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:50:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:50:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 7 @ 3927 updates, score 21.89) (writing took 2.7368366941809654 seconds)
2020-12-08 16:50:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-08 16:50:04 | INFO | train | epoch 007 | symm_mse 6.575 | loss 4.116 | nll_loss 1.719 | ppl 3.29 | wps 15579.4 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 0.000490877 | gnorm 1.181 | train_wall 349 | wall 0
2020-12-08 16:50:04 | INFO | fairseq.trainer | begin training epoch 8
2020-12-08 16:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:50:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:50:53 | INFO | train_inner | epoch 008:     73 / 561 symm_mse=6.293, loss=4.083, nll_loss=1.715, ppl=3.28, wps=11832.3, ups=1.12, wpb=10590, bsz=375, num_updates=4000, lr=0.0005, gnorm=1.114, train_wall=61, wall=0
2020-12-08 16:51:55 | INFO | train_inner | epoch 008:    173 / 561 symm_mse=6.724, loss=4.188, nll_loss=1.784, ppl=3.44, wps=16807, ups=1.6, wpb=10504.7, bsz=358.9, num_updates=4100, lr=0.000493865, gnorm=1.21, train_wall=62, wall=0
2020-12-08 16:52:58 | INFO | train_inner | epoch 008:    273 / 561 symm_mse=6.27, loss=4.129, nll_loss=1.771, ppl=3.41, wps=16687.4, ups=1.61, wpb=10367.5, bsz=366.4, num_updates=4200, lr=0.00048795, gnorm=1.156, train_wall=62, wall=0
2020-12-08 16:53:59 | INFO | train_inner | epoch 008:    373 / 561 symm_mse=6.047, loss=4.1, nll_loss=1.768, ppl=3.4, wps=16827.8, ups=1.62, wpb=10416.1, bsz=389.5, num_updates=4300, lr=0.000482243, gnorm=1.116, train_wall=62, wall=0
2020-12-08 16:55:02 | INFO | train_inner | epoch 008:    473 / 561 symm_mse=6.014, loss=4.106, nll_loss=1.781, ppl=3.44, wps=16993.7, ups=1.6, wpb=10648.7, bsz=379.6, num_updates=4400, lr=0.000476731, gnorm=1.094, train_wall=62, wall=0
2020-12-08 16:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 16:55:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:55:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:55:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:56:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 16:56:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 16:56:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 16:56:19 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_mse 0 | loss 5.287 | nll_loss 3.831 | ppl 14.23 | bleu 21.2 | wps 4258.4 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.68
2020-12-08 16:56:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 16:56:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:56:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:56:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 8 @ 4488 updates, score 21.2) (writing took 2.7460888884961605 seconds)
2020-12-08 16:56:22 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-08 16:56:22 | INFO | train | epoch 008 | symm_mse 6.292 | loss 4.139 | nll_loss 1.782 | ppl 3.44 | wps 15570.6 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 0.000472034 | gnorm 1.144 | train_wall 348 | wall 0
2020-12-08 16:56:22 | INFO | fairseq.trainer | begin training epoch 9
2020-12-08 16:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 16:56:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 16:56:33 | INFO | train_inner | epoch 009:     12 / 561 symm_mse=6.309, loss=4.204, nll_loss=1.855, ppl=3.62, wps=11401.5, ups=1.1, wpb=10320.4, bsz=352.8, num_updates=4500, lr=0.000471405, gnorm=1.149, train_wall=62, wall=0
2020-12-08 16:57:35 | INFO | train_inner | epoch 009:    112 / 561 symm_mse=6.059, loss=4.065, nll_loss=1.723, ppl=3.3, wps=16933.9, ups=1.61, wpb=10532.6, bsz=374.2, num_updates=4600, lr=0.000466252, gnorm=1.049, train_wall=62, wall=0
2020-12-08 16:58:37 | INFO | train_inner | epoch 009:    212 / 561 symm_mse=6.357, loss=4.166, nll_loss=1.801, ppl=3.49, wps=16851, ups=1.6, wpb=10528, bsz=345, num_updates=4700, lr=0.000461266, gnorm=1.127, train_wall=62, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 54 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
