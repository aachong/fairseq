nohup: ignoring input
criterion=r3f_closer_dropout_all
label_smoothing=0.1
dropout=0.3
lr=0.00001
lrscheduler=fixed
warmup_updates=0
max_epoch=200
r3f_lambda=0.08
layer_choice=normal
save_dir=./examples/entr/bash/../checkpoints/closer-all
2020-12-15 14:02:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:34 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15031
2020-12-15 14:02:35 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15031
2020-12-15 14:02:35 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15031
2020-12-15 14:02:35 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-15 14:02:35 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-15 14:02:36 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-15 14:02:39 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15031', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='normal', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[1e-05], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.08, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
2020-12-15 14:02:39 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-15 14:02:39 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-15 14:02:39 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-15 14:02:39 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-15 14:02:39 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-15 14:02:40 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-15 14:02:40 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-15 14:02:40 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-15 14:02:40 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2020-12-15 14:02:40 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-15 14:02:40 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-15 14:02:40 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-15 14:02:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-15 14:02:40 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-15 14:02:40 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-15 14:02:40 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-15 14:02:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-15 14:02:40 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-15 14:02:40 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-15 14:02:40 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-15 14:02:41 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-15 14:02:41 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-15 14:02:41 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-15 14:02:41 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-15 14:02:41 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-15 14:02:41 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-15 14:02:42 | INFO | fairseq.trainer | begin training epoch 1
2020-12-15 14:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:03:30 | INFO | train_inner | epoch 001:    100 / 561 symm_mse=19.35, loss=4.905, nll_loss=0.877, ppl=1.84, wps=24174.7, ups=2.28, wpb=10623.3, bsz=364.6, num_updates=100, lr=1e-05, gnorm=2.612, train_wall=44, wall=49
2020-12-15 14:04:13 | INFO | train_inner | epoch 001:    200 / 561 symm_mse=17.487, loss=4.705, nll_loss=0.895, ppl=1.86, wps=24181.9, ups=2.28, wpb=10583.4, bsz=369.8, num_updates=200, lr=1e-05, gnorm=2.402, train_wall=44, wall=93
2020-12-15 14:04:58 | INFO | train_inner | epoch 001:    300 / 561 symm_mse=15.973, loss=4.537, nll_loss=0.907, ppl=1.87, wps=23339.7, ups=2.26, wpb=10335, bsz=373, num_updates=300, lr=1e-05, gnorm=2.204, train_wall=44, wall=137
2020-12-15 14:05:43 | INFO | train_inner | epoch 001:    400 / 561 symm_mse=15.091, loss=4.432, nll_loss=0.907, ppl=1.88, wps=23581, ups=2.23, wpb=10571.8, bsz=388.4, num_updates=400, lr=1e-05, gnorm=2.045, train_wall=45, wall=182
2020-12-15 14:06:28 | INFO | train_inner | epoch 001:    500 / 561 symm_mse=15.054, loss=4.436, nll_loss=0.917, ppl=1.89, wps=23077.4, ups=2.22, wpb=10411.2, bsz=371.8, num_updates=500, lr=1e-05, gnorm=2.047, train_wall=45, wall=227
2020-12-15 14:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:07:17 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_mse 0 | loss 5.598 | nll_loss 4.05 | ppl 16.57 | bleu 22.31 | wps 4379.9 | wpb 7508.5 | bsz 272.7 | num_updates 561
2020-12-15 14:07:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:07:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:07:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:07:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.31) (writing took 2.2342119235545397 seconds)
2020-12-15 14:07:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-15 14:07:19 | INFO | train | epoch 001 | symm_mse 16.632 | loss 4.611 | nll_loss 0.905 | ppl 1.87 | wps 21493.7 | ups 2.05 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 1e-05 | gnorm 2.259 | train_wall 249 | wall 279
2020-12-15 14:07:19 | INFO | fairseq.trainer | begin training epoch 2
2020-12-15 14:07:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:07:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:07:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:07:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:07:40 | INFO | train_inner | epoch 002:     39 / 561 symm_mse=15.99, loss=4.549, nll_loss=0.924, ppl=1.9, wps=14301.7, ups=1.38, wpb=10345.6, bsz=358.4, num_updates=600, lr=1e-05, gnorm=2.146, train_wall=44, wall=300
2020-12-15 14:08:25 | INFO | train_inner | epoch 002:    139 / 561 symm_mse=15.653, loss=4.515, nll_loss=0.931, ppl=1.91, wps=23434.3, ups=2.22, wpb=10532.9, bsz=366.4, num_updates=700, lr=1e-05, gnorm=2.11, train_wall=45, wall=344
2020-12-15 14:09:10 | INFO | train_inner | epoch 002:    239 / 561 symm_mse=14.401, loss=4.361, nll_loss=0.919, ppl=1.89, wps=23223.5, ups=2.21, wpb=10499.4, bsz=369.4, num_updates=800, lr=1e-05, gnorm=1.918, train_wall=45, wall=390
2020-12-15 14:09:55 | INFO | train_inner | epoch 002:    339 / 561 symm_mse=14.577, loss=4.385, nll_loss=0.925, ppl=1.9, wps=23291.7, ups=2.21, wpb=10541.2, bsz=377.1, num_updates=900, lr=1e-05, gnorm=1.946, train_wall=45, wall=435
2020-12-15 14:10:41 | INFO | train_inner | epoch 002:    439 / 561 symm_mse=14.379, loss=4.372, nll_loss=0.934, ppl=1.91, wps=22990.8, ups=2.2, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1e-05, gnorm=1.921, train_wall=45, wall=481
2020-12-15 14:11:26 | INFO | train_inner | epoch 002:    539 / 561 symm_mse=14.536, loss=4.394, nll_loss=0.94, ppl=1.92, wps=23189.1, ups=2.22, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1e-05, gnorm=1.965, train_wall=45, wall=526
2020-12-15 14:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:11:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:11:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:11:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:11:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:11:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:11:57 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_mse 0 | loss 5.544 | nll_loss 4.004 | ppl 16.04 | bleu 22.49 | wps 4457.8 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.49
2020-12-15 14:11:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:11:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:11:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:12:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 2 @ 1122 updates, score 22.49) (writing took 5.609170524403453 seconds)
2020-12-15 14:12:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-15 14:12:03 | INFO | train | epoch 002 | symm_mse 14.733 | loss 4.407 | nll_loss 0.929 | ppl 1.9 | wps 20719.8 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1e-05 | gnorm 1.983 | train_wall 252 | wall 563
2020-12-15 14:12:03 | INFO | fairseq.trainer | begin training epoch 3
2020-12-15 14:12:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:12:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:12:41 | INFO | train_inner | epoch 003:     78 / 561 symm_mse=14.608, loss=4.394, nll_loss=0.935, ppl=1.91, wps=13990.6, ups=1.34, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1e-05, gnorm=1.98, train_wall=44, wall=600
2020-12-15 14:13:26 | INFO | train_inner | epoch 003:    178 / 561 symm_mse=13.913, loss=4.318, nll_loss=0.935, ppl=1.91, wps=23197.6, ups=2.23, wpb=10420.6, bsz=376, num_updates=1300, lr=1e-05, gnorm=1.877, train_wall=45, wall=645
2020-12-15 14:14:11 | INFO | train_inner | epoch 003:    278 / 561 symm_mse=13.92, loss=4.314, nll_loss=0.929, ppl=1.9, wps=23089.4, ups=2.2, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1e-05, gnorm=1.878, train_wall=45, wall=690
2020-12-15 14:14:56 | INFO | train_inner | epoch 003:    378 / 561 symm_mse=14.072, loss=4.349, nll_loss=0.953, ppl=1.94, wps=23138.2, ups=2.21, wpb=10472.3, bsz=374.7, num_updates=1500, lr=1e-05, gnorm=1.877, train_wall=45, wall=736
2020-12-15 14:15:42 | INFO | train_inner | epoch 003:    478 / 561 symm_mse=13.683, loss=4.291, nll_loss=0.937, ppl=1.91, wps=23293.1, ups=2.19, wpb=10650.7, bsz=373.4, num_updates=1600, lr=1e-05, gnorm=1.818, train_wall=46, wall=781
2020-12-15 14:16:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:16:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:16:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:16:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:16:42 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_mse 0 | loss 5.515 | nll_loss 3.978 | ppl 15.75 | bleu 22.58 | wps 4309.2 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.58
2020-12-15 14:16:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:16:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.58) (writing took 5.142370667308569 seconds)
2020-12-15 14:16:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-15 14:16:47 | INFO | train | epoch 003 | symm_mse 14.033 | loss 4.332 | nll_loss 0.938 | ppl 1.92 | wps 20726.4 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 1e-05 | gnorm 1.883 | train_wall 252 | wall 846
2020-12-15 14:16:47 | INFO | fairseq.trainer | begin training epoch 4
2020-12-15 14:16:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:16:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:16:58 | INFO | train_inner | epoch 004:     17 / 561 symm_mse=14.367, loss=4.369, nll_loss=0.939, ppl=1.92, wps=13794.9, ups=1.32, wpb=10447.8, bsz=352, num_updates=1700, lr=1e-05, gnorm=1.944, train_wall=45, wall=857
2020-12-15 14:17:43 | INFO | train_inner | epoch 004:    117 / 561 symm_mse=14.093, loss=4.347, nll_loss=0.949, ppl=1.93, wps=23052.9, ups=2.2, wpb=10469.1, bsz=365.6, num_updates=1800, lr=1e-05, gnorm=1.877, train_wall=45, wall=903
2020-12-15 14:18:29 | INFO | train_inner | epoch 004:    217 / 561 symm_mse=13.889, loss=4.327, nll_loss=0.95, ppl=1.93, wps=22263.3, ups=2.17, wpb=10271.1, bsz=367.4, num_updates=1900, lr=1e-05, gnorm=1.88, train_wall=46, wall=949
2020-12-15 14:19:16 | INFO | train_inner | epoch 004:    317 / 561 symm_mse=13.837, loss=4.31, nll_loss=0.939, ppl=1.92, wps=22700.6, ups=2.15, wpb=10571.4, bsz=356.9, num_updates=2000, lr=1e-05, gnorm=1.858, train_wall=46, wall=995
2020-12-15 14:20:02 | INFO | train_inner | epoch 004:    417 / 561 symm_mse=13.459, loss=4.268, nll_loss=0.94, ppl=1.92, wps=22673, ups=2.15, wpb=10532.7, bsz=370.6, num_updates=2100, lr=1e-05, gnorm=1.814, train_wall=46, wall=1042
2020-12-15 14:20:49 | INFO | train_inner | epoch 004:    517 / 561 symm_mse=12.985, loss=4.213, nll_loss=0.94, ppl=1.92, wps=22805.4, ups=2.15, wpb=10614.4, bsz=387.6, num_updates=2200, lr=1e-05, gnorm=1.764, train_wall=46, wall=1088
2020-12-15 14:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:21:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:21:37 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_mse 0 | loss 5.501 | nll_loss 3.961 | ppl 15.58 | bleu 22.48 | wps 3462.6 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.58
2020-12-15 14:21:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 4 @ 2244 updates, score 22.48) (writing took 3.414520274847746 seconds)
2020-12-15 14:21:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-15 14:21:40 | INFO | train | epoch 004 | symm_mse 13.564 | loss 4.281 | nll_loss 0.942 | ppl 1.92 | wps 20066.4 | ups 1.91 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 1e-05 | gnorm 1.829 | train_wall 257 | wall 1140
2020-12-15 14:21:40 | INFO | fairseq.trainer | begin training epoch 5
2020-12-15 14:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:21:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:22:10 | INFO | train_inner | epoch 005:     56 / 561 symm_mse=12.988, loss=4.205, nll_loss=0.932, ppl=1.91, wps=12918.1, ups=1.24, wpb=10433.5, bsz=373.3, num_updates=2300, lr=1e-05, gnorm=1.746, train_wall=45, wall=1169
2020-12-15 14:22:56 | INFO | train_inner | epoch 005:    156 / 561 symm_mse=13.202, loss=4.24, nll_loss=0.939, ppl=1.92, wps=22625.2, ups=2.17, wpb=10447.6, bsz=372.4, num_updates=2400, lr=1e-05, gnorm=1.787, train_wall=46, wall=1215
2020-12-15 14:23:42 | INFO | train_inner | epoch 005:    256 / 561 symm_mse=13.191, loss=4.24, nll_loss=0.943, ppl=1.92, wps=22745, ups=2.16, wpb=10524.3, bsz=363.8, num_updates=2500, lr=1e-05, gnorm=1.764, train_wall=46, wall=1262
2020-12-15 14:24:28 | INFO | train_inner | epoch 005:    356 / 561 symm_mse=13.89, loss=4.331, nll_loss=0.958, ppl=1.94, wps=22603.4, ups=2.17, wpb=10415.7, bsz=357.5, num_updates=2600, lr=1e-05, gnorm=1.858, train_wall=46, wall=1308
2020-12-15 14:25:14 | INFO | train_inner | epoch 005:    456 / 561 symm_mse=12.733, loss=4.178, nll_loss=0.932, ppl=1.91, wps=22795.9, ups=2.16, wpb=10565, bsz=383, num_updates=2700, lr=1e-05, gnorm=1.717, train_wall=46, wall=1354
2020-12-15 14:26:01 | INFO | train_inner | epoch 005:    556 / 561 symm_mse=13.104, loss=4.223, nll_loss=0.938, ppl=1.92, wps=22529.9, ups=2.15, wpb=10479.6, bsz=374.7, num_updates=2800, lr=1e-05, gnorm=1.768, train_wall=46, wall=1401
2020-12-15 14:26:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:26:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:26:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:26:31 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_mse 0 | loss 5.491 | nll_loss 3.949 | ppl 15.45 | bleu 22.67 | wps 3489.3 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.67
2020-12-15 14:26:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:26:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:26:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 5 @ 2805 updates, score 22.67) (writing took 5.436865787953138 seconds)
2020-12-15 14:26:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-15 14:26:36 | INFO | train | epoch 005 | symm_mse 13.251 | loss 4.245 | nll_loss 0.941 | ppl 1.92 | wps 19848.4 | ups 1.89 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 1e-05 | gnorm 1.782 | train_wall 257 | wall 1436
2020-12-15 14:26:36 | INFO | fairseq.trainer | begin training epoch 6
2020-12-15 14:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:27:24 | INFO | train_inner | epoch 006:     95 / 561 symm_mse=13.005, loss=4.213, nll_loss=0.938, ppl=1.92, wps=12380, ups=1.2, wpb=10318.1, bsz=377.4, num_updates=2900, lr=1e-05, gnorm=1.814, train_wall=46, wall=1484
2020-12-15 14:28:11 | INFO | train_inner | epoch 006:    195 / 561 symm_mse=13.105, loss=4.229, nll_loss=0.944, ppl=1.92, wps=23007.5, ups=2.15, wpb=10679.3, bsz=372.1, num_updates=3000, lr=1e-05, gnorm=1.74, train_wall=46, wall=1530
2020-12-15 14:28:57 | INFO | train_inner | epoch 006:    295 / 561 symm_mse=13.057, loss=4.22, nll_loss=0.938, ppl=1.92, wps=22533.9, ups=2.15, wpb=10477.8, bsz=365.4, num_updates=3100, lr=1e-05, gnorm=1.762, train_wall=46, wall=1577
2020-12-15 14:29:44 | INFO | train_inner | epoch 006:    395 / 561 symm_mse=13.251, loss=4.258, nll_loss=0.956, ppl=1.94, wps=22631.1, ups=2.15, wpb=10517.4, bsz=358, num_updates=3200, lr=1e-05, gnorm=1.767, train_wall=46, wall=1623
2020-12-15 14:30:30 | INFO | train_inner | epoch 006:    495 / 561 symm_mse=12.757, loss=4.174, nll_loss=0.928, ppl=1.9, wps=22571.8, ups=2.14, wpb=10534.9, bsz=372, num_updates=3300, lr=1e-05, gnorm=1.726, train_wall=46, wall=1670
2020-12-15 14:31:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:31:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:31:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:31:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:31:29 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_mse 0 | loss 5.477 | nll_loss 3.935 | ppl 15.29 | bleu 22.56 | wps 3359.6 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.67
2020-12-15 14:31:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 6 @ 3366 updates, score 22.56) (writing took 2.917700905352831 seconds)
2020-12-15 14:31:32 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-15 14:31:32 | INFO | train | epoch 006 | symm_mse 12.984 | loss 4.215 | nll_loss 0.943 | ppl 1.92 | wps 19897.9 | ups 1.9 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 1e-05 | gnorm 1.754 | train_wall 258 | wall 1731
2020-12-15 14:31:32 | INFO | fairseq.trainer | begin training epoch 7
2020-12-15 14:31:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:31:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:31:51 | INFO | train_inner | epoch 007:     34 / 561 symm_mse=12.41, loss=4.161, nll_loss=0.955, ppl=1.94, wps=12641, ups=1.23, wpb=10237, bsz=369, num_updates=3400, lr=1e-05, gnorm=1.682, train_wall=46, wall=1751
2020-12-15 14:32:38 | INFO | train_inner | epoch 007:    134 / 561 symm_mse=13.068, loss=4.225, nll_loss=0.942, ppl=1.92, wps=22590, ups=2.15, wpb=10508.4, bsz=371.6, num_updates=3500, lr=1e-05, gnorm=1.769, train_wall=46, wall=1797
2020-12-15 14:33:24 | INFO | train_inner | epoch 007:    234 / 561 symm_mse=12.781, loss=4.194, nll_loss=0.944, ppl=1.92, wps=22373.3, ups=2.15, wpb=10404.4, bsz=363.4, num_updates=3600, lr=1e-05, gnorm=1.726, train_wall=46, wall=1844
2020-12-15 14:34:10 | INFO | train_inner | epoch 007:    334 / 561 symm_mse=12.515, loss=4.164, nll_loss=0.945, ppl=1.93, wps=22833.6, ups=2.18, wpb=10456.6, bsz=375.4, num_updates=3700, lr=1e-05, gnorm=1.697, train_wall=45, wall=1890
2020-12-15 14:34:57 | INFO | train_inner | epoch 007:    434 / 561 symm_mse=13.114, loss=4.229, nll_loss=0.942, ppl=1.92, wps=22372.5, ups=2.14, wpb=10467.8, bsz=366.5, num_updates=3800, lr=1e-05, gnorm=1.803, train_wall=47, wall=1937
2020-12-15 14:35:43 | INFO | train_inner | epoch 007:    534 / 561 symm_mse=12.676, loss=4.178, nll_loss=0.941, ppl=1.92, wps=23280, ups=2.18, wpb=10688.9, bsz=373.3, num_updates=3900, lr=1e-05, gnorm=1.691, train_wall=46, wall=1982
2020-12-15 14:35:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:35:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:35:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:35:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:35:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:35:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:35:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:36:23 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_mse 0 | loss 5.469 | nll_loss 3.931 | ppl 15.25 | bleu 22.44 | wps 3525 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.67
2020-12-15 14:36:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:36:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 7 @ 3927 updates, score 22.44) (writing took 3.800820466130972 seconds)
2020-12-15 14:36:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-15 14:36:27 | INFO | train | epoch 007 | symm_mse 12.758 | loss 4.189 | nll_loss 0.943 | ppl 1.92 | wps 19947.7 | ups 1.9 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 1e-05 | gnorm 1.727 | train_wall 258 | wall 2026
2020-12-15 14:36:27 | INFO | fairseq.trainer | begin training epoch 8
2020-12-15 14:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:36:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:37:05 | INFO | train_inner | epoch 008:     73 / 561 symm_mse=12.429, loss=4.147, nll_loss=0.94, ppl=1.92, wps=12857.1, ups=1.21, wpb=10590, bsz=375, num_updates=4000, lr=1e-05, gnorm=1.67, train_wall=46, wall=2065
2020-12-15 14:37:52 | INFO | train_inner | epoch 008:    173 / 561 symm_mse=13.343, loss=4.259, nll_loss=0.947, ppl=1.93, wps=22594, ups=2.15, wpb=10504.7, bsz=358.9, num_updates=4100, lr=1e-05, gnorm=1.8, train_wall=46, wall=2111
2020-12-15 14:38:38 | INFO | train_inner | epoch 008:    273 / 561 symm_mse=12.375, loss=4.144, nll_loss=0.94, ppl=1.92, wps=22510, ups=2.17, wpb=10367.5, bsz=366.4, num_updates=4200, lr=1e-05, gnorm=1.687, train_wall=46, wall=2157
2020-12-15 14:39:24 | INFO | train_inner | epoch 008:    373 / 561 symm_mse=11.971, loss=4.089, nll_loss=0.932, ppl=1.91, wps=22302, ups=2.14, wpb=10416.1, bsz=389.5, num_updates=4300, lr=1e-05, gnorm=1.656, train_wall=46, wall=2204
2020-12-15 14:40:11 | INFO | train_inner | epoch 008:    473 / 561 symm_mse=12.076, loss=4.103, nll_loss=0.934, ppl=1.91, wps=22937.4, ups=2.15, wpb=10648.7, bsz=379.6, num_updates=4400, lr=1e-05, gnorm=1.615, train_wall=46, wall=2250
2020-12-15 14:40:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:40:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:40:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:40:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:41:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:41:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:41:19 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_mse 0 | loss 5.461 | nll_loss 3.921 | ppl 15.15 | bleu 22.73 | wps 3529.7 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.73
2020-12-15 14:41:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:41:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:41:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:41:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:41:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:41:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 8 @ 4488 updates, score 22.73) (writing took 6.037430051714182 seconds)
2020-12-15 14:41:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-15 14:41:25 | INFO | train | epoch 008 | symm_mse 12.564 | loss 4.167 | nll_loss 0.943 | ppl 1.92 | wps 19721.5 | ups 1.88 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 1e-05 | gnorm 1.703 | train_wall 258 | wall 2324
2020-12-15 14:41:25 | INFO | fairseq.trainer | begin training epoch 9
2020-12-15 14:41:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:41:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:41:35 | INFO | train_inner | epoch 009:     12 / 561 symm_mse=12.924, loss=4.221, nll_loss=0.959, ppl=1.94, wps=12206.9, ups=1.18, wpb=10320.4, bsz=352.8, num_updates=4500, lr=1e-05, gnorm=1.759, train_wall=46, wall=2335
2020-12-15 14:42:21 | INFO | train_inner | epoch 009:    112 / 561 symm_mse=12.565, loss=4.159, nll_loss=0.934, ppl=1.91, wps=22929.2, ups=2.18, wpb=10532.6, bsz=374.2, num_updates=4600, lr=1e-05, gnorm=1.692, train_wall=46, wall=2381
2020-12-15 14:43:08 | INFO | train_inner | epoch 009:    212 / 561 symm_mse=13.307, loss=4.268, nll_loss=0.961, ppl=1.95, wps=22750.1, ups=2.16, wpb=10528, bsz=345, num_updates=4700, lr=1e-05, gnorm=1.766, train_wall=46, wall=2427
2020-12-15 14:43:54 | INFO | train_inner | epoch 009:    312 / 561 symm_mse=11.923, loss=4.088, nll_loss=0.937, ppl=1.91, wps=22593.5, ups=2.16, wpb=10473.1, bsz=377.1, num_updates=4800, lr=1e-05, gnorm=1.625, train_wall=46, wall=2474
2020-12-15 14:44:41 | INFO | train_inner | epoch 009:    412 / 561 symm_mse=12.253, loss=4.127, nll_loss=0.938, ppl=1.92, wps=22500.8, ups=2.15, wpb=10483.1, bsz=368.3, num_updates=4900, lr=1e-05, gnorm=1.688, train_wall=46, wall=2520
2020-12-15 14:45:26 | INFO | train_inner | epoch 009:    512 / 561 symm_mse=11.916, loss=4.085, nll_loss=0.935, ppl=1.91, wps=23130.1, ups=2.2, wpb=10514.7, bsz=388.6, num_updates=5000, lr=1e-05, gnorm=1.642, train_wall=45, wall=2566
2020-12-15 14:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:46:09 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_mse 0 | loss 5.456 | nll_loss 3.915 | ppl 15.09 | bleu 22.57 | wps 4574.1 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.73
2020-12-15 14:46:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:46:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 9 @ 5049 updates, score 22.57) (writing took 2.922142408788204 seconds)
2020-12-15 14:46:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-15 14:46:12 | INFO | train | epoch 009 | symm_mse 12.401 | loss 4.148 | nll_loss 0.943 | ppl 1.92 | wps 20493.7 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 1e-05 | gnorm 1.684 | train_wall 256 | wall 2611
2020-12-15 14:46:12 | INFO | fairseq.trainer | begin training epoch 10
2020-12-15 14:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:46:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:46:38 | INFO | train_inner | epoch 010:     51 / 561 symm_mse=12.468, loss=4.166, nll_loss=0.953, ppl=1.94, wps=14448.7, ups=1.4, wpb=10353, bsz=356, num_updates=5100, lr=1e-05, gnorm=1.691, train_wall=44, wall=2637
2020-12-15 14:47:22 | INFO | train_inner | epoch 010:    151 / 561 symm_mse=12.281, loss=4.137, nll_loss=0.946, ppl=1.93, wps=23645.1, ups=2.24, wpb=10577.3, bsz=365.6, num_updates=5200, lr=1e-05, gnorm=1.65, train_wall=45, wall=2682
2020-12-15 14:48:07 | INFO | train_inner | epoch 010:    251 / 561 symm_mse=12.504, loss=4.163, nll_loss=0.945, ppl=1.93, wps=23417, ups=2.23, wpb=10501.8, bsz=363.6, num_updates=5300, lr=1e-05, gnorm=1.703, train_wall=45, wall=2727
2020-12-15 14:48:52 | INFO | train_inner | epoch 010:    351 / 561 symm_mse=12.332, loss=4.142, nll_loss=0.946, ppl=1.93, wps=23185.7, ups=2.22, wpb=10450.1, bsz=375.6, num_updates=5400, lr=1e-05, gnorm=1.685, train_wall=45, wall=2772
2020-12-15 14:49:37 | INFO | train_inner | epoch 010:    451 / 561 symm_mse=11.987, loss=4.09, nll_loss=0.933, ppl=1.91, wps=23329.9, ups=2.23, wpb=10472.1, bsz=373.6, num_updates=5500, lr=1e-05, gnorm=1.656, train_wall=45, wall=2817
2020-12-15 14:50:23 | INFO | train_inner | epoch 010:    551 / 561 symm_mse=11.968, loss=4.094, nll_loss=0.94, ppl=1.92, wps=23234.2, ups=2.21, wpb=10516.7, bsz=381.2, num_updates=5600, lr=1e-05, gnorm=1.638, train_wall=45, wall=2862
2020-12-15 14:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:50:48 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_mse 0 | loss 5.451 | nll_loss 3.911 | ppl 15.04 | bleu 22.7 | wps 4619.1 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.73
2020-12-15 14:50:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:50:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:50:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.7) (writing took 2.872894899919629 seconds)
2020-12-15 14:50:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-12-15 14:50:51 | INFO | train | epoch 010 | symm_mse 12.25 | loss 4.13 | nll_loss 0.942 | ppl 1.92 | wps 21087.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 1e-05 | gnorm 1.673 | train_wall 251 | wall 2890
2020-12-15 14:50:51 | INFO | fairseq.trainer | begin training epoch 11
2020-12-15 14:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:51:34 | INFO | train_inner | epoch 011:     90 / 561 symm_mse=12.64, loss=4.18, nll_loss=0.95, ppl=1.93, wps=14550.2, ups=1.4, wpb=10358.6, bsz=351.1, num_updates=5700, lr=1e-05, gnorm=1.711, train_wall=44, wall=2933
2020-12-15 14:52:19 | INFO | train_inner | epoch 011:    190 / 561 symm_mse=11.695, loss=4.056, nll_loss=0.932, ppl=1.91, wps=23393.5, ups=2.21, wpb=10564, bsz=383.6, num_updates=5800, lr=1e-05, gnorm=1.609, train_wall=45, wall=2978
2020-12-15 14:53:04 | INFO | train_inner | epoch 011:    290 / 561 symm_mse=12.514, loss=4.171, nll_loss=0.956, ppl=1.94, wps=23023.7, ups=2.21, wpb=10400.1, bsz=355.4, num_updates=5900, lr=1e-05, gnorm=1.695, train_wall=45, wall=3024
2020-12-15 14:53:50 | INFO | train_inner | epoch 011:    390 / 561 symm_mse=12.317, loss=4.15, nll_loss=0.957, ppl=1.94, wps=22858.3, ups=2.2, wpb=10394.2, bsz=372.6, num_updates=6000, lr=1e-05, gnorm=1.683, train_wall=45, wall=3069
2020-12-15 14:54:35 | INFO | train_inner | epoch 011:    490 / 561 symm_mse=11.741, loss=4.062, nll_loss=0.93, ppl=1.91, wps=23437.8, ups=2.2, wpb=10652.7, bsz=380.5, num_updates=6100, lr=1e-05, gnorm=1.615, train_wall=45, wall=3115
2020-12-15 14:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:55:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:55:29 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_mse 0 | loss 5.447 | nll_loss 3.906 | ppl 14.99 | bleu 22.57 | wps 4362.4 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 22.73
2020-12-15 14:55:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 14:55:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 11 @ 6171 updates, score 22.57) (writing took 2.9742484781891108 seconds)
2020-12-15 14:55:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-12-15 14:55:32 | INFO | train | epoch 011 | symm_mse 12.112 | loss 4.115 | nll_loss 0.944 | ppl 1.92 | wps 20900.1 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 1e-05 | gnorm 1.654 | train_wall 252 | wall 3172
2020-12-15 14:55:32 | INFO | fairseq.trainer | begin training epoch 12
2020-12-15 14:55:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:55:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:55:48 | INFO | train_inner | epoch 012:     29 / 561 symm_mse=11.951, loss=4.091, nll_loss=0.937, ppl=1.91, wps=14326.4, ups=1.37, wpb=10473.3, bsz=364.9, num_updates=6200, lr=1e-05, gnorm=1.632, train_wall=45, wall=3188
2020-12-15 14:56:33 | INFO | train_inner | epoch 012:    129 / 561 symm_mse=11.917, loss=4.09, nll_loss=0.94, ppl=1.92, wps=23306.5, ups=2.24, wpb=10400.1, bsz=369, num_updates=6300, lr=1e-05, gnorm=1.616, train_wall=44, wall=3232
2020-12-15 14:57:18 | INFO | train_inner | epoch 012:    229 / 561 symm_mse=12.277, loss=4.142, nll_loss=0.953, ppl=1.94, wps=23191, ups=2.22, wpb=10444.8, bsz=372, num_updates=6400, lr=1e-05, gnorm=1.698, train_wall=45, wall=3277
2020-12-15 14:58:03 | INFO | train_inner | epoch 012:    329 / 561 symm_mse=11.599, loss=4.045, nll_loss=0.931, ppl=1.91, wps=23440.6, ups=2.2, wpb=10631.9, bsz=382.4, num_updates=6500, lr=1e-05, gnorm=1.597, train_wall=45, wall=3323
2020-12-15 14:58:48 | INFO | train_inner | epoch 012:    429 / 561 symm_mse=12.421, loss=4.162, nll_loss=0.957, ppl=1.94, wps=23282.9, ups=2.21, wpb=10531, bsz=361.8, num_updates=6600, lr=1e-05, gnorm=1.68, train_wall=45, wall=3368
2020-12-15 14:59:34 | INFO | train_inner | epoch 012:    529 / 561 symm_mse=11.753, loss=4.068, nll_loss=0.938, ppl=1.92, wps=23171, ups=2.21, wpb=10493.3, bsz=369.8, num_updates=6700, lr=1e-05, gnorm=1.604, train_wall=45, wall=3413
2020-12-15 14:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 14:59:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:59:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:59:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 14:59:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:59:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:59:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 14:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 14:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 14:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 14:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:00:09 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_mse 0 | loss 5.439 | nll_loss 3.898 | ppl 14.91 | bleu 22.63 | wps 4573.9 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 22.73
2020-12-15 15:00:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:00:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 12 @ 6732 updates, score 22.63) (writing took 2.840236185118556 seconds)
2020-12-15 15:00:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-12-15 15:00:12 | INFO | train | epoch 012 | symm_mse 11.988 | loss 4.1 | nll_loss 0.943 | ppl 1.92 | wps 21005.1 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 1e-05 | gnorm 1.637 | train_wall 252 | wall 3452
2020-12-15 15:00:12 | INFO | fairseq.trainer | begin training epoch 13
2020-12-15 15:00:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:00:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:00:45 | INFO | train_inner | epoch 013:     68 / 561 symm_mse=12.526, loss=4.162, nll_loss=0.945, ppl=1.93, wps=14485.6, ups=1.4, wpb=10372.9, bsz=360.1, num_updates=6800, lr=1e-05, gnorm=1.709, train_wall=44, wall=3485
2020-12-15 15:01:30 | INFO | train_inner | epoch 013:    168 / 561 symm_mse=12.346, loss=4.149, nll_loss=0.95, ppl=1.93, wps=23431.8, ups=2.22, wpb=10571.4, bsz=349, num_updates=6900, lr=1e-05, gnorm=1.664, train_wall=45, wall=3530
2020-12-15 15:02:16 | INFO | train_inner | epoch 013:    268 / 561 symm_mse=11.744, loss=4.073, nll_loss=0.944, ppl=1.92, wps=23326.8, ups=2.21, wpb=10544.3, bsz=369.9, num_updates=7000, lr=1e-05, gnorm=1.594, train_wall=45, wall=3575
2020-12-15 15:03:01 | INFO | train_inner | epoch 013:    368 / 561 symm_mse=11.211, loss=3.997, nll_loss=0.926, ppl=1.9, wps=23201, ups=2.21, wpb=10490.1, bsz=389.7, num_updates=7100, lr=1e-05, gnorm=1.573, train_wall=45, wall=3620
2020-12-15 15:03:46 | INFO | train_inner | epoch 013:    468 / 561 symm_mse=11.675, loss=4.062, nll_loss=0.94, ppl=1.92, wps=23474.5, ups=2.23, wpb=10510.3, bsz=369.8, num_updates=7200, lr=1e-05, gnorm=1.597, train_wall=45, wall=3665
2020-12-15 15:04:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:04:50 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_mse 0 | loss 5.436 | nll_loss 3.894 | ppl 14.87 | bleu 22.77 | wps 4145.3 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 22.77
2020-12-15 15:04:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:04:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:04:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:04:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 13 @ 7293 updates, score 22.77) (writing took 5.157324332743883 seconds)
2020-12-15 15:04:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-12-15 15:04:56 | INFO | train | epoch 013 | symm_mse 11.873 | loss 4.087 | nll_loss 0.942 | ppl 1.92 | wps 20750.3 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 1e-05 | gnorm 1.628 | train_wall 251 | wall 3735
2020-12-15 15:04:56 | INFO | fairseq.trainer | begin training epoch 14
2020-12-15 15:04:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:04:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:05:02 | INFO | train_inner | epoch 014:      7 / 561 symm_mse=11.794, loss=4.08, nll_loss=0.945, ppl=1.93, wps=13531, ups=1.31, wpb=10327.2, bsz=373.9, num_updates=7300, lr=1e-05, gnorm=1.635, train_wall=45, wall=3741
2020-12-15 15:05:46 | INFO | train_inner | epoch 014:    107 / 561 symm_mse=12.213, loss=4.131, nll_loss=0.948, ppl=1.93, wps=23802.3, ups=2.25, wpb=10590, bsz=367, num_updates=7400, lr=1e-05, gnorm=1.651, train_wall=44, wall=3786
2020-12-15 15:06:32 | INFO | train_inner | epoch 014:    207 / 561 symm_mse=11.649, loss=4.053, nll_loss=0.935, ppl=1.91, wps=23221.9, ups=2.2, wpb=10574.3, bsz=374, num_updates=7500, lr=1e-05, gnorm=1.595, train_wall=45, wall=3831
2020-12-15 15:07:17 | INFO | train_inner | epoch 014:    307 / 561 symm_mse=12.056, loss=4.11, nll_loss=0.946, ppl=1.93, wps=23115.3, ups=2.23, wpb=10386.9, bsz=357.6, num_updates=7600, lr=1e-05, gnorm=1.666, train_wall=45, wall=3876
2020-12-15 15:08:02 | INFO | train_inner | epoch 014:    407 / 561 symm_mse=11.868, loss=4.094, nll_loss=0.953, ppl=1.94, wps=22992.2, ups=2.22, wpb=10338.9, bsz=367.2, num_updates=7700, lr=1e-05, gnorm=1.628, train_wall=45, wall=3921
2020-12-15 15:08:47 | INFO | train_inner | epoch 014:    507 / 561 symm_mse=11.6, loss=4.051, nll_loss=0.936, ppl=1.91, wps=23236.1, ups=2.19, wpb=10594.9, bsz=368.6, num_updates=7800, lr=1e-05, gnorm=1.573, train_wall=45, wall=3967
2020-12-15 15:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:09:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:09:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:09:34 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_mse 0 | loss 5.437 | nll_loss 3.895 | ppl 14.88 | bleu 22.56 | wps 4324.6 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 22.77
2020-12-15 15:09:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:09:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:09:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:09:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 14 @ 7854 updates, score 22.56) (writing took 3.160755604505539 seconds)
2020-12-15 15:09:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-12-15 15:09:37 | INFO | train | epoch 014 | symm_mse 11.773 | loss 4.075 | nll_loss 0.942 | ppl 1.92 | wps 20884.4 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 1e-05 | gnorm 1.615 | train_wall 252 | wall 4017
2020-12-15 15:09:37 | INFO | fairseq.trainer | begin training epoch 15
2020-12-15 15:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:09:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:10:01 | INFO | train_inner | epoch 015:     46 / 561 symm_mse=11.308, loss=4.024, nll_loss=0.946, ppl=1.93, wps=14148.2, ups=1.36, wpb=10380.4, bsz=382.1, num_updates=7900, lr=1e-05, gnorm=1.579, train_wall=44, wall=4040
2020-12-15 15:10:45 | INFO | train_inner | epoch 015:    146 / 561 symm_mse=11.84, loss=4.088, nll_loss=0.947, ppl=1.93, wps=23452.4, ups=2.24, wpb=10471, bsz=365.8, num_updates=8000, lr=1e-05, gnorm=1.62, train_wall=44, wall=4085
2020-12-15 15:11:31 | INFO | train_inner | epoch 015:    246 / 561 symm_mse=11.472, loss=4.029, nll_loss=0.931, ppl=1.91, wps=23049.8, ups=2.2, wpb=10485.3, bsz=382.9, num_updates=8100, lr=1e-05, gnorm=1.602, train_wall=45, wall=4130
2020-12-15 15:12:16 | INFO | train_inner | epoch 015:    346 / 561 symm_mse=11.722, loss=4.066, nll_loss=0.94, ppl=1.92, wps=23089, ups=2.19, wpb=10526.2, bsz=362.5, num_updates=8200, lr=1e-05, gnorm=1.607, train_wall=45, wall=4176
2020-12-15 15:13:02 | INFO | train_inner | epoch 015:    446 / 561 symm_mse=11.195, loss=4.006, nll_loss=0.938, ppl=1.92, wps=23165, ups=2.2, wpb=10527.6, bsz=374.3, num_updates=8300, lr=1e-05, gnorm=1.545, train_wall=45, wall=4222
2020-12-15 15:13:47 | INFO | train_inner | epoch 015:    546 / 561 symm_mse=12.156, loss=4.129, nll_loss=0.954, ppl=1.94, wps=23083.4, ups=2.2, wpb=10468.7, bsz=364.2, num_updates=8400, lr=1e-05, gnorm=1.665, train_wall=45, wall=4267
2020-12-15 15:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:14:15 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_mse 0 | loss 5.433 | nll_loss 3.891 | ppl 14.84 | bleu 22.65 | wps 4599.8 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 22.77
2020-12-15 15:14:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:14:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 15 @ 8415 updates, score 22.65) (writing took 3.0924939159303904 seconds)
2020-12-15 15:14:18 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-12-15 15:14:18 | INFO | train | epoch 015 | symm_mse 11.677 | loss 4.063 | nll_loss 0.941 | ppl 1.92 | wps 20914.3 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 1e-05 | gnorm 1.607 | train_wall 253 | wall 4298
2020-12-15 15:14:18 | INFO | fairseq.trainer | begin training epoch 16
2020-12-15 15:14:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:14:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:15:00 | INFO | train_inner | epoch 016:     85 / 561 symm_mse=11.407, loss=4.026, nll_loss=0.934, ppl=1.91, wps=14405.7, ups=1.38, wpb=10408.1, bsz=377.6, num_updates=8500, lr=1e-05, gnorm=1.596, train_wall=45, wall=4339
2020-12-15 15:15:45 | INFO | train_inner | epoch 016:    185 / 561 symm_mse=11.605, loss=4.056, nll_loss=0.944, ppl=1.92, wps=23041.7, ups=2.21, wpb=10433.6, bsz=372.2, num_updates=8600, lr=1e-05, gnorm=1.624, train_wall=45, wall=4384
2020-12-15 15:16:30 | INFO | train_inner | epoch 016:    285 / 561 symm_mse=11.703, loss=4.067, nll_loss=0.943, ppl=1.92, wps=23322.7, ups=2.21, wpb=10533.1, bsz=379.8, num_updates=8700, lr=1e-05, gnorm=1.611, train_wall=45, wall=4430
2020-12-15 15:17:16 | INFO | train_inner | epoch 016:    385 / 561 symm_mse=11.257, loss=4.001, nll_loss=0.927, ppl=1.9, wps=23137.7, ups=2.19, wpb=10551.2, bsz=375.2, num_updates=8800, lr=1e-05, gnorm=1.54, train_wall=45, wall=4475
2020-12-15 15:18:01 | INFO | train_inner | epoch 016:    485 / 561 symm_mse=11.738, loss=4.07, nll_loss=0.94, ppl=1.92, wps=23323.8, ups=2.22, wpb=10494.4, bsz=360.2, num_updates=8900, lr=1e-05, gnorm=1.614, train_wall=45, wall=4520
2020-12-15 15:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:18:57 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_mse 0 | loss 5.426 | nll_loss 3.885 | ppl 14.77 | bleu 22.66 | wps 4351 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 22.77
2020-12-15 15:18:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:18:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:18:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:19:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:19:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:19:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 16 @ 8976 updates, score 22.66) (writing took 3.0486197974532843 seconds)
2020-12-15 15:19:00 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-12-15 15:19:00 | INFO | train | epoch 016 | symm_mse 11.593 | loss 4.053 | nll_loss 0.941 | ppl 1.92 | wps 20882.4 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 1e-05 | gnorm 1.602 | train_wall 252 | wall 4580
2020-12-15 15:19:00 | INFO | fairseq.trainer | begin training epoch 17
2020-12-15 15:19:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:19:14 | INFO | train_inner | epoch 017:     24 / 561 symm_mse=11.43, loss=4.04, nll_loss=0.949, ppl=1.93, wps=14255.4, ups=1.36, wpb=10459.4, bsz=358.3, num_updates=9000, lr=1e-05, gnorm=1.58, train_wall=45, wall=4594
2020-12-15 15:19:59 | INFO | train_inner | epoch 017:    124 / 561 symm_mse=11.53, loss=4.046, nll_loss=0.94, ppl=1.92, wps=23294, ups=2.22, wpb=10489.1, bsz=374.8, num_updates=9100, lr=1e-05, gnorm=1.607, train_wall=45, wall=4639
2020-12-15 15:20:45 | INFO | train_inner | epoch 017:    224 / 561 symm_mse=11.725, loss=4.052, nll_loss=0.924, ppl=1.9, wps=23274.3, ups=2.2, wpb=10592.8, bsz=369.3, num_updates=9200, lr=1e-05, gnorm=1.624, train_wall=45, wall=4684
2020-12-15 15:21:30 | INFO | train_inner | epoch 017:    324 / 561 symm_mse=11.647, loss=4.055, nll_loss=0.937, ppl=1.91, wps=23312.7, ups=2.22, wpb=10507.6, bsz=365.5, num_updates=9300, lr=1e-05, gnorm=1.598, train_wall=45, wall=4729
2020-12-15 15:22:15 | INFO | train_inner | epoch 017:    424 / 561 symm_mse=11.051, loss=3.994, nll_loss=0.944, ppl=1.92, wps=23088.2, ups=2.22, wpb=10397.2, bsz=380.2, num_updates=9400, lr=1e-05, gnorm=1.555, train_wall=45, wall=4774
2020-12-15 15:23:00 | INFO | train_inner | epoch 017:    524 / 561 symm_mse=11.8, loss=4.094, nll_loss=0.959, ppl=1.94, wps=23266.6, ups=2.22, wpb=10491.6, bsz=362.4, num_updates=9500, lr=1e-05, gnorm=1.626, train_wall=45, wall=4819
2020-12-15 15:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:23:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:23:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:23:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:23:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:23:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:23:39 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_mse 0 | loss 5.422 | nll_loss 3.881 | ppl 14.73 | bleu 22.73 | wps 4346 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 22.77
2020-12-15 15:23:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:23:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:23:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 17 @ 9537 updates, score 22.73) (writing took 3.218540508300066 seconds)
2020-12-15 15:23:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-12-15 15:23:42 | INFO | train | epoch 017 | symm_mse 11.5 | loss 4.043 | nll_loss 0.941 | ppl 1.92 | wps 20862.9 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 1e-05 | gnorm 1.597 | train_wall 252 | wall 4861
2020-12-15 15:23:42 | INFO | fairseq.trainer | begin training epoch 18
2020-12-15 15:23:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:23:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:24:14 | INFO | train_inner | epoch 018:     63 / 561 symm_mse=11.63, loss=4.059, nll_loss=0.943, ppl=1.92, wps=14029.8, ups=1.35, wpb=10417.2, bsz=358.2, num_updates=9600, lr=1e-05, gnorm=1.608, train_wall=45, wall=4894
2020-12-15 15:25:00 | INFO | train_inner | epoch 018:    163 / 561 symm_mse=11.552, loss=4.051, nll_loss=0.944, ppl=1.92, wps=22663.4, ups=2.16, wpb=10492.8, bsz=370, num_updates=9700, lr=1e-05, gnorm=1.601, train_wall=46, wall=4940
2020-12-15 15:25:47 | INFO | train_inner | epoch 018:    263 / 561 symm_mse=11.611, loss=4.052, nll_loss=0.938, ppl=1.92, wps=22296.5, ups=2.13, wpb=10456, bsz=363.1, num_updates=9800, lr=1e-05, gnorm=1.609, train_wall=47, wall=4987
2020-12-15 15:26:33 | INFO | train_inner | epoch 018:    363 / 561 symm_mse=11.121, loss=3.999, nll_loss=0.942, ppl=1.92, wps=22404.3, ups=2.16, wpb=10380.2, bsz=372.9, num_updates=9900, lr=1e-05, gnorm=1.556, train_wall=46, wall=5033
2020-12-15 15:27:20 | INFO | train_inner | epoch 018:    463 / 561 symm_mse=11.033, loss=3.984, nll_loss=0.935, ppl=1.91, wps=22694.1, ups=2.14, wpb=10622, bsz=379.6, num_updates=10000, lr=1e-05, gnorm=1.504, train_wall=47, wall=5080
2020-12-15 15:28:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:28:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:28:33 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_mse 0 | loss 5.421 | nll_loss 3.88 | ppl 14.72 | bleu 22.72 | wps 3641.4 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 22.77
2020-12-15 15:28:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 18 @ 10098 updates, score 22.72) (writing took 2.911768604069948 seconds)
2020-12-15 15:28:36 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-12-15 15:28:36 | INFO | train | epoch 018 | symm_mse 11.403 | loss 4.031 | nll_loss 0.941 | ppl 1.92 | wps 20012 | ups 1.91 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 1e-05 | gnorm 1.573 | train_wall 259 | wall 5155
2020-12-15 15:28:36 | INFO | fairseq.trainer | begin training epoch 19
2020-12-15 15:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:28:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:28:40 | INFO | train_inner | epoch 019:      2 / 561 symm_mse=11.532, loss=4.054, nll_loss=0.95, ppl=1.93, wps=13061.8, ups=1.25, wpb=10417.9, bsz=365.4, num_updates=10100, lr=1e-05, gnorm=1.587, train_wall=46, wall=5160
2020-12-15 15:29:24 | INFO | train_inner | epoch 019:    102 / 561 symm_mse=11.198, loss=4.003, nll_loss=0.934, ppl=1.91, wps=23495.5, ups=2.25, wpb=10422.9, bsz=373.4, num_updates=10200, lr=1e-05, gnorm=1.557, train_wall=44, wall=5204
2020-12-15 15:30:10 | INFO | train_inner | epoch 019:    202 / 561 symm_mse=10.802, loss=3.953, nll_loss=0.93, ppl=1.91, wps=23453.5, ups=2.21, wpb=10632.5, bsz=380, num_updates=10300, lr=1e-05, gnorm=1.488, train_wall=45, wall=5249
2020-12-15 15:30:55 | INFO | train_inner | epoch 019:    302 / 561 symm_mse=11.728, loss=4.07, nll_loss=0.942, ppl=1.92, wps=23210.5, ups=2.21, wpb=10486.3, bsz=361.5, num_updates=10400, lr=1e-05, gnorm=1.647, train_wall=45, wall=5295
2020-12-15 15:31:40 | INFO | train_inner | epoch 019:    402 / 561 symm_mse=11.542, loss=4.055, nll_loss=0.949, ppl=1.93, wps=23114.9, ups=2.21, wpb=10482.8, bsz=369.7, num_updates=10500, lr=1e-05, gnorm=1.595, train_wall=45, wall=5340
2020-12-15 15:32:26 | INFO | train_inner | epoch 019:    502 / 561 symm_mse=11.427, loss=4.028, nll_loss=0.935, ppl=1.91, wps=22944, ups=2.2, wpb=10451.7, bsz=365.4, num_updates=10600, lr=1e-05, gnorm=1.607, train_wall=45, wall=5385
2020-12-15 15:32:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:32:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:32:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:32:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:33:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:33:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:33:15 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_mse 0 | loss 5.424 | nll_loss 3.881 | ppl 14.74 | bleu 22.6 | wps 4354 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 22.77
2020-12-15 15:33:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:33:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:33:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:33:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 19 @ 10659 updates, score 22.6) (writing took 4.179928030818701 seconds)
2020-12-15 15:33:19 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-12-15 15:33:19 | INFO | train | epoch 019 | symm_mse 11.335 | loss 4.023 | nll_loss 0.94 | ppl 1.92 | wps 20781.5 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 1e-05 | gnorm 1.582 | train_wall 252 | wall 5438
2020-12-15 15:33:19 | INFO | fairseq.trainer | begin training epoch 20
2020-12-15 15:33:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:33:40 | INFO | train_inner | epoch 020:     41 / 561 symm_mse=11.267, loss=4.015, nll_loss=0.942, ppl=1.92, wps=14063.3, ups=1.34, wpb=10477.3, bsz=372.4, num_updates=10700, lr=1e-05, gnorm=1.581, train_wall=44, wall=5460
2020-12-15 15:34:25 | INFO | train_inner | epoch 020:    141 / 561 symm_mse=11.359, loss=4.032, nll_loss=0.947, ppl=1.93, wps=23420.3, ups=2.22, wpb=10538.1, bsz=365.8, num_updates=10800, lr=1e-05, gnorm=1.574, train_wall=45, wall=5505
2020-12-15 15:35:11 | INFO | train_inner | epoch 020:    241 / 561 symm_mse=10.957, loss=3.977, nll_loss=0.937, ppl=1.92, wps=23144.1, ups=2.21, wpb=10496, bsz=377, num_updates=10900, lr=1e-05, gnorm=1.53, train_wall=45, wall=5550
2020-12-15 15:35:56 | INFO | train_inner | epoch 020:    341 / 561 symm_mse=11.72, loss=4.068, nll_loss=0.94, ppl=1.92, wps=23183.1, ups=2.2, wpb=10521.2, bsz=361.4, num_updates=11000, lr=1e-05, gnorm=1.621, train_wall=45, wall=5596
2020-12-15 15:36:41 | INFO | train_inner | epoch 020:    441 / 561 symm_mse=11.035, loss=3.989, nll_loss=0.942, ppl=1.92, wps=23227.6, ups=2.22, wpb=10445.5, bsz=372.4, num_updates=11100, lr=1e-05, gnorm=1.574, train_wall=45, wall=5641
2020-12-15 15:37:26 | INFO | train_inner | epoch 020:    541 / 561 symm_mse=11.423, loss=4.037, nll_loss=0.946, ppl=1.93, wps=23004, ups=2.21, wpb=10410.2, bsz=367.4, num_updates=11200, lr=1e-05, gnorm=1.62, train_wall=45, wall=5686
2020-12-15 15:37:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:37:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:37:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:37:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:37:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:37:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:37:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:37:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:37:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:37:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:37:58 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_mse 0 | loss 5.419 | nll_loss 3.876 | ppl 14.69 | bleu 22.65 | wps 4127.6 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 22.77
2020-12-15 15:37:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:37:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:37:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:38:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:38:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:38:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 20 @ 11220 updates, score 22.65) (writing took 3.025511721149087 seconds)
2020-12-15 15:38:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-12-15 15:38:01 | INFO | train | epoch 020 | symm_mse 11.264 | loss 4.015 | nll_loss 0.941 | ppl 1.92 | wps 20815.9 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 1e-05 | gnorm 1.578 | train_wall 252 | wall 5721
2020-12-15 15:38:01 | INFO | fairseq.trainer | begin training epoch 21
2020-12-15 15:38:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:38:40 | INFO | train_inner | epoch 021:     80 / 561 symm_mse=11.118, loss=4.003, nll_loss=0.945, ppl=1.93, wps=13957.5, ups=1.35, wpb=10315.2, bsz=376.1, num_updates=11300, lr=1e-05, gnorm=1.58, train_wall=44, wall=5760
2020-12-15 15:39:26 | INFO | train_inner | epoch 021:    180 / 561 symm_mse=11.312, loss=4.022, nll_loss=0.941, ppl=1.92, wps=23131.9, ups=2.2, wpb=10526.7, bsz=359.6, num_updates=11400, lr=1e-05, gnorm=1.567, train_wall=45, wall=5805
2020-12-15 15:40:11 | INFO | train_inner | epoch 021:    280 / 561 symm_mse=11.118, loss=3.991, nll_loss=0.933, ppl=1.91, wps=23102.4, ups=2.2, wpb=10514.2, bsz=377.1, num_updates=11500, lr=1e-05, gnorm=1.568, train_wall=45, wall=5851
2020-12-15 15:40:57 | INFO | train_inner | epoch 021:    380 / 561 symm_mse=10.965, loss=3.967, nll_loss=0.926, ppl=1.9, wps=23214.3, ups=2.19, wpb=10592.6, bsz=375.9, num_updates=11600, lr=1e-05, gnorm=1.532, train_wall=45, wall=5896
2020-12-15 15:41:42 | INFO | train_inner | epoch 021:    480 / 561 symm_mse=11.088, loss=3.998, nll_loss=0.943, ppl=1.92, wps=23064.7, ups=2.2, wpb=10501.6, bsz=373.3, num_updates=11700, lr=1e-05, gnorm=1.536, train_wall=45, wall=5942
2020-12-15 15:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:42:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:42:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:42:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:42:41 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_mse 0 | loss 5.414 | nll_loss 3.871 | ppl 14.63 | bleu 22.68 | wps 4478.2 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 22.77
2020-12-15 15:42:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 21 @ 11781 updates, score 22.68) (writing took 3.0562295448035 seconds)
2020-12-15 15:42:44 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-12-15 15:42:44 | INFO | train | epoch 021 | symm_mse 11.199 | loss 4.007 | nll_loss 0.94 | ppl 1.92 | wps 20798.9 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 1e-05 | gnorm 1.562 | train_wall 254 | wall 6004
2020-12-15 15:42:44 | INFO | fairseq.trainer | begin training epoch 22
2020-12-15 15:42:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:42:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:42:56 | INFO | train_inner | epoch 022:     19 / 561 symm_mse=11.523, loss=4.054, nll_loss=0.952, ppl=1.93, wps=14234.5, ups=1.36, wpb=10469.7, bsz=353, num_updates=11800, lr=1e-05, gnorm=1.577, train_wall=45, wall=6016
2020-12-15 15:43:41 | INFO | train_inner | epoch 022:    119 / 561 symm_mse=11.064, loss=3.986, nll_loss=0.934, ppl=1.91, wps=23527.8, ups=2.22, wpb=10589.6, bsz=376.8, num_updates=11900, lr=1e-05, gnorm=1.536, train_wall=45, wall=6061
2020-12-15 15:44:26 | INFO | train_inner | epoch 022:    219 / 561 symm_mse=10.834, loss=3.956, nll_loss=0.931, ppl=1.91, wps=23097.7, ups=2.2, wpb=10493.2, bsz=374.8, num_updates=12000, lr=1e-05, gnorm=1.506, train_wall=45, wall=6106
2020-12-15 15:45:12 | INFO | train_inner | epoch 022:    319 / 561 symm_mse=11.263, loss=4.025, nll_loss=0.953, ppl=1.94, wps=22920.6, ups=2.2, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1e-05, gnorm=1.559, train_wall=45, wall=6151
2020-12-15 15:45:57 | INFO | train_inner | epoch 022:    419 / 561 symm_mse=11.32, loss=4.01, nll_loss=0.927, ppl=1.9, wps=23108.2, ups=2.19, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1e-05, gnorm=1.568, train_wall=45, wall=6197
2020-12-15 15:46:42 | INFO | train_inner | epoch 022:    519 / 561 symm_mse=11.09, loss=4.002, nll_loss=0.949, ppl=1.93, wps=23187.1, ups=2.22, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1e-05, gnorm=1.559, train_wall=45, wall=6242
2020-12-15 15:47:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:47:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:47:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:47:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:47:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:47:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:47:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:47:25 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_mse 0 | loss 5.41 | nll_loss 3.868 | ppl 14.6 | bleu 22.77 | wps 4009.8 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 22.77
2020-12-15 15:47:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:47:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 22 @ 12342 updates, score 22.77) (writing took 9.161344019696116 seconds)
2020-12-15 15:47:34 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-12-15 15:47:34 | INFO | train | epoch 022 | symm_mse 11.138 | loss 4 | nll_loss 0.941 | ppl 1.92 | wps 20270.7 | ups 1.93 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1e-05 | gnorm 1.547 | train_wall 253 | wall 6294
2020-12-15 15:47:34 | INFO | fairseq.trainer | begin training epoch 23
2020-12-15 15:47:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:47:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:48:03 | INFO | train_inner | epoch 023:     58 / 561 symm_mse=11.846, loss=4.1, nll_loss=0.962, ppl=1.95, wps=12865.8, ups=1.24, wpb=10402.8, bsz=349, num_updates=12400, lr=1e-05, gnorm=1.639, train_wall=44, wall=6323
2020-12-15 15:48:49 | INFO | train_inner | epoch 023:    158 / 561 symm_mse=10.521, loss=3.927, nll_loss=0.939, ppl=1.92, wps=23147.8, ups=2.2, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1e-05, gnorm=1.483, train_wall=45, wall=6368
2020-12-15 15:49:34 | INFO | train_inner | epoch 023:    258 / 561 symm_mse=11.77, loss=4.084, nll_loss=0.953, ppl=1.94, wps=23068.6, ups=2.21, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1e-05, gnorm=1.639, train_wall=45, wall=6414
2020-12-15 15:50:19 | INFO | train_inner | epoch 023:    358 / 561 symm_mse=10.904, loss=3.968, nll_loss=0.935, ppl=1.91, wps=23393, ups=2.21, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1e-05, gnorm=1.51, train_wall=45, wall=6459
2020-12-15 15:51:04 | INFO | train_inner | epoch 023:    458 / 561 symm_mse=10.717, loss=3.948, nll_loss=0.936, ppl=1.91, wps=23169.6, ups=2.23, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1e-05, gnorm=1.494, train_wall=45, wall=6504
2020-12-15 15:51:50 | INFO | train_inner | epoch 023:    558 / 561 symm_mse=10.842, loss=3.953, nll_loss=0.925, ppl=1.9, wps=23029.7, ups=2.19, wpb=10521.5, bsz=381, num_updates=12900, lr=1e-05, gnorm=1.526, train_wall=45, wall=6549
2020-12-15 15:51:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:51:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:51:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:51:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:51:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:51:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:52:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:52:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:52:20 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_mse 0 | loss 5.411 | nll_loss 3.868 | ppl 14.6 | bleu 22.77 | wps 3355.4 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 22.77
2020-12-15 15:52:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:52:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:52:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:52:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:52:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:52:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 23 @ 12903 updates, score 22.77) (writing took 5.463963029906154 seconds)
2020-12-15 15:52:26 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-12-15 15:52:26 | INFO | train | epoch 023 | symm_mse 11.071 | loss 3.992 | nll_loss 0.94 | ppl 1.92 | wps 20164 | ups 1.92 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1e-05 | gnorm 1.546 | train_wall 252 | wall 6585
2020-12-15 15:52:26 | INFO | fairseq.trainer | begin training epoch 24
2020-12-15 15:52:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:52:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:53:13 | INFO | train_inner | epoch 024:     97 / 561 symm_mse=10.93, loss=3.973, nll_loss=0.936, ppl=1.91, wps=12640.9, ups=1.2, wpb=10536, bsz=373.3, num_updates=13000, lr=1e-05, gnorm=1.505, train_wall=45, wall=6633
2020-12-15 15:53:59 | INFO | train_inner | epoch 024:    197 / 561 symm_mse=11.331, loss=4.021, nll_loss=0.939, ppl=1.92, wps=22748.5, ups=2.18, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1e-05, gnorm=1.578, train_wall=46, wall=6679
2020-12-15 15:54:45 | INFO | train_inner | epoch 024:    297 / 561 symm_mse=10.901, loss=3.969, nll_loss=0.935, ppl=1.91, wps=22846.4, ups=2.17, wpb=10536, bsz=369, num_updates=13200, lr=1e-05, gnorm=1.522, train_wall=46, wall=6725
2020-12-15 15:55:31 | INFO | train_inner | epoch 024:    397 / 561 symm_mse=10.998, loss=3.989, nll_loss=0.946, ppl=1.93, wps=22568.4, ups=2.19, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1e-05, gnorm=1.53, train_wall=45, wall=6770
2020-12-15 15:56:16 | INFO | train_inner | epoch 024:    497 / 561 symm_mse=11.031, loss=3.995, nll_loss=0.95, ppl=1.93, wps=23112.1, ups=2.21, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1e-05, gnorm=1.552, train_wall=45, wall=6816
2020-12-15 15:56:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:56:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 15:56:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 15:56:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 15:57:08 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_mse 0 | loss 5.413 | nll_loss 3.868 | ppl 14.6 | bleu 22.65 | wps 4481.7 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 22.77
2020-12-15 15:57:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 15:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:57:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:57:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:57:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 24 @ 13464 updates, score 22.65) (writing took 3.1803401187062263 seconds)
2020-12-15 15:57:12 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-12-15 15:57:12 | INFO | train | epoch 024 | symm_mse 11.012 | loss 3.984 | nll_loss 0.939 | ppl 1.92 | wps 20582 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1e-05 | gnorm 1.535 | train_wall 255 | wall 6871
2020-12-15 15:57:12 | INFO | fairseq.trainer | begin training epoch 25
2020-12-15 15:57:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 15:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 15:57:31 | INFO | train_inner | epoch 025:     36 / 561 symm_mse=11.018, loss=3.975, nll_loss=0.927, ppl=1.9, wps=14027.1, ups=1.33, wpb=10574, bsz=365.8, num_updates=13500, lr=1e-05, gnorm=1.539, train_wall=45, wall=6891
2020-12-15 15:58:17 | INFO | train_inner | epoch 025:    136 / 561 symm_mse=11.09, loss=3.989, nll_loss=0.932, ppl=1.91, wps=23275, ups=2.2, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1e-05, gnorm=1.551, train_wall=45, wall=6936
2020-12-15 15:59:03 | INFO | train_inner | epoch 025:    236 / 561 symm_mse=10.735, loss=3.953, nll_loss=0.939, ppl=1.92, wps=22743.4, ups=2.19, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1e-05, gnorm=1.498, train_wall=45, wall=6982
2020-12-15 15:59:48 | INFO | train_inner | epoch 025:    336 / 561 symm_mse=11.187, loss=4.006, nll_loss=0.94, ppl=1.92, wps=23004.5, ups=2.19, wpb=10508.8, bsz=363, num_updates=13800, lr=1e-05, gnorm=1.567, train_wall=45, wall=7028
2020-12-15 16:00:34 | INFO | train_inner | epoch 025:    436 / 561 symm_mse=10.535, loss=3.932, nll_loss=0.941, ppl=1.92, wps=23268.9, ups=2.19, wpb=10632, bsz=381.2, num_updates=13900, lr=1e-05, gnorm=1.464, train_wall=45, wall=7073
2020-12-15 16:01:20 | INFO | train_inner | epoch 025:    536 / 561 symm_mse=10.756, loss=3.953, nll_loss=0.937, ppl=1.91, wps=22726.2, ups=2.19, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1e-05, gnorm=1.518, train_wall=45, wall=7119
2020-12-15 16:01:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:01:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:01:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:01:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:01:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:01:55 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_mse 0 | loss 5.407 | nll_loss 3.864 | ppl 14.56 | bleu 22.59 | wps 4028.8 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 22.77
2020-12-15 16:01:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:01:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:01:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:01:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:01:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 25 @ 14025 updates, score 22.59) (writing took 2.9692208785563707 seconds)
2020-12-15 16:01:58 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-12-15 16:01:58 | INFO | train | epoch 025 | symm_mse 10.947 | loss 3.977 | nll_loss 0.938 | ppl 1.92 | wps 20552.2 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1e-05 | gnorm 1.532 | train_wall 254 | wall 7157
2020-12-15 16:01:58 | INFO | fairseq.trainer | begin training epoch 26
2020-12-15 16:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:02:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:02:34 | INFO | train_inner | epoch 026:     75 / 561 symm_mse=10.42, loss=3.902, nll_loss=0.923, ppl=1.9, wps=14023.5, ups=1.35, wpb=10395, bsz=387.4, num_updates=14100, lr=1e-05, gnorm=1.48, train_wall=44, wall=7193
2020-12-15 16:03:18 | INFO | train_inner | epoch 026:    175 / 561 symm_mse=11.422, loss=4.033, nll_loss=0.942, ppl=1.92, wps=23558.2, ups=2.26, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1e-05, gnorm=1.592, train_wall=44, wall=7238
2020-12-15 16:04:03 | INFO | train_inner | epoch 026:    275 / 561 symm_mse=10.486, loss=3.916, nll_loss=0.929, ppl=1.9, wps=23856.2, ups=2.24, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1e-05, gnorm=1.468, train_wall=45, wall=7282
2020-12-15 16:04:47 | INFO | train_inner | epoch 026:    375 / 561 symm_mse=11.21, loss=4.014, nll_loss=0.947, ppl=1.93, wps=23628.5, ups=2.24, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1e-05, gnorm=1.562, train_wall=44, wall=7327
2020-12-15 16:05:32 | INFO | train_inner | epoch 026:    475 / 561 symm_mse=11.142, loss=4.003, nll_loss=0.942, ppl=1.92, wps=23496.4, ups=2.23, wpb=10526, bsz=359.1, num_updates=14500, lr=1e-05, gnorm=1.557, train_wall=45, wall=7372
2020-12-15 16:06:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:06:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:06:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:06:31 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_mse 0 | loss 5.402 | nll_loss 3.86 | ppl 14.52 | bleu 22.68 | wps 4538.8 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 22.77
2020-12-15 16:06:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:06:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 26 @ 14586 updates, score 22.68) (writing took 2.964990980923176 seconds)
2020-12-15 16:06:34 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-12-15 16:06:34 | INFO | train | epoch 026 | symm_mse 10.892 | loss 3.97 | nll_loss 0.939 | ppl 1.92 | wps 21261.8 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1e-05 | gnorm 1.529 | train_wall 248 | wall 7434
2020-12-15 16:06:34 | INFO | fairseq.trainer | begin training epoch 27
2020-12-15 16:06:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:06:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:06:44 | INFO | train_inner | epoch 027:     14 / 561 symm_mse=10.874, loss=3.978, nll_loss=0.951, ppl=1.93, wps=14332.5, ups=1.39, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1e-05, gnorm=1.553, train_wall=44, wall=7443
2020-12-15 16:07:28 | INFO | train_inner | epoch 027:    114 / 561 symm_mse=11.05, loss=3.995, nll_loss=0.946, ppl=1.93, wps=23685.7, ups=2.25, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1e-05, gnorm=1.545, train_wall=44, wall=7488
2020-12-15 16:08:13 | INFO | train_inner | epoch 027:    214 / 561 symm_mse=11.036, loss=3.99, nll_loss=0.942, ppl=1.92, wps=23551.2, ups=2.23, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1e-05, gnorm=1.551, train_wall=45, wall=7533
2020-12-15 16:08:58 | INFO | train_inner | epoch 027:    314 / 561 symm_mse=10.526, loss=3.924, nll_loss=0.935, ppl=1.91, wps=23382.8, ups=2.23, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1e-05, gnorm=1.483, train_wall=45, wall=7577
2020-12-15 16:09:43 | INFO | train_inner | epoch 027:    414 / 561 symm_mse=10.631, loss=3.932, nll_loss=0.93, ppl=1.91, wps=23386, ups=2.21, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1e-05, gnorm=1.503, train_wall=45, wall=7623
2020-12-15 16:10:28 | INFO | train_inner | epoch 027:    514 / 561 symm_mse=11.07, loss=3.992, nll_loss=0.942, ppl=1.92, wps=23152.6, ups=2.22, wpb=10447, bsz=358.8, num_updates=15100, lr=1e-05, gnorm=1.546, train_wall=45, wall=7668
2020-12-15 16:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:10:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:10:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:10:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:10:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:10:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:10:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:10:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:10:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:10:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:10:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:11:10 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_mse 0 | loss 5.405 | nll_loss 3.862 | ppl 14.54 | bleu 22.69 | wps 4673.2 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 22.77
2020-12-15 16:11:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:11:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:11:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:11:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:11:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:11:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 27 @ 15147 updates, score 22.69) (writing took 3.0368227250874043 seconds)
2020-12-15 16:11:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-12-15 16:11:13 | INFO | train | epoch 027 | symm_mse 10.836 | loss 3.964 | nll_loss 0.94 | ppl 1.92 | wps 21133.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1e-05 | gnorm 1.528 | train_wall 250 | wall 7712
2020-12-15 16:11:13 | INFO | fairseq.trainer | begin training epoch 28
2020-12-15 16:11:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:11:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:11:39 | INFO | train_inner | epoch 028:     53 / 561 symm_mse=10.421, loss=3.905, nll_loss=0.926, ppl=1.9, wps=14537.6, ups=1.41, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1e-05, gnorm=1.499, train_wall=44, wall=7739
2020-12-15 16:12:24 | INFO | train_inner | epoch 028:    153 / 561 symm_mse=11.123, loss=4.001, nll_loss=0.943, ppl=1.92, wps=23301.5, ups=2.22, wpb=10482.5, bsz=354, num_updates=15300, lr=1e-05, gnorm=1.551, train_wall=45, wall=7784
2020-12-15 16:13:09 | INFO | train_inner | epoch 028:    253 / 561 symm_mse=10.821, loss=3.958, nll_loss=0.934, ppl=1.91, wps=23198.4, ups=2.2, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1e-05, gnorm=1.522, train_wall=45, wall=7829
2020-12-15 16:13:55 | INFO | train_inner | epoch 028:    353 / 561 symm_mse=11.094, loss=3.994, nll_loss=0.939, ppl=1.92, wps=23071.6, ups=2.19, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1e-05, gnorm=1.541, train_wall=45, wall=7875
2020-12-15 16:14:40 | INFO | train_inner | epoch 028:    453 / 561 symm_mse=10.534, loss=3.939, nll_loss=0.951, ppl=1.93, wps=23112.3, ups=2.21, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1e-05, gnorm=1.462, train_wall=45, wall=7920
2020-12-15 16:15:26 | INFO | train_inner | epoch 028:    553 / 561 symm_mse=10.633, loss=3.948, nll_loss=0.946, ppl=1.93, wps=23291.3, ups=2.21, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1e-05, gnorm=1.485, train_wall=45, wall=7965
2020-12-15 16:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:15:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:15:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:15:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:15:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:15:50 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_mse 0 | loss 5.4 | nll_loss 3.856 | ppl 14.48 | bleu 22.72 | wps 4645.2 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 22.77
2020-12-15 16:15:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:15:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:15:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 28 @ 15708 updates, score 22.72) (writing took 3.062165107578039 seconds)
2020-12-15 16:15:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-12-15 16:15:53 | INFO | train | epoch 028 | symm_mse 10.78 | loss 3.958 | nll_loss 0.939 | ppl 1.92 | wps 20987.8 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1e-05 | gnorm 1.509 | train_wall 252 | wall 7992
2020-12-15 16:15:53 | INFO | fairseq.trainer | begin training epoch 29
2020-12-15 16:15:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:15:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:15:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:16:37 | INFO | train_inner | epoch 029:     92 / 561 symm_mse=10.556, loss=3.929, nll_loss=0.935, ppl=1.91, wps=14683.3, ups=1.4, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1e-05, gnorm=1.507, train_wall=44, wall=8037
2020-12-15 16:17:22 | INFO | train_inner | epoch 029:    192 / 561 symm_mse=10.898, loss=3.969, nll_loss=0.938, ppl=1.92, wps=23307.3, ups=2.21, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1e-05, gnorm=1.525, train_wall=45, wall=8082
2020-12-15 16:18:07 | INFO | train_inner | epoch 029:    292 / 561 symm_mse=10.761, loss=3.96, nll_loss=0.944, ppl=1.92, wps=23379.9, ups=2.22, wpb=10552.2, bsz=366, num_updates=16000, lr=1e-05, gnorm=1.485, train_wall=45, wall=8127
2020-12-15 16:18:53 | INFO | train_inner | epoch 029:    392 / 561 symm_mse=10.589, loss=3.928, nll_loss=0.931, ppl=1.91, wps=22833.4, ups=2.2, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1e-05, gnorm=1.518, train_wall=45, wall=8172
2020-12-15 16:19:39 | INFO | train_inner | epoch 029:    492 / 561 symm_mse=10.799, loss=3.96, nll_loss=0.937, ppl=1.91, wps=22711.7, ups=2.19, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1e-05, gnorm=1.53, train_wall=45, wall=8218
2020-12-15 16:20:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:20:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:20:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:20:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:20:32 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_mse 0 | loss 5.397 | nll_loss 3.854 | ppl 14.46 | bleu 22.65 | wps 4318.9 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 22.77
2020-12-15 16:20:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:20:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 29 @ 16269 updates, score 22.65) (writing took 2.792223270982504 seconds)
2020-12-15 16:20:34 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-12-15 16:20:34 | INFO | train | epoch 029 | symm_mse 10.735 | loss 3.952 | nll_loss 0.938 | ppl 1.92 | wps 20894.1 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1e-05 | gnorm 1.51 | train_wall 252 | wall 8274
2020-12-15 16:20:34 | INFO | fairseq.trainer | begin training epoch 30
2020-12-15 16:20:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:20:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:20:51 | INFO | train_inner | epoch 030:     31 / 561 symm_mse=10.586, loss=3.935, nll_loss=0.938, ppl=1.92, wps=14395, ups=1.38, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1e-05, gnorm=1.493, train_wall=45, wall=8291
2020-12-15 16:21:36 | INFO | train_inner | epoch 030:    131 / 561 symm_mse=10.783, loss=3.955, nll_loss=0.936, ppl=1.91, wps=23691.7, ups=2.24, wpb=10585.6, bsz=366, num_updates=16400, lr=1e-05, gnorm=1.509, train_wall=44, wall=8335
2020-12-15 16:22:21 | INFO | train_inner | epoch 030:    231 / 561 symm_mse=10.712, loss=3.95, nll_loss=0.94, ppl=1.92, wps=22968.4, ups=2.2, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1e-05, gnorm=1.506, train_wall=45, wall=8381
2020-12-15 16:23:07 | INFO | train_inner | epoch 030:    331 / 561 symm_mse=10.988, loss=3.987, nll_loss=0.946, ppl=1.93, wps=22955.3, ups=2.21, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1e-05, gnorm=1.54, train_wall=45, wall=8426
2020-12-15 16:23:52 | INFO | train_inner | epoch 030:    431 / 561 symm_mse=10.578, loss=3.929, nll_loss=0.934, ppl=1.91, wps=23371.4, ups=2.2, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1e-05, gnorm=1.486, train_wall=45, wall=8471
2020-12-15 16:24:37 | INFO | train_inner | epoch 030:    531 / 561 symm_mse=10.366, loss=3.91, nll_loss=0.938, ppl=1.92, wps=23224.9, ups=2.21, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1e-05, gnorm=1.445, train_wall=45, wall=8517
2020-12-15 16:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:24:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:24:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:24:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:24:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:24:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:24:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:25:12 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_mse 0 | loss 5.394 | nll_loss 3.851 | ppl 14.43 | bleu 22.68 | wps 4443.3 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 22.77
2020-12-15 16:25:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-15 16:25:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:25:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:25:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 30 @ 16830 updates, score 22.68) (writing took 3.1054048482328653 seconds)
2020-12-15 16:25:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-12-15 16:25:15 | INFO | train | epoch 030 | symm_mse 10.692 | loss 3.947 | nll_loss 0.939 | ppl 1.92 | wps 20954.8 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1e-05 | gnorm 1.502 | train_wall 252 | wall 8555
2020-12-15 16:25:15 | INFO | fairseq.trainer | begin training epoch 31
2020-12-15 16:25:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:25:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:25:49 | INFO | train_inner | epoch 031:     70 / 561 symm_mse=11.126, loss=4.003, nll_loss=0.945, ppl=1.93, wps=14467.3, ups=1.39, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1e-05, gnorm=1.57, train_wall=44, wall=8589
2020-12-15 16:26:35 | INFO | train_inner | epoch 031:    170 / 561 symm_mse=10.291, loss=3.895, nll_loss=0.932, ppl=1.91, wps=22915.8, ups=2.17, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1e-05, gnorm=1.456, train_wall=46, wall=8635
2020-12-15 16:27:22 | INFO | train_inner | epoch 031:    270 / 561 symm_mse=10.616, loss=3.934, nll_loss=0.934, ppl=1.91, wps=22477.7, ups=2.14, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1e-05, gnorm=1.498, train_wall=46, wall=8682
2020-12-15 16:28:08 | INFO | train_inner | epoch 031:    370 / 561 symm_mse=11.26, loss=4.023, nll_loss=0.95, ppl=1.93, wps=22559.5, ups=2.17, wpb=10374.8, bsz=340, num_updates=17200, lr=1e-05, gnorm=1.562, train_wall=46, wall=8728
2020-12-15 16:28:54 | INFO | train_inner | epoch 031:    470 / 561 symm_mse=10.412, loss=3.907, nll_loss=0.931, ppl=1.91, wps=22889.7, ups=2.17, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1e-05, gnorm=1.469, train_wall=46, wall=8774
2020-12-15 16:29:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-15 16:29:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:29:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:29:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-15 16:29:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:29:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-15 16:29:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-15 16:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-15 16:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-15 16:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 186 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
