nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/closer-all
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07'
2021-01-01 11:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:28 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:17196
2021-01-01 11:44:28 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17196
2021-01-01 11:44:28 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17196
2021-01-01 11:44:28 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17196
2021-01-01 11:44:28 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-01 11:44:29 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2021-01-01 11:44:29 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-01 11:44:29 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-01 11:44:33 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:17196', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-01 11:44:33 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-01 11:44:33 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-01 11:44:33 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-01 11:44:33 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-01 11:44:33 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-01 11:44:34 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-01 11:44:34 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-01 11:44:34 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-01 11:44:34 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-01 11:44:34 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-01 11:44:34 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-01 11:44:34 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-01 11:44:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-01 11:44:34 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 11:44:34 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 11:44:34 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 11:44:34 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-01 11:44:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2021-01-01 11:44:34 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2021-01-01 11:44:34 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-01 11:44:34 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-01 11:44:35 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-01 11:44:35 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-01 11:44:35 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-01 11:44:35 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-01 11:44:35 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-01 11:44:35 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-01 11:44:35 | INFO | fairseq.trainer | begin training epoch 1
2021-01-01 11:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:44:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:44:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:45:40 | INFO | train_inner | epoch 001:    100 / 421 symm_kl=1.092, self_kl=0, self_cv=0, loss=4.236, nll_loss=0.844, ppl=1.8, wps=23301.8, ups=1.65, wpb=14145.4, bsz=485.9, num_updates=100, lr=1.43e-06, gnorm=2.409, train_wall=61, wall=66
2021-01-01 11:46:41 | INFO | train_inner | epoch 001:    200 / 421 symm_kl=0.992, self_kl=0, self_cv=0, loss=4.094, nll_loss=0.857, ppl=1.81, wps=22816.4, ups=1.63, wpb=13995.2, bsz=501.2, num_updates=200, lr=2.76e-06, gnorm=2.336, train_wall=61, wall=127
2021-01-01 11:47:43 | INFO | train_inner | epoch 001:    300 / 421 symm_kl=0.874, self_kl=0, self_cv=0, loss=3.941, nll_loss=0.892, ppl=1.86, wps=22671.5, ups=1.62, wpb=13972.8, bsz=508.7, num_updates=300, lr=4.09e-06, gnorm=1.914, train_wall=61, wall=189
2021-01-01 11:48:44 | INFO | train_inner | epoch 001:    400 / 421 symm_kl=0.821, self_kl=0, self_cv=0, loss=3.879, nll_loss=0.903, ppl=1.87, wps=22526.7, ups=1.62, wpb=13886.9, bsz=487.4, num_updates=400, lr=5.42e-06, gnorm=1.556, train_wall=61, wall=250
2021-01-01 11:48:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 11:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:49:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:49:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:49:14 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.677 | nll_loss 4.115 | ppl 17.33 | bleu 22.33 | wps 5951.6 | wpb 10324.2 | bsz 375 | num_updates 421
2021-01-01 11:49:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 11:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:49:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 1 @ 421 updates, score 22.33) (writing took 2.148538548499346 seconds)
2021-01-01 11:49:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-01 11:49:16 | INFO | train | epoch 001 | symm_kl 0.939 | self_kl 0 | self_cv 0 | loss 4.031 | nll_loss 0.876 | ppl 1.84 | wps 21210.6 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 421 | lr 5.6993e-06 | gnorm 2.027 | train_wall 258 | wall 282
2021-01-01 11:49:16 | INFO | fairseq.trainer | begin training epoch 2
2021-01-01 11:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:49:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:49:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:50:08 | INFO | train_inner | epoch 002:     79 / 421 symm_kl=0.788, self_kl=0, self_cv=0, loss=3.836, nll_loss=0.912, ppl=1.88, wps=16584.4, ups=1.2, wpb=13861.1, bsz=487.7, num_updates=500, lr=6.75e-06, gnorm=1.45, train_wall=61, wall=334
2021-01-01 11:51:10 | INFO | train_inner | epoch 002:    179 / 421 symm_kl=0.747, self_kl=0, self_cv=0, loss=3.775, nll_loss=0.915, ppl=1.89, wps=22732.7, ups=1.62, wpb=14007.7, bsz=489.9, num_updates=600, lr=8.08e-06, gnorm=1.355, train_wall=61, wall=396
2021-01-01 11:52:11 | INFO | train_inner | epoch 002:    279 / 421 symm_kl=0.727, self_kl=0, self_cv=0, loss=3.758, nll_loss=0.93, ppl=1.9, wps=22687.7, ups=1.62, wpb=14033.1, bsz=489, num_updates=700, lr=9.41e-06, gnorm=1.304, train_wall=62, wall=457
2021-01-01 11:53:13 | INFO | train_inner | epoch 002:    379 / 421 symm_kl=0.716, self_kl=0, self_cv=0, loss=3.764, nll_loss=0.958, ppl=1.94, wps=22646.9, ups=1.63, wpb=13928.4, bsz=496.1, num_updates=800, lr=1.074e-05, gnorm=1.266, train_wall=61, wall=519
2021-01-01 11:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 11:53:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:53:55 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.545 | nll_loss 4.009 | ppl 16.1 | bleu 22.41 | wps 6341.4 | wpb 10324.2 | bsz 375 | num_updates 842 | best_bleu 22.41
2021-01-01 11:53:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 11:53:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:53:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:53:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 2 @ 842 updates, score 22.41) (writing took 4.66116008721292 seconds)
2021-01-01 11:53:59 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-01 11:53:59 | INFO | train | epoch 002 | symm_kl 0.736 | self_kl 0 | self_cv 0 | loss 3.774 | nll_loss 0.932 | ppl 1.91 | wps 20769.1 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 842 | lr 1.12986e-05 | gnorm 1.327 | train_wall 258 | wall 565
2021-01-01 11:53:59 | INFO | fairseq.trainer | begin training epoch 3
2021-01-01 11:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:54:38 | INFO | train_inner | epoch 003:     58 / 421 symm_kl=0.687, self_kl=0, self_cv=0, loss=3.715, nll_loss=0.952, ppl=1.93, wps=16323.7, ups=1.17, wpb=13895.5, bsz=482.9, num_updates=900, lr=1.207e-05, gnorm=1.234, train_wall=61, wall=604
2021-01-01 11:55:39 | INFO | train_inner | epoch 003:    158 / 421 symm_kl=0.674, self_kl=0, self_cv=0, loss=3.706, nll_loss=0.962, ppl=1.95, wps=22677.7, ups=1.63, wpb=13938, bsz=507, num_updates=1000, lr=1.34e-05, gnorm=1.195, train_wall=61, wall=665
2021-01-01 11:56:41 | INFO | train_inner | epoch 003:    258 / 421 symm_kl=0.668, self_kl=0, self_cv=0, loss=3.706, nll_loss=0.975, ppl=1.97, wps=22714.9, ups=1.63, wpb=13957.9, bsz=482.5, num_updates=1100, lr=1.473e-05, gnorm=1.178, train_wall=61, wall=727
2021-01-01 11:57:43 | INFO | train_inner | epoch 003:    358 / 421 symm_kl=0.645, self_kl=0, self_cv=0, loss=3.668, nll_loss=0.971, ppl=1.96, wps=22884.3, ups=1.62, wpb=14135, bsz=507.8, num_updates=1200, lr=1.606e-05, gnorm=1.131, train_wall=62, wall=789
2021-01-01 11:58:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 11:58:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 11:58:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 11:58:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 11:58:38 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.48 | nll_loss 3.95 | ppl 15.45 | bleu 22.65 | wps 6018.5 | wpb 10324.2 | bsz 375 | num_updates 1263 | best_bleu 22.65
2021-01-01 11:58:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 11:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:58:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 3 @ 1263 updates, score 22.65) (writing took 4.652964115142822 seconds)
2021-01-01 11:58:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-01 11:58:43 | INFO | train | epoch 003 | symm_kl 0.662 | self_kl 0 | self_cv 0 | loss 3.69 | nll_loss 0.967 | ppl 1.95 | wps 20765.4 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 1263 | lr 1.68979e-05 | gnorm 1.172 | train_wall 258 | wall 849
2021-01-01 11:58:43 | INFO | fairseq.trainer | begin training epoch 4
2021-01-01 11:58:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 11:58:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 11:59:08 | INFO | train_inner | epoch 004:     37 / 421 symm_kl=0.643, self_kl=0, self_cv=0, loss=3.669, nll_loss=0.976, ppl=1.97, wps=16250.5, ups=1.17, wpb=13896.1, bsz=467.7, num_updates=1300, lr=1.739e-05, gnorm=1.152, train_wall=61, wall=874
2021-01-01 12:00:10 | INFO | train_inner | epoch 004:    137 / 421 symm_kl=0.629, self_kl=0, self_cv=0, loss=3.65, nll_loss=0.975, ppl=1.97, wps=22312.7, ups=1.62, wpb=13735.4, bsz=496.9, num_updates=1400, lr=1.872e-05, gnorm=1.132, train_wall=61, wall=936
2021-01-01 12:01:11 | INFO | train_inner | epoch 004:    237 / 421 symm_kl=0.62, self_kl=0, self_cv=0, loss=3.641, nll_loss=0.979, ppl=1.97, wps=22905.9, ups=1.63, wpb=14090.6, bsz=475.5, num_updates=1500, lr=2.005e-05, gnorm=1.1, train_wall=61, wall=997
2021-01-01 12:02:13 | INFO | train_inner | epoch 004:    337 / 421 symm_kl=0.605, self_kl=0, self_cv=0, loss=3.619, nll_loss=0.98, ppl=1.97, wps=22896.5, ups=1.63, wpb=14057, bsz=499.7, num_updates=1600, lr=2.138e-05, gnorm=1.072, train_wall=61, wall=1059
2021-01-01 12:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:03:21 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.448 | nll_loss 3.917 | ppl 15.11 | bleu 22.67 | wps 5938.5 | wpb 10324.2 | bsz 375 | num_updates 1684 | best_bleu 22.67
2021-01-01 12:03:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 4 @ 1684 updates, score 22.67) (writing took 4.896917959675193 seconds)
2021-01-01 12:03:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-01 12:03:26 | INFO | train | epoch 004 | symm_kl 0.616 | self_kl 0 | self_cv 0 | loss 3.633 | nll_loss 0.979 | ppl 1.97 | wps 20755.4 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 1684 | lr 2.24972e-05 | gnorm 1.098 | train_wall 258 | wall 1132
2021-01-01 12:03:26 | INFO | fairseq.trainer | begin training epoch 5
2021-01-01 12:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:03:39 | INFO | train_inner | epoch 005:     16 / 421 symm_kl=0.596, self_kl=0, self_cv=0, loss=3.609, nll_loss=0.985, ppl=1.98, wps=16205.7, ups=1.16, wpb=13973, bsz=503, num_updates=1700, lr=2.271e-05, gnorm=1.066, train_wall=61, wall=1145
2021-01-01 12:04:40 | INFO | train_inner | epoch 005:    116 / 421 symm_kl=0.589, self_kl=0, self_cv=0, loss=3.592, nll_loss=0.975, ppl=1.97, wps=22693.7, ups=1.63, wpb=13947.8, bsz=499.7, num_updates=1800, lr=2.404e-05, gnorm=1.059, train_wall=61, wall=1206
2021-01-01 12:05:42 | INFO | train_inner | epoch 005:    216 / 421 symm_kl=0.587, self_kl=0, self_cv=0, loss=3.6, nll_loss=0.989, ppl=1.99, wps=22736.8, ups=1.62, wpb=14054.4, bsz=484.6, num_updates=1900, lr=2.537e-05, gnorm=1.045, train_wall=62, wall=1268
2021-01-01 12:06:44 | INFO | train_inner | epoch 005:    316 / 421 symm_kl=0.58, self_kl=0, self_cv=0, loss=3.595, nll_loss=0.996, ppl=1.99, wps=22577.9, ups=1.62, wpb=13937.1, bsz=501.2, num_updates=2000, lr=2.67e-05, gnorm=1.041, train_wall=62, wall=1330
2021-01-01 12:07:46 | INFO | train_inner | epoch 005:    416 / 421 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.566, nll_loss=0.978, ppl=1.97, wps=22729, ups=1.62, wpb=14006.7, bsz=492.6, num_updates=2100, lr=2.803e-05, gnorm=1.029, train_wall=61, wall=1392
2021-01-01 12:07:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:07:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:07:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:07:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:07:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:07:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:07:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:07:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:07:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:08:06 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.423 | nll_loss 3.894 | ppl 14.87 | bleu 22.62 | wps 5702.8 | wpb 10324.2 | bsz 375 | num_updates 2105 | best_bleu 22.67
2021-01-01 12:08:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:08:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:08:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:08:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:08:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 5 @ 2105 updates, score 22.62) (writing took 2.3712779004126787 seconds)
2021-01-01 12:08:08 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-01 12:08:08 | INFO | train | epoch 005 | symm_kl 0.582 | self_kl 0 | self_cv 0 | loss 3.59 | nll_loss 0.985 | ppl 1.98 | wps 20833.5 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 2105 | lr 2.80965e-05 | gnorm 1.046 | train_wall 258 | wall 1414
2021-01-01 12:08:08 | INFO | fairseq.trainer | begin training epoch 6
2021-01-01 12:08:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:08:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:08:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:08:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:08:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:09:10 | INFO | train_inner | epoch 006:     95 / 421 symm_kl=0.562, self_kl=0, self_cv=0, loss=3.561, nll_loss=0.984, ppl=1.98, wps=16392.9, ups=1.19, wpb=13827.8, bsz=506.6, num_updates=2200, lr=2.936e-05, gnorm=1.022, train_wall=61, wall=1476
2021-01-01 12:10:12 | INFO | train_inner | epoch 006:    195 / 421 symm_kl=0.56, self_kl=0, self_cv=0, loss=3.562, nll_loss=0.989, ppl=1.99, wps=22705.2, ups=1.61, wpb=14087.8, bsz=489.6, num_updates=2300, lr=3.069e-05, gnorm=1.006, train_wall=62, wall=1538
2021-01-01 12:11:13 | INFO | train_inner | epoch 006:    295 / 421 symm_kl=0.557, self_kl=0, self_cv=0, loss=3.567, nll_loss=1, ppl=2, wps=22844.6, ups=1.63, wpb=14036.1, bsz=477.8, num_updates=2400, lr=3.202e-05, gnorm=1.004, train_wall=61, wall=1599
2021-01-01 12:12:15 | INFO | train_inner | epoch 006:    395 / 421 symm_kl=0.542, self_kl=0, self_cv=0, loss=3.533, nll_loss=0.987, ppl=1.98, wps=22610.4, ups=1.62, wpb=13994.5, bsz=506.2, num_updates=2500, lr=3.335e-05, gnorm=0.991, train_wall=62, wall=1661
2021-01-01 12:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:12:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:12:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:12:50 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.397 | nll_loss 3.864 | ppl 14.56 | bleu 22.74 | wps 5009.6 | wpb 10324.2 | bsz 375 | num_updates 2526 | best_bleu 22.74
2021-01-01 12:12:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:12:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 6 @ 2526 updates, score 22.74) (writing took 4.848401710391045 seconds)
2021-01-01 12:12:55 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-01 12:12:55 | INFO | train | epoch 006 | symm_kl 0.555 | self_kl 0 | self_cv 0 | loss 3.557 | nll_loss 0.992 | ppl 1.99 | wps 20509.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 2526 | lr 3.36958e-05 | gnorm 1.006 | train_wall 259 | wall 1701
2021-01-01 12:12:55 | INFO | fairseq.trainer | begin training epoch 7
2021-01-01 12:12:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:13:43 | INFO | train_inner | epoch 007:     74 / 421 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.545, nll_loss=0.998, ppl=2, wps=15613.1, ups=1.13, wpb=13772.9, bsz=486.4, num_updates=2600, lr=3.468e-05, gnorm=1.001, train_wall=61, wall=1749
2021-01-01 12:14:45 | INFO | train_inner | epoch 007:    174 / 421 symm_kl=0.54, self_kl=0, self_cv=0, loss=3.54, nll_loss=0.998, ppl=2, wps=22645.6, ups=1.63, wpb=13910.1, bsz=482.8, num_updates=2700, lr=3.601e-05, gnorm=0.987, train_wall=61, wall=1811
2021-01-01 12:15:46 | INFO | train_inner | epoch 007:    274 / 421 symm_kl=0.535, self_kl=0, self_cv=0, loss=3.531, nll_loss=0.995, ppl=1.99, wps=22572.4, ups=1.63, wpb=13879.3, bsz=492.2, num_updates=2800, lr=3.734e-05, gnorm=0.978, train_wall=61, wall=1872
2021-01-01 12:16:48 | INFO | train_inner | epoch 007:    374 / 421 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.522, nll_loss=0.997, ppl=2, wps=22927.3, ups=1.62, wpb=14151.4, bsz=491.7, num_updates=2900, lr=3.867e-05, gnorm=0.971, train_wall=62, wall=1934
2021-01-01 12:17:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:17:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:17:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:17:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:17:34 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.382 | nll_loss 3.849 | ppl 14.41 | bleu 22.73 | wps 5874.5 | wpb 10324.2 | bsz 375 | num_updates 2947 | best_bleu 22.74
2021-01-01 12:17:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:17:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 7 @ 2947 updates, score 22.73) (writing took 2.5955790486186743 seconds)
2021-01-01 12:17:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-01 12:17:37 | INFO | train | epoch 007 | symm_kl 0.533 | self_kl 0 | self_cv 0 | loss 3.53 | nll_loss 0.996 | ppl 2 | wps 20887.4 | ups 1.5 | wpb 13969.5 | bsz 492.6 | num_updates 2947 | lr 3.92951e-05 | gnorm 0.979 | train_wall 258 | wall 1983
2021-01-01 12:17:37 | INFO | fairseq.trainer | begin training epoch 8
2021-01-01 12:17:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:17:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:17:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:18:12 | INFO | train_inner | epoch 008:     53 / 421 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.507, nll_loss=0.997, ppl=2, wps=16779.1, ups=1.19, wpb=14109.9, bsz=506.2, num_updates=3000, lr=4e-05, gnorm=0.956, train_wall=61, wall=2018
2021-01-01 12:19:14 | INFO | train_inner | epoch 008:    153 / 421 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.515, nll_loss=0.994, ppl=1.99, wps=22655.5, ups=1.62, wpb=13958.9, bsz=475.6, num_updates=3100, lr=3.93496e-05, gnorm=0.965, train_wall=61, wall=2080
2021-01-01 12:20:16 | INFO | train_inner | epoch 008:    253 / 421 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.504, nll_loss=0.999, ppl=2, wps=22327.3, ups=1.61, wpb=13843, bsz=502.1, num_updates=3200, lr=3.87298e-05, gnorm=0.951, train_wall=62, wall=2142
2021-01-01 12:21:18 | INFO | train_inner | epoch 008:    353 / 421 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.484, nll_loss=0.994, ppl=1.99, wps=22756, ups=1.61, wpb=14141.4, bsz=514.8, num_updates=3300, lr=3.81385e-05, gnorm=0.927, train_wall=62, wall=2204
2021-01-01 12:22:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:22:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:22:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:22:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:22:17 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.372 | nll_loss 3.839 | ppl 14.31 | bleu 22.66 | wps 5968.6 | wpb 10324.2 | bsz 375 | num_updates 3368 | best_bleu 22.74
2021-01-01 12:22:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 8 @ 3368 updates, score 22.66) (writing took 2.8908525817096233 seconds)
2021-01-01 12:22:20 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-01 12:22:20 | INFO | train | epoch 008 | symm_kl 0.515 | self_kl 0 | self_cv 0 | loss 3.505 | nll_loss 0.999 | ppl 2 | wps 20782.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 3368 | lr 3.77515e-05 | gnorm 0.949 | train_wall 259 | wall 2266
2021-01-01 12:22:20 | INFO | fairseq.trainer | begin training epoch 9
2021-01-01 12:22:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:22:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:22:42 | INFO | train_inner | epoch 009:     32 / 421 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.515, nll_loss=1.013, ppl=2.02, wps=16377.5, ups=1.19, wpb=13812.4, bsz=478.6, num_updates=3400, lr=3.75735e-05, gnorm=0.951, train_wall=61, wall=2288
2021-01-01 12:23:44 | INFO | train_inner | epoch 009:    132 / 421 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.491, nll_loss=0.996, ppl=1.99, wps=22801.7, ups=1.63, wpb=13981.2, bsz=478.7, num_updates=3500, lr=3.70328e-05, gnorm=0.936, train_wall=61, wall=2350
2021-01-01 12:24:45 | INFO | train_inner | epoch 009:    232 / 421 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.473, nll_loss=0.994, ppl=1.99, wps=22788.7, ups=1.62, wpb=14040.5, bsz=491.4, num_updates=3600, lr=3.65148e-05, gnorm=0.92, train_wall=61, wall=2411
2021-01-01 12:25:47 | INFO | train_inner | epoch 009:    332 / 421 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.001, ppl=2, wps=22502, ups=1.61, wpb=14010.8, bsz=506.9, num_updates=3700, lr=3.6018e-05, gnorm=0.921, train_wall=62, wall=2474
2021-01-01 12:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:26:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:26:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:26:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:26:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:26:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:26:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:26:59 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.362 | nll_loss 3.827 | ppl 14.19 | bleu 22.87 | wps 5968.4 | wpb 10324.2 | bsz 375 | num_updates 3789 | best_bleu 22.87
2021-01-01 12:26:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:27:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:27:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:27:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:27:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 9 @ 3789 updates, score 22.87) (writing took 4.331704441457987 seconds)
2021-01-01 12:27:03 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-01 12:27:03 | INFO | train | epoch 009 | symm_kl 0.499 | self_kl 0 | self_cv 0 | loss 3.483 | nll_loss 0.999 | ppl 2 | wps 20739.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 3789 | lr 3.55925e-05 | gnorm 0.928 | train_wall 258 | wall 2549
2021-01-01 12:27:03 | INFO | fairseq.trainer | begin training epoch 10
2021-01-01 12:27:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:27:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:27:13 | INFO | train_inner | epoch 010:     11 / 421 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.486, nll_loss=1.009, ppl=2.01, wps=16088.1, ups=1.17, wpb=13765.2, bsz=485.4, num_updates=3800, lr=3.55409e-05, gnorm=0.931, train_wall=61, wall=2559
2021-01-01 12:28:14 | INFO | train_inner | epoch 010:    111 / 421 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.471, nll_loss=0.998, ppl=2, wps=23044.3, ups=1.63, wpb=14110.2, bsz=485.5, num_updates=3900, lr=3.50823e-05, gnorm=0.91, train_wall=61, wall=2620
2021-01-01 12:29:16 | INFO | train_inner | epoch 010:    211 / 421 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.003, ppl=2, wps=22445.5, ups=1.61, wpb=13947.9, bsz=478.8, num_updates=4000, lr=3.4641e-05, gnorm=0.92, train_wall=62, wall=2682
2021-01-01 12:30:18 | INFO | train_inner | epoch 010:    311 / 421 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.441, nll_loss=0.987, ppl=1.98, wps=22688.5, ups=1.62, wpb=14032.4, bsz=515.1, num_updates=4100, lr=3.4216e-05, gnorm=0.888, train_wall=62, wall=2744
2021-01-01 12:31:20 | INFO | train_inner | epoch 010:    411 / 421 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.004, ppl=2.01, wps=22527.2, ups=1.62, wpb=13937.4, bsz=501.3, num_updates=4200, lr=3.38062e-05, gnorm=0.9, train_wall=62, wall=2806
2021-01-01 12:31:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:31:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:31:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:31:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:31:43 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.347 | nll_loss 3.813 | ppl 14.05 | bleu 22.83 | wps 5868.3 | wpb 10324.2 | bsz 375 | num_updates 4210 | best_bleu 22.87
2021-01-01 12:31:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:31:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:31:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 10 @ 4210 updates, score 22.83) (writing took 2.8930257577449083 seconds)
2021-01-01 12:31:46 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-01 12:31:46 | INFO | train | epoch 010 | symm_kl 0.487 | self_kl 0 | self_cv 0 | loss 3.465 | nll_loss 0.999 | ppl 2 | wps 20793.8 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 4210 | lr 3.3766e-05 | gnorm 0.907 | train_wall 259 | wall 2832
2021-01-01 12:31:46 | INFO | fairseq.trainer | begin training epoch 11
2021-01-01 12:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:31:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:32:44 | INFO | train_inner | epoch 011:     90 / 421 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.457, nll_loss=0.995, ppl=1.99, wps=16444.7, ups=1.19, wpb=13795, bsz=468.2, num_updates=4300, lr=3.34108e-05, gnorm=0.915, train_wall=61, wall=2890
2021-01-01 12:33:46 | INFO | train_inner | epoch 011:    190 / 421 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.435, nll_loss=0.988, ppl=1.98, wps=22794.5, ups=1.62, wpb=14072.6, bsz=503.6, num_updates=4400, lr=3.30289e-05, gnorm=0.889, train_wall=62, wall=2952
2021-01-01 12:34:47 | INFO | train_inner | epoch 011:    290 / 421 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.021, ppl=2.03, wps=22445.6, ups=1.62, wpb=13821.1, bsz=488.1, num_updates=4500, lr=3.26599e-05, gnorm=0.903, train_wall=61, wall=3013
2021-01-01 12:35:49 | INFO | train_inner | epoch 011:    390 / 421 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.438, nll_loss=0.992, ppl=1.99, wps=22835.7, ups=1.61, wpb=14171.3, bsz=501.5, num_updates=4600, lr=3.23029e-05, gnorm=0.882, train_wall=62, wall=3075
2021-01-01 12:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:36:27 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.347 | nll_loss 3.811 | ppl 14.04 | bleu 22.81 | wps 5173.7 | wpb 10324.2 | bsz 375 | num_updates 4631 | best_bleu 22.87
2021-01-01 12:36:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:36:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 11 @ 4631 updates, score 22.81) (writing took 2.966539043933153 seconds)
2021-01-01 12:36:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-01 12:36:30 | INFO | train | epoch 011 | symm_kl 0.477 | self_kl 0 | self_cv 0 | loss 3.451 | nll_loss 0.998 | ppl 2 | wps 20696.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 4631 | lr 3.21946e-05 | gnorm 0.895 | train_wall 258 | wall 3116
2021-01-01 12:36:30 | INFO | fairseq.trainer | begin training epoch 12
2021-01-01 12:36:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:36:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:37:15 | INFO | train_inner | epoch 012:     69 / 421 symm_kl=0.47, self_kl=0, self_cv=0, loss=3.434, nll_loss=0.99, ppl=1.99, wps=16131, ups=1.16, wpb=13876.4, bsz=488.4, num_updates=4700, lr=3.19574e-05, gnorm=0.886, train_wall=61, wall=3161
2021-01-01 12:38:17 | INFO | train_inner | epoch 012:    169 / 421 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.445, nll_loss=0.999, ppl=2, wps=22454.1, ups=1.62, wpb=13884.5, bsz=497.5, num_updates=4800, lr=3.16228e-05, gnorm=0.886, train_wall=62, wall=3223
2021-01-01 12:39:19 | INFO | train_inner | epoch 012:    269 / 421 symm_kl=0.464, self_kl=0, self_cv=0, loss=3.419, nll_loss=0.984, ppl=1.98, wps=23044, ups=1.62, wpb=14222.4, bsz=501.7, num_updates=4900, lr=3.12984e-05, gnorm=0.867, train_wall=62, wall=3285
2021-01-01 12:40:21 | INFO | train_inner | epoch 012:    369 / 421 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.01, ppl=2.01, wps=22582.6, ups=1.62, wpb=13899.9, bsz=487.4, num_updates=5000, lr=3.09839e-05, gnorm=0.884, train_wall=61, wall=3347
2021-01-01 12:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:41:09 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.337 | nll_loss 3.799 | ppl 13.92 | bleu 22.91 | wps 6349.5 | wpb 10324.2 | bsz 375 | num_updates 5052 | best_bleu 22.91
2021-01-01 12:41:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:41:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:41:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:41:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:41:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 12 @ 5052 updates, score 22.91) (writing took 4.771150138229132 seconds)
2021-01-01 12:41:13 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-01 12:41:13 | INFO | train | epoch 012 | symm_kl 0.469 | self_kl 0 | self_cv 0 | loss 3.437 | nll_loss 0.996 | ppl 1.99 | wps 20756 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 5052 | lr 3.0824e-05 | gnorm 0.881 | train_wall 259 | wall 3399
2021-01-01 12:41:13 | INFO | fairseq.trainer | begin training epoch 13
2021-01-01 12:41:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:41:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:41:46 | INFO | train_inner | epoch 013:     48 / 421 symm_kl=0.468, self_kl=0, self_cv=0, loss=3.432, nll_loss=0.994, ppl=1.99, wps=16298.7, ups=1.17, wpb=13889.5, bsz=482.8, num_updates=5100, lr=3.06786e-05, gnorm=0.886, train_wall=61, wall=3432
2021-01-01 12:42:47 | INFO | train_inner | epoch 013:    148 / 421 symm_kl=0.466, self_kl=0, self_cv=0, loss=3.431, nll_loss=0.993, ppl=1.99, wps=22877.9, ups=1.62, wpb=14105.2, bsz=473, num_updates=5200, lr=3.03822e-05, gnorm=0.874, train_wall=61, wall=3493
2021-01-01 12:43:49 | INFO | train_inner | epoch 013:    248 / 421 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.421, nll_loss=0.993, ppl=1.99, wps=22555, ups=1.62, wpb=13944.9, bsz=504.8, num_updates=5300, lr=3.00942e-05, gnorm=0.872, train_wall=62, wall=3555
2021-01-01 12:44:51 | INFO | train_inner | epoch 013:    348 / 421 symm_kl=0.457, self_kl=0, self_cv=0, loss=3.418, nll_loss=0.993, ppl=1.99, wps=22887.9, ups=1.63, wpb=14073.3, bsz=500.6, num_updates=5400, lr=2.98142e-05, gnorm=0.86, train_wall=61, wall=3617
2021-01-01 12:45:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:45:52 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.326 | nll_loss 3.791 | ppl 13.84 | bleu 22.81 | wps 6059.6 | wpb 10324.2 | bsz 375 | num_updates 5473 | best_bleu 22.91
2021-01-01 12:45:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:45:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:45:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 13 @ 5473 updates, score 22.81) (writing took 2.9130032677203417 seconds)
2021-01-01 12:45:55 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-01 12:45:55 | INFO | train | epoch 013 | symm_kl 0.462 | self_kl 0 | self_cv 0 | loss 3.426 | nll_loss 0.995 | ppl 1.99 | wps 20877.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 5473 | lr 2.96147e-05 | gnorm 0.873 | train_wall 258 | wall 3681
2021-01-01 12:45:55 | INFO | fairseq.trainer | begin training epoch 14
2021-01-01 12:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:46:15 | INFO | train_inner | epoch 014:     27 / 421 symm_kl=0.461, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.002, ppl=2, wps=16431.4, ups=1.19, wpb=13804.5, bsz=491.3, num_updates=5500, lr=2.9542e-05, gnorm=0.88, train_wall=61, wall=3701
2021-01-01 12:47:16 | INFO | train_inner | epoch 014:    127 / 421 symm_kl=0.458, self_kl=0, self_cv=0, loss=3.416, nll_loss=0.989, ppl=1.98, wps=22932.9, ups=1.62, wpb=14140.2, bsz=491.6, num_updates=5600, lr=2.9277e-05, gnorm=0.863, train_wall=61, wall=3762
2021-01-01 12:48:18 | INFO | train_inner | epoch 014:    227 / 421 symm_kl=0.455, self_kl=0, self_cv=0, loss=3.413, nll_loss=0.992, ppl=1.99, wps=22548.4, ups=1.62, wpb=13893.4, bsz=488.5, num_updates=5700, lr=2.90191e-05, gnorm=0.858, train_wall=61, wall=3824
2021-01-01 12:49:20 | INFO | train_inner | epoch 014:    327 / 421 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.432, nll_loss=1.005, ppl=2.01, wps=22497.3, ups=1.62, wpb=13861.2, bsz=485.7, num_updates=5800, lr=2.87678e-05, gnorm=0.873, train_wall=61, wall=3886
2021-01-01 12:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:50:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:50:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:50:36 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.329 | nll_loss 3.79 | ppl 13.84 | bleu 23.02 | wps 5347.1 | wpb 10324.2 | bsz 375 | num_updates 5894 | best_bleu 23.02
2021-01-01 12:50:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 14 @ 5894 updates, score 23.02) (writing took 4.791122799739242 seconds)
2021-01-01 12:50:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-01 12:50:40 | INFO | train | epoch 014 | symm_kl 0.456 | self_kl 0 | self_cv 0 | loss 3.416 | nll_loss 0.994 | ppl 1.99 | wps 20616.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 5894 | lr 2.85375e-05 | gnorm 0.863 | train_wall 258 | wall 3966
2021-01-01 12:50:40 | INFO | fairseq.trainer | begin training epoch 15
2021-01-01 12:50:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:50:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:50:47 | INFO | train_inner | epoch 015:      6 / 421 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.408, nll_loss=0.995, ppl=1.99, wps=15893.7, ups=1.14, wpb=13964.8, bsz=507.2, num_updates=5900, lr=2.8523e-05, gnorm=0.861, train_wall=61, wall=3974
2021-01-01 12:51:49 | INFO | train_inner | epoch 015:    106 / 421 symm_kl=0.454, self_kl=0, self_cv=0, loss=3.418, nll_loss=0.998, ppl=2, wps=22560.3, ups=1.62, wpb=13909.8, bsz=489.7, num_updates=6000, lr=2.82843e-05, gnorm=0.859, train_wall=61, wall=4035
2021-01-01 12:52:51 | INFO | train_inner | epoch 015:    206 / 421 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.386, nll_loss=0.976, ppl=1.97, wps=22763.4, ups=1.62, wpb=14025.9, bsz=508.1, num_updates=6100, lr=2.80515e-05, gnorm=0.857, train_wall=61, wall=4097
2021-01-01 12:53:52 | INFO | train_inner | epoch 015:    306 / 421 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.411, nll_loss=0.997, ppl=2, wps=22644.7, ups=1.62, wpb=13965.9, bsz=486.5, num_updates=6200, lr=2.78243e-05, gnorm=0.848, train_wall=61, wall=4158
2021-01-01 12:54:54 | INFO | train_inner | epoch 015:    406 / 421 symm_kl=0.453, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.006, ppl=2.01, wps=22728.5, ups=1.62, wpb=14000.5, bsz=488.7, num_updates=6300, lr=2.76026e-05, gnorm=0.87, train_wall=61, wall=4220
2021-01-01 12:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:55:20 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.33 | nll_loss 3.79 | ppl 13.83 | bleu 22.94 | wps 5943.6 | wpb 10324.2 | bsz 375 | num_updates 6315 | best_bleu 23.02
2021-01-01 12:55:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 12:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:55:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 15 @ 6315 updates, score 22.94) (writing took 2.8935622591525316 seconds)
2021-01-01 12:55:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-01 12:55:23 | INFO | train | epoch 015 | symm_kl 0.45 | self_kl 0 | self_cv 0 | loss 3.407 | nll_loss 0.993 | ppl 1.99 | wps 20824.2 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 6315 | lr 2.75698e-05 | gnorm 0.859 | train_wall 259 | wall 4249
2021-01-01 12:55:23 | INFO | fairseq.trainer | begin training epoch 16
2021-01-01 12:55:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:55:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:56:18 | INFO | train_inner | epoch 016:     85 / 421 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.389, nll_loss=0.983, ppl=1.98, wps=16523, ups=1.19, wpb=13920.5, bsz=499.7, num_updates=6400, lr=2.73861e-05, gnorm=0.843, train_wall=61, wall=4304
2021-01-01 12:57:20 | INFO | train_inner | epoch 016:    185 / 421 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.405, nll_loss=0.993, ppl=1.99, wps=22645.5, ups=1.62, wpb=13953.5, bsz=496.1, num_updates=6500, lr=2.71746e-05, gnorm=0.853, train_wall=61, wall=4366
2021-01-01 12:58:22 | INFO | train_inner | epoch 016:    285 / 421 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.382, nll_loss=0.982, ppl=1.98, wps=22745.8, ups=1.62, wpb=14059.6, bsz=504.2, num_updates=6600, lr=2.6968e-05, gnorm=0.841, train_wall=62, wall=4428
2021-01-01 12:59:23 | INFO | train_inner | epoch 016:    385 / 421 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.408, nll_loss=0.997, ppl=2, wps=22652.2, ups=1.63, wpb=13933.8, bsz=478.3, num_updates=6700, lr=2.6766e-05, gnorm=0.854, train_wall=61, wall=4489
2021-01-01 12:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 12:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 12:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 12:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 12:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 12:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 12:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:00:04 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.318 | nll_loss 3.78 | ppl 13.74 | bleu 22.77 | wps 5252.5 | wpb 10324.2 | bsz 375 | num_updates 6736 | best_bleu 23.02
2021-01-01 13:00:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:00:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:00:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:00:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:00:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:00:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:00:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:00:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 16 @ 6736 updates, score 22.77) (writing took 2.961532212793827 seconds)
2021-01-01 13:00:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-01 13:00:07 | INFO | train | epoch 016 | symm_kl 0.445 | self_kl 0 | self_cv 0 | loss 3.4 | nll_loss 0.992 | ppl 1.99 | wps 20728.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 6736 | lr 2.66944e-05 | gnorm 0.848 | train_wall 258 | wall 4533
2021-01-01 13:00:07 | INFO | fairseq.trainer | begin training epoch 17
2021-01-01 13:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:00:49 | INFO | train_inner | epoch 017:     64 / 421 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.406, nll_loss=1, ppl=2, wps=16131.2, ups=1.16, wpb=13884.8, bsz=477.7, num_updates=6800, lr=2.65684e-05, gnorm=0.854, train_wall=61, wall=4575
2021-01-01 13:01:51 | INFO | train_inner | epoch 017:    164 / 421 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.358, nll_loss=0.965, ppl=1.95, wps=22937.5, ups=1.62, wpb=14155.6, bsz=509.1, num_updates=6900, lr=2.63752e-05, gnorm=0.831, train_wall=62, wall=4637
2021-01-01 13:02:53 | INFO | train_inner | epoch 017:    264 / 421 symm_kl=0.44, self_kl=0, self_cv=0, loss=3.384, nll_loss=0.982, ppl=1.98, wps=22727.7, ups=1.62, wpb=14019.6, bsz=491.8, num_updates=7000, lr=2.61861e-05, gnorm=0.835, train_wall=61, wall=4699
2021-01-01 13:03:54 | INFO | train_inner | epoch 017:    364 / 421 symm_kl=0.444, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.005, ppl=2.01, wps=22634.1, ups=1.63, wpb=13911.3, bsz=494.4, num_updates=7100, lr=2.60011e-05, gnorm=0.85, train_wall=61, wall=4760
2021-01-01 13:04:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:04:46 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.317 | nll_loss 3.779 | ppl 13.73 | bleu 22.8 | wps 5890.6 | wpb 10324.2 | bsz 375 | num_updates 7157 | best_bleu 23.02
2021-01-01 13:04:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:04:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 17 @ 7157 updates, score 22.8) (writing took 2.913896942511201 seconds)
2021-01-01 13:04:49 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-01 13:04:49 | INFO | train | epoch 017 | symm_kl 0.441 | self_kl 0 | self_cv 0 | loss 3.392 | nll_loss 0.991 | ppl 1.99 | wps 20817.8 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 7157 | lr 2.58973e-05 | gnorm 0.842 | train_wall 259 | wall 4815
2021-01-01 13:04:49 | INFO | fairseq.trainer | begin training epoch 18
2021-01-01 13:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:05:19 | INFO | train_inner | epoch 018:     43 / 421 symm_kl=0.444, self_kl=0, self_cv=0, loss=3.405, nll_loss=1, ppl=2, wps=16442, ups=1.18, wpb=13876, bsz=478.1, num_updates=7200, lr=2.58199e-05, gnorm=0.854, train_wall=61, wall=4845
2021-01-01 13:06:20 | INFO | train_inner | epoch 018:    143 / 421 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.381, nll_loss=0.984, ppl=1.98, wps=22695.4, ups=1.62, wpb=14024.4, bsz=489.7, num_updates=7300, lr=2.56424e-05, gnorm=0.838, train_wall=62, wall=4906
2021-01-01 13:07:22 | INFO | train_inner | epoch 018:    243 / 421 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.386, nll_loss=0.988, ppl=1.98, wps=22359.6, ups=1.62, wpb=13842.7, bsz=494.3, num_updates=7400, lr=2.54686e-05, gnorm=0.84, train_wall=62, wall=4968
2021-01-01 13:08:24 | INFO | train_inner | epoch 018:    343 / 421 symm_kl=0.434, self_kl=0, self_cv=0, loss=3.381, nll_loss=0.99, ppl=1.99, wps=22839, ups=1.62, wpb=14074.6, bsz=501.3, num_updates=7500, lr=2.52982e-05, gnorm=0.825, train_wall=61, wall=5030
2021-01-01 13:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:09:29 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.313 | nll_loss 3.775 | ppl 13.69 | bleu 22.84 | wps 5959.8 | wpb 10324.2 | bsz 375 | num_updates 7578 | best_bleu 23.02
2021-01-01 13:09:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 18 @ 7578 updates, score 22.84) (writing took 2.9966424591839314 seconds)
2021-01-01 13:09:32 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-01 13:09:32 | INFO | train | epoch 018 | symm_kl 0.437 | self_kl 0 | self_cv 0 | loss 3.385 | nll_loss 0.989 | ppl 1.99 | wps 20814.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 7578 | lr 2.51677e-05 | gnorm 0.839 | train_wall 259 | wall 5098
2021-01-01 13:09:32 | INFO | fairseq.trainer | begin training epoch 19
2021-01-01 13:09:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:09:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:09:48 | INFO | train_inner | epoch 019:     22 / 421 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.393, nll_loss=0.998, ppl=2, wps=16380.7, ups=1.18, wpb=13829.2, bsz=486.7, num_updates=7600, lr=2.51312e-05, gnorm=0.846, train_wall=61, wall=5114
2021-01-01 13:10:50 | INFO | train_inner | epoch 019:    122 / 421 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.372, nll_loss=0.982, ppl=1.98, wps=22794.3, ups=1.62, wpb=14075, bsz=504.1, num_updates=7700, lr=2.49675e-05, gnorm=0.829, train_wall=62, wall=5176
2021-01-01 13:11:52 | INFO | train_inner | epoch 019:    222 / 421 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.376, nll_loss=0.984, ppl=1.98, wps=22794.8, ups=1.63, wpb=14017.6, bsz=488.3, num_updates=7800, lr=2.48069e-05, gnorm=0.826, train_wall=61, wall=5238
2021-01-01 13:12:53 | INFO | train_inner | epoch 019:    322 / 421 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.392, nll_loss=0.996, ppl=1.99, wps=22661.9, ups=1.63, wpb=13925.5, bsz=485.6, num_updates=7900, lr=2.46494e-05, gnorm=0.832, train_wall=61, wall=5299
2021-01-01 13:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:13:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:13:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:13:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:13:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:14:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:14:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:14:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:14:10 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.313 | nll_loss 3.775 | ppl 13.69 | bleu 22.84 | wps 6028.7 | wpb 10324.2 | bsz 375 | num_updates 7999 | best_bleu 23.02
2021-01-01 13:14:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:14:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:14:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:14:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:14:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 19 @ 7999 updates, score 22.84) (writing took 2.3913080617785454 seconds)
2021-01-01 13:14:13 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-01 13:14:13 | INFO | train | epoch 019 | symm_kl 0.433 | self_kl 0 | self_cv 0 | loss 3.379 | nll_loss 0.988 | ppl 1.98 | wps 20925.1 | ups 1.5 | wpb 13969.5 | bsz 492.6 | num_updates 7999 | lr 2.44964e-05 | gnorm 0.835 | train_wall 258 | wall 5379
2021-01-01 13:14:13 | INFO | fairseq.trainer | begin training epoch 20
2021-01-01 13:14:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:14:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:14:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:14:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:14:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:14:17 | INFO | train_inner | epoch 020:      1 / 421 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.375, nll_loss=0.99, ppl=1.99, wps=16658.4, ups=1.2, wpb=13936.5, bsz=492.6, num_updates=8000, lr=2.44949e-05, gnorm=0.852, train_wall=61, wall=5383
2021-01-01 13:15:18 | INFO | train_inner | epoch 020:    101 / 421 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.376, nll_loss=0.987, ppl=1.98, wps=22865.5, ups=1.63, wpb=14022.1, bsz=493.9, num_updates=8100, lr=2.43432e-05, gnorm=0.82, train_wall=61, wall=5444
2021-01-01 13:16:20 | INFO | train_inner | epoch 020:    201 / 421 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.367, nll_loss=0.982, ppl=1.97, wps=22545.2, ups=1.61, wpb=13989.7, bsz=488, num_updates=8200, lr=2.41943e-05, gnorm=0.831, train_wall=62, wall=5506
2021-01-01 13:17:21 | INFO | train_inner | epoch 020:    301 / 421 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.38, nll_loss=0.993, ppl=1.99, wps=23001.4, ups=1.63, wpb=14118.4, bsz=497.3, num_updates=8300, lr=2.40481e-05, gnorm=0.817, train_wall=61, wall=5567
2021-01-01 13:18:23 | INFO | train_inner | epoch 020:    401 / 421 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.378, nll_loss=0.991, ppl=1.99, wps=22311.5, ups=1.62, wpb=13791.4, bsz=493.2, num_updates=8400, lr=2.39046e-05, gnorm=0.835, train_wall=62, wall=5629
2021-01-01 13:18:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:18:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:18:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:18:52 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.313 | nll_loss 3.775 | ppl 13.69 | bleu 23.03 | wps 6032.5 | wpb 10324.2 | bsz 375 | num_updates 8420 | best_bleu 23.03
2021-01-01 13:18:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:18:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 20 @ 8420 updates, score 23.03) (writing took 4.302384275943041 seconds)
2021-01-01 13:18:56 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-01 13:18:56 | INFO | train | epoch 020 | symm_kl 0.43 | self_kl 0 | self_cv 0 | loss 3.374 | nll_loss 0.987 | ppl 1.98 | wps 20726.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 8420 | lr 2.38762e-05 | gnorm 0.827 | train_wall 259 | wall 5662
2021-01-01 13:18:56 | INFO | fairseq.trainer | begin training epoch 21
2021-01-01 13:18:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:18:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:19:49 | INFO | train_inner | epoch 021:     80 / 421 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.378, nll_loss=0.993, ppl=1.99, wps=16051.6, ups=1.17, wpb=13696.4, bsz=490.3, num_updates=8500, lr=2.37635e-05, gnorm=0.841, train_wall=61, wall=5715
2021-01-01 13:20:50 | INFO | train_inner | epoch 021:    180 / 421 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.366, nll_loss=0.983, ppl=1.98, wps=22760, ups=1.62, wpb=14078.8, bsz=487.9, num_updates=8600, lr=2.3625e-05, gnorm=0.824, train_wall=62, wall=5776
2021-01-01 13:21:52 | INFO | train_inner | epoch 021:    280 / 421 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.342, nll_loss=0.97, ppl=1.96, wps=22947.9, ups=1.62, wpb=14138.2, bsz=508.5, num_updates=8700, lr=2.34888e-05, gnorm=0.799, train_wall=61, wall=5838
2021-01-01 13:22:53 | INFO | train_inner | epoch 021:    380 / 421 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.376, nll_loss=0.992, ppl=1.99, wps=22813.1, ups=1.63, wpb=14011.6, bsz=493.4, num_updates=8800, lr=2.3355e-05, gnorm=0.822, train_wall=61, wall=5899
2021-01-01 13:23:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:23:35 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.308 | nll_loss 3.768 | ppl 13.62 | bleu 23 | wps 6014.6 | wpb 10324.2 | bsz 375 | num_updates 8841 | best_bleu 23.03
2021-01-01 13:23:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:23:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:23:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 21 @ 8841 updates, score 23.0) (writing took 2.9551257006824017 seconds)
2021-01-01 13:23:38 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-01 13:23:38 | INFO | train | epoch 021 | symm_kl 0.427 | self_kl 0 | self_cv 0 | loss 3.368 | nll_loss 0.987 | ppl 1.98 | wps 20880 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 8841 | lr 2.33008e-05 | gnorm 0.822 | train_wall 258 | wall 5944
2021-01-01 13:23:38 | INFO | fairseq.trainer | begin training epoch 22
2021-01-01 13:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:24:17 | INFO | train_inner | epoch 022:     59 / 421 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.373, nll_loss=0.991, ppl=1.99, wps=16599.1, ups=1.19, wpb=13937.8, bsz=479, num_updates=8900, lr=2.32234e-05, gnorm=0.819, train_wall=61, wall=5983
2021-01-01 13:25:19 | INFO | train_inner | epoch 022:    159 / 421 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.345, nll_loss=0.974, ppl=1.96, wps=22707.2, ups=1.62, wpb=14049.6, bsz=507.4, num_updates=9000, lr=2.3094e-05, gnorm=0.804, train_wall=62, wall=6045
2021-01-01 13:26:21 | INFO | train_inner | epoch 022:    259 / 421 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.374, nll_loss=0.993, ppl=1.99, wps=22621.4, ups=1.63, wpb=13902.5, bsz=489.7, num_updates=9100, lr=2.29668e-05, gnorm=0.827, train_wall=61, wall=6107
2021-01-01 13:27:22 | INFO | train_inner | epoch 022:    359 / 421 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.36, nll_loss=0.983, ppl=1.98, wps=22805.6, ups=1.63, wpb=14032.6, bsz=487.8, num_updates=9200, lr=2.28416e-05, gnorm=0.814, train_wall=61, wall=6168
2021-01-01 13:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:28:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:28:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:28:17 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.308 | nll_loss 3.766 | ppl 13.61 | bleu 22.98 | wps 6043.2 | wpb 10324.2 | bsz 375 | num_updates 9262 | best_bleu 23.03
2021-01-01 13:28:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 22 @ 9262 updates, score 22.98) (writing took 2.9172968324273825 seconds)
2021-01-01 13:28:20 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-01 13:28:20 | INFO | train | epoch 022 | symm_kl 0.424 | self_kl 0 | self_cv 0 | loss 3.363 | nll_loss 0.985 | ppl 1.98 | wps 20867.8 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 9262 | lr 2.2765e-05 | gnorm 0.816 | train_wall 258 | wall 6226
2021-01-01 13:28:20 | INFO | fairseq.trainer | begin training epoch 23
2021-01-01 13:28:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:28:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:28:46 | INFO | train_inner | epoch 023:     38 / 421 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.388, nll_loss=1, ppl=2, wps=16491.4, ups=1.19, wpb=13834.9, bsz=470.7, num_updates=9300, lr=2.27185e-05, gnorm=0.832, train_wall=61, wall=6252
2021-01-01 13:29:48 | INFO | train_inner | epoch 023:    138 / 421 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.356, nll_loss=0.983, ppl=1.98, wps=22529.2, ups=1.61, wpb=13964.3, bsz=496.2, num_updates=9400, lr=2.25973e-05, gnorm=0.81, train_wall=62, wall=6314
2021-01-01 13:30:50 | INFO | train_inner | epoch 023:    238 / 421 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.371, nll_loss=0.994, ppl=1.99, wps=22682.8, ups=1.62, wpb=13991.9, bsz=478, num_updates=9500, lr=2.24781e-05, gnorm=0.82, train_wall=61, wall=6376
2021-01-01 13:31:51 | INFO | train_inner | epoch 023:    338 / 421 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.35, nll_loss=0.979, ppl=1.97, wps=22748.4, ups=1.63, wpb=13972.2, bsz=506.8, num_updates=9600, lr=2.23607e-05, gnorm=0.806, train_wall=61, wall=6437
2021-01-01 13:32:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:32:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:32:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:32:59 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.303 | nll_loss 3.765 | ppl 13.59 | bleu 22.8 | wps 6037.7 | wpb 10324.2 | bsz 375 | num_updates 9683 | best_bleu 23.03
2021-01-01 13:32:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:33:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:33:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:33:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 23 @ 9683 updates, score 22.8) (writing took 3.0521272234618664 seconds)
2021-01-01 13:33:02 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-01 13:33:02 | INFO | train | epoch 023 | symm_kl 0.421 | self_kl 0 | self_cv 0 | loss 3.358 | nll_loss 0.984 | ppl 1.98 | wps 20837.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 9683 | lr 2.22646e-05 | gnorm 0.813 | train_wall 258 | wall 6508
2021-01-01 13:33:02 | INFO | fairseq.trainer | begin training epoch 24
2021-01-01 13:33:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:33:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:33:16 | INFO | train_inner | epoch 024:     17 / 421 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.339, nll_loss=0.973, ppl=1.96, wps=16526.8, ups=1.18, wpb=13990.7, bsz=508.2, num_updates=9700, lr=2.22451e-05, gnorm=0.801, train_wall=61, wall=6522
2021-01-01 13:34:18 | INFO | train_inner | epoch 024:    117 / 421 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.358, nll_loss=0.983, ppl=1.98, wps=22728.9, ups=1.62, wpb=14043.6, bsz=489.9, num_updates=9800, lr=2.21313e-05, gnorm=0.815, train_wall=62, wall=6584
2021-01-01 13:35:20 | INFO | train_inner | epoch 024:    217 / 421 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.346, nll_loss=0.976, ppl=1.97, wps=22602.9, ups=1.62, wpb=13992.3, bsz=488.8, num_updates=9900, lr=2.20193e-05, gnorm=0.813, train_wall=62, wall=6646
2021-01-01 13:36:21 | INFO | train_inner | epoch 024:    317 / 421 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.995, ppl=1.99, wps=22227, ups=1.62, wpb=13726.7, bsz=487.9, num_updates=10000, lr=2.19089e-05, gnorm=0.819, train_wall=62, wall=6707
2021-01-01 13:37:23 | INFO | train_inner | epoch 024:    417 / 421 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.346, nll_loss=0.982, ppl=1.97, wps=22853.3, ups=1.61, wpb=14165.6, bsz=504.5, num_updates=10100, lr=2.18002e-05, gnorm=0.8, train_wall=62, wall=6769
2021-01-01 13:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:37:43 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.306 | nll_loss 3.766 | ppl 13.61 | bleu 22.98 | wps 5953.5 | wpb 10324.2 | bsz 375 | num_updates 10104 | best_bleu 23.03
2021-01-01 13:37:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:37:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 24 @ 10104 updates, score 22.98) (writing took 2.9338782783597708 seconds)
2021-01-01 13:37:46 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-01 13:37:46 | INFO | train | epoch 024 | symm_kl 0.418 | self_kl 0 | self_cv 0 | loss 3.354 | nll_loss 0.984 | ppl 1.98 | wps 20754.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10104 | lr 2.17959e-05 | gnorm 0.812 | train_wall 260 | wall 6792
2021-01-01 13:37:46 | INFO | fairseq.trainer | begin training epoch 25
2021-01-01 13:37:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:37:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:38:48 | INFO | train_inner | epoch 025:     96 / 421 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.343, nll_loss=0.972, ppl=1.96, wps=16487, ups=1.18, wpb=13965.8, bsz=477.5, num_updates=10200, lr=2.1693e-05, gnorm=0.82, train_wall=62, wall=6854
2021-01-01 13:39:50 | INFO | train_inner | epoch 025:    196 / 421 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.356, nll_loss=0.988, ppl=1.98, wps=22559.9, ups=1.62, wpb=13916.1, bsz=500.1, num_updates=10300, lr=2.15875e-05, gnorm=0.808, train_wall=61, wall=6916
2021-01-01 13:40:51 | INFO | train_inner | epoch 025:    296 / 421 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.356, nll_loss=0.988, ppl=1.98, wps=22815.2, ups=1.62, wpb=14064.9, bsz=487, num_updates=10400, lr=2.14834e-05, gnorm=0.805, train_wall=61, wall=6977
2021-01-01 13:41:53 | INFO | train_inner | epoch 025:    396 / 421 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.343, nll_loss=0.981, ppl=1.97, wps=22659.4, ups=1.62, wpb=14008.2, bsz=508.2, num_updates=10500, lr=2.13809e-05, gnorm=0.797, train_wall=62, wall=7039
2021-01-01 13:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:42:26 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.301 | nll_loss 3.762 | ppl 13.57 | bleu 22.88 | wps 5883.5 | wpb 10324.2 | bsz 375 | num_updates 10525 | best_bleu 23.03
2021-01-01 13:42:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:42:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 25 @ 10525 updates, score 22.88) (writing took 2.914011560380459 seconds)
2021-01-01 13:42:28 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-01 13:42:28 | INFO | train | epoch 025 | symm_kl 0.416 | self_kl 0 | self_cv 0 | loss 3.35 | nll_loss 0.983 | ppl 1.98 | wps 20786.5 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10525 | lr 2.13555e-05 | gnorm 0.808 | train_wall 259 | wall 7074
2021-01-01 13:42:28 | INFO | fairseq.trainer | begin training epoch 26
2021-01-01 13:42:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:42:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:42:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:43:18 | INFO | train_inner | epoch 026:     75 / 421 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.331, nll_loss=0.97, ppl=1.96, wps=16313.5, ups=1.18, wpb=13766.8, bsz=507.2, num_updates=10600, lr=2.12798e-05, gnorm=0.81, train_wall=61, wall=7124
2021-01-01 13:44:19 | INFO | train_inner | epoch 026:    175 / 421 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.338, nll_loss=0.975, ppl=1.97, wps=22834.9, ups=1.62, wpb=14063.1, bsz=499.9, num_updates=10700, lr=2.11801e-05, gnorm=0.797, train_wall=61, wall=7185
2021-01-01 13:45:21 | INFO | train_inner | epoch 026:    275 / 421 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.351, nll_loss=0.986, ppl=1.98, wps=22815, ups=1.62, wpb=14095.9, bsz=494.4, num_updates=10800, lr=2.10819e-05, gnorm=0.794, train_wall=62, wall=7247
2021-01-01 13:46:23 | INFO | train_inner | epoch 026:    375 / 421 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.358, nll_loss=0.99, ppl=1.99, wps=22369.6, ups=1.61, wpb=13928.6, bsz=473.6, num_updates=10900, lr=2.09849e-05, gnorm=0.812, train_wall=62, wall=7309
2021-01-01 13:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:46:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:46:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:46:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:46:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:46:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:47:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:47:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:47:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:47:08 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.3 | nll_loss 3.761 | ppl 13.55 | bleu 22.97 | wps 6246.4 | wpb 10324.2 | bsz 375 | num_updates 10946 | best_bleu 23.03
2021-01-01 13:47:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:47:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:47:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:47:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:47:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:47:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 26 @ 10946 updates, score 22.97) (writing took 2.907202582806349 seconds)
2021-01-01 13:47:11 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-01 13:47:11 | INFO | train | epoch 026 | symm_kl 0.414 | self_kl 0 | self_cv 0 | loss 3.346 | nll_loss 0.982 | ppl 1.97 | wps 20829 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 10946 | lr 2.09408e-05 | gnorm 0.803 | train_wall 259 | wall 7357
2021-01-01 13:47:11 | INFO | fairseq.trainer | begin training epoch 27
2021-01-01 13:47:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:47:47 | INFO | train_inner | epoch 027:     54 / 421 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.992, ppl=1.99, wps=16547.7, ups=1.19, wpb=13886.4, bsz=483.5, num_updates=11000, lr=2.08893e-05, gnorm=0.806, train_wall=61, wall=7393
2021-01-01 13:48:49 | INFO | train_inner | epoch 027:    154 / 421 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.347, nll_loss=0.981, ppl=1.97, wps=22591.4, ups=1.61, wpb=14008.5, bsz=483.7, num_updates=11100, lr=2.0795e-05, gnorm=0.801, train_wall=62, wall=7455
2021-01-01 13:49:51 | INFO | train_inner | epoch 027:    254 / 421 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.334, nll_loss=0.978, ppl=1.97, wps=22822.6, ups=1.62, wpb=14065.1, bsz=498.8, num_updates=11200, lr=2.0702e-05, gnorm=0.793, train_wall=61, wall=7517
2021-01-01 13:50:53 | INFO | train_inner | epoch 027:    354 / 421 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.338, nll_loss=0.978, ppl=1.97, wps=22528, ups=1.62, wpb=13936.2, bsz=501.4, num_updates=11300, lr=2.06102e-05, gnorm=0.8, train_wall=62, wall=7579
2021-01-01 13:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:51:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:51:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:51:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:51:50 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.296 | nll_loss 3.756 | ppl 13.51 | bleu 23.11 | wps 6074 | wpb 10324.2 | bsz 375 | num_updates 11367 | best_bleu 23.11
2021-01-01 13:51:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:51:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:51:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 27 @ 11367 updates, score 23.11) (writing took 4.871380232274532 seconds)
2021-01-01 13:51:55 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-01 13:51:55 | INFO | train | epoch 027 | symm_kl 0.412 | self_kl 0 | self_cv 0 | loss 3.343 | nll_loss 0.982 | ppl 1.97 | wps 20670.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 11367 | lr 2.05493e-05 | gnorm 0.801 | train_wall 259 | wall 7641
2021-01-01 13:51:55 | INFO | fairseq.trainer | begin training epoch 28
2021-01-01 13:51:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:51:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:52:19 | INFO | train_inner | epoch 028:     33 / 421 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.973, ppl=1.96, wps=15993, ups=1.16, wpb=13736.5, bsz=489.8, num_updates=11400, lr=2.05196e-05, gnorm=0.813, train_wall=61, wall=7665
2021-01-01 13:53:20 | INFO | train_inner | epoch 028:    133 / 421 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.343, nll_loss=0.981, ppl=1.97, wps=22680.9, ups=1.62, wpb=13970.3, bsz=473.9, num_updates=11500, lr=2.04302e-05, gnorm=0.795, train_wall=61, wall=7726
2021-01-01 13:54:22 | INFO | train_inner | epoch 028:    233 / 421 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.332, nll_loss=0.974, ppl=1.96, wps=22801.5, ups=1.62, wpb=14051.6, bsz=492.8, num_updates=11600, lr=2.03419e-05, gnorm=0.788, train_wall=61, wall=7788
2021-01-01 13:55:24 | INFO | train_inner | epoch 028:    333 / 421 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.348, nll_loss=0.992, ppl=1.99, wps=22722.3, ups=1.62, wpb=14062.4, bsz=500.3, num_updates=11700, lr=2.02548e-05, gnorm=0.785, train_wall=62, wall=7850
2021-01-01 13:56:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 13:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 13:56:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 13:56:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 13:56:34 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.297 | nll_loss 3.756 | ppl 13.51 | bleu 23.09 | wps 6318.7 | wpb 10324.2 | bsz 375 | num_updates 11788 | best_bleu 23.11
2021-01-01 13:56:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 13:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 28 @ 11788 updates, score 23.09) (writing took 2.943541906774044 seconds)
2021-01-01 13:56:37 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-01 13:56:37 | INFO | train | epoch 028 | symm_kl 0.409 | self_kl 0 | self_cv 0 | loss 3.338 | nll_loss 0.98 | ppl 1.97 | wps 20877.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 11788 | lr 2.0179e-05 | gnorm 0.794 | train_wall 259 | wall 7923
2021-01-01 13:56:37 | INFO | fairseq.trainer | begin training epoch 29
2021-01-01 13:56:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 13:56:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 13:56:48 | INFO | train_inner | epoch 029:     12 / 421 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.342, nll_loss=0.985, ppl=1.98, wps=16528.1, ups=1.19, wpb=13890.5, bsz=495.7, num_updates=11800, lr=2.01688e-05, gnorm=0.806, train_wall=62, wall=7934
2021-01-01 13:57:49 | INFO | train_inner | epoch 029:    112 / 421 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.977, ppl=1.97, wps=22965.4, ups=1.63, wpb=14129.1, bsz=498.7, num_updates=11900, lr=2.00839e-05, gnorm=0.794, train_wall=61, wall=7995
2021-01-01 13:58:51 | INFO | train_inner | epoch 029:    212 / 421 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.339, nll_loss=0.979, ppl=1.97, wps=22809.9, ups=1.63, wpb=14014.8, bsz=486.4, num_updates=12000, lr=2e-05, gnorm=0.804, train_wall=61, wall=8057
2021-01-01 13:59:52 | INFO | train_inner | epoch 029:    312 / 421 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.978, ppl=1.97, wps=22565.3, ups=1.62, wpb=13916.7, bsz=507.4, num_updates=12100, lr=1.99172e-05, gnorm=0.788, train_wall=61, wall=8118
2021-01-01 14:00:54 | INFO | train_inner | epoch 029:    412 / 421 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.345, nll_loss=0.986, ppl=1.98, wps=22353.5, ups=1.61, wpb=13860.4, bsz=482.7, num_updates=12200, lr=1.98354e-05, gnorm=0.805, train_wall=62, wall=8180
2021-01-01 14:01:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:01:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:01:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:01:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:01:17 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.295 | nll_loss 3.754 | ppl 13.5 | bleu 23.08 | wps 5953.5 | wpb 10324.2 | bsz 375 | num_updates 12209 | best_bleu 23.11
2021-01-01 14:01:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:01:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:01:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 29 @ 12209 updates, score 23.08) (writing took 2.961985332891345 seconds)
2021-01-01 14:01:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-01 14:01:20 | INFO | train | epoch 029 | symm_kl 0.408 | self_kl 0 | self_cv 0 | loss 3.335 | nll_loss 0.98 | ppl 1.97 | wps 20803 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 12209 | lr 1.98281e-05 | gnorm 0.798 | train_wall 259 | wall 8206
2021-01-01 14:01:20 | INFO | fairseq.trainer | begin training epoch 30
2021-01-01 14:01:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:01:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:02:19 | INFO | train_inner | epoch 030:     91 / 421 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.969, ppl=1.96, wps=16561.7, ups=1.18, wpb=14010.2, bsz=498.8, num_updates=12300, lr=1.97546e-05, gnorm=0.784, train_wall=61, wall=8265
2021-01-01 14:03:21 | INFO | train_inner | epoch 030:    191 / 421 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.327, nll_loss=0.976, ppl=1.97, wps=22513.3, ups=1.62, wpb=13933.2, bsz=483.3, num_updates=12400, lr=1.96748e-05, gnorm=0.792, train_wall=62, wall=8327
2021-01-01 14:04:23 | INFO | train_inner | epoch 030:    291 / 421 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.342, nll_loss=0.985, ppl=1.98, wps=22468.2, ups=1.61, wpb=13950.6, bsz=497.5, num_updates=12500, lr=1.95959e-05, gnorm=0.799, train_wall=62, wall=8389
2021-01-01 14:05:25 | INFO | train_inner | epoch 030:    391 / 421 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.979, ppl=1.97, wps=22498.7, ups=1.6, wpb=14044.2, bsz=493.8, num_updates=12600, lr=1.9518e-05, gnorm=0.786, train_wall=62, wall=8451
2021-01-01 14:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:06:00 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.298 | nll_loss 3.756 | ppl 13.51 | bleu 23.09 | wps 5964.8 | wpb 10324.2 | bsz 375 | num_updates 12630 | best_bleu 23.11
2021-01-01 14:06:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:06:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:06:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:06:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:06:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:06:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:06:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:06:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 30 @ 12630 updates, score 23.09) (writing took 2.882099749520421 seconds)
2021-01-01 14:06:03 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-01 14:06:03 | INFO | train | epoch 030 | symm_kl 0.406 | self_kl 0 | self_cv 0 | loss 3.331 | nll_loss 0.979 | ppl 1.97 | wps 20739.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 12630 | lr 1.94948e-05 | gnorm 0.792 | train_wall 260 | wall 8489
2021-01-01 14:06:03 | INFO | fairseq.trainer | begin training epoch 31
2021-01-01 14:06:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:06:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:06:49 | INFO | train_inner | epoch 031:     70 / 421 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.345, nll_loss=0.987, ppl=1.98, wps=16513.8, ups=1.19, wpb=13873.1, bsz=481.6, num_updates=12700, lr=1.9441e-05, gnorm=0.806, train_wall=61, wall=8535
2021-01-01 14:07:51 | INFO | train_inner | epoch 031:    170 / 421 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.315, nll_loss=0.97, ppl=1.96, wps=22804.1, ups=1.62, wpb=14050.2, bsz=507.7, num_updates=12800, lr=1.93649e-05, gnorm=0.782, train_wall=61, wall=8597
2021-01-01 14:08:53 | INFO | train_inner | epoch 031:    270 / 421 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.339, nll_loss=0.984, ppl=1.98, wps=22358.9, ups=1.61, wpb=13924.8, bsz=465.5, num_updates=12900, lr=1.92897e-05, gnorm=0.799, train_wall=62, wall=8659
2021-01-01 14:09:55 | INFO | train_inner | epoch 031:    370 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.32, nll_loss=0.973, ppl=1.96, wps=22571.5, ups=1.61, wpb=14000.3, bsz=501.4, num_updates=13000, lr=1.92154e-05, gnorm=0.784, train_wall=62, wall=8721
2021-01-01 14:10:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:10:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:10:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:10:44 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.295 | nll_loss 3.755 | ppl 13.5 | bleu 23.1 | wps 5831.3 | wpb 10324.2 | bsz 375 | num_updates 13051 | best_bleu 23.11
2021-01-01 14:10:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:10:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 31 @ 13051 updates, score 23.1) (writing took 2.9115008376538754 seconds)
2021-01-01 14:10:47 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-01 14:10:47 | INFO | train | epoch 031 | symm_kl 0.404 | self_kl 0 | self_cv 0 | loss 3.328 | nll_loss 0.978 | ppl 1.97 | wps 20722.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 13051 | lr 1.91778e-05 | gnorm 0.791 | train_wall 260 | wall 8773
2021-01-01 14:10:47 | INFO | fairseq.trainer | begin training epoch 32
2021-01-01 14:10:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:10:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:11:20 | INFO | train_inner | epoch 032:     49 / 421 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.333, nll_loss=0.984, ppl=1.98, wps=16162.9, ups=1.18, wpb=13736.1, bsz=497.8, num_updates=13100, lr=1.91419e-05, gnorm=0.796, train_wall=62, wall=8806
2021-01-01 14:12:22 | INFO | train_inner | epoch 032:    149 / 421 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.316, nll_loss=0.971, ppl=1.96, wps=22964.1, ups=1.62, wpb=14205.9, bsz=491.4, num_updates=13200, lr=1.90693e-05, gnorm=0.77, train_wall=62, wall=8868
2021-01-01 14:13:24 | INFO | train_inner | epoch 032:    249 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.316, nll_loss=0.97, ppl=1.96, wps=22767.6, ups=1.62, wpb=14086.9, bsz=488.9, num_updates=13300, lr=1.89974e-05, gnorm=0.786, train_wall=62, wall=8930
2021-01-01 14:14:26 | INFO | train_inner | epoch 032:    349 / 421 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.338, nll_loss=0.982, ppl=1.98, wps=22382.4, ups=1.62, wpb=13848.6, bsz=472.5, num_updates=13400, lr=1.89264e-05, gnorm=0.808, train_wall=62, wall=8992
2021-01-01 14:15:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:15:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:15:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:15:27 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.294 | nll_loss 3.753 | ppl 13.48 | bleu 22.98 | wps 6011.6 | wpb 10324.2 | bsz 375 | num_updates 13472 | best_bleu 23.11
2021-01-01 14:15:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 32 @ 13472 updates, score 22.98) (writing took 2.877210147678852 seconds)
2021-01-01 14:15:30 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-01 14:15:30 | INFO | train | epoch 032 | symm_kl 0.402 | self_kl 0 | self_cv 0 | loss 3.325 | nll_loss 0.977 | ppl 1.97 | wps 20797 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 13472 | lr 1.88758e-05 | gnorm 0.789 | train_wall 259 | wall 9056
2021-01-01 14:15:30 | INFO | fairseq.trainer | begin training epoch 33
2021-01-01 14:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:15:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:15:50 | INFO | train_inner | epoch 033:     28 / 421 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.326, nll_loss=0.983, ppl=1.98, wps=16469.8, ups=1.18, wpb=13913.9, bsz=525.4, num_updates=13500, lr=1.88562e-05, gnorm=0.785, train_wall=61, wall=9076
2021-01-01 14:16:52 | INFO | train_inner | epoch 033:    128 / 421 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.31, nll_loss=0.969, ppl=1.96, wps=22675, ups=1.62, wpb=14000, bsz=500, num_updates=13600, lr=1.87867e-05, gnorm=0.773, train_wall=62, wall=9138
2021-01-01 14:17:54 | INFO | train_inner | epoch 033:    228 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.976, ppl=1.97, wps=22563.2, ups=1.61, wpb=14017.7, bsz=502.2, num_updates=13700, lr=1.8718e-05, gnorm=0.78, train_wall=62, wall=9200
2021-01-01 14:18:56 | INFO | train_inner | epoch 033:    328 / 421 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.335, nll_loss=0.982, ppl=1.98, wps=22419.5, ups=1.61, wpb=13934, bsz=474.8, num_updates=13800, lr=1.86501e-05, gnorm=0.797, train_wall=62, wall=9262
2021-01-01 14:19:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:19:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:19:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:19:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:19:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:19:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:19:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:19:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:19:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:20:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:20:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:20:11 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.291 | nll_loss 3.749 | ppl 13.44 | bleu 23.02 | wps 5952.2 | wpb 10324.2 | bsz 375 | num_updates 13893 | best_bleu 23.11
2021-01-01 14:20:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:20:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:20:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:20:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:20:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:20:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 33 @ 13893 updates, score 23.02) (writing took 2.886405633762479 seconds)
2021-01-01 14:20:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-01 14:20:14 | INFO | train | epoch 033 | symm_kl 0.401 | self_kl 0 | self_cv 0 | loss 3.323 | nll_loss 0.977 | ppl 1.97 | wps 20712.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 13893 | lr 1.85876e-05 | gnorm 0.784 | train_wall 260 | wall 9340
2021-01-01 14:20:14 | INFO | fairseq.trainer | begin training epoch 34
2021-01-01 14:20:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:20:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:20:21 | INFO | train_inner | epoch 034:      7 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.324, nll_loss=0.979, ppl=1.97, wps=16359.1, ups=1.18, wpb=13912, bsz=487.7, num_updates=13900, lr=1.85829e-05, gnorm=0.789, train_wall=62, wall=9347
2021-01-01 14:21:23 | INFO | train_inner | epoch 034:    107 / 421 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.326, nll_loss=0.977, ppl=1.97, wps=22768.1, ups=1.63, wpb=13986, bsz=504.2, num_updates=14000, lr=1.85164e-05, gnorm=0.786, train_wall=61, wall=9409
2021-01-01 14:22:25 | INFO | train_inner | epoch 034:    207 / 421 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.325, nll_loss=0.98, ppl=1.97, wps=22616.8, ups=1.62, wpb=13983.3, bsz=494.8, num_updates=14100, lr=1.84506e-05, gnorm=0.784, train_wall=62, wall=9471
2021-01-01 14:23:27 | INFO | train_inner | epoch 034:    307 / 421 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.97, ppl=1.96, wps=22309.8, ups=1.6, wpb=13929, bsz=504.4, num_updates=14200, lr=1.83855e-05, gnorm=0.777, train_wall=62, wall=9533
2021-01-01 14:24:29 | INFO | train_inner | epoch 034:    407 / 421 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.319, nll_loss=0.976, ppl=1.97, wps=22521, ups=1.61, wpb=13997.4, bsz=476.1, num_updates=14300, lr=1.83211e-05, gnorm=0.78, train_wall=62, wall=9595
2021-01-01 14:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:24:55 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.291 | nll_loss 3.748 | ppl 13.44 | bleu 22.96 | wps 5879.6 | wpb 10324.2 | bsz 375 | num_updates 14314 | best_bleu 23.11
2021-01-01 14:24:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:24:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 34 @ 14314 updates, score 22.96) (writing took 2.906587252393365 seconds)
2021-01-01 14:24:58 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-01 14:24:58 | INFO | train | epoch 034 | symm_kl 0.399 | self_kl 0 | self_cv 0 | loss 3.32 | nll_loss 0.976 | ppl 1.97 | wps 20730.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14314 | lr 1.83122e-05 | gnorm 0.783 | train_wall 260 | wall 9624
2021-01-01 14:24:58 | INFO | fairseq.trainer | begin training epoch 35
2021-01-01 14:24:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:25:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:25:54 | INFO | train_inner | epoch 035:     86 / 421 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.966, ppl=1.95, wps=16471, ups=1.18, wpb=13962.1, bsz=477.3, num_updates=14400, lr=1.82574e-05, gnorm=0.778, train_wall=62, wall=9680
2021-01-01 14:26:56 | INFO | train_inner | epoch 035:    186 / 421 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.963, ppl=1.95, wps=22361.4, ups=1.6, wpb=13963.9, bsz=507.5, num_updates=14500, lr=1.81944e-05, gnorm=0.776, train_wall=62, wall=9743
2021-01-01 14:27:58 | INFO | train_inner | epoch 035:    286 / 421 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.326, nll_loss=0.98, ppl=1.97, wps=22747.2, ups=1.61, wpb=14103.9, bsz=479.2, num_updates=14600, lr=1.81319e-05, gnorm=0.785, train_wall=62, wall=9805
2021-01-01 14:29:01 | INFO | train_inner | epoch 035:    386 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.982, ppl=1.98, wps=22332.2, ups=1.6, wpb=13917.1, bsz=499.5, num_updates=14700, lr=1.80702e-05, gnorm=0.783, train_wall=62, wall=9867
2021-01-01 14:29:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:29:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:29:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:29:39 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.287 | nll_loss 3.745 | ppl 13.41 | bleu 23.05 | wps 5918.6 | wpb 10324.2 | bsz 375 | num_updates 14735 | best_bleu 23.11
2021-01-01 14:29:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:29:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:29:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 35 @ 14735 updates, score 23.05) (writing took 2.9128658939152956 seconds)
2021-01-01 14:29:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-01 14:29:42 | INFO | train | epoch 035 | symm_kl 0.398 | self_kl 0 | self_cv 0 | loss 3.317 | nll_loss 0.975 | ppl 1.97 | wps 20664.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 14735 | lr 1.80487e-05 | gnorm 0.781 | train_wall 261 | wall 9908
2021-01-01 14:29:42 | INFO | fairseq.trainer | begin training epoch 36
2021-01-01 14:29:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:29:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:30:25 | INFO | train_inner | epoch 036:     65 / 421 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.321, nll_loss=0.979, ppl=1.97, wps=16266, ups=1.18, wpb=13745.1, bsz=484.9, num_updates=14800, lr=1.8009e-05, gnorm=0.792, train_wall=61, wall=9951
2021-01-01 14:31:27 | INFO | train_inner | epoch 036:    165 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.981, ppl=1.97, wps=22408.7, ups=1.61, wpb=13932.1, bsz=482.5, num_updates=14900, lr=1.79485e-05, gnorm=0.789, train_wall=62, wall=10014
2021-01-01 14:32:29 | INFO | train_inner | epoch 036:    265 / 421 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.969, ppl=1.96, wps=22798.1, ups=1.62, wpb=14089.7, bsz=509.1, num_updates=15000, lr=1.78885e-05, gnorm=0.778, train_wall=62, wall=10075
2021-01-01 14:33:32 | INFO | train_inner | epoch 036:    365 / 421 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.967, ppl=1.95, wps=22835.9, ups=1.6, wpb=14257.4, bsz=501.1, num_updates=15100, lr=1.78292e-05, gnorm=0.76, train_wall=62, wall=10138
2021-01-01 14:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:34:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:34:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:34:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:34:23 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.293 | nll_loss 3.751 | ppl 13.46 | bleu 22.94 | wps 5943 | wpb 10324.2 | bsz 375 | num_updates 15156 | best_bleu 23.11
2021-01-01 14:34:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 36 @ 15156 updates, score 22.94) (writing took 2.8949569948017597 seconds)
2021-01-01 14:34:26 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-01 14:34:26 | INFO | train | epoch 036 | symm_kl 0.396 | self_kl 0 | self_cv 0 | loss 3.314 | nll_loss 0.974 | ppl 1.96 | wps 20714.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15156 | lr 1.77962e-05 | gnorm 0.78 | train_wall 260 | wall 10192
2021-01-01 14:34:26 | INFO | fairseq.trainer | begin training epoch 37
2021-01-01 14:34:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:34:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:34:56 | INFO | train_inner | epoch 037:     44 / 421 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.984, ppl=1.98, wps=16307.1, ups=1.19, wpb=13760.8, bsz=468.6, num_updates=15200, lr=1.77705e-05, gnorm=0.789, train_wall=61, wall=10222
2021-01-01 14:35:58 | INFO | train_inner | epoch 037:    144 / 421 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.32, nll_loss=0.978, ppl=1.97, wps=22236.8, ups=1.61, wpb=13844.6, bsz=487.3, num_updates=15300, lr=1.77123e-05, gnorm=0.788, train_wall=62, wall=10284
2021-01-01 14:37:00 | INFO | train_inner | epoch 037:    244 / 421 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.966, ppl=1.95, wps=22586.1, ups=1.61, wpb=14021.7, bsz=500.2, num_updates=15400, lr=1.76547e-05, gnorm=0.77, train_wall=62, wall=10346
2021-01-01 14:38:02 | INFO | train_inner | epoch 037:    344 / 421 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.971, ppl=1.96, wps=22808.7, ups=1.62, wpb=14112, bsz=500.6, num_updates=15500, lr=1.75977e-05, gnorm=0.762, train_wall=62, wall=10408
2021-01-01 14:38:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:38:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:38:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:38:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:39:07 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.293 | nll_loss 3.749 | ppl 13.45 | bleu 23.13 | wps 5981.6 | wpb 10324.2 | bsz 375 | num_updates 15577 | best_bleu 23.13
2021-01-01 14:39:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:39:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:39:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:39:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:39:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:39:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:39:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:39:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 37 @ 15577 updates, score 23.13) (writing took 4.763176331296563 seconds)
2021-01-01 14:39:11 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-01 14:39:11 | INFO | train | epoch 037 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.312 | nll_loss 0.973 | ppl 1.96 | wps 20605.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15577 | lr 1.75541e-05 | gnorm 0.776 | train_wall 260 | wall 10478
2021-01-01 14:39:11 | INFO | fairseq.trainer | begin training epoch 38
2021-01-01 14:39:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:39:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:39:29 | INFO | train_inner | epoch 038:     23 / 421 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.983, ppl=1.98, wps=15908.8, ups=1.16, wpb=13763.3, bsz=486, num_updates=15600, lr=1.75412e-05, gnorm=0.79, train_wall=62, wall=10495
2021-01-01 14:40:30 | INFO | train_inner | epoch 038:    123 / 421 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.965, ppl=1.95, wps=22543, ups=1.63, wpb=13863.9, bsz=510.3, num_updates=15700, lr=1.74852e-05, gnorm=0.776, train_wall=61, wall=10556
2021-01-01 14:41:33 | INFO | train_inner | epoch 038:    223 / 421 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.974, ppl=1.96, wps=22585.4, ups=1.6, wpb=14077.8, bsz=498.1, num_updates=15800, lr=1.74298e-05, gnorm=0.767, train_wall=62, wall=10619
2021-01-01 14:42:35 | INFO | train_inner | epoch 038:    323 / 421 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.323, nll_loss=0.984, ppl=1.98, wps=22637.4, ups=1.61, wpb=14103.4, bsz=481.4, num_updates=15900, lr=1.73749e-05, gnorm=0.776, train_wall=62, wall=10681
2021-01-01 14:43:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:43:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:43:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:43:53 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.293 | nll_loss 3.751 | ppl 13.46 | bleu 23.09 | wps 5936.4 | wpb 10324.2 | bsz 375 | num_updates 15998 | best_bleu 23.13
2021-01-01 14:43:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 38 @ 15998 updates, score 23.09) (writing took 2.9126672372221947 seconds)
2021-01-01 14:43:56 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-01 14:43:56 | INFO | train | epoch 038 | symm_kl 0.394 | self_kl 0 | self_cv 0 | loss 3.31 | nll_loss 0.973 | ppl 1.96 | wps 20659.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 15998 | lr 1.73216e-05 | gnorm 0.775 | train_wall 261 | wall 10762
2021-01-01 14:43:56 | INFO | fairseq.trainer | begin training epoch 39
2021-01-01 14:43:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:43:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:43:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:44:01 | INFO | train_inner | epoch 039:      2 / 421 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.966, ppl=1.95, wps=16232.2, ups=1.17, wpb=13913.6, bsz=491.7, num_updates=16000, lr=1.73205e-05, gnorm=0.777, train_wall=63, wall=10767
2021-01-01 14:45:02 | INFO | train_inner | epoch 039:    102 / 421 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.31, nll_loss=0.974, ppl=1.96, wps=22699.4, ups=1.62, wpb=13976.1, bsz=494.8, num_updates=16100, lr=1.72666e-05, gnorm=0.769, train_wall=61, wall=10828
2021-01-01 14:46:05 | INFO | train_inner | epoch 039:    202 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.955, ppl=1.94, wps=22575.7, ups=1.6, wpb=14067.4, bsz=497.4, num_updates=16200, lr=1.72133e-05, gnorm=0.766, train_wall=62, wall=10891
2021-01-01 14:47:07 | INFO | train_inner | epoch 039:    302 / 421 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.319, nll_loss=0.982, ppl=1.98, wps=22544.3, ups=1.6, wpb=14046.5, bsz=496.7, num_updates=16300, lr=1.71604e-05, gnorm=0.764, train_wall=62, wall=10953
2021-01-01 14:48:09 | INFO | train_inner | epoch 039:    402 / 421 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.975, ppl=1.97, wps=22264.9, ups=1.6, wpb=13881.8, bsz=485.9, num_updates=16400, lr=1.7108e-05, gnorm=0.778, train_wall=62, wall=11015
2021-01-01 14:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:48:38 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.289 | nll_loss 3.747 | ppl 13.43 | bleu 23.09 | wps 5808.6 | wpb 10324.2 | bsz 375 | num_updates 16419 | best_bleu 23.13
2021-01-01 14:48:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:48:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:48:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 39 @ 16419 updates, score 23.09) (writing took 2.8743799794465303 seconds)
2021-01-01 14:48:41 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-01 14:48:41 | INFO | train | epoch 039 | symm_kl 0.392 | self_kl 0 | self_cv 0 | loss 3.307 | nll_loss 0.972 | ppl 1.96 | wps 20669.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 16419 | lr 1.70981e-05 | gnorm 0.772 | train_wall 261 | wall 11047
2021-01-01 14:48:41 | INFO | fairseq.trainer | begin training epoch 40
2021-01-01 14:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:48:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:49:34 | INFO | train_inner | epoch 040:     81 / 421 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.32, nll_loss=0.978, ppl=1.97, wps=16259.2, ups=1.18, wpb=13775.3, bsz=458.8, num_updates=16500, lr=1.70561e-05, gnorm=0.787, train_wall=61, wall=11100
2021-01-01 14:50:36 | INFO | train_inner | epoch 040:    181 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.968, ppl=1.96, wps=22443.6, ups=1.6, wpb=13989.1, bsz=509.5, num_updates=16600, lr=1.70046e-05, gnorm=0.77, train_wall=62, wall=11162
2021-01-01 14:51:39 | INFO | train_inner | epoch 040:    281 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.965, ppl=1.95, wps=22563.2, ups=1.6, wpb=14114.3, bsz=492.2, num_updates=16700, lr=1.69536e-05, gnorm=0.765, train_wall=62, wall=11225
2021-01-01 14:52:41 | INFO | train_inner | epoch 040:    381 / 421 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.317, nll_loss=0.983, ppl=1.98, wps=22559.7, ups=1.61, wpb=14010.3, bsz=512.3, num_updates=16800, lr=1.69031e-05, gnorm=0.771, train_wall=62, wall=11287
2021-01-01 14:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:53:24 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.288 | nll_loss 3.746 | ppl 13.42 | bleu 23.03 | wps 5264.3 | wpb 10324.2 | bsz 375 | num_updates 16840 | best_bleu 23.13
2021-01-01 14:53:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:53:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:53:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 40 @ 16840 updates, score 23.03) (writing took 3.0019406974315643 seconds)
2021-01-01 14:53:27 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-01 14:53:27 | INFO | train | epoch 040 | symm_kl 0.391 | self_kl 0 | self_cv 0 | loss 3.305 | nll_loss 0.972 | ppl 1.96 | wps 20510.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 16840 | lr 1.6883e-05 | gnorm 0.772 | train_wall 261 | wall 11333
2021-01-01 14:53:27 | INFO | fairseq.trainer | begin training epoch 41
2021-01-01 14:53:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:53:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:54:07 | INFO | train_inner | epoch 041:     60 / 421 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.969, ppl=1.96, wps=16015.2, ups=1.16, wpb=13852.1, bsz=485.2, num_updates=16900, lr=1.6853e-05, gnorm=0.772, train_wall=62, wall=11373
2021-01-01 14:55:10 | INFO | train_inner | epoch 041:    160 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.964, ppl=1.95, wps=22526.1, ups=1.6, wpb=14070.9, bsz=484.6, num_updates=17000, lr=1.68034e-05, gnorm=0.77, train_wall=62, wall=11436
2021-01-01 14:56:12 | INFO | train_inner | epoch 041:    260 / 421 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.97, ppl=1.96, wps=22322.4, ups=1.6, wpb=13910.3, bsz=522.1, num_updates=17100, lr=1.67542e-05, gnorm=0.764, train_wall=62, wall=11498
2021-01-01 14:57:14 | INFO | train_inner | epoch 041:    360 / 421 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.314, nll_loss=0.978, ppl=1.97, wps=22483.9, ups=1.61, wpb=13988.5, bsz=486.2, num_updates=17200, lr=1.67054e-05, gnorm=0.78, train_wall=62, wall=11560
2021-01-01 14:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 14:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:57:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:57:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:57:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:58:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:58:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:58:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:58:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:58:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:58:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:58:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 14:58:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 14:58:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 14:58:09 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.288 | nll_loss 3.744 | ppl 13.4 | bleu 23.09 | wps 6024.8 | wpb 10324.2 | bsz 375 | num_updates 17261 | best_bleu 23.13
2021-01-01 14:58:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 14:58:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:58:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:58:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:58:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:58:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:58:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:58:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 41 @ 17261 updates, score 23.09) (writing took 2.9270681850612164 seconds)
2021-01-01 14:58:12 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-01 14:58:12 | INFO | train | epoch 041 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.302 | nll_loss 0.971 | ppl 1.96 | wps 20637.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 17261 | lr 1.66758e-05 | gnorm 0.772 | train_wall 261 | wall 11618
2021-01-01 14:58:12 | INFO | fairseq.trainer | begin training epoch 42
2021-01-01 14:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 14:58:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 14:58:39 | INFO | train_inner | epoch 042:     39 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.969, ppl=1.96, wps=16448.4, ups=1.18, wpb=13992.9, bsz=466.6, num_updates=17300, lr=1.6657e-05, gnorm=0.775, train_wall=62, wall=11646
2021-01-01 14:59:42 | INFO | train_inner | epoch 042:    139 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.971, ppl=1.96, wps=22104.2, ups=1.59, wpb=13864.2, bsz=490.1, num_updates=17400, lr=1.66091e-05, gnorm=0.772, train_wall=63, wall=11708
2021-01-01 15:00:45 | INFO | train_inner | epoch 042:    239 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.966, ppl=1.95, wps=22449.8, ups=1.6, wpb=14054, bsz=497.4, num_updates=17500, lr=1.65616e-05, gnorm=0.769, train_wall=62, wall=11771
2021-01-01 15:01:47 | INFO | train_inner | epoch 042:    339 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.97, ppl=1.96, wps=22609.8, ups=1.61, wpb=14062.4, bsz=506.2, num_updates=17600, lr=1.65145e-05, gnorm=0.764, train_wall=62, wall=11833
2021-01-01 15:02:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:02:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:02:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:02:56 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.288 | nll_loss 3.748 | ppl 13.43 | bleu 23.01 | wps 5765 | wpb 10324.2 | bsz 375 | num_updates 17682 | best_bleu 23.13
2021-01-01 15:02:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:02:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:02:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 42 @ 17682 updates, score 23.01) (writing took 2.8974648900330067 seconds)
2021-01-01 15:02:59 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-01 15:02:59 | INFO | train | epoch 042 | symm_kl 0.389 | self_kl 0 | self_cv 0 | loss 3.301 | nll_loss 0.971 | ppl 1.96 | wps 20550 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 17682 | lr 1.64761e-05 | gnorm 0.773 | train_wall 262 | wall 11905
2021-01-01 15:02:59 | INFO | fairseq.trainer | begin training epoch 43
2021-01-01 15:02:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:03:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:03:13 | INFO | train_inner | epoch 043:     18 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.977, ppl=1.97, wps=16082.4, ups=1.17, wpb=13796.6, bsz=485.9, num_updates=17700, lr=1.64677e-05, gnorm=0.786, train_wall=62, wall=11919
2021-01-01 15:04:15 | INFO | train_inner | epoch 043:    118 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.97, ppl=1.96, wps=22592.4, ups=1.61, wpb=13998.6, bsz=490.6, num_updates=17800, lr=1.64214e-05, gnorm=0.761, train_wall=62, wall=11981
2021-01-01 15:05:17 | INFO | train_inner | epoch 043:    218 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.969, ppl=1.96, wps=22392.9, ups=1.61, wpb=13921.6, bsz=480.3, num_updates=17900, lr=1.63755e-05, gnorm=0.778, train_wall=62, wall=12043
2021-01-01 15:06:20 | INFO | train_inner | epoch 043:    318 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.973, ppl=1.96, wps=22367, ups=1.6, wpb=14013.3, bsz=509.1, num_updates=18000, lr=1.63299e-05, gnorm=0.762, train_wall=62, wall=12106
2021-01-01 15:07:22 | INFO | train_inner | epoch 043:    418 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.973, ppl=1.96, wps=22407.6, ups=1.59, wpb=14072.7, bsz=493.7, num_updates=18100, lr=1.62848e-05, gnorm=0.761, train_wall=63, wall=12168
2021-01-01 15:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:07:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:07:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:07:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:07:41 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.284 | nll_loss 3.742 | ppl 13.38 | bleu 23.1 | wps 5950.8 | wpb 10324.2 | bsz 375 | num_updates 18103 | best_bleu 23.13
2021-01-01 15:07:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:07:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:07:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 43 @ 18103 updates, score 23.1) (writing took 2.8690588250756264 seconds)
2021-01-01 15:07:44 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-01 15:07:44 | INFO | train | epoch 043 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.298 | nll_loss 0.97 | ppl 1.96 | wps 20619.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18103 | lr 1.62834e-05 | gnorm 0.769 | train_wall 261 | wall 12190
2021-01-01 15:07:44 | INFO | fairseq.trainer | begin training epoch 44
2021-01-01 15:07:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:07:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:08:47 | INFO | train_inner | epoch 044:     97 / 421 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.959, ppl=1.94, wps=16337.3, ups=1.18, wpb=13866.6, bsz=492.2, num_updates=18200, lr=1.624e-05, gnorm=0.778, train_wall=62, wall=12253
2021-01-01 15:09:50 | INFO | train_inner | epoch 044:    197 / 421 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.952, ppl=1.93, wps=22365.3, ups=1.59, wpb=14058.6, bsz=492.3, num_updates=18300, lr=1.61955e-05, gnorm=0.759, train_wall=63, wall=12316
2021-01-01 15:10:52 | INFO | train_inner | epoch 044:    297 / 421 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.979, ppl=1.97, wps=22378.1, ups=1.61, wpb=13940.4, bsz=512, num_updates=18400, lr=1.61515e-05, gnorm=0.758, train_wall=62, wall=12378
2021-01-01 15:11:54 | INFO | train_inner | epoch 044:    397 / 421 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.317, nll_loss=0.982, ppl=1.97, wps=22647.3, ups=1.61, wpb=14035.6, bsz=473.9, num_updates=18500, lr=1.61077e-05, gnorm=0.777, train_wall=62, wall=12440
2021-01-01 15:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:12:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:12:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:12:26 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.284 | nll_loss 3.74 | ppl 13.37 | bleu 23.05 | wps 5912 | wpb 10324.2 | bsz 375 | num_updates 18524 | best_bleu 23.13
2021-01-01 15:12:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:12:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:12:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 44 @ 18524 updates, score 23.05) (writing took 2.9388423953205347 seconds)
2021-01-01 15:12:29 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-01 15:12:29 | INFO | train | epoch 044 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.296 | nll_loss 0.97 | ppl 1.96 | wps 20608 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18524 | lr 1.60973e-05 | gnorm 0.766 | train_wall 261 | wall 12475
2021-01-01 15:12:29 | INFO | fairseq.trainer | begin training epoch 45
2021-01-01 15:12:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:12:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:13:20 | INFO | train_inner | epoch 045:     76 / 421 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.977, ppl=1.97, wps=16159.6, ups=1.17, wpb=13771.3, bsz=483, num_updates=18600, lr=1.60644e-05, gnorm=0.772, train_wall=62, wall=12526
2021-01-01 15:14:22 | INFO | train_inner | epoch 045:    176 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.972, ppl=1.96, wps=22608.8, ups=1.62, wpb=13992.1, bsz=510.8, num_updates=18700, lr=1.60214e-05, gnorm=0.765, train_wall=62, wall=12588
2021-01-01 15:15:23 | INFO | train_inner | epoch 045:    276 / 421 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.974, ppl=1.96, wps=22591.9, ups=1.62, wpb=13974.4, bsz=476, num_updates=18800, lr=1.59787e-05, gnorm=0.774, train_wall=62, wall=12649
2021-01-01 15:16:26 | INFO | train_inner | epoch 045:    376 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.961, ppl=1.95, wps=22519.1, ups=1.6, wpb=14092, bsz=499.6, num_updates=18900, lr=1.59364e-05, gnorm=0.756, train_wall=62, wall=12712
2021-01-01 15:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:16:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:16:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:16:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:16:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:17:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:17:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:17:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:17:11 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.284 | nll_loss 3.743 | ppl 13.39 | bleu 23.1 | wps 5937.4 | wpb 10324.2 | bsz 375 | num_updates 18945 | best_bleu 23.13
2021-01-01 15:17:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:17:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 45 @ 18945 updates, score 23.1) (writing took 2.924404554069042 seconds)
2021-01-01 15:17:14 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-01 15:17:14 | INFO | train | epoch 045 | symm_kl 0.386 | self_kl 0 | self_cv 0 | loss 3.294 | nll_loss 0.969 | ppl 1.96 | wps 20685.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 18945 | lr 1.59174e-05 | gnorm 0.768 | train_wall 260 | wall 12760
2021-01-01 15:17:14 | INFO | fairseq.trainer | begin training epoch 46
2021-01-01 15:17:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:17:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:17:50 | INFO | train_inner | epoch 046:     55 / 421 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.961, ppl=1.95, wps=16504.5, ups=1.18, wpb=13951.3, bsz=487.3, num_updates=19000, lr=1.58944e-05, gnorm=0.778, train_wall=61, wall=12797
2021-01-01 15:18:53 | INFO | train_inner | epoch 046:    155 / 421 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.975, ppl=1.97, wps=22232.8, ups=1.6, wpb=13867.7, bsz=496.7, num_updates=19100, lr=1.58527e-05, gnorm=0.767, train_wall=62, wall=12859
2021-01-01 15:19:55 | INFO | train_inner | epoch 046:    255 / 421 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.293, nll_loss=0.967, ppl=1.95, wps=22532.8, ups=1.6, wpb=14102.5, bsz=494.7, num_updates=19200, lr=1.58114e-05, gnorm=0.754, train_wall=62, wall=12921
2021-01-01 15:20:57 | INFO | train_inner | epoch 046:    355 / 421 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.293, nll_loss=0.97, ppl=1.96, wps=22637.6, ups=1.62, wpb=13996.6, bsz=487.2, num_updates=19300, lr=1.57704e-05, gnorm=0.762, train_wall=62, wall=12983
2021-01-01 15:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:21:55 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.286 | nll_loss 3.743 | ppl 13.39 | bleu 23.03 | wps 5930.5 | wpb 10324.2 | bsz 375 | num_updates 19366 | best_bleu 23.13
2021-01-01 15:21:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:21:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:21:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 46 @ 19366 updates, score 23.03) (writing took 2.8722113594412804 seconds)
2021-01-01 15:21:58 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-01 15:21:58 | INFO | train | epoch 046 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.293 | nll_loss 0.969 | ppl 1.96 | wps 20708.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 19366 | lr 1.57435e-05 | gnorm 0.763 | train_wall 260 | wall 13044
2021-01-01 15:21:58 | INFO | fairseq.trainer | begin training epoch 47
2021-01-01 15:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:22:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:22:22 | INFO | train_inner | epoch 047:     34 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.969, ppl=1.96, wps=16561.4, ups=1.19, wpb=13956.4, bsz=499.5, num_updates=19400, lr=1.57297e-05, gnorm=0.761, train_wall=61, wall=13068
2021-01-01 15:23:24 | INFO | train_inner | epoch 047:    134 / 421 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.976, ppl=1.97, wps=22387.8, ups=1.6, wpb=13959.6, bsz=462.2, num_updates=19500, lr=1.56893e-05, gnorm=0.777, train_wall=62, wall=13130
2021-01-01 15:24:26 | INFO | train_inner | epoch 047:    234 / 421 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.966, ppl=1.95, wps=22512.2, ups=1.61, wpb=13996.9, bsz=506.4, num_updates=19600, lr=1.56492e-05, gnorm=0.759, train_wall=62, wall=13192
2021-01-01 15:25:29 | INFO | train_inner | epoch 047:    334 / 421 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.968, ppl=1.96, wps=22252.3, ups=1.59, wpb=13997.8, bsz=500.8, num_updates=19700, lr=1.56094e-05, gnorm=0.755, train_wall=63, wall=13255
2021-01-01 15:26:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:26:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:26:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:26:42 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.282 | nll_loss 3.74 | ppl 13.36 | bleu 23.16 | wps 5221 | wpb 10324.2 | bsz 375 | num_updates 19787 | best_bleu 23.16
2021-01-01 15:26:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 47 @ 19787 updates, score 23.16) (writing took 4.788635220378637 seconds)
2021-01-01 15:26:47 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-01 15:26:47 | INFO | train | epoch 047 | symm_kl 0.384 | self_kl 0 | self_cv 0 | loss 3.291 | nll_loss 0.969 | ppl 1.96 | wps 20347.9 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 19787 | lr 1.55751e-05 | gnorm 0.766 | train_wall 262 | wall 13333
2021-01-01 15:26:47 | INFO | fairseq.trainer | begin training epoch 48
2021-01-01 15:26:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:26:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:26:58 | INFO | train_inner | epoch 048:     13 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.967, ppl=1.95, wps=15599.4, ups=1.13, wpb=13865.8, bsz=501.2, num_updates=19800, lr=1.557e-05, gnorm=0.775, train_wall=62, wall=13344
2021-01-01 15:28:00 | INFO | train_inner | epoch 048:    113 / 421 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.97, ppl=1.96, wps=22491.2, ups=1.61, wpb=13942.5, bsz=492.8, num_updates=19900, lr=1.55308e-05, gnorm=0.77, train_wall=62, wall=13406
2021-01-01 15:29:03 | INFO | train_inner | epoch 048:    213 / 421 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.293, nll_loss=0.973, ppl=1.96, wps=22216.9, ups=1.59, wpb=13986.9, bsz=473.4, num_updates=20000, lr=1.54919e-05, gnorm=0.761, train_wall=63, wall=13469
2021-01-01 15:30:05 | INFO | train_inner | epoch 048:    313 / 421 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.969, ppl=1.96, wps=22352.8, ups=1.6, wpb=13999.2, bsz=503.7, num_updates=20100, lr=1.54533e-05, gnorm=0.758, train_wall=62, wall=13531
2021-01-01 15:31:08 | INFO | train_inner | epoch 048:    413 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.962, ppl=1.95, wps=22305.6, ups=1.6, wpb=13966.9, bsz=498.5, num_updates=20200, lr=1.5415e-05, gnorm=0.762, train_wall=62, wall=13594
2021-01-01 15:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:31:30 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.284 | nll_loss 3.742 | ppl 13.38 | bleu 23.06 | wps 5964.2 | wpb 10324.2 | bsz 375 | num_updates 20208 | best_bleu 23.16
2021-01-01 15:31:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:31:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:31:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 48 @ 20208 updates, score 23.06) (writing took 2.902153702452779 seconds)
2021-01-01 15:31:33 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-01 15:31:33 | INFO | train | epoch 048 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.289 | nll_loss 0.968 | ppl 1.96 | wps 20546.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 20208 | lr 1.5412e-05 | gnorm 0.762 | train_wall 262 | wall 13619
2021-01-01 15:31:33 | INFO | fairseq.trainer | begin training epoch 49
2021-01-01 15:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:32:33 | INFO | train_inner | epoch 049:     92 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.958, ppl=1.94, wps=16358.4, ups=1.17, wpb=13934.4, bsz=503, num_updates=20300, lr=1.5377e-05, gnorm=0.759, train_wall=62, wall=13679
2021-01-01 15:33:36 | INFO | train_inner | epoch 049:    192 / 421 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.982, ppl=1.98, wps=22250.8, ups=1.6, wpb=13889.8, bsz=475.2, num_updates=20400, lr=1.53393e-05, gnorm=0.774, train_wall=62, wall=13742
2021-01-01 15:34:38 | INFO | train_inner | epoch 049:    292 / 421 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.965, ppl=1.95, wps=22304.2, ups=1.6, wpb=13923.3, bsz=497, num_updates=20500, lr=1.53018e-05, gnorm=0.752, train_wall=62, wall=13804
2021-01-01 15:35:40 | INFO | train_inner | epoch 049:    392 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.964, ppl=1.95, wps=22667, ups=1.6, wpb=14132.9, bsz=483.7, num_updates=20600, lr=1.52647e-05, gnorm=0.757, train_wall=62, wall=13866
2021-01-01 15:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:36:16 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.28 | nll_loss 3.738 | ppl 13.34 | bleu 23.12 | wps 5793.2 | wpb 10324.2 | bsz 375 | num_updates 20629 | best_bleu 23.16
2021-01-01 15:36:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:36:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 49 @ 20629 updates, score 23.12) (writing took 2.9208668768405914 seconds)
2021-01-01 15:36:19 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-01 15:36:19 | INFO | train | epoch 049 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.287 | nll_loss 0.967 | ppl 1.96 | wps 20563.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 20629 | lr 1.52539e-05 | gnorm 0.761 | train_wall 262 | wall 13905
2021-01-01 15:36:19 | INFO | fairseq.trainer | begin training epoch 50
2021-01-01 15:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:37:06 | INFO | train_inner | epoch 050:     71 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.966, ppl=1.95, wps=16348.7, ups=1.17, wpb=13974.8, bsz=500.3, num_updates=20700, lr=1.52277e-05, gnorm=0.762, train_wall=62, wall=13952
2021-01-01 15:38:08 | INFO | train_inner | epoch 050:    171 / 421 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.965, ppl=1.95, wps=22225.9, ups=1.6, wpb=13849.1, bsz=471, num_updates=20800, lr=1.51911e-05, gnorm=0.771, train_wall=62, wall=14014
2021-01-01 15:39:11 | INFO | train_inner | epoch 050:    271 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.955, ppl=1.94, wps=22611.6, ups=1.6, wpb=14131.2, bsz=510.4, num_updates=20900, lr=1.51547e-05, gnorm=0.747, train_wall=62, wall=14077
2021-01-01 15:40:13 | INFO | train_inner | epoch 050:    371 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.292, nll_loss=0.973, ppl=1.96, wps=22378.4, ups=1.6, wpb=13993.5, bsz=510.5, num_updates=21000, lr=1.51186e-05, gnorm=0.761, train_wall=62, wall=14139
2021-01-01 15:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:40:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:40:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:40:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:40:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:40:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:40:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:40:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:40:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:41:01 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.281 | nll_loss 3.739 | ppl 13.35 | bleu 23.04 | wps 5896.9 | wpb 10324.2 | bsz 375 | num_updates 21050 | best_bleu 23.16
2021-01-01 15:41:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:41:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:41:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:41:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 50 @ 21050 updates, score 23.04) (writing took 2.8619282133877277 seconds)
2021-01-01 15:41:04 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-01 15:41:04 | INFO | train | epoch 050 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.286 | nll_loss 0.967 | ppl 1.95 | wps 20616.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 21050 | lr 1.51006e-05 | gnorm 0.763 | train_wall 261 | wall 14190
2021-01-01 15:41:04 | INFO | fairseq.trainer | begin training epoch 51
2021-01-01 15:41:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:41:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:41:38 | INFO | train_inner | epoch 051:     50 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.31, nll_loss=0.982, ppl=1.97, wps=16305.8, ups=1.18, wpb=13798.6, bsz=464.4, num_updates=21100, lr=1.50827e-05, gnorm=0.781, train_wall=61, wall=14224
2021-01-01 15:42:41 | INFO | train_inner | epoch 051:    150 / 421 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.967, ppl=1.96, wps=22436.8, ups=1.6, wpb=14049.9, bsz=505.3, num_updates=21200, lr=1.50471e-05, gnorm=0.75, train_wall=62, wall=14287
2021-01-01 15:43:43 | INFO | train_inner | epoch 051:    250 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.971, ppl=1.96, wps=22280.7, ups=1.6, wpb=13949.7, bsz=501, num_updates=21300, lr=1.50117e-05, gnorm=0.758, train_wall=62, wall=14349
2021-01-01 15:44:46 | INFO | train_inner | epoch 051:    350 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.959, ppl=1.94, wps=22262.5, ups=1.59, wpb=13994.4, bsz=484.5, num_updates=21400, lr=1.49766e-05, gnorm=0.76, train_wall=63, wall=14412
2021-01-01 15:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:45:47 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.281 | nll_loss 3.738 | ppl 13.34 | bleu 23.17 | wps 5911.3 | wpb 10324.2 | bsz 375 | num_updates 21471 | best_bleu 23.17
2021-01-01 15:45:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:45:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 51 @ 21471 updates, score 23.17) (writing took 4.769746808335185 seconds)
2021-01-01 15:45:52 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-01 15:45:52 | INFO | train | epoch 051 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.284 | nll_loss 0.967 | ppl 1.95 | wps 20433 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 21471 | lr 1.49518e-05 | gnorm 0.759 | train_wall 262 | wall 14478
2021-01-01 15:45:52 | INFO | fairseq.trainer | begin training epoch 52
2021-01-01 15:45:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:45:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:46:13 | INFO | train_inner | epoch 052:     29 / 421 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.958, ppl=1.94, wps=16088.9, ups=1.15, wpb=14005.4, bsz=491.4, num_updates=21500, lr=1.49417e-05, gnorm=0.754, train_wall=62, wall=14499
2021-01-01 15:47:15 | INFO | train_inner | epoch 052:    129 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.965, ppl=1.95, wps=22455.1, ups=1.6, wpb=14011.3, bsz=500.1, num_updates=21600, lr=1.49071e-05, gnorm=0.75, train_wall=62, wall=14561
2021-01-01 15:48:18 | INFO | train_inner | epoch 052:    229 / 421 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.985, ppl=1.98, wps=22328.9, ups=1.6, wpb=13925.4, bsz=462.5, num_updates=21700, lr=1.48727e-05, gnorm=0.771, train_wall=62, wall=14624
2021-01-01 15:49:20 | INFO | train_inner | epoch 052:    329 / 421 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.958, ppl=1.94, wps=22342.6, ups=1.61, wpb=13906.2, bsz=492.3, num_updates=21800, lr=1.48386e-05, gnorm=0.758, train_wall=62, wall=14686
2021-01-01 15:50:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:50:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:50:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:50:34 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.281 | nll_loss 3.739 | ppl 13.35 | bleu 23.14 | wps 6030.5 | wpb 10324.2 | bsz 375 | num_updates 21892 | best_bleu 23.17
2021-01-01 15:50:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 52 @ 21892 updates, score 23.14) (writing took 2.9133014120161533 seconds)
2021-01-01 15:50:37 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-01 15:50:37 | INFO | train | epoch 052 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.282 | nll_loss 0.965 | ppl 1.95 | wps 20644.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 21892 | lr 1.48074e-05 | gnorm 0.758 | train_wall 261 | wall 14763
2021-01-01 15:50:37 | INFO | fairseq.trainer | begin training epoch 53
2021-01-01 15:50:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:50:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:50:45 | INFO | train_inner | epoch 053:      8 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.955, ppl=1.94, wps=16481, ups=1.18, wpb=13998.2, bsz=519.4, num_updates=21900, lr=1.48047e-05, gnorm=0.755, train_wall=62, wall=14771
2021-01-01 15:51:47 | INFO | train_inner | epoch 053:    108 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.969, ppl=1.96, wps=22697.2, ups=1.62, wpb=14041.4, bsz=485.9, num_updates=22000, lr=1.4771e-05, gnorm=0.754, train_wall=62, wall=14833
2021-01-01 15:52:49 | INFO | train_inner | epoch 053:    208 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.958, ppl=1.94, wps=22233.8, ups=1.6, wpb=13904.8, bsz=492.6, num_updates=22100, lr=1.47375e-05, gnorm=0.758, train_wall=62, wall=14895
2021-01-01 15:53:53 | INFO | train_inner | epoch 053:    308 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.956, ppl=1.94, wps=22248.8, ups=1.58, wpb=14051.7, bsz=498.5, num_updates=22200, lr=1.47043e-05, gnorm=0.756, train_wall=63, wall=14959
2021-01-01 15:54:55 | INFO | train_inner | epoch 053:    408 / 421 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.977, ppl=1.97, wps=22291.2, ups=1.6, wpb=13918.4, bsz=491.2, num_updates=22300, lr=1.46713e-05, gnorm=0.764, train_wall=62, wall=15021
2021-01-01 15:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:55:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:55:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:55:20 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.283 | nll_loss 3.741 | ppl 13.37 | bleu 23.02 | wps 5902.2 | wpb 10324.2 | bsz 375 | num_updates 22313 | best_bleu 23.17
2021-01-01 15:55:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 15:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:55:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 53 @ 22313 updates, score 23.02) (writing took 2.8587620854377747 seconds)
2021-01-01 15:55:23 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-01 15:55:23 | INFO | train | epoch 053 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.281 | nll_loss 0.965 | ppl 1.95 | wps 20563 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 22313 | lr 1.4667e-05 | gnorm 0.758 | train_wall 262 | wall 15049
2021-01-01 15:55:23 | INFO | fairseq.trainer | begin training epoch 54
2021-01-01 15:55:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:55:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:56:20 | INFO | train_inner | epoch 054:     87 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.962, ppl=1.95, wps=16490.8, ups=1.18, wpb=13991.3, bsz=490.6, num_updates=22400, lr=1.46385e-05, gnorm=0.765, train_wall=61, wall=15106
2021-01-01 15:57:23 | INFO | train_inner | epoch 054:    187 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.971, ppl=1.96, wps=22228.2, ups=1.59, wpb=13999.8, bsz=486.5, num_updates=22500, lr=1.46059e-05, gnorm=0.754, train_wall=63, wall=15169
2021-01-01 15:58:26 | INFO | train_inner | epoch 054:    287 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.962, ppl=1.95, wps=22362.6, ups=1.59, wpb=14070.5, bsz=509, num_updates=22600, lr=1.45736e-05, gnorm=0.744, train_wall=63, wall=15232
2021-01-01 15:59:28 | INFO | train_inner | epoch 054:    387 / 421 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.967, ppl=1.96, wps=22234.7, ups=1.6, wpb=13862.2, bsz=486.7, num_updates=22700, lr=1.45414e-05, gnorm=0.759, train_wall=62, wall=15294
2021-01-01 15:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 15:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 15:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 15:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 15:59:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 15:59:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 15:59:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:00:06 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.282 | nll_loss 3.74 | ppl 13.36 | bleu 23.06 | wps 5853.4 | wpb 10324.2 | bsz 375 | num_updates 22734 | best_bleu 23.17
2021-01-01 16:00:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:00:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 54 @ 22734 updates, score 23.06) (writing took 2.9377930238842964 seconds)
2021-01-01 16:00:09 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-01 16:00:09 | INFO | train | epoch 054 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.279 | nll_loss 0.964 | ppl 1.95 | wps 20534.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 22734 | lr 1.45306e-05 | gnorm 0.756 | train_wall 262 | wall 15335
2021-01-01 16:00:09 | INFO | fairseq.trainer | begin training epoch 55
2021-01-01 16:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:00:53 | INFO | train_inner | epoch 055:     66 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.973, ppl=1.96, wps=16352, ups=1.18, wpb=13841.5, bsz=488.1, num_updates=22800, lr=1.45095e-05, gnorm=0.764, train_wall=61, wall=15379
2021-01-01 16:01:55 | INFO | train_inner | epoch 055:    166 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.959, ppl=1.94, wps=22410.5, ups=1.59, wpb=14052.4, bsz=517.9, num_updates=22900, lr=1.44778e-05, gnorm=0.744, train_wall=62, wall=15441
2021-01-01 16:02:58 | INFO | train_inner | epoch 055:    266 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.96, ppl=1.95, wps=22415.6, ups=1.59, wpb=14060.1, bsz=471.7, num_updates=23000, lr=1.44463e-05, gnorm=0.756, train_wall=63, wall=15504
2021-01-01 16:04:01 | INFO | train_inner | epoch 055:    366 / 421 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.968, ppl=1.96, wps=22130.4, ups=1.59, wpb=13892.3, bsz=496.7, num_updates=23100, lr=1.4415e-05, gnorm=0.756, train_wall=63, wall=15567
2021-01-01 16:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:04:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:04:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:04:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:04:52 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.28 | nll_loss 3.739 | ppl 13.35 | bleu 23.06 | wps 5938 | wpb 10324.2 | bsz 375 | num_updates 23155 | best_bleu 23.17
2021-01-01 16:04:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:04:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 55 @ 23155 updates, score 23.06) (writing took 2.9162604715675116 seconds)
2021-01-01 16:04:55 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-01 16:04:55 | INFO | train | epoch 055 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.278 | nll_loss 0.965 | ppl 1.95 | wps 20588.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23155 | lr 1.43979e-05 | gnorm 0.757 | train_wall 262 | wall 15621
2021-01-01 16:04:55 | INFO | fairseq.trainer | begin training epoch 56
2021-01-01 16:04:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:04:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:05:26 | INFO | train_inner | epoch 056:     45 / 421 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.969, ppl=1.96, wps=16285.8, ups=1.18, wpb=13788.6, bsz=481.8, num_updates=23200, lr=1.43839e-05, gnorm=0.775, train_wall=62, wall=15652
2021-01-01 16:06:28 | INFO | train_inner | epoch 056:    145 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.957, ppl=1.94, wps=22627.7, ups=1.59, wpb=14199.1, bsz=519.6, num_updates=23300, lr=1.4353e-05, gnorm=0.735, train_wall=63, wall=15714
2021-01-01 16:07:31 | INFO | train_inner | epoch 056:    245 / 421 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.964, ppl=1.95, wps=22438.9, ups=1.61, wpb=13964, bsz=488.3, num_updates=23400, lr=1.43223e-05, gnorm=0.758, train_wall=62, wall=15777
2021-01-01 16:08:33 | INFO | train_inner | epoch 056:    345 / 421 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.963, ppl=1.95, wps=22077.1, ups=1.6, wpb=13808, bsz=486.3, num_updates=23500, lr=1.42918e-05, gnorm=0.765, train_wall=62, wall=15839
2021-01-01 16:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:09:37 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.278 | nll_loss 3.736 | ppl 13.32 | bleu 23.02 | wps 5821.5 | wpb 10324.2 | bsz 375 | num_updates 23576 | best_bleu 23.17
2021-01-01 16:09:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 56 @ 23576 updates, score 23.02) (writing took 2.9020585995167494 seconds)
2021-01-01 16:09:40 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-01 16:09:40 | INFO | train | epoch 056 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.276 | nll_loss 0.964 | ppl 1.95 | wps 20612.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 23576 | lr 1.42687e-05 | gnorm 0.755 | train_wall 261 | wall 15906
2021-01-01 16:09:40 | INFO | fairseq.trainer | begin training epoch 57
2021-01-01 16:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:09:58 | INFO | train_inner | epoch 057:     24 / 421 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.973, ppl=1.96, wps=16432.5, ups=1.18, wpb=13962, bsz=488.1, num_updates=23600, lr=1.42615e-05, gnorm=0.756, train_wall=62, wall=15924
2021-01-01 16:11:00 | INFO | train_inner | epoch 057:    124 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.966, ppl=1.95, wps=22420.9, ups=1.61, wpb=13953.8, bsz=500.8, num_updates=23700, lr=1.42314e-05, gnorm=0.742, train_wall=62, wall=15986
2021-01-01 16:12:03 | INFO | train_inner | epoch 057:    224 / 421 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.959, ppl=1.94, wps=22236.6, ups=1.6, wpb=13941.2, bsz=473.1, num_updates=23800, lr=1.42014e-05, gnorm=0.756, train_wall=62, wall=16049
2021-01-01 16:13:06 | INFO | train_inner | epoch 057:    324 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.96, ppl=1.95, wps=22447.6, ups=1.6, wpb=14044.8, bsz=504.2, num_updates=23900, lr=1.41717e-05, gnorm=0.747, train_wall=62, wall=16112
2021-01-01 16:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:14:23 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.28 | nll_loss 3.737 | ppl 13.33 | bleu 23.16 | wps 5969.9 | wpb 10324.2 | bsz 375 | num_updates 23997 | best_bleu 23.17
2021-01-01 16:14:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:14:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 57 @ 23997 updates, score 23.16) (writing took 2.8717649690806866 seconds)
2021-01-01 16:14:26 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-01 16:14:26 | INFO | train | epoch 057 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.275 | nll_loss 0.964 | ppl 1.95 | wps 20595 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 23997 | lr 1.4143e-05 | gnorm 0.751 | train_wall 262 | wall 16192
2021-01-01 16:14:26 | INFO | fairseq.trainer | begin training epoch 58
2021-01-01 16:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:14:31 | INFO | train_inner | epoch 058:      3 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.963, ppl=1.95, wps=16375.2, ups=1.17, wpb=13965.8, bsz=483.9, num_updates=24000, lr=1.41421e-05, gnorm=0.757, train_wall=62, wall=16197
2021-01-01 16:15:33 | INFO | train_inner | epoch 058:    103 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.952, ppl=1.94, wps=22911.6, ups=1.62, wpb=14125.7, bsz=513.5, num_updates=24100, lr=1.41128e-05, gnorm=0.741, train_wall=61, wall=16259
2021-01-01 16:16:35 | INFO | train_inner | epoch 058:    203 / 421 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.976, ppl=1.97, wps=22100.6, ups=1.59, wpb=13886.5, bsz=477.5, num_updates=24200, lr=1.40836e-05, gnorm=0.767, train_wall=63, wall=16321
2021-01-01 16:17:38 | INFO | train_inner | epoch 058:    303 / 421 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.964, ppl=1.95, wps=22185.5, ups=1.59, wpb=13979.3, bsz=488.4, num_updates=24300, lr=1.40546e-05, gnorm=0.754, train_wall=63, wall=16384
2021-01-01 16:18:41 | INFO | train_inner | epoch 058:    403 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.964, ppl=1.95, wps=22357.3, ups=1.61, wpb=13927.2, bsz=482.3, num_updates=24400, lr=1.40257e-05, gnorm=0.75, train_wall=62, wall=16447
2021-01-01 16:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:18:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:18:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:18:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:18:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:18:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:19:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:19:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:19:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:19:09 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.277 | nll_loss 3.735 | ppl 13.31 | bleu 23.06 | wps 5970.2 | wpb 10324.2 | bsz 375 | num_updates 24418 | best_bleu 23.17
2021-01-01 16:19:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:19:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 58 @ 24418 updates, score 23.06) (writing took 2.998379047960043 seconds)
2021-01-01 16:19:12 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-01 16:19:12 | INFO | train | epoch 058 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.273 | nll_loss 0.963 | ppl 1.95 | wps 20570.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 24418 | lr 1.40206e-05 | gnorm 0.752 | train_wall 262 | wall 16478
2021-01-01 16:19:12 | INFO | fairseq.trainer | begin training epoch 59
2021-01-01 16:19:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:19:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:20:06 | INFO | train_inner | epoch 059:     82 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.964, ppl=1.95, wps=16470.7, ups=1.18, wpb=13975.5, bsz=496.1, num_updates=24500, lr=1.39971e-05, gnorm=0.759, train_wall=62, wall=16532
2021-01-01 16:21:08 | INFO | train_inner | epoch 059:    182 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.96, ppl=1.95, wps=22243.5, ups=1.6, wpb=13932.4, bsz=486.6, num_updates=24600, lr=1.39686e-05, gnorm=0.746, train_wall=62, wall=16594
2021-01-01 16:22:10 | INFO | train_inner | epoch 059:    282 / 421 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.966, ppl=1.95, wps=22352.1, ups=1.61, wpb=13891, bsz=476.6, num_updates=24700, lr=1.39403e-05, gnorm=0.761, train_wall=62, wall=16656
2021-01-01 16:23:12 | INFO | train_inner | epoch 059:    382 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.957, ppl=1.94, wps=22771.4, ups=1.61, wpb=14152.9, bsz=520.6, num_updates=24800, lr=1.39122e-05, gnorm=0.733, train_wall=62, wall=16718
2021-01-01 16:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:23:54 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.278 | nll_loss 3.735 | ppl 13.31 | bleu 23.08 | wps 5793.7 | wpb 10324.2 | bsz 375 | num_updates 24839 | best_bleu 23.17
2021-01-01 16:23:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:23:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:23:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:23:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 59 @ 24839 updates, score 23.08) (writing took 2.8745309077203274 seconds)
2021-01-01 16:23:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-01 16:23:57 | INFO | train | epoch 059 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.273 | nll_loss 0.963 | ppl 1.95 | wps 20615.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 24839 | lr 1.39012e-05 | gnorm 0.751 | train_wall 261 | wall 16763
2021-01-01 16:23:57 | INFO | fairseq.trainer | begin training epoch 60
2021-01-01 16:23:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:24:37 | INFO | train_inner | epoch 060:     61 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.965, ppl=1.95, wps=16368.6, ups=1.18, wpb=13915, bsz=494.5, num_updates=24900, lr=1.38842e-05, gnorm=0.75, train_wall=62, wall=16803
2021-01-01 16:25:40 | INFO | train_inner | epoch 060:    161 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.963, ppl=1.95, wps=22335.2, ups=1.6, wpb=13949.2, bsz=495.3, num_updates=25000, lr=1.38564e-05, gnorm=0.75, train_wall=62, wall=16866
2021-01-01 16:26:43 | INFO | train_inner | epoch 060:    261 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.962, ppl=1.95, wps=22186.4, ups=1.59, wpb=13980, bsz=487, num_updates=25100, lr=1.38288e-05, gnorm=0.75, train_wall=63, wall=16929
2021-01-01 16:27:45 | INFO | train_inner | epoch 060:    361 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.963, ppl=1.95, wps=22580.1, ups=1.61, wpb=14002.2, bsz=497.2, num_updates=25200, lr=1.38013e-05, gnorm=0.746, train_wall=62, wall=16991
2021-01-01 16:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:28:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:28:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:28:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:28:40 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.734 | ppl 13.31 | bleu 23.16 | wps 5839.2 | wpb 10324.2 | bsz 375 | num_updates 25260 | best_bleu 23.17
2021-01-01 16:28:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:28:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:28:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 60 @ 25260 updates, score 23.16) (writing took 2.8745010644197464 seconds)
2021-01-01 16:28:42 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-01 16:28:42 | INFO | train | epoch 060 | symm_kl 0.373 | self_kl 0 | self_cv 0 | loss 3.27 | nll_loss 0.962 | ppl 1.95 | wps 20591.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 25260 | lr 1.37849e-05 | gnorm 0.752 | train_wall 262 | wall 17049
2021-01-01 16:28:42 | INFO | fairseq.trainer | begin training epoch 61
2021-01-01 16:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:29:10 | INFO | train_inner | epoch 061:     40 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.959, ppl=1.94, wps=16370.5, ups=1.17, wpb=13960.9, bsz=497.4, num_updates=25300, lr=1.3774e-05, gnorm=0.761, train_wall=62, wall=17076
2021-01-01 16:30:13 | INFO | train_inner | epoch 061:    140 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.965, ppl=1.95, wps=22156.5, ups=1.6, wpb=13876.7, bsz=488.5, num_updates=25400, lr=1.37469e-05, gnorm=0.751, train_wall=62, wall=17139
2021-01-01 16:31:15 | INFO | train_inner | epoch 061:    240 / 421 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.965, ppl=1.95, wps=22277.1, ups=1.6, wpb=13922.9, bsz=496.8, num_updates=25500, lr=1.37199e-05, gnorm=0.759, train_wall=62, wall=17201
2021-01-01 16:32:18 | INFO | train_inner | epoch 061:    340 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.956, ppl=1.94, wps=22583.2, ups=1.59, wpb=14164, bsz=486.6, num_updates=25600, lr=1.36931e-05, gnorm=0.747, train_wall=63, wall=17264
2021-01-01 16:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:33:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:33:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:33:25 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.735 | ppl 13.31 | bleu 23.17 | wps 5953.9 | wpb 10324.2 | bsz 375 | num_updates 25681 | best_bleu 23.17
2021-01-01 16:33:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:33:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 61 @ 25681 updates, score 23.17) (writing took 4.832084508612752 seconds)
2021-01-01 16:33:30 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-01 16:33:30 | INFO | train | epoch 061 | symm_kl 0.373 | self_kl 0 | self_cv 0 | loss 3.27 | nll_loss 0.962 | ppl 1.95 | wps 20456.4 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 25681 | lr 1.36715e-05 | gnorm 0.752 | train_wall 262 | wall 17336
2021-01-01 16:33:30 | INFO | fairseq.trainer | begin training epoch 62
2021-01-01 16:33:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:33:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:33:45 | INFO | train_inner | epoch 062:     19 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.967, ppl=1.95, wps=15933, ups=1.15, wpb=13835, bsz=493.3, num_updates=25700, lr=1.36664e-05, gnorm=0.755, train_wall=62, wall=17351
2021-01-01 16:34:47 | INFO | train_inner | epoch 062:    119 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.956, ppl=1.94, wps=22544.6, ups=1.61, wpb=13986.2, bsz=487.6, num_updates=25800, lr=1.36399e-05, gnorm=0.748, train_wall=62, wall=17413
2021-01-01 16:35:49 | INFO | train_inner | epoch 062:    219 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.963, ppl=1.95, wps=22573.9, ups=1.61, wpb=14050.9, bsz=497.9, num_updates=25900, lr=1.36135e-05, gnorm=0.744, train_wall=62, wall=17475
2021-01-01 16:36:52 | INFO | train_inner | epoch 062:    319 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.959, ppl=1.94, wps=22282.1, ups=1.6, wpb=13962.5, bsz=495.5, num_updates=26000, lr=1.35873e-05, gnorm=0.753, train_wall=62, wall=17538
2021-01-01 16:37:54 | INFO | train_inner | epoch 062:    419 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.967, ppl=1.95, wps=22511.9, ups=1.61, wpb=13977.3, bsz=489.4, num_updates=26100, lr=1.35613e-05, gnorm=0.747, train_wall=62, wall=17600
2021-01-01 16:37:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:37:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:37:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:37:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:37:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:37:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:37:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:37:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:37:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:38:12 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.735 | ppl 13.31 | bleu 23.1 | wps 5946.3 | wpb 10324.2 | bsz 375 | num_updates 26102 | best_bleu 23.17
2021-01-01 16:38:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:38:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:38:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:38:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:38:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:38:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:38:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:38:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 62 @ 26102 updates, score 23.1) (writing took 2.8965428583323956 seconds)
2021-01-01 16:38:15 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-01 16:38:15 | INFO | train | epoch 062 | symm_kl 0.372 | self_kl 0 | self_cv 0 | loss 3.268 | nll_loss 0.962 | ppl 1.95 | wps 20642.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 26102 | lr 1.35608e-05 | gnorm 0.75 | train_wall 261 | wall 17621
2021-01-01 16:38:15 | INFO | fairseq.trainer | begin training epoch 63
2021-01-01 16:38:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:38:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:39:19 | INFO | train_inner | epoch 063:     98 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.952, ppl=1.93, wps=16528.9, ups=1.17, wpb=14090.7, bsz=472.6, num_updates=26200, lr=1.35354e-05, gnorm=0.756, train_wall=62, wall=17685
2021-01-01 16:40:22 | INFO | train_inner | epoch 063:    198 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.967, ppl=1.95, wps=22297.6, ups=1.6, wpb=13897.7, bsz=494.5, num_updates=26300, lr=1.35096e-05, gnorm=0.748, train_wall=62, wall=17748
2021-01-01 16:41:24 | INFO | train_inner | epoch 063:    298 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.96, ppl=1.95, wps=22169, ups=1.59, wpb=13936.7, bsz=513.1, num_updates=26400, lr=1.3484e-05, gnorm=0.745, train_wall=63, wall=17810
2021-01-01 16:42:27 | INFO | train_inner | epoch 063:    398 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.969, ppl=1.96, wps=22283.5, ups=1.6, wpb=13957.4, bsz=498.3, num_updates=26500, lr=1.34585e-05, gnorm=0.745, train_wall=62, wall=17873
2021-01-01 16:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:42:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:42:58 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.734 | ppl 13.3 | bleu 23.12 | wps 6017 | wpb 10324.2 | bsz 375 | num_updates 26523 | best_bleu 23.17
2021-01-01 16:42:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:42:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:42:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:43:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 63 @ 26523 updates, score 23.12) (writing took 2.889331690967083 seconds)
2021-01-01 16:43:01 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-01 16:43:01 | INFO | train | epoch 063 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.267 | nll_loss 0.961 | ppl 1.95 | wps 20576.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 26523 | lr 1.34527e-05 | gnorm 0.747 | train_wall 262 | wall 17907
2021-01-01 16:43:01 | INFO | fairseq.trainer | begin training epoch 64
2021-01-01 16:43:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:43:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:43:51 | INFO | train_inner | epoch 064:     77 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.95, ppl=1.93, wps=16504.5, ups=1.19, wpb=13916.8, bsz=482.8, num_updates=26600, lr=1.34332e-05, gnorm=0.748, train_wall=61, wall=17957
2021-01-01 16:44:54 | INFO | train_inner | epoch 064:    177 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.966, ppl=1.95, wps=22449.5, ups=1.61, wpb=13975.7, bsz=487.3, num_updates=26700, lr=1.3408e-05, gnorm=0.748, train_wall=62, wall=18020
2021-01-01 16:45:56 | INFO | train_inner | epoch 064:    277 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.961, ppl=1.95, wps=22567, ups=1.6, wpb=14128.6, bsz=497.4, num_updates=26800, lr=1.3383e-05, gnorm=0.738, train_wall=62, wall=18082
2021-01-01 16:46:59 | INFO | train_inner | epoch 064:    377 / 421 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.965, ppl=1.95, wps=22188.8, ups=1.6, wpb=13883.7, bsz=493.9, num_updates=26900, lr=1.33581e-05, gnorm=0.749, train_wall=62, wall=18145
2021-01-01 16:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:47:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:47:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:47:43 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.275 | nll_loss 3.732 | ppl 13.28 | bleu 23.11 | wps 5882.1 | wpb 10324.2 | bsz 375 | num_updates 26944 | best_bleu 23.17
2021-01-01 16:47:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:47:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 64 @ 26944 updates, score 23.11) (writing took 2.919923273846507 seconds)
2021-01-01 16:47:46 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-01 16:47:46 | INFO | train | epoch 064 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.265 | nll_loss 0.961 | ppl 1.95 | wps 20593.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 26944 | lr 1.33472e-05 | gnorm 0.745 | train_wall 262 | wall 18192
2021-01-01 16:47:46 | INFO | fairseq.trainer | begin training epoch 65
2021-01-01 16:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:47:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:47:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:48:24 | INFO | train_inner | epoch 065:     56 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.962, ppl=1.95, wps=16216, ups=1.18, wpb=13770.9, bsz=483.4, num_updates=27000, lr=1.33333e-05, gnorm=0.756, train_wall=62, wall=18230
2021-01-01 16:49:26 | INFO | train_inner | epoch 065:    156 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.957, ppl=1.94, wps=22514.1, ups=1.6, wpb=14040.4, bsz=480.6, num_updates=27100, lr=1.33087e-05, gnorm=0.739, train_wall=62, wall=18292
2021-01-01 16:50:28 | INFO | train_inner | epoch 065:    256 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.963, ppl=1.95, wps=22461.8, ups=1.61, wpb=13927, bsz=516.4, num_updates=27200, lr=1.32842e-05, gnorm=0.747, train_wall=62, wall=18354
2021-01-01 16:51:30 | INFO | train_inner | epoch 065:    356 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.962, ppl=1.95, wps=22531.6, ups=1.6, wpb=14052, bsz=486.2, num_updates=27300, lr=1.32599e-05, gnorm=0.738, train_wall=62, wall=18416
2021-01-01 16:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:52:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:52:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:52:29 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.278 | nll_loss 3.734 | ppl 13.31 | bleu 23.13 | wps 5580.2 | wpb 10324.2 | bsz 375 | num_updates 27365 | best_bleu 23.17
2021-01-01 16:52:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 65 @ 27365 updates, score 23.13) (writing took 2.9302429016679525 seconds)
2021-01-01 16:52:32 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-01 16:52:32 | INFO | train | epoch 065 | symm_kl 0.37 | self_kl 0 | self_cv 0 | loss 3.265 | nll_loss 0.961 | ppl 1.95 | wps 20600.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27365 | lr 1.32441e-05 | gnorm 0.744 | train_wall 261 | wall 18478
2021-01-01 16:52:32 | INFO | fairseq.trainer | begin training epoch 66
2021-01-01 16:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:52:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:52:56 | INFO | train_inner | epoch 066:     35 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.964, ppl=1.95, wps=16137.2, ups=1.16, wpb=13884.4, bsz=501.7, num_updates=27400, lr=1.32357e-05, gnorm=0.752, train_wall=62, wall=18502
2021-01-01 16:53:59 | INFO | train_inner | epoch 066:    135 / 421 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.948, ppl=1.93, wps=22391.8, ups=1.6, wpb=14031.2, bsz=494.3, num_updates=27500, lr=1.32116e-05, gnorm=0.738, train_wall=62, wall=18565
2021-01-01 16:55:02 | INFO | train_inner | epoch 066:    235 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.953, ppl=1.94, wps=22222.3, ups=1.59, wpb=14005.9, bsz=498.6, num_updates=27600, lr=1.31876e-05, gnorm=0.747, train_wall=63, wall=18628
2021-01-01 16:56:05 | INFO | train_inner | epoch 066:    335 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.959, ppl=1.94, wps=22457.2, ups=1.59, wpb=14104.2, bsz=488.2, num_updates=27700, lr=1.31638e-05, gnorm=0.741, train_wall=63, wall=18691
2021-01-01 16:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 16:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:56:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 16:57:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 16:57:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 16:57:15 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.275 | nll_loss 3.732 | ppl 13.29 | bleu 23.07 | wps 5947 | wpb 10324.2 | bsz 375 | num_updates 27786 | best_bleu 23.17
2021-01-01 16:57:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 16:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:57:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:57:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 66 @ 27786 updates, score 23.07) (writing took 2.892045997083187 seconds)
2021-01-01 16:57:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-01 16:57:18 | INFO | train | epoch 066 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.263 | nll_loss 0.96 | ppl 1.95 | wps 20544.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 27786 | lr 1.31434e-05 | gnorm 0.747 | train_wall 262 | wall 18764
2021-01-01 16:57:18 | INFO | fairseq.trainer | begin training epoch 67
2021-01-01 16:57:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 16:57:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 16:57:30 | INFO | train_inner | epoch 067:     14 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.969, ppl=1.96, wps=16283.2, ups=1.18, wpb=13844.4, bsz=484.9, num_updates=27800, lr=1.31401e-05, gnorm=0.757, train_wall=62, wall=18776
2021-01-01 16:58:32 | INFO | train_inner | epoch 067:    114 / 421 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.973, ppl=1.96, wps=22505.2, ups=1.61, wpb=13962.4, bsz=482.9, num_updates=27900, lr=1.31165e-05, gnorm=0.754, train_wall=62, wall=18838
2021-01-01 16:59:35 | INFO | train_inner | epoch 067:    214 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.957, ppl=1.94, wps=22509.4, ups=1.6, wpb=14065.6, bsz=497.8, num_updates=28000, lr=1.30931e-05, gnorm=0.741, train_wall=62, wall=18901
2021-01-01 17:00:37 | INFO | train_inner | epoch 067:    314 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.951, ppl=1.93, wps=22510.9, ups=1.61, wpb=14011.8, bsz=495.5, num_updates=28100, lr=1.30698e-05, gnorm=0.744, train_wall=62, wall=18963
2021-01-01 17:01:39 | INFO | train_inner | epoch 067:    414 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.963, ppl=1.95, wps=22382.7, ups=1.62, wpb=13845.2, bsz=495.4, num_updates=28200, lr=1.30466e-05, gnorm=0.747, train_wall=62, wall=19025
2021-01-01 17:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:01:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:01:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:01:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:01:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:01:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:01:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:01:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:01:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:02:00 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.733 | ppl 13.29 | bleu 23.09 | wps 5997.6 | wpb 10324.2 | bsz 375 | num_updates 28207 | best_bleu 23.17
2021-01-01 17:02:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:02:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:02:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:02:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:02:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 67 @ 28207 updates, score 23.09) (writing took 2.9393760319799185 seconds)
2021-01-01 17:02:03 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-01 17:02:03 | INFO | train | epoch 067 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.262 | nll_loss 0.96 | ppl 1.94 | wps 20664.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 28207 | lr 1.30449e-05 | gnorm 0.747 | train_wall 261 | wall 19049
2021-01-01 17:02:03 | INFO | fairseq.trainer | begin training epoch 68
2021-01-01 17:02:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:02:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:03:04 | INFO | train_inner | epoch 068:     93 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.949, ppl=1.93, wps=16356.7, ups=1.18, wpb=13895.4, bsz=502.8, num_updates=28300, lr=1.30235e-05, gnorm=0.743, train_wall=62, wall=19110
2021-01-01 17:04:06 | INFO | train_inner | epoch 068:    193 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.958, ppl=1.94, wps=22161.2, ups=1.59, wpb=13941.5, bsz=498.7, num_updates=28400, lr=1.30005e-05, gnorm=0.747, train_wall=63, wall=19173
2021-01-01 17:05:09 | INFO | train_inner | epoch 068:    293 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.964, ppl=1.95, wps=22474.2, ups=1.6, wpb=14061.8, bsz=483.7, num_updates=28500, lr=1.29777e-05, gnorm=0.744, train_wall=62, wall=19235
2021-01-01 17:06:11 | INFO | train_inner | epoch 068:    393 / 421 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.965, ppl=1.95, wps=22470.9, ups=1.6, wpb=14016.9, bsz=485.7, num_updates=28600, lr=1.2955e-05, gnorm=0.737, train_wall=62, wall=19297
2021-01-01 17:06:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:06:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:06:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:06:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:06:45 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.28 | nll_loss 3.737 | ppl 13.34 | bleu 23.04 | wps 6015.7 | wpb 10324.2 | bsz 375 | num_updates 28628 | best_bleu 23.17
2021-01-01 17:06:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:06:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:06:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 68 @ 28628 updates, score 23.04) (writing took 2.952711360529065 seconds)
2021-01-01 17:06:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-01 17:06:48 | INFO | train | epoch 068 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.261 | nll_loss 0.959 | ppl 1.94 | wps 20603.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 28628 | lr 1.29487e-05 | gnorm 0.744 | train_wall 262 | wall 19334
2021-01-01 17:06:48 | INFO | fairseq.trainer | begin training epoch 69
2021-01-01 17:06:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:06:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:07:36 | INFO | train_inner | epoch 069:     72 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.963, ppl=1.95, wps=16396.9, ups=1.18, wpb=13856.8, bsz=488.3, num_updates=28700, lr=1.29324e-05, gnorm=0.753, train_wall=61, wall=19382
2021-01-01 17:08:38 | INFO | train_inner | epoch 069:    172 / 421 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.959, ppl=1.94, wps=22440.4, ups=1.6, wpb=13998.6, bsz=496.1, num_updates=28800, lr=1.29099e-05, gnorm=0.741, train_wall=62, wall=19444
2021-01-01 17:09:41 | INFO | train_inner | epoch 069:    272 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.954, ppl=1.94, wps=22758.5, ups=1.61, wpb=14161.2, bsz=500.7, num_updates=28900, lr=1.28876e-05, gnorm=0.736, train_wall=62, wall=19507
2021-01-01 17:10:43 | INFO | train_inner | epoch 069:    372 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.963, ppl=1.95, wps=22167.6, ups=1.6, wpb=13844.5, bsz=482.7, num_updates=29000, lr=1.28654e-05, gnorm=0.748, train_wall=62, wall=19569
2021-01-01 17:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:11:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:11:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:11:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:11:30 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.273 | nll_loss 3.73 | ppl 13.27 | bleu 23.15 | wps 5984.9 | wpb 10324.2 | bsz 375 | num_updates 29049 | best_bleu 23.17
2021-01-01 17:11:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:11:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:11:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 69 @ 29049 updates, score 23.15) (writing took 2.814060030505061 seconds)
2021-01-01 17:11:33 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-01 17:11:33 | INFO | train | epoch 069 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.26 | nll_loss 0.96 | ppl 1.95 | wps 20647.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 29049 | lr 1.28545e-05 | gnorm 0.744 | train_wall 261 | wall 19619
2021-01-01 17:11:33 | INFO | fairseq.trainer | begin training epoch 70
2021-01-01 17:11:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:12:07 | INFO | train_inner | epoch 070:     51 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.951, ppl=1.93, wps=16503.6, ups=1.18, wpb=13940.7, bsz=497.4, num_updates=29100, lr=1.28432e-05, gnorm=0.748, train_wall=62, wall=19654
2021-01-01 17:13:10 | INFO | train_inner | epoch 070:    151 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.954, ppl=1.94, wps=22514.5, ups=1.6, wpb=14040.9, bsz=491.7, num_updates=29200, lr=1.28212e-05, gnorm=0.746, train_wall=62, wall=19716
2021-01-01 17:14:12 | INFO | train_inner | epoch 070:    251 / 421 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.969, ppl=1.96, wps=22462.9, ups=1.61, wpb=13968.4, bsz=492.8, num_updates=29300, lr=1.27993e-05, gnorm=0.755, train_wall=62, wall=19778
2021-01-01 17:15:15 | INFO | train_inner | epoch 070:    351 / 421 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.949, ppl=1.93, wps=22380.3, ups=1.6, wpb=14021.5, bsz=494.1, num_updates=29400, lr=1.27775e-05, gnorm=0.737, train_wall=62, wall=19841
2021-01-01 17:15:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:15:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:15:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:15:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:15:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:16:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:16:15 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.73 | ppl 13.27 | bleu 23.04 | wps 6056.8 | wpb 10324.2 | bsz 375 | num_updates 29470 | best_bleu 23.17
2021-01-01 17:16:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:16:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 70 @ 29470 updates, score 23.04) (writing took 2.8934267722070217 seconds)
2021-01-01 17:16:18 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-01 17:16:18 | INFO | train | epoch 070 | symm_kl 0.367 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.959 | ppl 1.94 | wps 20623.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 29470 | lr 1.27623e-05 | gnorm 0.747 | train_wall 262 | wall 19904
2021-01-01 17:16:18 | INFO | fairseq.trainer | begin training epoch 71
2021-01-01 17:16:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:16:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:16:40 | INFO | train_inner | epoch 071:     30 / 421 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.976, ppl=1.97, wps=16216.7, ups=1.18, wpb=13792.5, bsz=475.6, num_updates=29500, lr=1.27559e-05, gnorm=0.756, train_wall=62, wall=19926
2021-01-01 17:17:42 | INFO | train_inner | epoch 071:    130 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.96, ppl=1.94, wps=22563.6, ups=1.61, wpb=14042.9, bsz=493.1, num_updates=29600, lr=1.27343e-05, gnorm=0.732, train_wall=62, wall=19988
2021-01-01 17:18:44 | INFO | train_inner | epoch 071:    230 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.964, ppl=1.95, wps=22428.8, ups=1.61, wpb=13948, bsz=496.8, num_updates=29700, lr=1.27128e-05, gnorm=0.749, train_wall=62, wall=20050
2021-01-01 17:19:46 | INFO | train_inner | epoch 071:    330 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.961, ppl=1.95, wps=22619.3, ups=1.61, wpb=14070.8, bsz=493.8, num_updates=29800, lr=1.26915e-05, gnorm=0.745, train_wall=62, wall=20112
2021-01-01 17:20:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:20:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:20:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:20:59 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.278 | nll_loss 3.735 | ppl 13.32 | bleu 23.08 | wps 5971.5 | wpb 10324.2 | bsz 375 | num_updates 29891 | best_bleu 23.17
2021-01-01 17:20:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:21:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 71 @ 29891 updates, score 23.08) (writing took 2.7953091468662024 seconds)
2021-01-01 17:21:02 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-01 17:21:02 | INFO | train | epoch 071 | symm_kl 0.367 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.959 | ppl 1.94 | wps 20709.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 29891 | lr 1.26722e-05 | gnorm 0.745 | train_wall 260 | wall 20188
2021-01-01 17:21:02 | INFO | fairseq.trainer | begin training epoch 72
2021-01-01 17:21:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:21:11 | INFO | train_inner | epoch 072:      9 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.947, ppl=1.93, wps=16354.2, ups=1.18, wpb=13812, bsz=497.7, num_updates=29900, lr=1.26702e-05, gnorm=0.753, train_wall=61, wall=20197
2021-01-01 17:22:13 | INFO | train_inner | epoch 072:    109 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.96, ppl=1.94, wps=22416.2, ups=1.61, wpb=13899.2, bsz=500.5, num_updates=30000, lr=1.26491e-05, gnorm=0.737, train_wall=62, wall=20259
2021-01-01 17:23:15 | INFO | train_inner | epoch 072:    209 / 421 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.969, ppl=1.96, wps=22486.1, ups=1.6, wpb=14056.5, bsz=479, num_updates=30100, lr=1.26281e-05, gnorm=0.75, train_wall=62, wall=20321
2021-01-01 17:24:18 | INFO | train_inner | epoch 072:    309 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.941, ppl=1.92, wps=22658.6, ups=1.61, wpb=14090.8, bsz=503.7, num_updates=30200, lr=1.26072e-05, gnorm=0.734, train_wall=62, wall=20384
2021-01-01 17:25:19 | INFO | train_inner | epoch 072:    409 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.954, ppl=1.94, wps=22598.4, ups=1.62, wpb=13971.2, bsz=488.9, num_updates=30300, lr=1.25863e-05, gnorm=0.751, train_wall=62, wall=20445
2021-01-01 17:25:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:25:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:25:43 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.732 | ppl 13.29 | bleu 23.15 | wps 5918 | wpb 10324.2 | bsz 375 | num_updates 30312 | best_bleu 23.17
2021-01-01 17:25:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:25:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 72 @ 30312 updates, score 23.15) (writing took 2.904188798740506 seconds)
2021-01-01 17:25:46 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-01 17:25:46 | INFO | train | epoch 072 | symm_kl 0.366 | self_kl 0 | self_cv 0 | loss 3.256 | nll_loss 0.957 | ppl 1.94 | wps 20690.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 30312 | lr 1.25838e-05 | gnorm 0.744 | train_wall 260 | wall 20472
2021-01-01 17:25:46 | INFO | fairseq.trainer | begin training epoch 73
2021-01-01 17:25:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:25:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:26:44 | INFO | train_inner | epoch 073:     88 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.961, ppl=1.95, wps=16288.2, ups=1.18, wpb=13804.3, bsz=486.8, num_updates=30400, lr=1.25656e-05, gnorm=0.741, train_wall=62, wall=20530
2021-01-01 17:27:46 | INFO | train_inner | epoch 073:    188 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.957, ppl=1.94, wps=22437.7, ups=1.62, wpb=13883.7, bsz=482.3, num_updates=30500, lr=1.2545e-05, gnorm=0.746, train_wall=62, wall=20592
2021-01-01 17:28:48 | INFO | train_inner | epoch 073:    288 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.941, ppl=1.92, wps=22605.6, ups=1.6, wpb=14107.1, bsz=514.2, num_updates=30600, lr=1.25245e-05, gnorm=0.727, train_wall=62, wall=20654
2021-01-01 17:29:51 | INFO | train_inner | epoch 073:    388 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.965, ppl=1.95, wps=22553.7, ups=1.61, wpb=14021.4, bsz=489.6, num_updates=30700, lr=1.25041e-05, gnorm=0.739, train_wall=62, wall=20717
2021-01-01 17:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:30:28 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.273 | nll_loss 3.731 | ppl 13.28 | bleu 23.12 | wps 5974.1 | wpb 10324.2 | bsz 375 | num_updates 30733 | best_bleu 23.17
2021-01-01 17:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:30:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 73 @ 30733 updates, score 23.12) (writing took 2.9088130109012127 seconds)
2021-01-01 17:30:31 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-01 17:30:31 | INFO | train | epoch 073 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.254 | nll_loss 0.957 | ppl 1.94 | wps 20681.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 30733 | lr 1.24974e-05 | gnorm 0.739 | train_wall 261 | wall 20757
2021-01-01 17:30:31 | INFO | fairseq.trainer | begin training epoch 74
2021-01-01 17:30:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:30:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:31:15 | INFO | train_inner | epoch 074:     67 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.964, ppl=1.95, wps=16489.9, ups=1.18, wpb=13947.4, bsz=487, num_updates=30800, lr=1.24838e-05, gnorm=0.742, train_wall=61, wall=20801
2021-01-01 17:32:18 | INFO | train_inner | epoch 074:    167 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.947, ppl=1.93, wps=22568.3, ups=1.6, wpb=14112, bsz=503.8, num_updates=30900, lr=1.24635e-05, gnorm=0.728, train_wall=62, wall=20864
2021-01-01 17:33:20 | INFO | train_inner | epoch 074:    267 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.968, ppl=1.96, wps=22549.8, ups=1.61, wpb=13972.7, bsz=481.2, num_updates=31000, lr=1.24434e-05, gnorm=0.741, train_wall=62, wall=20926
2021-01-01 17:34:22 | INFO | train_inner | epoch 074:    367 / 421 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.962, ppl=1.95, wps=22364.8, ups=1.61, wpb=13875.6, bsz=488.2, num_updates=31100, lr=1.24234e-05, gnorm=0.746, train_wall=62, wall=20988
2021-01-01 17:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:34:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:34:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:34:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:34:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:35:11 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.733 | ppl 13.29 | bleu 23.08 | wps 6008 | wpb 10324.2 | bsz 375 | num_updates 31154 | best_bleu 23.17
2021-01-01 17:35:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:35:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 74 @ 31154 updates, score 23.08) (writing took 2.9800368044525385 seconds)
2021-01-01 17:35:14 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-01 17:35:14 | INFO | train | epoch 074 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.254 | nll_loss 0.958 | ppl 1.94 | wps 20732.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 31154 | lr 1.24126e-05 | gnorm 0.739 | train_wall 260 | wall 21040
2021-01-01 17:35:14 | INFO | fairseq.trainer | begin training epoch 75
2021-01-01 17:35:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:35:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:35:46 | INFO | train_inner | epoch 075:     46 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.948, ppl=1.93, wps=16474.3, ups=1.19, wpb=13850.8, bsz=498.8, num_updates=31200, lr=1.24035e-05, gnorm=0.743, train_wall=61, wall=21072
2021-01-01 17:36:48 | INFO | train_inner | epoch 075:    146 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.945, ppl=1.93, wps=22597.9, ups=1.6, wpb=14149.4, bsz=504.3, num_updates=31300, lr=1.23836e-05, gnorm=0.735, train_wall=62, wall=21134
2021-01-01 17:37:50 | INFO | train_inner | epoch 075:    246 / 421 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.964, ppl=1.95, wps=22574.5, ups=1.61, wpb=13988.6, bsz=487.1, num_updates=31400, lr=1.23639e-05, gnorm=0.747, train_wall=62, wall=21196
2021-01-01 17:38:52 | INFO | train_inner | epoch 075:    346 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.958, ppl=1.94, wps=22350.6, ups=1.61, wpb=13842.7, bsz=481.1, num_updates=31500, lr=1.23443e-05, gnorm=0.745, train_wall=62, wall=21258
2021-01-01 17:39:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:39:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:39:56 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.273 | nll_loss 3.731 | ppl 13.28 | bleu 23.08 | wps 5923.3 | wpb 10324.2 | bsz 375 | num_updates 31575 | best_bleu 23.17
2021-01-01 17:39:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:39:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:39:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 75 @ 31575 updates, score 23.08) (writing took 2.848102943971753 seconds)
2021-01-01 17:39:58 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-01 17:39:58 | INFO | train | epoch 075 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.253 | nll_loss 0.956 | ppl 1.94 | wps 20710.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 31575 | lr 1.23296e-05 | gnorm 0.743 | train_wall 260 | wall 21324
2021-01-01 17:39:58 | INFO | fairseq.trainer | begin training epoch 76
2021-01-01 17:39:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:40:17 | INFO | train_inner | epoch 076:     25 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.958, ppl=1.94, wps=16373.9, ups=1.18, wpb=13898.5, bsz=505.8, num_updates=31600, lr=1.23247e-05, gnorm=0.746, train_wall=62, wall=21343
2021-01-01 17:41:19 | INFO | train_inner | epoch 076:    125 / 421 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.957, ppl=1.94, wps=22833.6, ups=1.62, wpb=14096.4, bsz=515.1, num_updates=31700, lr=1.23053e-05, gnorm=0.731, train_wall=62, wall=21405
2021-01-01 17:42:21 | INFO | train_inner | epoch 076:    225 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.951, ppl=1.93, wps=22555.2, ups=1.61, wpb=13973.1, bsz=492.4, num_updates=31800, lr=1.22859e-05, gnorm=0.738, train_wall=62, wall=21467
2021-01-01 17:43:23 | INFO | train_inner | epoch 076:    325 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.959, ppl=1.94, wps=22381.3, ups=1.6, wpb=13963, bsz=461.8, num_updates=31900, lr=1.22666e-05, gnorm=0.748, train_wall=62, wall=21529
2021-01-01 17:44:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:44:40 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.729 | ppl 13.26 | bleu 23.1 | wps 5558.6 | wpb 10324.2 | bsz 375 | num_updates 31996 | best_bleu 23.17
2021-01-01 17:44:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 76 @ 31996 updates, score 23.1) (writing took 2.940716937184334 seconds)
2021-01-01 17:44:43 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-01 17:44:43 | INFO | train | epoch 076 | symm_kl 0.364 | self_kl 0 | self_cv 0 | loss 3.252 | nll_loss 0.957 | ppl 1.94 | wps 20638.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 31996 | lr 1.22482e-05 | gnorm 0.739 | train_wall 260 | wall 21609
2021-01-01 17:44:43 | INFO | fairseq.trainer | begin training epoch 77
2021-01-01 17:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:44:49 | INFO | train_inner | epoch 077:      4 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.966, ppl=1.95, wps=16155.1, ups=1.17, wpb=13844.7, bsz=485, num_updates=32000, lr=1.22474e-05, gnorm=0.745, train_wall=62, wall=21615
2021-01-01 17:45:50 | INFO | train_inner | epoch 077:    104 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.947, ppl=1.93, wps=22793.5, ups=1.63, wpb=13985.2, bsz=500.6, num_updates=32100, lr=1.22284e-05, gnorm=0.731, train_wall=61, wall=21676
2021-01-01 17:46:53 | INFO | train_inner | epoch 077:    204 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.952, ppl=1.93, wps=22484.7, ups=1.6, wpb=14043.8, bsz=486.1, num_updates=32200, lr=1.22094e-05, gnorm=0.732, train_wall=62, wall=21739
2021-01-01 17:47:55 | INFO | train_inner | epoch 077:    304 / 421 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.962, ppl=1.95, wps=22262.9, ups=1.6, wpb=13904.1, bsz=480.2, num_updates=32300, lr=1.21904e-05, gnorm=0.748, train_wall=62, wall=21801
2021-01-01 17:48:58 | INFO | train_inner | epoch 077:    404 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.965, ppl=1.95, wps=22468.7, ups=1.6, wpb=14050.1, bsz=507.9, num_updates=32400, lr=1.21716e-05, gnorm=0.733, train_wall=62, wall=21864
2021-01-01 17:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:49:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:49:26 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.728 | ppl 13.25 | bleu 23.08 | wps 5321.8 | wpb 10324.2 | bsz 375 | num_updates 32417 | best_bleu 23.17
2021-01-01 17:49:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:49:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:49:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 77 @ 32417 updates, score 23.08) (writing took 2.9302163403481245 seconds)
2021-01-01 17:49:29 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-01 17:49:29 | INFO | train | epoch 077 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.251 | nll_loss 0.956 | ppl 1.94 | wps 20569.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 32417 | lr 1.21684e-05 | gnorm 0.739 | train_wall 261 | wall 21895
2021-01-01 17:49:29 | INFO | fairseq.trainer | begin training epoch 78
2021-01-01 17:49:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:49:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:50:23 | INFO | train_inner | epoch 078:     83 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.96, ppl=1.94, wps=16075, ups=1.17, wpb=13787.4, bsz=490.6, num_updates=32500, lr=1.21529e-05, gnorm=0.748, train_wall=61, wall=21950
2021-01-01 17:51:26 | INFO | train_inner | epoch 078:    183 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.956, ppl=1.94, wps=22651, ups=1.61, wpb=14097.3, bsz=477.3, num_updates=32600, lr=1.21342e-05, gnorm=0.738, train_wall=62, wall=22012
2021-01-01 17:52:28 | INFO | train_inner | epoch 078:    283 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.941, ppl=1.92, wps=22403.1, ups=1.6, wpb=13992.1, bsz=506.1, num_updates=32700, lr=1.21157e-05, gnorm=0.737, train_wall=62, wall=22074
2021-01-01 17:53:30 | INFO | train_inner | epoch 078:    383 / 421 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.965, ppl=1.95, wps=22667, ups=1.62, wpb=14019.5, bsz=506.6, num_updates=32800, lr=1.20972e-05, gnorm=0.741, train_wall=62, wall=22136
2021-01-01 17:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:53:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:53:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:54:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:54:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:54:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:54:10 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.275 | nll_loss 3.732 | ppl 13.28 | bleu 23.2 | wps 5988 | wpb 10324.2 | bsz 375 | num_updates 32838 | best_bleu 23.2
2021-01-01 17:54:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:54:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:54:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:54:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:54:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 78 @ 32838 updates, score 23.2) (writing took 4.93453168310225 seconds)
2021-01-01 17:54:15 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-01 17:54:15 | INFO | train | epoch 078 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.25 | nll_loss 0.956 | ppl 1.94 | wps 20573.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 32838 | lr 1.20902e-05 | gnorm 0.741 | train_wall 260 | wall 22181
2021-01-01 17:54:15 | INFO | fairseq.trainer | begin training epoch 79
2021-01-01 17:54:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:54:56 | INFO | train_inner | epoch 079:     62 / 421 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.956, ppl=1.94, wps=16060.3, ups=1.17, wpb=13781.3, bsz=469.8, num_updates=32900, lr=1.20788e-05, gnorm=0.751, train_wall=61, wall=22222
2021-01-01 17:55:58 | INFO | train_inner | epoch 079:    162 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.952, ppl=1.94, wps=22568, ups=1.61, wpb=14053.4, bsz=495.8, num_updates=33000, lr=1.20605e-05, gnorm=0.734, train_wall=62, wall=22284
2021-01-01 17:57:01 | INFO | train_inner | epoch 079:    262 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.952, ppl=1.93, wps=22339.7, ups=1.59, wpb=14036.6, bsz=502, num_updates=33100, lr=1.20422e-05, gnorm=0.73, train_wall=63, wall=22347
2021-01-01 17:58:03 | INFO | train_inner | epoch 079:    362 / 421 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.96, ppl=1.94, wps=22441.5, ups=1.61, wpb=13914.4, bsz=489, num_updates=33200, lr=1.20241e-05, gnorm=0.743, train_wall=62, wall=22409
2021-01-01 17:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 17:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 17:58:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 17:58:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 17:58:56 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.12 | wps 5959 | wpb 10324.2 | bsz 375 | num_updates 33259 | best_bleu 23.2
2021-01-01 17:58:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 17:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 79 @ 33259 updates, score 23.12) (writing took 2.8859288040548563 seconds)
2021-01-01 17:58:59 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-01 17:58:59 | INFO | train | epoch 079 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.249 | nll_loss 0.955 | ppl 1.94 | wps 20694.6 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 33259 | lr 1.20134e-05 | gnorm 0.739 | train_wall 260 | wall 22465
2021-01-01 17:58:59 | INFO | fairseq.trainer | begin training epoch 80
2021-01-01 17:59:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 17:59:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 17:59:27 | INFO | train_inner | epoch 080:     41 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.955, ppl=1.94, wps=16434.8, ups=1.18, wpb=13895.2, bsz=496.2, num_updates=33300, lr=1.2006e-05, gnorm=0.743, train_wall=61, wall=22494
2021-01-01 18:00:30 | INFO | train_inner | epoch 080:    141 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.952, ppl=1.93, wps=22507.7, ups=1.61, wpb=14015.3, bsz=502.4, num_updates=33400, lr=1.1988e-05, gnorm=0.729, train_wall=62, wall=22556
2021-01-01 18:01:32 | INFO | train_inner | epoch 080:    241 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.961, ppl=1.95, wps=22617.2, ups=1.6, wpb=14112, bsz=494.1, num_updates=33500, lr=1.19701e-05, gnorm=0.734, train_wall=62, wall=22618
2021-01-01 18:02:35 | INFO | train_inner | epoch 080:    341 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.954, ppl=1.94, wps=22353.5, ups=1.6, wpb=13952.8, bsz=476.2, num_updates=33600, lr=1.19523e-05, gnorm=0.737, train_wall=62, wall=22681
2021-01-01 18:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:03:41 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.22 | wps 5833 | wpb 10324.2 | bsz 375 | num_updates 33680 | best_bleu 23.22
2021-01-01 18:03:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:03:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:03:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 80 @ 33680 updates, score 23.22) (writing took 4.930979754775763 seconds)
2021-01-01 18:03:46 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-01 18:03:46 | INFO | train | epoch 080 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.248 | nll_loss 0.955 | ppl 1.94 | wps 20483.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 33680 | lr 1.19381e-05 | gnorm 0.736 | train_wall 261 | wall 22752
2021-01-01 18:03:46 | INFO | fairseq.trainer | begin training epoch 81
2021-01-01 18:03:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:04:02 | INFO | train_inner | epoch 081:     20 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.957, ppl=1.94, wps=15896.8, ups=1.15, wpb=13861.8, bsz=491.9, num_updates=33700, lr=1.19345e-05, gnorm=0.749, train_wall=62, wall=22768
2021-01-01 18:05:03 | INFO | train_inner | epoch 081:    120 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.949, ppl=1.93, wps=22644.8, ups=1.62, wpb=13969.8, bsz=502, num_updates=33800, lr=1.19169e-05, gnorm=0.734, train_wall=61, wall=22830
2021-01-01 18:06:06 | INFO | train_inner | epoch 081:    220 / 421 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.959, ppl=1.94, wps=22318.9, ups=1.61, wpb=13903.9, bsz=499.9, num_updates=33900, lr=1.18993e-05, gnorm=0.739, train_wall=62, wall=22892
2021-01-01 18:07:08 | INFO | train_inner | epoch 081:    320 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.955, ppl=1.94, wps=22821.1, ups=1.62, wpb=14120.4, bsz=487, num_updates=34000, lr=1.18818e-05, gnorm=0.731, train_wall=62, wall=22954
2021-01-01 18:08:10 | INFO | train_inner | epoch 081:    420 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.955, ppl=1.94, wps=22352.8, ups=1.6, wpb=13969.7, bsz=489.8, num_updates=34100, lr=1.18643e-05, gnorm=0.738, train_wall=62, wall=23016
2021-01-01 18:08:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:08:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:08:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:08:27 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.02 | wps 5944.9 | wpb 10324.2 | bsz 375 | num_updates 34101 | best_bleu 23.22
2021-01-01 18:08:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:08:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:08:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 81 @ 34101 updates, score 23.02) (writing took 3.0093119591474533 seconds)
2021-01-01 18:08:30 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-01 18:08:30 | INFO | train | epoch 081 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.247 | nll_loss 0.955 | ppl 1.94 | wps 20705.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 34101 | lr 1.18642e-05 | gnorm 0.738 | train_wall 260 | wall 23036
2021-01-01 18:08:30 | INFO | fairseq.trainer | begin training epoch 82
2021-01-01 18:08:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:08:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:09:35 | INFO | train_inner | epoch 082:     99 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.958, ppl=1.94, wps=16480.9, ups=1.18, wpb=13925.6, bsz=481.2, num_updates=34200, lr=1.1847e-05, gnorm=0.747, train_wall=61, wall=23101
2021-01-01 18:10:37 | INFO | train_inner | epoch 082:    199 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.954, ppl=1.94, wps=22371.3, ups=1.6, wpb=13999.5, bsz=488, num_updates=34300, lr=1.18297e-05, gnorm=0.74, train_wall=62, wall=23163
2021-01-01 18:11:39 | INFO | train_inner | epoch 082:    299 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.954, ppl=1.94, wps=22530.4, ups=1.61, wpb=13976.5, bsz=488.2, num_updates=34400, lr=1.18125e-05, gnorm=0.733, train_wall=62, wall=23225
2021-01-01 18:12:41 | INFO | train_inner | epoch 082:    399 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.952, ppl=1.94, wps=22437.8, ups=1.61, wpb=13959.8, bsz=515.1, num_updates=34500, lr=1.17954e-05, gnorm=0.731, train_wall=62, wall=23288
2021-01-01 18:12:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:12:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:12:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:12:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:12:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:13:12 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.728 | ppl 13.25 | bleu 23.13 | wps 5971.7 | wpb 10324.2 | bsz 375 | num_updates 34522 | best_bleu 23.22
2021-01-01 18:13:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:13:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:13:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:13:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:13:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:13:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:13:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:13:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 82 @ 34522 updates, score 23.13) (writing took 2.898964013904333 seconds)
2021-01-01 18:13:15 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-01 18:13:15 | INFO | train | epoch 082 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.246 | nll_loss 0.954 | ppl 1.94 | wps 20694.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 34522 | lr 1.17916e-05 | gnorm 0.737 | train_wall 261 | wall 23321
2021-01-01 18:13:15 | INFO | fairseq.trainer | begin training epoch 83
2021-01-01 18:13:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:13:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:14:06 | INFO | train_inner | epoch 083:     78 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.96, ppl=1.95, wps=16470.9, ups=1.19, wpb=13862, bsz=480.6, num_updates=34600, lr=1.17783e-05, gnorm=0.743, train_wall=61, wall=23372
2021-01-01 18:15:08 | INFO | train_inner | epoch 083:    178 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.959, ppl=1.94, wps=22342, ups=1.61, wpb=13916.6, bsz=488.6, num_updates=34700, lr=1.17613e-05, gnorm=0.737, train_wall=62, wall=23434
2021-01-01 18:16:10 | INFO | train_inner | epoch 083:    278 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.954, ppl=1.94, wps=22411.5, ups=1.6, wpb=13982, bsz=495, num_updates=34800, lr=1.17444e-05, gnorm=0.733, train_wall=62, wall=23496
2021-01-01 18:17:13 | INFO | train_inner | epoch 083:    378 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.95, ppl=1.93, wps=22543, ups=1.6, wpb=14049.8, bsz=508, num_updates=34900, lr=1.17276e-05, gnorm=0.731, train_wall=62, wall=23559
2021-01-01 18:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:17:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:17:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:17:56 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.731 | ppl 13.27 | bleu 23.09 | wps 5963.7 | wpb 10324.2 | bsz 375 | num_updates 34943 | best_bleu 23.22
2021-01-01 18:17:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 83 @ 34943 updates, score 23.09) (writing took 2.9519661087542772 seconds)
2021-01-01 18:17:59 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-01 18:17:59 | INFO | train | epoch 083 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.245 | nll_loss 0.954 | ppl 1.94 | wps 20678.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 34943 | lr 1.17203e-05 | gnorm 0.736 | train_wall 261 | wall 23605
2021-01-01 18:17:59 | INFO | fairseq.trainer | begin training epoch 84
2021-01-01 18:18:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:18:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:18:38 | INFO | train_inner | epoch 084:     57 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.946, ppl=1.93, wps=16455.7, ups=1.18, wpb=13975, bsz=481.6, num_updates=35000, lr=1.17108e-05, gnorm=0.74, train_wall=62, wall=23644
2021-01-01 18:19:40 | INFO | train_inner | epoch 084:    157 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.955, ppl=1.94, wps=22319.4, ups=1.6, wpb=13918.9, bsz=487.9, num_updates=35100, lr=1.16941e-05, gnorm=0.733, train_wall=62, wall=23706
2021-01-01 18:20:42 | INFO | train_inner | epoch 084:    257 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.949, ppl=1.93, wps=22492.8, ups=1.61, wpb=13950.9, bsz=496.1, num_updates=35200, lr=1.16775e-05, gnorm=0.741, train_wall=62, wall=23768
2021-01-01 18:21:44 | INFO | train_inner | epoch 084:    357 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.957, ppl=1.94, wps=22687.5, ups=1.61, wpb=14065.2, bsz=497.5, num_updates=35300, lr=1.16609e-05, gnorm=0.741, train_wall=62, wall=23830
2021-01-01 18:22:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:22:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:22:41 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.73 | ppl 13.27 | bleu 23.06 | wps 5883.7 | wpb 10324.2 | bsz 375 | num_updates 35364 | best_bleu 23.22
2021-01-01 18:22:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:22:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:22:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 84 @ 35364 updates, score 23.06) (writing took 2.9601584170013666 seconds)
2021-01-01 18:22:44 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-01 18:22:44 | INFO | train | epoch 084 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.244 | nll_loss 0.953 | ppl 1.94 | wps 20667.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 35364 | lr 1.16504e-05 | gnorm 0.739 | train_wall 261 | wall 23890
2021-01-01 18:22:44 | INFO | fairseq.trainer | begin training epoch 85
2021-01-01 18:22:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:22:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:23:09 | INFO | train_inner | epoch 085:     36 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.939, ppl=1.92, wps=16428.5, ups=1.18, wpb=13976.4, bsz=506.2, num_updates=35400, lr=1.16445e-05, gnorm=0.733, train_wall=62, wall=23915
2021-01-01 18:24:11 | INFO | train_inner | epoch 085:    136 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.945, ppl=1.93, wps=22612.9, ups=1.62, wpb=13980.4, bsz=498.1, num_updates=35500, lr=1.1628e-05, gnorm=0.729, train_wall=62, wall=23977
2021-01-01 18:25:13 | INFO | train_inner | epoch 085:    236 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.954, ppl=1.94, wps=22642.7, ups=1.61, wpb=14089.2, bsz=493.7, num_updates=35600, lr=1.16117e-05, gnorm=0.732, train_wall=62, wall=24039
2021-01-01 18:26:15 | INFO | train_inner | epoch 085:    336 / 421 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.964, ppl=1.95, wps=22497.7, ups=1.62, wpb=13923.5, bsz=483.7, num_updates=35700, lr=1.15954e-05, gnorm=0.74, train_wall=62, wall=24101
2021-01-01 18:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:27:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:27:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:27:24 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.729 | ppl 13.26 | bleu 23.19 | wps 6031.4 | wpb 10324.2 | bsz 375 | num_updates 35785 | best_bleu 23.22
2021-01-01 18:27:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:27:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 85 @ 35785 updates, score 23.19) (writing took 2.8906576987355947 seconds)
2021-01-01 18:27:27 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-01 18:27:27 | INFO | train | epoch 085 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.243 | nll_loss 0.954 | ppl 1.94 | wps 20756.6 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 35785 | lr 1.15816e-05 | gnorm 0.737 | train_wall 260 | wall 24173
2021-01-01 18:27:27 | INFO | fairseq.trainer | begin training epoch 86
2021-01-01 18:27:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:27:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:27:39 | INFO | train_inner | epoch 086:     15 / 421 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.961, ppl=1.95, wps=16416, ups=1.18, wpb=13875.6, bsz=480.7, num_updates=35800, lr=1.15792e-05, gnorm=0.754, train_wall=62, wall=24186
2021-01-01 18:28:41 | INFO | train_inner | epoch 086:    115 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.942, ppl=1.92, wps=22654.2, ups=1.62, wpb=13984.8, bsz=487.7, num_updates=35900, lr=1.15631e-05, gnorm=0.739, train_wall=62, wall=24247
2021-01-01 18:29:43 | INFO | train_inner | epoch 086:    215 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.954, ppl=1.94, wps=22534.4, ups=1.62, wpb=13938.5, bsz=498.3, num_updates=36000, lr=1.1547e-05, gnorm=0.724, train_wall=62, wall=24309
2021-01-01 18:30:46 | INFO | train_inner | epoch 086:    315 / 421 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.967, ppl=1.96, wps=22363.6, ups=1.6, wpb=14000.8, bsz=477.6, num_updates=36100, lr=1.1531e-05, gnorm=0.744, train_wall=62, wall=24372
2021-01-01 18:31:48 | INFO | train_inner | epoch 086:    415 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.95, ppl=1.93, wps=22586, ups=1.61, wpb=14012.9, bsz=511.5, num_updates=36200, lr=1.15151e-05, gnorm=0.728, train_wall=62, wall=24434
2021-01-01 18:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:31:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:31:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:32:08 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.73 | ppl 13.27 | bleu 23.13 | wps 5836.5 | wpb 10324.2 | bsz 375 | num_updates 36206 | best_bleu 23.22
2021-01-01 18:32:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:32:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 86 @ 36206 updates, score 23.13) (writing took 2.9338191021233797 seconds)
2021-01-01 18:32:11 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-01 18:32:11 | INFO | train | epoch 086 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.242 | nll_loss 0.953 | ppl 1.94 | wps 20690.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 36206 | lr 1.15141e-05 | gnorm 0.737 | train_wall 260 | wall 24457
2021-01-01 18:32:11 | INFO | fairseq.trainer | begin training epoch 87
2021-01-01 18:32:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:32:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:33:12 | INFO | train_inner | epoch 087:     94 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.949, ppl=1.93, wps=16300.7, ups=1.18, wpb=13761.8, bsz=487.9, num_updates=36300, lr=1.14992e-05, gnorm=0.754, train_wall=61, wall=24518
2021-01-01 18:34:14 | INFO | train_inner | epoch 087:    194 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.944, ppl=1.92, wps=22674.2, ups=1.62, wpb=14010.6, bsz=480, num_updates=36400, lr=1.14834e-05, gnorm=0.738, train_wall=62, wall=24580
2021-01-01 18:35:16 | INFO | train_inner | epoch 087:    294 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.95, ppl=1.93, wps=22605.7, ups=1.6, wpb=14131.5, bsz=494.4, num_updates=36500, lr=1.14676e-05, gnorm=0.737, train_wall=62, wall=24642
2021-01-01 18:36:19 | INFO | train_inner | epoch 087:    394 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.964, ppl=1.95, wps=22391.6, ups=1.61, wpb=13941.3, bsz=509.6, num_updates=36600, lr=1.1452e-05, gnorm=0.736, train_wall=62, wall=24705
2021-01-01 18:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:36:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:36:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:36:53 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.728 | ppl 13.25 | bleu 23.21 | wps 5552.9 | wpb 10324.2 | bsz 375 | num_updates 36627 | best_bleu 23.22
2021-01-01 18:36:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:36:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 87 @ 36627 updates, score 23.21) (writing took 3.0258138608187437 seconds)
2021-01-01 18:36:56 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-01 18:36:56 | INFO | train | epoch 087 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.952 | ppl 1.93 | wps 20627.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 36627 | lr 1.14477e-05 | gnorm 0.739 | train_wall 260 | wall 24742
2021-01-01 18:36:56 | INFO | fairseq.trainer | begin training epoch 88
2021-01-01 18:36:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:36:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:37:44 | INFO | train_inner | epoch 088:     73 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.955, ppl=1.94, wps=16257.1, ups=1.17, wpb=13922, bsz=483, num_updates=36700, lr=1.14364e-05, gnorm=0.742, train_wall=62, wall=24790
2021-01-01 18:38:46 | INFO | train_inner | epoch 088:    173 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.94, ppl=1.92, wps=22328.8, ups=1.61, wpb=13837.3, bsz=483.8, num_updates=36800, lr=1.14208e-05, gnorm=0.73, train_wall=62, wall=24852
2021-01-01 18:39:49 | INFO | train_inner | epoch 088:    273 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.962, ppl=1.95, wps=22476.1, ups=1.59, wpb=14095, bsz=506.8, num_updates=36900, lr=1.14053e-05, gnorm=0.733, train_wall=63, wall=24915
2021-01-01 18:40:51 | INFO | train_inner | epoch 088:    373 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.956, ppl=1.94, wps=22367.3, ups=1.6, wpb=13973.8, bsz=490.4, num_updates=37000, lr=1.13899e-05, gnorm=0.742, train_wall=62, wall=24978
2021-01-01 18:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:41:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:41:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:41:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:41:39 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.727 | ppl 13.24 | bleu 23.07 | wps 5208.8 | wpb 10324.2 | bsz 375 | num_updates 37048 | best_bleu 23.22
2021-01-01 18:41:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:41:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:41:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 88 @ 37048 updates, score 23.07) (writing took 2.8515460565686226 seconds)
2021-01-01 18:41:42 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-01 18:41:42 | INFO | train | epoch 088 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.952 | ppl 1.94 | wps 20563.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 37048 | lr 1.13825e-05 | gnorm 0.736 | train_wall 260 | wall 25028
2021-01-01 18:41:42 | INFO | fairseq.trainer | begin training epoch 89
2021-01-01 18:41:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:41:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:42:17 | INFO | train_inner | epoch 089:     52 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.948, ppl=1.93, wps=16265.9, ups=1.16, wpb=13986.3, bsz=500.3, num_updates=37100, lr=1.13745e-05, gnorm=0.726, train_wall=61, wall=25064
2021-01-01 18:43:19 | INFO | train_inner | epoch 089:    152 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.942, ppl=1.92, wps=22498.8, ups=1.62, wpb=13927.9, bsz=491.8, num_updates=37200, lr=1.13592e-05, gnorm=0.728, train_wall=62, wall=25125
2021-01-01 18:44:21 | INFO | train_inner | epoch 089:    252 / 421 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.962, ppl=1.95, wps=22639.7, ups=1.62, wpb=13999.4, bsz=495.4, num_updates=37300, lr=1.1344e-05, gnorm=0.736, train_wall=62, wall=25187
2021-01-01 18:45:23 | INFO | train_inner | epoch 089:    352 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.951, ppl=1.93, wps=22744.9, ups=1.61, wpb=14106.2, bsz=471, num_updates=37400, lr=1.13288e-05, gnorm=0.739, train_wall=62, wall=25249
2021-01-01 18:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:46:22 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.729 | ppl 13.26 | bleu 23.23 | wps 5981.7 | wpb 10324.2 | bsz 375 | num_updates 37469 | best_bleu 23.23
2021-01-01 18:46:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 89 @ 37469 updates, score 23.23) (writing took 4.911104507744312 seconds)
2021-01-01 18:46:27 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-01 18:46:27 | INFO | train | epoch 089 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.24 | nll_loss 0.952 | ppl 1.93 | wps 20632.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 37469 | lr 1.13184e-05 | gnorm 0.733 | train_wall 259 | wall 25313
2021-01-01 18:46:27 | INFO | fairseq.trainer | begin training epoch 90
2021-01-01 18:46:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:46:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:46:50 | INFO | train_inner | epoch 090:     31 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.952, ppl=1.93, wps=16025, ups=1.16, wpb=13829.1, bsz=499.7, num_updates=37500, lr=1.13137e-05, gnorm=0.737, train_wall=61, wall=25336
2021-01-01 18:47:52 | INFO | train_inner | epoch 090:    131 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.946, ppl=1.93, wps=22450.7, ups=1.61, wpb=13971.9, bsz=468, num_updates=37600, lr=1.12987e-05, gnorm=0.735, train_wall=62, wall=25398
2021-01-01 18:48:54 | INFO | train_inner | epoch 090:    231 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.951, ppl=1.93, wps=22770.5, ups=1.62, wpb=14079.9, bsz=510.4, num_updates=37700, lr=1.12837e-05, gnorm=0.718, train_wall=62, wall=25460
2021-01-01 18:49:55 | INFO | train_inner | epoch 090:    331 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.959, ppl=1.94, wps=22347.4, ups=1.62, wpb=13773.4, bsz=492.6, num_updates=37800, lr=1.12687e-05, gnorm=0.745, train_wall=61, wall=25521
2021-01-01 18:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:51:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:51:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:51:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:51:08 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.729 | ppl 13.26 | bleu 23.19 | wps 5830.2 | wpb 10324.2 | bsz 375 | num_updates 37890 | best_bleu 23.23
2021-01-01 18:51:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:51:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:51:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:51:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:51:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 90 @ 37890 updates, score 23.19) (writing took 2.9064951352775097 seconds)
2021-01-01 18:51:11 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-01 18:51:11 | INFO | train | epoch 090 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.239 | nll_loss 0.951 | ppl 1.93 | wps 20717.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 37890 | lr 1.12553e-05 | gnorm 0.732 | train_wall 260 | wall 25597
2021-01-01 18:51:11 | INFO | fairseq.trainer | begin training epoch 91
2021-01-01 18:51:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:51:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:51:21 | INFO | train_inner | epoch 091:     10 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.953, ppl=1.94, wps=16446.1, ups=1.17, wpb=14051.5, bsz=501.5, num_updates=37900, lr=1.12538e-05, gnorm=0.727, train_wall=62, wall=25607
2021-01-01 18:52:22 | INFO | train_inner | epoch 091:    110 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.954, ppl=1.94, wps=22657.9, ups=1.62, wpb=13964.4, bsz=498.9, num_updates=38000, lr=1.1239e-05, gnorm=0.734, train_wall=61, wall=25668
2021-01-01 18:53:24 | INFO | train_inner | epoch 091:    210 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.962, ppl=1.95, wps=22447.5, ups=1.62, wpb=13883.8, bsz=503.3, num_updates=38100, lr=1.12243e-05, gnorm=0.742, train_wall=62, wall=25730
2021-01-01 18:54:26 | INFO | train_inner | epoch 091:    310 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.94, ppl=1.92, wps=22768.5, ups=1.61, wpb=14130.5, bsz=481.8, num_updates=38200, lr=1.12096e-05, gnorm=0.735, train_wall=62, wall=25792
2021-01-01 18:55:28 | INFO | train_inner | epoch 091:    410 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.955, ppl=1.94, wps=22650.6, ups=1.62, wpb=13954.4, bsz=488, num_updates=38300, lr=1.11949e-05, gnorm=0.74, train_wall=61, wall=25854
2021-01-01 18:55:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 18:55:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 18:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 18:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 18:55:52 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.727 | ppl 13.25 | bleu 23.16 | wps 5878.1 | wpb 10324.2 | bsz 375 | num_updates 38311 | best_bleu 23.23
2021-01-01 18:55:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 18:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:55:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 91 @ 38311 updates, score 23.16) (writing took 2.931363059207797 seconds)
2021-01-01 18:55:55 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-01 18:55:55 | INFO | train | epoch 091 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.239 | nll_loss 0.952 | ppl 1.94 | wps 20754.8 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 38311 | lr 1.11933e-05 | gnorm 0.738 | train_wall 259 | wall 25881
2021-01-01 18:55:55 | INFO | fairseq.trainer | begin training epoch 92
2021-01-01 18:55:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 18:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 18:56:53 | INFO | train_inner | epoch 092:     89 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.948, ppl=1.93, wps=16292, ups=1.17, wpb=13872.5, bsz=487.1, num_updates=38400, lr=1.11803e-05, gnorm=0.743, train_wall=62, wall=25939
2021-01-01 18:57:55 | INFO | train_inner | epoch 092:    189 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.955, ppl=1.94, wps=22553.5, ups=1.62, wpb=13956.8, bsz=481.9, num_updates=38500, lr=1.11658e-05, gnorm=0.731, train_wall=62, wall=26001
2021-01-01 18:58:57 | INFO | train_inner | epoch 092:    289 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.95, ppl=1.93, wps=22636.2, ups=1.61, wpb=14079.7, bsz=508.1, num_updates=38600, lr=1.11513e-05, gnorm=0.73, train_wall=62, wall=26063
2021-01-01 18:59:59 | INFO | train_inner | epoch 092:    389 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.955, ppl=1.94, wps=22530.9, ups=1.62, wpb=13919.4, bsz=495.5, num_updates=38700, lr=1.11369e-05, gnorm=0.733, train_wall=62, wall=26125
2021-01-01 19:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:00:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:00:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:00:35 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.731 | ppl 13.28 | bleu 23.08 | wps 5979.7 | wpb 10324.2 | bsz 375 | num_updates 38732 | best_bleu 23.23
2021-01-01 19:00:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:00:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 92 @ 38732 updates, score 23.08) (writing took 2.9090070836246014 seconds)
2021-01-01 19:00:38 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-01 19:00:38 | INFO | train | epoch 092 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.237 | nll_loss 0.951 | ppl 1.93 | wps 20727.7 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 38732 | lr 1.11323e-05 | gnorm 0.735 | train_wall 260 | wall 26164
2021-01-01 19:00:38 | INFO | fairseq.trainer | begin training epoch 93
2021-01-01 19:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:01:23 | INFO | train_inner | epoch 093:     68 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.943, ppl=1.92, wps=16477.9, ups=1.18, wpb=13931.8, bsz=499.8, num_updates=38800, lr=1.11226e-05, gnorm=0.732, train_wall=61, wall=26209
2021-01-01 19:02:26 | INFO | train_inner | epoch 093:    168 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.947, ppl=1.93, wps=22318.5, ups=1.61, wpb=13886.1, bsz=487.4, num_updates=38900, lr=1.11083e-05, gnorm=0.738, train_wall=62, wall=26272
2021-01-01 19:03:27 | INFO | train_inner | epoch 093:    268 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.959, ppl=1.94, wps=22495.4, ups=1.62, wpb=13850.6, bsz=485.8, num_updates=39000, lr=1.1094e-05, gnorm=0.74, train_wall=61, wall=26333
2021-01-01 19:04:29 | INFO | train_inner | epoch 093:    368 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.947, ppl=1.93, wps=23095.2, ups=1.62, wpb=14222.5, bsz=489.6, num_updates=39100, lr=1.10798e-05, gnorm=0.728, train_wall=61, wall=26395
2021-01-01 19:05:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:05:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:05:18 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.272 | nll_loss 3.73 | ppl 13.27 | bleu 23.08 | wps 5971.7 | wpb 10324.2 | bsz 375 | num_updates 39153 | best_bleu 23.23
2021-01-01 19:05:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:05:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 93 @ 39153 updates, score 23.08) (writing took 2.8983504958450794 seconds)
2021-01-01 19:05:21 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-01 19:05:21 | INFO | train | epoch 093 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.237 | nll_loss 0.951 | ppl 1.93 | wps 20804.2 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 39153 | lr 1.10723e-05 | gnorm 0.733 | train_wall 259 | wall 26447
2021-01-01 19:05:21 | INFO | fairseq.trainer | begin training epoch 94
2021-01-01 19:05:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:05:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:05:53 | INFO | train_inner | epoch 094:     47 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.947, ppl=1.93, wps=16627.6, ups=1.19, wpb=13992.9, bsz=499.8, num_updates=39200, lr=1.10657e-05, gnorm=0.731, train_wall=61, wall=26479
2021-01-01 19:06:55 | INFO | train_inner | epoch 094:    147 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.945, ppl=1.93, wps=22711.9, ups=1.62, wpb=14008.8, bsz=483.8, num_updates=39300, lr=1.10516e-05, gnorm=0.721, train_wall=61, wall=26541
2021-01-01 19:07:57 | INFO | train_inner | epoch 094:    247 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.953, ppl=1.94, wps=22638.7, ups=1.61, wpb=14073.1, bsz=497.6, num_updates=39400, lr=1.10375e-05, gnorm=0.732, train_wall=62, wall=26603
2021-01-01 19:08:59 | INFO | train_inner | epoch 094:    347 / 421 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.97, ppl=1.96, wps=22203.6, ups=1.61, wpb=13782.1, bsz=478.6, num_updates=39500, lr=1.10236e-05, gnorm=0.737, train_wall=62, wall=26665
2021-01-01 19:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:09:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:09:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:09:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:09:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:09:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:09:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:09:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:09:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:10:02 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.17 | wps 5861.7 | wpb 10324.2 | bsz 375 | num_updates 39574 | best_bleu 23.23
2021-01-01 19:10:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:10:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:10:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:10:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:10:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:10:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:10:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:10:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 94 @ 39574 updates, score 23.17) (writing took 2.920178020372987 seconds)
2021-01-01 19:10:05 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-01 19:10:05 | INFO | train | epoch 094 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.236 | nll_loss 0.951 | ppl 1.93 | wps 20728.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 39574 | lr 1.10133e-05 | gnorm 0.732 | train_wall 260 | wall 26731
2021-01-01 19:10:05 | INFO | fairseq.trainer | begin training epoch 95
2021-01-01 19:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:10:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:10:24 | INFO | train_inner | epoch 095:     26 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.943, ppl=1.92, wps=16422, ups=1.17, wpb=13986.8, bsz=521.4, num_updates=39600, lr=1.10096e-05, gnorm=0.735, train_wall=62, wall=26750
2021-01-01 19:11:26 | INFO | train_inner | epoch 095:    126 / 421 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.955, ppl=1.94, wps=22461.5, ups=1.62, wpb=13861.6, bsz=476.5, num_updates=39700, lr=1.09958e-05, gnorm=0.734, train_wall=62, wall=26812
2021-01-01 19:12:27 | INFO | train_inner | epoch 095:    226 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.942, ppl=1.92, wps=22569.7, ups=1.62, wpb=13904.9, bsz=482.6, num_updates=39800, lr=1.09819e-05, gnorm=0.737, train_wall=61, wall=26873
2021-01-01 19:13:29 | INFO | train_inner | epoch 095:    326 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.951, ppl=1.93, wps=22922.1, ups=1.62, wpb=14181.4, bsz=509.4, num_updates=39900, lr=1.09682e-05, gnorm=0.719, train_wall=62, wall=26935
2021-01-01 19:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:14:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:14:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:14:46 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.23 | wps 5723.1 | wpb 10324.2 | bsz 375 | num_updates 39995 | best_bleu 23.23
2021-01-01 19:14:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:14:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 95 @ 39995 updates, score 23.23) (writing took 4.9342607613652945 seconds)
2021-01-01 19:14:50 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-01 19:14:50 | INFO | train | epoch 095 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.235 | nll_loss 0.951 | ppl 1.93 | wps 20584.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 39995 | lr 1.09551e-05 | gnorm 0.733 | train_wall 259 | wall 27017
2021-01-01 19:14:50 | INFO | fairseq.trainer | begin training epoch 96
2021-01-01 19:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:14:57 | INFO | train_inner | epoch 096:      5 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.956, ppl=1.94, wps=15843.8, ups=1.14, wpb=13880.1, bsz=488.7, num_updates=40000, lr=1.09545e-05, gnorm=0.742, train_wall=62, wall=27023
2021-01-01 19:15:58 | INFO | train_inner | epoch 096:    105 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.941, ppl=1.92, wps=23127.9, ups=1.63, wpb=14175.6, bsz=499, num_updates=40100, lr=1.09408e-05, gnorm=0.716, train_wall=61, wall=27084
2021-01-01 19:17:00 | INFO | train_inner | epoch 096:    205 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.955, ppl=1.94, wps=22194.5, ups=1.61, wpb=13801.8, bsz=502.2, num_updates=40200, lr=1.09272e-05, gnorm=0.731, train_wall=62, wall=27146
2021-01-01 19:18:02 | INFO | train_inner | epoch 096:    305 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.959, ppl=1.94, wps=22591.6, ups=1.61, wpb=14033.4, bsz=490.9, num_updates=40300, lr=1.09136e-05, gnorm=0.732, train_wall=62, wall=27208
2021-01-01 19:19:04 | INFO | train_inner | epoch 096:    405 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.949, ppl=1.93, wps=22590.6, ups=1.62, wpb=13939.7, bsz=479.9, num_updates=40400, lr=1.09001e-05, gnorm=0.735, train_wall=62, wall=27270
2021-01-01 19:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:19:31 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.729 | ppl 13.26 | bleu 23.11 | wps 5902 | wpb 10324.2 | bsz 375 | num_updates 40416 | best_bleu 23.23
2021-01-01 19:19:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:19:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 96 @ 40416 updates, score 23.11) (writing took 2.8983851708471775 seconds)
2021-01-01 19:19:34 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-01 19:19:34 | INFO | train | epoch 096 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.235 | nll_loss 0.95 | ppl 1.93 | wps 20752.9 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 40416 | lr 1.08979e-05 | gnorm 0.729 | train_wall 259 | wall 27300
2021-01-01 19:19:34 | INFO | fairseq.trainer | begin training epoch 97
2021-01-01 19:19:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:19:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:20:29 | INFO | train_inner | epoch 097:     84 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.953, ppl=1.94, wps=16620.2, ups=1.18, wpb=14091.7, bsz=512.2, num_updates=40500, lr=1.08866e-05, gnorm=0.718, train_wall=62, wall=27355
2021-01-01 19:21:31 | INFO | train_inner | epoch 097:    184 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.957, ppl=1.94, wps=22400.4, ups=1.62, wpb=13802.1, bsz=472.5, num_updates=40600, lr=1.08732e-05, gnorm=0.736, train_wall=61, wall=27417
2021-01-01 19:22:33 | INFO | train_inner | epoch 097:    284 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.947, ppl=1.93, wps=22641.1, ups=1.61, wpb=14048.6, bsz=500.6, num_updates=40700, lr=1.08598e-05, gnorm=0.728, train_wall=62, wall=27479
2021-01-01 19:23:35 | INFO | train_inner | epoch 097:    384 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.946, ppl=1.93, wps=22443.1, ups=1.6, wpb=13995.7, bsz=497.8, num_updates=40800, lr=1.08465e-05, gnorm=0.737, train_wall=62, wall=27541
2021-01-01 19:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:24:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:24:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:24:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:24:15 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.727 | ppl 13.24 | bleu 23.05 | wps 5936.6 | wpb 10324.2 | bsz 375 | num_updates 40837 | best_bleu 23.23
2021-01-01 19:24:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:24:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:24:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:24:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:24:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:24:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 97 @ 40837 updates, score 23.05) (writing took 2.8917746245861053 seconds)
2021-01-01 19:24:18 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-01 19:24:18 | INFO | train | epoch 097 | symm_kl 0.355 | self_kl 0 | self_cv 0 | loss 3.234 | nll_loss 0.95 | ppl 1.93 | wps 20727.9 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 40837 | lr 1.08416e-05 | gnorm 0.732 | train_wall 260 | wall 27584
2021-01-01 19:24:18 | INFO | fairseq.trainer | begin training epoch 98
2021-01-01 19:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:25:00 | INFO | train_inner | epoch 098:     63 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.937, ppl=1.92, wps=16346, ups=1.18, wpb=13843.1, bsz=482.2, num_updates=40900, lr=1.08333e-05, gnorm=0.733, train_wall=62, wall=27626
2021-01-01 19:26:02 | INFO | train_inner | epoch 098:    163 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.948, ppl=1.93, wps=22435.3, ups=1.61, wpb=13929.2, bsz=503.2, num_updates=41000, lr=1.082e-05, gnorm=0.737, train_wall=62, wall=27688
2021-01-01 19:27:04 | INFO | train_inner | epoch 098:    263 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.966, ppl=1.95, wps=22399.3, ups=1.61, wpb=13939.5, bsz=483, num_updates=41100, lr=1.08069e-05, gnorm=0.734, train_wall=62, wall=27750
2021-01-01 19:28:06 | INFO | train_inner | epoch 098:    363 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.942, ppl=1.92, wps=22508.8, ups=1.6, wpb=14068.5, bsz=492.2, num_updates=41200, lr=1.07937e-05, gnorm=0.727, train_wall=62, wall=27812
2021-01-01 19:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:28:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:29:00 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.729 | ppl 13.26 | bleu 23.11 | wps 5808.4 | wpb 10324.2 | bsz 375 | num_updates 41258 | best_bleu 23.23
2021-01-01 19:29:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:29:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:29:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:29:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:29:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:29:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:29:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:29:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 98 @ 41258 updates, score 23.11) (writing took 2.92667980492115 seconds)
2021-01-01 19:29:03 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-01 19:29:03 | INFO | train | epoch 098 | symm_kl 0.355 | self_kl 0 | self_cv 0 | loss 3.233 | nll_loss 0.95 | ppl 1.93 | wps 20636.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 41258 | lr 1.07862e-05 | gnorm 0.731 | train_wall 261 | wall 27869
2021-01-01 19:29:03 | INFO | fairseq.trainer | begin training epoch 99
2021-01-01 19:29:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:29:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:29:32 | INFO | train_inner | epoch 099:     42 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.948, ppl=1.93, wps=16292.3, ups=1.17, wpb=13872, bsz=489.7, num_updates=41300, lr=1.07807e-05, gnorm=0.734, train_wall=62, wall=27898
2021-01-01 19:30:33 | INFO | train_inner | epoch 099:    142 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.949, ppl=1.93, wps=22734.8, ups=1.62, wpb=14015.1, bsz=501.8, num_updates=41400, lr=1.07676e-05, gnorm=0.728, train_wall=61, wall=27959
2021-01-01 19:31:35 | INFO | train_inner | epoch 099:    242 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.949, ppl=1.93, wps=22560.2, ups=1.61, wpb=14021.7, bsz=480, num_updates=41500, lr=1.07547e-05, gnorm=0.73, train_wall=62, wall=28021
2021-01-01 19:32:38 | INFO | train_inner | epoch 099:    342 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.959, ppl=1.94, wps=22226.8, ups=1.6, wpb=13915.6, bsz=489.7, num_updates=41600, lr=1.07417e-05, gnorm=0.735, train_wall=62, wall=28084
2021-01-01 19:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:33:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:33:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:33:44 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.271 | nll_loss 3.728 | ppl 13.25 | bleu 22.99 | wps 5972.8 | wpb 10324.2 | bsz 375 | num_updates 41679 | best_bleu 23.23
2021-01-01 19:33:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 99 @ 41679 updates, score 22.99) (writing took 2.9102217257022858 seconds)
2021-01-01 19:33:47 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-01 19:33:47 | INFO | train | epoch 099 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.949 | ppl 1.93 | wps 20703.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 41679 | lr 1.07315e-05 | gnorm 0.729 | train_wall 260 | wall 28153
2021-01-01 19:33:47 | INFO | fairseq.trainer | begin training epoch 100
2021-01-01 19:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:33:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:33:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:34:03 | INFO | train_inner | epoch 100:     21 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.941, ppl=1.92, wps=16497.8, ups=1.18, wpb=13985.8, bsz=496.4, num_updates=41700, lr=1.07288e-05, gnorm=0.723, train_wall=62, wall=28169
2021-01-01 19:35:04 | INFO | train_inner | epoch 100:    121 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.209, nll_loss=0.933, ppl=1.91, wps=22897.2, ups=1.62, wpb=14118.8, bsz=510.7, num_updates=41800, lr=1.0716e-05, gnorm=0.713, train_wall=61, wall=28230
2021-01-01 19:36:06 | INFO | train_inner | epoch 100:    221 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.945, ppl=1.93, wps=22612.9, ups=1.61, wpb=14026.1, bsz=489.2, num_updates=41900, lr=1.07032e-05, gnorm=0.725, train_wall=62, wall=28293
2021-01-01 19:37:08 | INFO | train_inner | epoch 100:    321 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.958, ppl=1.94, wps=22619.5, ups=1.62, wpb=13989.4, bsz=484.4, num_updates=42000, lr=1.06904e-05, gnorm=0.737, train_wall=62, wall=28354
2021-01-01 19:38:11 | INFO | train_inner | epoch 100:    421 / 421 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.965, ppl=1.95, wps=22105.6, ups=1.61, wpb=13759.9, bsz=487.8, num_updates=42100, lr=1.06777e-05, gnorm=0.745, train_wall=62, wall=28417
2021-01-01 19:38:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:38:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:38:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:38:27 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.727 | ppl 13.24 | bleu 23.05 | wps 5930.6 | wpb 10324.2 | bsz 375 | num_updates 42100 | best_bleu 23.23
2021-01-01 19:38:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:38:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 100 @ 42100 updates, score 23.05) (writing took 2.962638135999441 seconds)
2021-01-01 19:38:30 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-01 19:38:30 | INFO | train | epoch 100 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.231 | nll_loss 0.949 | ppl 1.93 | wps 20740 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 42100 | lr 1.06777e-05 | gnorm 0.73 | train_wall 260 | wall 28436
2021-01-01 19:38:30 | INFO | fairseq.trainer | begin training epoch 101
2021-01-01 19:38:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:38:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:39:35 | INFO | train_inner | epoch 101:    100 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.21, nll_loss=0.935, ppl=1.91, wps=16365.4, ups=1.18, wpb=13883.2, bsz=499.1, num_updates=42200, lr=1.06651e-05, gnorm=0.733, train_wall=62, wall=28501
2021-01-01 19:40:37 | INFO | train_inner | epoch 101:    200 / 421 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.957, ppl=1.94, wps=22769.6, ups=1.62, wpb=14079.7, bsz=491, num_updates=42300, lr=1.06525e-05, gnorm=0.73, train_wall=62, wall=28563
2021-01-01 19:41:39 | INFO | train_inner | epoch 101:    300 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.945, ppl=1.93, wps=22749.6, ups=1.62, wpb=14068.5, bsz=485.1, num_updates=42400, lr=1.06399e-05, gnorm=0.729, train_wall=62, wall=28625
2021-01-01 19:42:41 | INFO | train_inner | epoch 101:    400 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.955, ppl=1.94, wps=22370.5, ups=1.61, wpb=13890.9, bsz=496, num_updates=42500, lr=1.06274e-05, gnorm=0.733, train_wall=62, wall=28687
2021-01-01 19:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:42:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:42:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:42:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:42:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:43:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:43:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:43:11 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.728 | ppl 13.25 | bleu 23.11 | wps 6044.6 | wpb 10324.2 | bsz 375 | num_updates 42521 | best_bleu 23.23
2021-01-01 19:43:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:43:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:43:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 101 @ 42521 updates, score 23.11) (writing took 2.9186250995844603 seconds)
2021-01-01 19:43:14 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-01 19:43:14 | INFO | train | epoch 101 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.23 | nll_loss 0.948 | ppl 1.93 | wps 20750.3 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 42521 | lr 1.06248e-05 | gnorm 0.732 | train_wall 260 | wall 28720
2021-01-01 19:43:14 | INFO | fairseq.trainer | begin training epoch 102
2021-01-01 19:43:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:43:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:44:05 | INFO | train_inner | epoch 102:     79 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.954, ppl=1.94, wps=16599.1, ups=1.19, wpb=13969.1, bsz=481.5, num_updates=42600, lr=1.06149e-05, gnorm=0.746, train_wall=61, wall=28771
2021-01-01 19:45:07 | INFO | train_inner | epoch 102:    179 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.942, ppl=1.92, wps=22537.3, ups=1.62, wpb=13930.9, bsz=507, num_updates=42700, lr=1.06025e-05, gnorm=0.725, train_wall=62, wall=28833
2021-01-01 19:46:09 | INFO | train_inner | epoch 102:    279 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.946, ppl=1.93, wps=22445.2, ups=1.62, wpb=13897.1, bsz=483, num_updates=42800, lr=1.05901e-05, gnorm=0.726, train_wall=62, wall=28895
2021-01-01 19:47:11 | INFO | train_inner | epoch 102:    379 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.947, ppl=1.93, wps=22746.1, ups=1.62, wpb=14020.9, bsz=498.7, num_updates=42900, lr=1.05777e-05, gnorm=0.728, train_wall=61, wall=28957
2021-01-01 19:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:47:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:47:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:47:53 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.725 | ppl 13.22 | bleu 23.23 | wps 5950.6 | wpb 10324.2 | bsz 375 | num_updates 42942 | best_bleu 23.23
2021-01-01 19:47:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:47:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:47:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:47:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 102 @ 42942 updates, score 23.23) (writing took 4.920315841212869 seconds)
2021-01-01 19:47:58 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-01 19:47:58 | INFO | train | epoch 102 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.229 | nll_loss 0.949 | ppl 1.93 | wps 20661 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 42942 | lr 1.05725e-05 | gnorm 0.731 | train_wall 259 | wall 29004
2021-01-01 19:47:58 | INFO | fairseq.trainer | begin training epoch 103
2021-01-01 19:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:48:37 | INFO | train_inner | epoch 103:     58 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.952, ppl=1.93, wps=16180, ups=1.16, wpb=14003.3, bsz=502, num_updates=43000, lr=1.05654e-05, gnorm=0.737, train_wall=61, wall=29043
2021-01-01 19:49:39 | INFO | train_inner | epoch 103:    158 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.944, ppl=1.92, wps=22438.9, ups=1.62, wpb=13884.4, bsz=487.8, num_updates=43100, lr=1.05531e-05, gnorm=0.728, train_wall=62, wall=29105
2021-01-01 19:50:41 | INFO | train_inner | epoch 103:    258 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.945, ppl=1.92, wps=22795.4, ups=1.62, wpb=14078.5, bsz=477.8, num_updates=43200, lr=1.05409e-05, gnorm=0.727, train_wall=62, wall=29167
2021-01-01 19:51:43 | INFO | train_inner | epoch 103:    358 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.955, ppl=1.94, wps=22686.3, ups=1.61, wpb=14069.8, bsz=494.8, num_updates=43300, lr=1.05287e-05, gnorm=0.729, train_wall=62, wall=29229
2021-01-01 19:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:52:38 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.726 | ppl 13.24 | bleu 23.12 | wps 5946.9 | wpb 10324.2 | bsz 375 | num_updates 43363 | best_bleu 23.23
2021-01-01 19:52:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:52:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 103 @ 43363 updates, score 23.12) (writing took 2.9526431784033775 seconds)
2021-01-01 19:52:41 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-01 19:52:41 | INFO | train | epoch 103 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.23 | nll_loss 0.949 | ppl 1.93 | wps 20770.7 | ups 1.49 | wpb 13969.5 | bsz 492.6 | num_updates 43363 | lr 1.05211e-05 | gnorm 0.731 | train_wall 259 | wall 29287
2021-01-01 19:52:41 | INFO | fairseq.trainer | begin training epoch 104
2021-01-01 19:52:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:53:08 | INFO | train_inner | epoch 104:     37 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.955, ppl=1.94, wps=16355.3, ups=1.18, wpb=13858.5, bsz=500.9, num_updates=43400, lr=1.05166e-05, gnorm=0.735, train_wall=61, wall=29314
2021-01-01 19:54:10 | INFO | train_inner | epoch 104:    137 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.939, ppl=1.92, wps=22553.1, ups=1.61, wpb=13991.8, bsz=485.7, num_updates=43500, lr=1.05045e-05, gnorm=0.729, train_wall=62, wall=29376
2021-01-01 19:55:12 | INFO | train_inner | epoch 104:    237 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.949, ppl=1.93, wps=22599.1, ups=1.61, wpb=13997.3, bsz=499.1, num_updates=43600, lr=1.04925e-05, gnorm=0.733, train_wall=62, wall=29438
2021-01-01 19:56:14 | INFO | train_inner | epoch 104:    337 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.951, ppl=1.93, wps=22393.7, ups=1.6, wpb=13981.3, bsz=497, num_updates=43700, lr=1.04804e-05, gnorm=0.726, train_wall=62, wall=29500
2021-01-01 19:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 19:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 19:57:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 19:57:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 19:57:24 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.726 | ppl 13.23 | bleu 23.12 | wps 5870.1 | wpb 10324.2 | bsz 375 | num_updates 43784 | best_bleu 23.23
2021-01-01 19:57:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 19:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 104 @ 43784 updates, score 23.12) (writing took 2.90696220099926 seconds)
2021-01-01 19:57:27 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-01 19:57:27 | INFO | train | epoch 104 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.227 | nll_loss 0.947 | ppl 1.93 | wps 20625.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 43784 | lr 1.04704e-05 | gnorm 0.729 | train_wall 261 | wall 29573
2021-01-01 19:57:27 | INFO | fairseq.trainer | begin training epoch 105
2021-01-01 19:57:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 19:57:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 19:57:40 | INFO | train_inner | epoch 105:     16 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.946, ppl=1.93, wps=16168.4, ups=1.17, wpb=13856.2, bsz=485.6, num_updates=43800, lr=1.04685e-05, gnorm=0.726, train_wall=62, wall=29586
2021-01-01 19:58:41 | INFO | train_inner | epoch 105:    116 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.946, ppl=1.93, wps=22661, ups=1.62, wpb=13988.2, bsz=507, num_updates=43900, lr=1.04565e-05, gnorm=0.721, train_wall=62, wall=29648
2021-01-01 19:59:44 | INFO | train_inner | epoch 105:    216 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.946, ppl=1.93, wps=22387.5, ups=1.6, wpb=13991.7, bsz=483.1, num_updates=44000, lr=1.04447e-05, gnorm=0.729, train_wall=62, wall=29710
2021-01-01 20:00:46 | INFO | train_inner | epoch 105:    316 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.954, ppl=1.94, wps=22501.4, ups=1.6, wpb=14043.8, bsz=485.8, num_updates=44100, lr=1.04328e-05, gnorm=0.73, train_wall=62, wall=29772
2021-01-01 20:01:48 | INFO | train_inner | epoch 105:    416 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.948, ppl=1.93, wps=22609, ups=1.62, wpb=13989, bsz=497.6, num_updates=44200, lr=1.0421e-05, gnorm=0.718, train_wall=62, wall=29834
2021-01-01 20:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:01:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:01:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:01:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:01:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:01:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:01:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:01:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:01:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:01:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:01:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:01:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:02:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:02:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:02:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:02:08 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.725 | ppl 13.22 | bleu 23.2 | wps 5959.3 | wpb 10324.2 | bsz 375 | num_updates 44205 | best_bleu 23.23
2021-01-01 20:02:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:02:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:02:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:02:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:02:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 105 @ 44205 updates, score 23.2) (writing took 2.901444748044014 seconds)
2021-01-01 20:02:11 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-01 20:02:11 | INFO | train | epoch 105 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.228 | nll_loss 0.948 | ppl 1.93 | wps 20684.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 44205 | lr 1.04204e-05 | gnorm 0.725 | train_wall 261 | wall 29857
2021-01-01 20:02:11 | INFO | fairseq.trainer | begin training epoch 106
2021-01-01 20:02:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:02:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:02:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:02:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:03:13 | INFO | train_inner | epoch 106:     95 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.947, ppl=1.93, wps=16375.8, ups=1.18, wpb=13822.1, bsz=478.9, num_updates=44300, lr=1.04092e-05, gnorm=0.741, train_wall=61, wall=29919
2021-01-01 20:04:15 | INFO | train_inner | epoch 106:    195 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.209, nll_loss=0.937, ppl=1.91, wps=22487.8, ups=1.61, wpb=14001.9, bsz=513.3, num_updates=44400, lr=1.03975e-05, gnorm=0.716, train_wall=62, wall=29981
2021-01-01 20:05:17 | INFO | train_inner | epoch 106:    295 / 421 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.962, ppl=1.95, wps=22450.5, ups=1.61, wpb=13964, bsz=483.5, num_updates=44500, lr=1.03858e-05, gnorm=0.73, train_wall=62, wall=30043
2021-01-01 20:06:19 | INFO | train_inner | epoch 106:    395 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.941, ppl=1.92, wps=22598.9, ups=1.61, wpb=14026.1, bsz=500.2, num_updates=44600, lr=1.03742e-05, gnorm=0.719, train_wall=62, wall=30105
2021-01-01 20:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:06:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:06:53 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.728 | ppl 13.25 | bleu 23.14 | wps 5514.3 | wpb 10324.2 | bsz 375 | num_updates 44626 | best_bleu 23.23
2021-01-01 20:06:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:06:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:06:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 106 @ 44626 updates, score 23.14) (writing took 2.9285868760198355 seconds)
2021-01-01 20:06:56 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-01 20:06:56 | INFO | train | epoch 106 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.227 | nll_loss 0.947 | ppl 1.93 | wps 20632.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 44626 | lr 1.03711e-05 | gnorm 0.727 | train_wall 260 | wall 30142
2021-01-01 20:06:56 | INFO | fairseq.trainer | begin training epoch 107
2021-01-01 20:06:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:06:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:07:45 | INFO | train_inner | epoch 107:     74 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.948, ppl=1.93, wps=16347.8, ups=1.17, wpb=14000, bsz=476.7, num_updates=44700, lr=1.03626e-05, gnorm=0.724, train_wall=62, wall=30191
2021-01-01 20:08:47 | INFO | train_inner | epoch 107:    174 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.942, ppl=1.92, wps=22554.9, ups=1.61, wpb=14039.2, bsz=499.8, num_updates=44800, lr=1.0351e-05, gnorm=0.727, train_wall=62, wall=30253
2021-01-01 20:09:49 | INFO | train_inner | epoch 107:    274 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.95, ppl=1.93, wps=22581.5, ups=1.62, wpb=13956.4, bsz=498.3, num_updates=44900, lr=1.03395e-05, gnorm=0.734, train_wall=62, wall=30315
2021-01-01 20:10:51 | INFO | train_inner | epoch 107:    374 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.957, ppl=1.94, wps=22250.9, ups=1.61, wpb=13812.2, bsz=496, num_updates=45000, lr=1.0328e-05, gnorm=0.724, train_wall=62, wall=30377
2021-01-01 20:11:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:11:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:11:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:11:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:11:37 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.725 | ppl 13.22 | bleu 23.14 | wps 6042.8 | wpb 10324.2 | bsz 375 | num_updates 45047 | best_bleu 23.23
2021-01-01 20:11:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:11:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:11:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 107 @ 45047 updates, score 23.14) (writing took 2.946645000949502 seconds)
2021-01-01 20:11:40 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-01 20:11:40 | INFO | train | epoch 107 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.226 | nll_loss 0.948 | ppl 1.93 | wps 20727.1 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 45047 | lr 1.03226e-05 | gnorm 0.726 | train_wall 260 | wall 30426
2021-01-01 20:11:40 | INFO | fairseq.trainer | begin training epoch 108
2021-01-01 20:11:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:12:16 | INFO | train_inner | epoch 108:     53 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.204, nll_loss=0.933, ppl=1.91, wps=16694.2, ups=1.18, wpb=14121.7, bsz=497.7, num_updates=45100, lr=1.03165e-05, gnorm=0.725, train_wall=62, wall=30462
2021-01-01 20:13:18 | INFO | train_inner | epoch 108:    153 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.949, ppl=1.93, wps=22336.7, ups=1.61, wpb=13887.2, bsz=471.7, num_updates=45200, lr=1.03051e-05, gnorm=0.73, train_wall=62, wall=30524
2021-01-01 20:14:20 | INFO | train_inner | epoch 108:    253 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.946, ppl=1.93, wps=22330.7, ups=1.6, wpb=13926.7, bsz=491.3, num_updates=45300, lr=1.02937e-05, gnorm=0.738, train_wall=62, wall=30586
2021-01-01 20:15:23 | INFO | train_inner | epoch 108:    353 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.949, ppl=1.93, wps=22283.5, ups=1.59, wpb=13971.7, bsz=485, num_updates=45400, lr=1.02824e-05, gnorm=0.733, train_wall=63, wall=30649
2021-01-01 20:16:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:16:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:16:24 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.724 | ppl 13.21 | bleu 23.13 | wps 5094.5 | wpb 10324.2 | bsz 375 | num_updates 45468 | best_bleu 23.23
2021-01-01 20:16:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:16:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 108 @ 45468 updates, score 23.13) (writing took 2.855168690904975 seconds)
2021-01-01 20:16:27 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-01 20:16:27 | INFO | train | epoch 108 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.226 | nll_loss 0.947 | ppl 1.93 | wps 20468.3 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 45468 | lr 1.02747e-05 | gnorm 0.732 | train_wall 261 | wall 30713
2021-01-01 20:16:27 | INFO | fairseq.trainer | begin training epoch 109
2021-01-01 20:16:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:16:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:16:50 | INFO | train_inner | epoch 109:     32 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.956, ppl=1.94, wps=15950.1, ups=1.15, wpb=13911, bsz=510.6, num_updates=45500, lr=1.02711e-05, gnorm=0.733, train_wall=62, wall=30736
2021-01-01 20:17:52 | INFO | train_inner | epoch 109:    132 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.954, ppl=1.94, wps=22625.9, ups=1.62, wpb=13987.2, bsz=497.9, num_updates=45600, lr=1.02598e-05, gnorm=0.721, train_wall=62, wall=30798
2021-01-01 20:18:54 | INFO | train_inner | epoch 109:    232 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.945, ppl=1.93, wps=22408.4, ups=1.61, wpb=13961.6, bsz=489.7, num_updates=45700, lr=1.02486e-05, gnorm=0.732, train_wall=62, wall=30860
2021-01-01 20:19:57 | INFO | train_inner | epoch 109:    332 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.94, ppl=1.92, wps=22427.7, ups=1.6, wpb=13998.7, bsz=486.6, num_updates=45800, lr=1.02374e-05, gnorm=0.732, train_wall=62, wall=30923
2021-01-01 20:20:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:20:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:20:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:20:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:20:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:20:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:20:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:20:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:20:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:21:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:21:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:21:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:21:09 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.727 | ppl 13.25 | bleu 23.04 | wps 5867.2 | wpb 10324.2 | bsz 375 | num_updates 45889 | best_bleu 23.23
2021-01-01 20:21:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:21:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:21:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:21:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:21:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 109 @ 45889 updates, score 23.04) (writing took 2.9217397533357143 seconds)
2021-01-01 20:21:12 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-01 20:21:12 | INFO | train | epoch 109 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.225 | nll_loss 0.947 | ppl 1.93 | wps 20630.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 45889 | lr 1.02274e-05 | gnorm 0.729 | train_wall 261 | wall 30998
2021-01-01 20:21:12 | INFO | fairseq.trainer | begin training epoch 110
2021-01-01 20:21:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:21:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:21:22 | INFO | train_inner | epoch 110:     11 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.947, ppl=1.93, wps=16294.8, ups=1.17, wpb=13957.2, bsz=495.4, num_updates=45900, lr=1.02262e-05, gnorm=0.727, train_wall=62, wall=31008
2021-01-01 20:22:24 | INFO | train_inner | epoch 110:    111 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.945, ppl=1.92, wps=22451, ups=1.61, wpb=13930.8, bsz=481.6, num_updates=46000, lr=1.02151e-05, gnorm=0.727, train_wall=62, wall=31070
2021-01-01 20:23:27 | INFO | train_inner | epoch 110:    211 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.941, ppl=1.92, wps=22456.1, ups=1.6, wpb=14039.4, bsz=478.6, num_updates=46100, lr=1.0204e-05, gnorm=0.729, train_wall=62, wall=31133
2021-01-01 20:24:29 | INFO | train_inner | epoch 110:    311 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.957, ppl=1.94, wps=22350.7, ups=1.61, wpb=13881.6, bsz=507.3, num_updates=46200, lr=1.01929e-05, gnorm=0.724, train_wall=62, wall=31195
2021-01-01 20:25:31 | INFO | train_inner | epoch 110:    411 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.216, nll_loss=0.942, ppl=1.92, wps=22721.1, ups=1.61, wpb=14092.8, bsz=505.5, num_updates=46300, lr=1.01819e-05, gnorm=0.723, train_wall=62, wall=31257
2021-01-01 20:25:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:25:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:25:54 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.726 | ppl 13.23 | bleu 23.14 | wps 5979.1 | wpb 10324.2 | bsz 375 | num_updates 46310 | best_bleu 23.23
2021-01-01 20:25:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:25:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 110 @ 46310 updates, score 23.14) (writing took 2.8737481124699116 seconds)
2021-01-01 20:25:57 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-01 20:25:57 | INFO | train | epoch 110 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.224 | nll_loss 0.946 | ppl 1.93 | wps 20653.5 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 46310 | lr 1.01808e-05 | gnorm 0.727 | train_wall 261 | wall 31283
2021-01-01 20:25:57 | INFO | fairseq.trainer | begin training epoch 111
2021-01-01 20:25:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:25:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:26:56 | INFO | train_inner | epoch 111:     90 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.945, ppl=1.92, wps=16662.1, ups=1.18, wpb=14116, bsz=495.6, num_updates=46400, lr=1.0171e-05, gnorm=0.72, train_wall=62, wall=31342
2021-01-01 20:27:58 | INFO | train_inner | epoch 111:    190 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.949, ppl=1.93, wps=22405.1, ups=1.6, wpb=13991.1, bsz=489.8, num_updates=46500, lr=1.016e-05, gnorm=0.722, train_wall=62, wall=31404
2021-01-01 20:29:00 | INFO | train_inner | epoch 111:    290 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.95, ppl=1.93, wps=22330.5, ups=1.61, wpb=13902.4, bsz=483.1, num_updates=46600, lr=1.01491e-05, gnorm=0.726, train_wall=62, wall=31466
2021-01-01 20:30:03 | INFO | train_inner | epoch 111:    390 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.946, ppl=1.93, wps=22229.8, ups=1.59, wpb=13943.7, bsz=496.4, num_updates=46700, lr=1.01382e-05, gnorm=0.722, train_wall=63, wall=31529
2021-01-01 20:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:30:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:30:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:30:41 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.726 | ppl 13.24 | bleu 23.09 | wps 5400.3 | wpb 10324.2 | bsz 375 | num_updates 46731 | best_bleu 23.23
2021-01-01 20:30:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:30:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 111 @ 46731 updates, score 23.09) (writing took 2.8637250028550625 seconds)
2021-01-01 20:30:43 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-01 20:30:43 | INFO | train | epoch 111 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.223 | nll_loss 0.947 | ppl 1.93 | wps 20525 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 46731 | lr 1.01349e-05 | gnorm 0.725 | train_wall 262 | wall 31569
2021-01-01 20:30:43 | INFO | fairseq.trainer | begin training epoch 112
2021-01-01 20:30:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:30:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:31:30 | INFO | train_inner | epoch 112:     69 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.941, ppl=1.92, wps=16065.7, ups=1.16, wpb=13896.2, bsz=496.3, num_updates=46800, lr=1.01274e-05, gnorm=0.734, train_wall=62, wall=31616
2021-01-01 20:32:32 | INFO | train_inner | epoch 112:    169 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.943, ppl=1.92, wps=22282.7, ups=1.59, wpb=13971.6, bsz=489.3, num_updates=46900, lr=1.01166e-05, gnorm=0.725, train_wall=62, wall=31678
2021-01-01 20:33:35 | INFO | train_inner | epoch 112:    269 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.955, ppl=1.94, wps=22251, ups=1.6, wpb=13883.7, bsz=472.8, num_updates=47000, lr=1.01058e-05, gnorm=0.733, train_wall=62, wall=31741
2021-01-01 20:34:37 | INFO | train_inner | epoch 112:    369 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.942, ppl=1.92, wps=22579.6, ups=1.61, wpb=14044.4, bsz=503.4, num_updates=47100, lr=1.00951e-05, gnorm=0.72, train_wall=62, wall=31803
2021-01-01 20:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:35:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:35:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:35:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:35:26 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.723 | ppl 13.21 | bleu 23.19 | wps 6042.4 | wpb 10324.2 | bsz 375 | num_updates 47152 | best_bleu 23.23
2021-01-01 20:35:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:35:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 112 @ 47152 updates, score 23.19) (writing took 2.9266191348433495 seconds)
2021-01-01 20:35:29 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-01 20:35:29 | INFO | train | epoch 112 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.223 | nll_loss 0.946 | ppl 1.93 | wps 20618.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 47152 | lr 1.00895e-05 | gnorm 0.724 | train_wall 261 | wall 31855
2021-01-01 20:35:29 | INFO | fairseq.trainer | begin training epoch 113
2021-01-01 20:35:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:35:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:36:01 | INFO | train_inner | epoch 113:     48 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.952, ppl=1.93, wps=16507.8, ups=1.18, wpb=13952, bsz=495.1, num_updates=47200, lr=1.00844e-05, gnorm=0.724, train_wall=61, wall=31887
2021-01-01 20:37:04 | INFO | train_inner | epoch 113:    148 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.942, ppl=1.92, wps=22474.4, ups=1.6, wpb=14018, bsz=501.3, num_updates=47300, lr=1.00737e-05, gnorm=0.719, train_wall=62, wall=31950
2021-01-01 20:38:06 | INFO | train_inner | epoch 113:    248 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.205, nll_loss=0.933, ppl=1.91, wps=22460.3, ups=1.59, wpb=14084.4, bsz=507.6, num_updates=47400, lr=1.00631e-05, gnorm=0.713, train_wall=63, wall=32012
2021-01-01 20:39:09 | INFO | train_inner | epoch 113:    348 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.947, ppl=1.93, wps=22309.1, ups=1.6, wpb=13916.6, bsz=498.7, num_updates=47500, lr=1.00525e-05, gnorm=0.726, train_wall=62, wall=32075
2021-01-01 20:39:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:39:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:39:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:39:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:39:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:40:11 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.725 | ppl 13.22 | bleu 23.17 | wps 5906.3 | wpb 10324.2 | bsz 375 | num_updates 47573 | best_bleu 23.23
2021-01-01 20:40:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:40:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:40:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 113 @ 47573 updates, score 23.17) (writing took 2.917297003790736 seconds)
2021-01-01 20:40:14 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-01 20:40:14 | INFO | train | epoch 113 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.222 | nll_loss 0.946 | ppl 1.93 | wps 20583.2 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 47573 | lr 1.00448e-05 | gnorm 0.726 | train_wall 262 | wall 32140
2021-01-01 20:40:14 | INFO | fairseq.trainer | begin training epoch 114
2021-01-01 20:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:40:34 | INFO | train_inner | epoch 114:     27 / 421 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.957, ppl=1.94, wps=16265.8, ups=1.17, wpb=13894.5, bsz=464.8, num_updates=47600, lr=1.00419e-05, gnorm=0.744, train_wall=62, wall=32160
2021-01-01 20:41:36 | INFO | train_inner | epoch 114:    127 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.96, ppl=1.95, wps=22465.3, ups=1.62, wpb=13900.1, bsz=483, num_updates=47700, lr=1.00314e-05, gnorm=0.737, train_wall=62, wall=32222
2021-01-01 20:42:38 | INFO | train_inner | epoch 114:    227 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.946, ppl=1.93, wps=22557.2, ups=1.61, wpb=14030.9, bsz=489.1, num_updates=47800, lr=1.00209e-05, gnorm=0.73, train_wall=62, wall=32284
2021-01-01 20:43:40 | INFO | train_inner | epoch 114:    327 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.942, ppl=1.92, wps=22433.9, ups=1.61, wpb=13927.4, bsz=480.9, num_updates=47900, lr=1.00104e-05, gnorm=0.728, train_wall=62, wall=32346
2021-01-01 20:44:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:44:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:44:56 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.725 | ppl 13.22 | bleu 23.2 | wps 5845.5 | wpb 10324.2 | bsz 375 | num_updates 47994 | best_bleu 23.23
2021-01-01 20:44:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:44:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:44:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:44:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 114 @ 47994 updates, score 23.2) (writing took 2.8801500909030437 seconds)
2021-01-01 20:44:59 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-01 20:44:59 | INFO | train | epoch 114 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.222 | nll_loss 0.945 | ppl 1.93 | wps 20640.2 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 47994 | lr 1.00006e-05 | gnorm 0.729 | train_wall 261 | wall 32425
2021-01-01 20:44:59 | INFO | fairseq.trainer | begin training epoch 115
2021-01-01 20:45:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:45:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:45:06 | INFO | train_inner | epoch 115:      6 / 421 symm_kl=0.34, self_kl=0, self_cv=0, loss=3.194, nll_loss=0.931, ppl=1.91, wps=16234.3, ups=1.16, wpb=13966.9, bsz=528.5, num_updates=48000, lr=1e-05, gnorm=0.719, train_wall=63, wall=32432
2021-01-01 20:46:08 | INFO | train_inner | epoch 115:    106 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.949, ppl=1.93, wps=22602, ups=1.62, wpb=13974.8, bsz=486.3, num_updates=48100, lr=9.9896e-06, gnorm=0.722, train_wall=62, wall=32494
2021-01-01 20:47:11 | INFO | train_inner | epoch 115:    206 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.946, ppl=1.93, wps=22335, ups=1.6, wpb=13948.4, bsz=491.4, num_updates=48200, lr=9.97923e-06, gnorm=0.722, train_wall=62, wall=32557
2021-01-01 20:48:13 | INFO | train_inner | epoch 115:    306 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.938, ppl=1.92, wps=22265, ups=1.6, wpb=13922.8, bsz=493, num_updates=48300, lr=9.9689e-06, gnorm=0.727, train_wall=62, wall=32619
2021-01-01 20:49:16 | INFO | train_inner | epoch 115:    406 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.944, ppl=1.92, wps=22624.4, ups=1.6, wpb=14107, bsz=498.6, num_updates=48400, lr=9.95859e-06, gnorm=0.718, train_wall=62, wall=32682
2021-01-01 20:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:49:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:49:42 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.725 | ppl 13.22 | bleu 23.04 | wps 5937.4 | wpb 10324.2 | bsz 375 | num_updates 48415 | best_bleu 23.23
2021-01-01 20:49:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:49:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:49:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 115 @ 48415 updates, score 23.04) (writing took 2.873166412115097 seconds)
2021-01-01 20:49:45 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-01 20:49:45 | INFO | train | epoch 115 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.221 | nll_loss 0.945 | ppl 1.93 | wps 20619.4 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48415 | lr 9.95705e-06 | gnorm 0.725 | train_wall 261 | wall 32711
2021-01-01 20:49:45 | INFO | fairseq.trainer | begin training epoch 116
2021-01-01 20:49:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:49:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:50:40 | INFO | train_inner | epoch 116:     85 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.942, ppl=1.92, wps=16486, ups=1.18, wpb=13963.5, bsz=489.8, num_updates=48500, lr=9.94832e-06, gnorm=0.724, train_wall=61, wall=32766
2021-01-01 20:51:43 | INFO | train_inner | epoch 116:    185 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.202, nll_loss=0.93, ppl=1.91, wps=22554.2, ups=1.59, wpb=14169.6, bsz=503.2, num_updates=48600, lr=9.93808e-06, gnorm=0.716, train_wall=63, wall=32829
2021-01-01 20:52:45 | INFO | train_inner | epoch 116:    285 / 421 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.956, ppl=1.94, wps=22082.1, ups=1.61, wpb=13708, bsz=476.4, num_updates=48700, lr=9.92787e-06, gnorm=0.74, train_wall=62, wall=32891
2021-01-01 20:53:48 | INFO | train_inner | epoch 116:    385 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.947, ppl=1.93, wps=22511.6, ups=1.6, wpb=14043.3, bsz=500.8, num_updates=48800, lr=9.91769e-06, gnorm=0.719, train_wall=62, wall=32954
2021-01-01 20:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:54:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:54:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:54:27 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.725 | ppl 13.22 | bleu 23.08 | wps 5887.9 | wpb 10324.2 | bsz 375 | num_updates 48836 | best_bleu 23.23
2021-01-01 20:54:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:54:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 116 @ 48836 updates, score 23.08) (writing took 2.8450761940330267 seconds)
2021-01-01 20:54:30 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-01 20:54:30 | INFO | train | epoch 116 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.22 | nll_loss 0.945 | ppl 1.93 | wps 20626.8 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 48836 | lr 9.91404e-06 | gnorm 0.725 | train_wall 261 | wall 32996
2021-01-01 20:54:30 | INFO | fairseq.trainer | begin training epoch 117
2021-01-01 20:54:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:54:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:55:12 | INFO | train_inner | epoch 117:     64 / 421 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.962, ppl=1.95, wps=16369, ups=1.18, wpb=13880.7, bsz=472.5, num_updates=48900, lr=9.90755e-06, gnorm=0.742, train_wall=61, wall=33038
2021-01-01 20:56:15 | INFO | train_inner | epoch 117:    164 / 421 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.204, nll_loss=0.935, ppl=1.91, wps=22375.5, ups=1.59, wpb=14082.2, bsz=504.9, num_updates=49000, lr=9.89743e-06, gnorm=0.71, train_wall=63, wall=33101
2021-01-01 20:57:17 | INFO | train_inner | epoch 117:    264 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.216, nll_loss=0.941, ppl=1.92, wps=22554.1, ups=1.61, wpb=14012, bsz=506.2, num_updates=49100, lr=9.88735e-06, gnorm=0.724, train_wall=62, wall=33164
2021-01-01 20:58:20 | INFO | train_inner | epoch 117:    364 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.943, ppl=1.92, wps=22167.9, ups=1.6, wpb=13879.7, bsz=483.9, num_updates=49200, lr=9.8773e-06, gnorm=0.729, train_wall=62, wall=33226
2021-01-01 20:58:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 20:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:58:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 20:59:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 20:59:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 20:59:13 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.264 | nll_loss 3.722 | ppl 13.19 | bleu 23.09 | wps 5970.1 | wpb 10324.2 | bsz 375 | num_updates 49257 | best_bleu 23.23
2021-01-01 20:59:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 20:59:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:59:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:59:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:59:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:59:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:59:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:59:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 117 @ 49257 updates, score 23.09) (writing took 2.958517087623477 seconds)
2021-01-01 20:59:16 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-01 20:59:16 | INFO | train | epoch 117 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.22 | nll_loss 0.945 | ppl 1.92 | wps 20555.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 49257 | lr 9.87158e-06 | gnorm 0.725 | train_wall 262 | wall 33282
2021-01-01 20:59:16 | INFO | fairseq.trainer | begin training epoch 118
2021-01-01 20:59:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 20:59:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 20:59:46 | INFO | train_inner | epoch 118:     43 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.953, ppl=1.94, wps=16345.5, ups=1.17, wpb=13980.3, bsz=496.7, num_updates=49300, lr=9.86727e-06, gnorm=0.727, train_wall=62, wall=33312
2021-01-01 21:00:49 | INFO | train_inner | epoch 118:    143 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.944, ppl=1.92, wps=22175.8, ups=1.59, wpb=13971.2, bsz=478.3, num_updates=49400, lr=9.85728e-06, gnorm=0.724, train_wall=63, wall=33375
2021-01-01 21:01:51 | INFO | train_inner | epoch 118:    243 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.946, ppl=1.93, wps=22262.4, ups=1.6, wpb=13927.7, bsz=482.2, num_updates=49500, lr=9.84732e-06, gnorm=0.733, train_wall=62, wall=33437
2021-01-01 21:02:54 | INFO | train_inner | epoch 118:    343 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.946, ppl=1.93, wps=22165.6, ups=1.6, wpb=13889.3, bsz=503.8, num_updates=49600, lr=9.83739e-06, gnorm=0.728, train_wall=62, wall=33500
2021-01-01 21:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:03:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:03:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:03:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:03:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:04:00 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.726 | ppl 13.23 | bleu 23.13 | wps 5932.2 | wpb 10324.2 | bsz 375 | num_updates 49678 | best_bleu 23.23
2021-01-01 21:04:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:04:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:04:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:04:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:04:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:04:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:04:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:04:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 118 @ 49678 updates, score 23.13) (writing took 2.9192362911999226 seconds)
2021-01-01 21:04:03 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-01 21:04:03 | INFO | train | epoch 118 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.219 | nll_loss 0.944 | ppl 1.92 | wps 20482.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 49678 | lr 9.82966e-06 | gnorm 0.726 | train_wall 263 | wall 33569
2021-01-01 21:04:03 | INFO | fairseq.trainer | begin training epoch 119
2021-01-01 21:04:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:04:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:04:20 | INFO | train_inner | epoch 119:     22 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.946, ppl=1.93, wps=16162.5, ups=1.16, wpb=13910.5, bsz=498.5, num_updates=49700, lr=9.82749e-06, gnorm=0.722, train_wall=63, wall=33586
2021-01-01 21:05:22 | INFO | train_inner | epoch 119:    122 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.212, nll_loss=0.938, ppl=1.92, wps=22494, ups=1.6, wpb=14040.1, bsz=488.4, num_updates=49800, lr=9.81761e-06, gnorm=0.721, train_wall=62, wall=33648
2021-01-01 21:06:25 | INFO | train_inner | epoch 119:    222 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.945, ppl=1.93, wps=22478.9, ups=1.6, wpb=14013, bsz=472.1, num_updates=49900, lr=9.80777e-06, gnorm=0.731, train_wall=62, wall=33711
2021-01-01 21:07:27 | INFO | train_inner | epoch 119:    322 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.954, ppl=1.94, wps=22245.7, ups=1.59, wpb=13969, bsz=510.7, num_updates=50000, lr=9.79796e-06, gnorm=0.73, train_wall=63, wall=33773
2021-01-01 21:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:08:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:08:46 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.264 | nll_loss 3.722 | ppl 13.2 | bleu 23.02 | wps 5917.9 | wpb 10324.2 | bsz 375 | num_updates 50099 | best_bleu 23.23
2021-01-01 21:08:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 119 @ 50099 updates, score 23.02) (writing took 2.893622148782015 seconds)
2021-01-01 21:08:49 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-01 21:08:49 | INFO | train | epoch 119 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.218 | nll_loss 0.944 | ppl 1.92 | wps 20548.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50099 | lr 9.78827e-06 | gnorm 0.727 | train_wall 262 | wall 33855
2021-01-01 21:08:49 | INFO | fairseq.trainer | begin training epoch 120
2021-01-01 21:08:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:08:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:08:53 | INFO | train_inner | epoch 120:      1 / 421 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.204, nll_loss=0.935, ppl=1.91, wps=16295.2, ups=1.17, wpb=13969.4, bsz=502.6, num_updates=50100, lr=9.78818e-06, gnorm=0.729, train_wall=62, wall=33859
2021-01-01 21:09:55 | INFO | train_inner | epoch 120:    101 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.211, nll_loss=0.938, ppl=1.92, wps=22590.3, ups=1.61, wpb=14059.1, bsz=491.9, num_updates=50200, lr=9.77842e-06, gnorm=0.728, train_wall=62, wall=33921
2021-01-01 21:10:58 | INFO | train_inner | epoch 120:    201 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.212, nll_loss=0.94, ppl=1.92, wps=22103.9, ups=1.59, wpb=13908.9, bsz=492.2, num_updates=50300, lr=9.7687e-06, gnorm=0.728, train_wall=63, wall=33984
2021-01-01 21:12:01 | INFO | train_inner | epoch 120:    301 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.939, ppl=1.92, wps=22366.2, ups=1.6, wpb=13996.1, bsz=476, num_updates=50400, lr=9.759e-06, gnorm=0.728, train_wall=62, wall=34047
2021-01-01 21:13:04 | INFO | train_inner | epoch 120:    401 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.959, ppl=1.94, wps=22378.6, ups=1.6, wpb=14006, bsz=494.6, num_updates=50500, lr=9.74933e-06, gnorm=0.718, train_wall=62, wall=34110
2021-01-01 21:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:13:33 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.721 | ppl 13.19 | bleu 23.14 | wps 5946.9 | wpb 10324.2 | bsz 375 | num_updates 50520 | best_bleu 23.23
2021-01-01 21:13:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:13:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:13:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 120 @ 50520 updates, score 23.14) (writing took 2.9282106030732393 seconds)
2021-01-01 21:13:36 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-01 21:13:36 | INFO | train | epoch 120 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.217 | nll_loss 0.944 | ppl 1.92 | wps 20515.8 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50520 | lr 9.7474e-06 | gnorm 0.728 | train_wall 263 | wall 34142
2021-01-01 21:13:36 | INFO | fairseq.trainer | begin training epoch 121
2021-01-01 21:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:14:29 | INFO | train_inner | epoch 121:     80 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.205, nll_loss=0.935, ppl=1.91, wps=16277.2, ups=1.17, wpb=13853.9, bsz=502, num_updates=50600, lr=9.7397e-06, gnorm=0.729, train_wall=62, wall=34195
2021-01-01 21:15:31 | INFO | train_inner | epoch 121:    180 / 421 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.945, ppl=1.93, wps=22149.3, ups=1.6, wpb=13886.3, bsz=493, num_updates=50700, lr=9.73009e-06, gnorm=0.735, train_wall=62, wall=34257
2021-01-01 21:16:34 | INFO | train_inner | epoch 121:    280 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.211, nll_loss=0.941, ppl=1.92, wps=22506.4, ups=1.59, wpb=14115.8, bsz=502.8, num_updates=50800, lr=9.7205e-06, gnorm=0.712, train_wall=63, wall=34320
2021-01-01 21:17:37 | INFO | train_inner | epoch 121:    380 / 421 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.953, ppl=1.94, wps=22219, ups=1.59, wpb=13937.7, bsz=495.9, num_updates=50900, lr=9.71095e-06, gnorm=0.733, train_wall=63, wall=34383
2021-01-01 21:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:18:19 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.725 | ppl 13.22 | bleu 22.97 | wps 5952.6 | wpb 10324.2 | bsz 375 | num_updates 50941 | best_bleu 23.23
2021-01-01 21:18:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:18:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:18:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 121 @ 50941 updates, score 22.97) (writing took 3.000023975968361 seconds)
2021-01-01 21:18:22 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-01 21:18:22 | INFO | train | epoch 121 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.217 | nll_loss 0.944 | ppl 1.92 | wps 20514.1 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 50941 | lr 9.70704e-06 | gnorm 0.726 | train_wall 263 | wall 34429
2021-01-01 21:18:22 | INFO | fairseq.trainer | begin training epoch 122
2021-01-01 21:18:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:18:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:19:02 | INFO | train_inner | epoch 122:     59 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.942, ppl=1.92, wps=16279.7, ups=1.17, wpb=13933, bsz=477.6, num_updates=51000, lr=9.70143e-06, gnorm=0.73, train_wall=62, wall=34468
2021-01-01 21:20:06 | INFO | train_inner | epoch 122:    159 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.943, ppl=1.92, wps=22388.8, ups=1.58, wpb=14155.5, bsz=499.6, num_updates=51100, lr=9.69193e-06, gnorm=0.712, train_wall=63, wall=34532
2021-01-01 21:21:09 | INFO | train_inner | epoch 122:    259 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.944, ppl=1.92, wps=22255.8, ups=1.59, wpb=14008.1, bsz=513.3, num_updates=51200, lr=9.68246e-06, gnorm=0.714, train_wall=63, wall=34595
2021-01-01 21:22:11 | INFO | train_inner | epoch 122:    359 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.208, nll_loss=0.936, ppl=1.91, wps=22449.6, ups=1.6, wpb=14063.1, bsz=482.8, num_updates=51300, lr=9.67302e-06, gnorm=0.72, train_wall=62, wall=34657
2021-01-01 21:22:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:22:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:22:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:22:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:23:06 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.266 | nll_loss 3.724 | ppl 13.21 | bleu 23.18 | wps 5989.6 | wpb 10324.2 | bsz 375 | num_updates 51362 | best_bleu 23.23
2021-01-01 21:23:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:23:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 122 @ 51362 updates, score 23.18) (writing took 2.881809152662754 seconds)
2021-01-01 21:23:09 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2021-01-01 21:23:09 | INFO | train | epoch 122 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.216 | nll_loss 0.943 | ppl 1.92 | wps 20518.7 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 51362 | lr 9.66718e-06 | gnorm 0.724 | train_wall 263 | wall 34715
2021-01-01 21:23:09 | INFO | fairseq.trainer | begin training epoch 123
2021-01-01 21:23:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:23:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:23:36 | INFO | train_inner | epoch 123:     38 / 421 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.954, ppl=1.94, wps=16128.8, ups=1.18, wpb=13655.5, bsz=474.9, num_updates=51400, lr=9.6636e-06, gnorm=0.749, train_wall=61, wall=34742
2021-01-01 21:24:39 | INFO | train_inner | epoch 123:    138 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.946, ppl=1.93, wps=22256.1, ups=1.59, wpb=13976.4, bsz=481.8, num_updates=51500, lr=9.65422e-06, gnorm=0.729, train_wall=63, wall=34805
2021-01-01 21:25:41 | INFO | train_inner | epoch 123:    238 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.942, ppl=1.92, wps=22480.5, ups=1.6, wpb=14039.3, bsz=500.6, num_updates=51600, lr=9.64486e-06, gnorm=0.72, train_wall=62, wall=34867
2021-01-01 21:26:44 | INFO | train_inner | epoch 123:    338 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.21, nll_loss=0.941, ppl=1.92, wps=22357.7, ups=1.6, wpb=13996.7, bsz=503.3, num_updates=51700, lr=9.63552e-06, gnorm=0.712, train_wall=62, wall=34930
2021-01-01 21:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:27:53 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.724 | ppl 13.22 | bleu 23.11 | wps 6002 | wpb 10324.2 | bsz 375 | num_updates 51783 | best_bleu 23.23
2021-01-01 21:27:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:27:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 123 @ 51783 updates, score 23.11) (writing took 2.850932417437434 seconds)
2021-01-01 21:27:55 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2021-01-01 21:27:55 | INFO | train | epoch 123 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.216 | nll_loss 0.943 | ppl 1.92 | wps 20541.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 51783 | lr 9.6278e-06 | gnorm 0.723 | train_wall 262 | wall 35001
2021-01-01 21:27:55 | INFO | fairseq.trainer | begin training epoch 124
2021-01-01 21:27:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:27:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:28:09 | INFO | train_inner | epoch 124:     17 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.94, ppl=1.92, wps=16214.8, ups=1.17, wpb=13896, bsz=488.3, num_updates=51800, lr=9.62622e-06, gnorm=0.725, train_wall=62, wall=35015
2021-01-01 21:29:11 | INFO | train_inner | epoch 124:    117 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.935, ppl=1.91, wps=22733.3, ups=1.62, wpb=14064.5, bsz=493.2, num_updates=51900, lr=9.61694e-06, gnorm=0.729, train_wall=62, wall=35077
2021-01-01 21:30:14 | INFO | train_inner | epoch 124:    217 / 421 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.202, nll_loss=0.934, ppl=1.91, wps=22350.2, ups=1.6, wpb=14007.4, bsz=492.8, num_updates=52000, lr=9.60769e-06, gnorm=0.715, train_wall=62, wall=35140
2021-01-01 21:31:17 | INFO | train_inner | epoch 124:    317 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.947, ppl=1.93, wps=22323.9, ups=1.6, wpb=13972, bsz=515, num_updates=52100, lr=9.59846e-06, gnorm=0.717, train_wall=62, wall=35203
2021-01-01 21:32:19 | INFO | train_inner | epoch 124:    417 / 421 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.955, ppl=1.94, wps=22426.5, ups=1.61, wpb=13928.4, bsz=475.2, num_updates=52200, lr=9.58927e-06, gnorm=0.731, train_wall=62, wall=35265
2021-01-01 21:32:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:32:38 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.724 | ppl 13.21 | bleu 22.97 | wps 5876.7 | wpb 10324.2 | bsz 375 | num_updates 52204 | best_bleu 23.23
2021-01-01 21:32:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:32:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:32:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 124 @ 52204 updates, score 22.97) (writing took 2.9328571874648333 seconds)
2021-01-01 21:32:41 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2021-01-01 21:32:41 | INFO | train | epoch 124 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.215 | nll_loss 0.943 | ppl 1.92 | wps 20600.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 52204 | lr 9.5889e-06 | gnorm 0.725 | train_wall 261 | wall 35287
2021-01-01 21:32:41 | INFO | fairseq.trainer | begin training epoch 125
2021-01-01 21:32:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:32:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:33:43 | INFO | train_inner | epoch 125:     96 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.939, ppl=1.92, wps=16323.5, ups=1.18, wpb=13835, bsz=484.3, num_updates=52300, lr=9.58009e-06, gnorm=0.733, train_wall=61, wall=35349
2021-01-01 21:34:46 | INFO | train_inner | epoch 125:    196 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.216, nll_loss=0.944, ppl=1.92, wps=22216.2, ups=1.59, wpb=13957.3, bsz=502.6, num_updates=52400, lr=9.57095e-06, gnorm=0.716, train_wall=63, wall=35412
2021-01-01 21:35:49 | INFO | train_inner | epoch 125:    296 / 421 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.94, ppl=1.92, wps=22450.4, ups=1.59, wpb=14079, bsz=480.6, num_updates=52500, lr=9.56183e-06, gnorm=0.726, train_wall=63, wall=35475
2021-01-01 21:36:51 | INFO | train_inner | epoch 125:    396 / 421 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.945, ppl=1.93, wps=22542.8, ups=1.61, wpb=14031.2, bsz=496.6, num_updates=52600, lr=9.55274e-06, gnorm=0.718, train_wall=62, wall=35537
2021-01-01 21:37:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:37:23 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.264 | nll_loss 3.721 | ppl 13.19 | bleu 23.1 | wps 5911.2 | wpb 10324.2 | bsz 375 | num_updates 52625 | best_bleu 23.23
2021-01-01 21:37:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:37:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 125 @ 52625 updates, score 23.1) (writing took 2.878543760627508 seconds)
2021-01-01 21:37:26 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2021-01-01 21:37:26 | INFO | train | epoch 125 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.214 | nll_loss 0.943 | ppl 1.92 | wps 20604.5 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 52625 | lr 9.55047e-06 | gnorm 0.723 | train_wall 261 | wall 35572
2021-01-01 21:37:26 | INFO | fairseq.trainer | begin training epoch 126
2021-01-01 21:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:37:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:38:16 | INFO | train_inner | epoch 126:     75 / 421 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.206, nll_loss=0.939, ppl=1.92, wps=16538.4, ups=1.18, wpb=13991, bsz=505.5, num_updates=52700, lr=9.54367e-06, gnorm=0.722, train_wall=61, wall=35622
2021-01-01 21:39:18 | INFO | train_inner | epoch 126:    175 / 421 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.939, ppl=1.92, wps=22395, ups=1.6, wpb=14009.5, bsz=506.2, num_updates=52800, lr=9.53463e-06, gnorm=0.715, train_wall=62, wall=35684
2021-01-01 21:40:21 | INFO | train_inner | epoch 126:    275 / 421 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.958, ppl=1.94, wps=22068.5, ups=1.6, wpb=13824.1, bsz=477.8, num_updates=52900, lr=9.52561e-06, gnorm=0.736, train_wall=62, wall=35747
2021-01-01 21:41:23 | INFO | train_inner | epoch 126:    375 / 421 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.941, ppl=1.92, wps=22361.1, ups=1.61, wpb=13908.7, bsz=493.2, num_updates=53000, lr=9.51662e-06, gnorm=0.725, train_wall=62, wall=35809
2021-01-01 21:41:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-01 21:41:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:41:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:41:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:41:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:41:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-01 21:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-01 21:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-01 21:42:09 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.723 | ppl 13.2 | bleu 23.15 | wps 5943.4 | wpb 10324.2 | bsz 375 | num_updates 53046 | best_bleu 23.23
2021-01-01 21:42:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-01 21:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:42:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 126 @ 53046 updates, score 23.15) (writing took 3.0168024078011513 seconds)
2021-01-01 21:42:12 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2021-01-01 21:42:12 | INFO | train | epoch 126 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.214 | nll_loss 0.943 | ppl 1.92 | wps 20604.4 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 53046 | lr 9.51249e-06 | gnorm 0.722 | train_wall 261 | wall 35858
2021-01-01 21:42:12 | INFO | fairseq.trainer | begin training epoch 127
2021-01-01 21:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-01 21:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-01 21:42:48 | INFO | train_inner | epoch 127:     54 / 421 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.212, nll_loss=0.94, ppl=1.92, wps=16411.4, ups=1.18, wpb=13942.5, bsz=473, num_updates=53100, lr=9.50765e-06, gnorm=0.721, train_wall=62, wall=35894
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1056 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
