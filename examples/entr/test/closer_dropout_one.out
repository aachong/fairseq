--r3f-lambda 0.05

2020-12-08 10:49:12 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout_all', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:18528', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_choice='allone', layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=220, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.05, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-one', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-08 10:49:12 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-08 10:49:12 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-08 10:49:12 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-08 10:49:12 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-08 10:49:12 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-08 10:49:13 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-08 10:49:13 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-08 10:49:13 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-08 10:49:13 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout_all (R3fCloserDropoutAll)
2020-12-08 10:49:13 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-08 10:49:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-08 10:49:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-08 10:49:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-08 10:49:13 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 10:49:13 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 10:49:13 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 10:49:13 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-08 10:49:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-08 10:49:13 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2020-12-08 10:49:13 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-08 10:49:14 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-08 10:49:14 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-08 10:49:14 | INFO | fairseq.trainer | loading train data for epoch 106
2020-12-08 10:49:14 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-08 10:49:14 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-08 10:49:14 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-08 10:49:14 | INFO | fairseq.trainer | begin training epoch 106
2020-12-08 10:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:50:20 | INFO | train_inner | epoch 106:    100 / 421 loss=5.1, nll_loss=0.884, symm_mse=39.115, ppl=1.85, wps=18909.8, ups=1.28, wpb=14798.4, bsz=510.6, num_updates=100, lr=1.25975e-05, gnorm=3.716, train_wall=67, wall=0
2020-12-08 10:51:23 | INFO | train_inner | epoch 106:    200 / 421 loss=4.748, nll_loss=0.997, symm_mse=27.816, ppl=2, wps=22380.8, ups=1.6, wpb=13987.5, bsz=509.7, num_updates=200, lr=2.5095e-05, gnorm=2.229, train_wall=62, wall=0
2020-12-08 10:52:26 | INFO | train_inner | epoch 106:    300 / 421 loss=4.689, nll_loss=1.165, symm_mse=25.392, ppl=2.24, wps=22110.9, ups=1.58, wpb=13964.3, bsz=483.9, num_updates=300, lr=3.75925e-05, gnorm=1.868, train_wall=63, wall=0
2020-12-08 10:53:29 | INFO | train_inner | epoch 106:    400 / 421 loss=4.535, nll_loss=1.225, symm_mse=22.588, ppl=2.34, wps=22302.5, ups=1.59, wpb=14030.6, bsz=494.1, num_updates=400, lr=5.009e-05, gnorm=1.666, train_wall=63, wall=0
2020-12-08 10:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 10:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:53:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:53:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:53:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:53:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:53:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:54:00 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_mse 0 | loss 5.422 | nll_loss 3.975 | ppl 15.72 | bleu 22.49 | wps 5655.4 | wpb 10324.2 | bsz 375 | num_updates 421
2020-12-08 10:54:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 10:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:54:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_best.pt (epoch 106 @ 421 updates, score 22.49) (writing took 3.634272951632738 seconds)
2020-12-08 10:54:03 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2020-12-08 10:54:03 | INFO | train | epoch 106 | loss 3.768 | nll_loss 0.976 | symm_mse 28.307 | ppl 1.97 | wps 28161.4 | ups 1.59 | wpb 17741.1 | bsz 625.6 | num_updates 421 | lr 5.27145e-05 | gnorm 1.703 | train_wall 372 | wall 0
2020-12-08 10:54:03 | INFO | fairseq.trainer | begin training epoch 107
2020-12-08 10:54:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:54:55 | INFO | train_inner | epoch 107:     79 / 421 loss=4.415, nll_loss=1.244, symm_mse=20.625, ppl=2.37, wps=16155.3, ups=1.15, wpb=13998.9, bsz=483.7, num_updates=500, lr=6.25875e-05, gnorm=1.51, train_wall=62, wall=0
2020-12-08 10:55:58 | INFO | train_inner | epoch 107:    179 / 421 loss=4.342, nll_loss=1.267, symm_mse=19.303, ppl=2.41, wps=22378.3, ups=1.59, wpb=14055.1, bsz=506.5, num_updates=600, lr=7.5085e-05, gnorm=1.439, train_wall=63, wall=0
2020-12-08 10:57:01 | INFO | train_inner | epoch 107:    279 / 421 loss=4.326, nll_loss=1.293, symm_mse=18.743, ppl=2.45, wps=22193.2, ups=1.6, wpb=13904.1, bsz=488.4, num_updates=700, lr=8.75825e-05, gnorm=1.419, train_wall=62, wall=0
2020-12-08 10:58:03 | INFO | train_inner | epoch 107:    379 / 421 loss=4.257, nll_loss=1.307, symm_mse=17.597, ppl=2.47, wps=22150.9, ups=1.6, wpb=13867.9, bsz=498.9, num_updates=800, lr=0.00010008, gnorm=1.379, train_wall=62, wall=0
2020-12-08 10:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 10:58:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 10:58:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 10:58:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 10:58:45 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_mse 0 | loss 5.395 | nll_loss 3.943 | ppl 15.38 | bleu 22.31 | wps 6620.4 | wpb 10324.2 | bsz 375 | num_updates 842 | best_bleu 22.49
2020-12-08 10:58:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 10:58:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:58:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 107 @ 842 updates, score 22.31) (writing took 3.1298086270689964 seconds)
2020-12-08 10:58:48 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2020-12-08 10:58:48 | INFO | train | epoch 107 | loss 4.326 | nll_loss 1.284 | symm_mse 18.863 | ppl 2.43 | wps 20626.3 | ups 1.48 | wpb 13969.5 | bsz 492.6 | num_updates 842 | lr 0.000105329 | gnorm 1.428 | train_wall 263 | wall 0
2020-12-08 10:58:48 | INFO | fairseq.trainer | begin training epoch 108
2020-12-08 10:58:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 10:58:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 10:59:27 | INFO | train_inner | epoch 108:     58 / 421 loss=4.189, nll_loss=1.285, symm_mse=16.851, ppl=2.44, wps=16746.9, ups=1.19, wpb=14067.6, bsz=496.4, num_updates=900, lr=0.000112578, gnorm=1.306, train_wall=62, wall=0
2020-12-08 11:00:30 | INFO | train_inner | epoch 108:    158 / 421 loss=4.227, nll_loss=1.324, symm_mse=16.922, ppl=2.5, wps=22156.5, ups=1.59, wpb=13919, bsz=476.1, num_updates=1000, lr=0.000125075, gnorm=1.322, train_wall=63, wall=0
2020-12-08 11:01:33 | INFO | train_inner | epoch 108:    258 / 421 loss=4.21, nll_loss=1.35, symm_mse=16.336, ppl=2.55, wps=22047.8, ups=1.59, wpb=13904.3, bsz=484.6, num_updates=1100, lr=0.000137573, gnorm=1.352, train_wall=63, wall=0
2020-12-08 11:02:36 | INFO | train_inner | epoch 108:    358 / 421 loss=4.183, nll_loss=1.362, symm_mse=15.803, ppl=2.57, wps=22275.6, ups=1.59, wpb=14023.7, bsz=486.3, num_updates=1200, lr=0.00015007, gnorm=1.274, train_wall=63, wall=0
2020-12-08 11:03:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:03:32 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_mse 0 | loss 5.368 | nll_loss 3.905 | ppl 14.98 | bleu 21.91 | wps 5876.4 | wpb 10324.2 | bsz 375 | num_updates 1263 | best_bleu 22.49
2020-12-08 11:03:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:03:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:03:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 108 @ 1263 updates, score 21.91) (writing took 3.1858661267906427 seconds)
2020-12-08 11:03:36 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2020-12-08 11:03:36 | INFO | train | epoch 108 | loss 4.183 | nll_loss 1.335 | symm_mse 16.132 | ppl 2.52 | wps 20468.6 | ups 1.47 | wpb 13969.5 | bsz 492.6 | num_updates 1263 | lr 0.000157943 | gnorm 1.294 | train_wall 263 | wall 0
2020-12-08 11:03:36 | INFO | fairseq.trainer | begin training epoch 109
2020-12-08 11:03:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:03:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:04:01 | INFO | train_inner | epoch 109:     37 / 421 loss=4.12, nll_loss=1.353, symm_mse=15.019, ppl=2.55, wps=16296, ups=1.17, wpb=13870, bsz=511.4, num_updates=1300, lr=0.000162568, gnorm=1.231, train_wall=62, wall=0
2020-12-08 11:05:04 | INFO | train_inner | epoch 109:    137 / 421 loss=4.118, nll_loss=1.355, symm_mse=14.931, ppl=2.56, wps=22231.4, ups=1.59, wpb=14008.2, bsz=498.2, num_updates=1400, lr=0.000175065, gnorm=1.222, train_wall=63, wall=0
2020-12-08 11:06:07 | INFO | train_inner | epoch 109:    237 / 421 loss=4.116, nll_loss=1.377, symm_mse=14.654, ppl=2.6, wps=22172.3, ups=1.59, wpb=13912.4, bsz=489.6, num_updates=1500, lr=0.000187563, gnorm=1.224, train_wall=63, wall=0
2020-12-08 11:07:10 | INFO | train_inner | epoch 109:    337 / 421 loss=4.13, nll_loss=1.406, symm_mse=14.535, ppl=2.65, wps=22242, ups=1.59, wpb=14018.4, bsz=489.3, num_updates=1600, lr=0.00020006, gnorm=1.251, train_wall=63, wall=0
2020-12-08 11:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:08:20 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_mse 0 | loss 5.349 | nll_loss 3.894 | ppl 14.87 | bleu 21.32 | wps 5717.7 | wpb 10324.2 | bsz 375 | num_updates 1684 | best_bleu 22.49
2020-12-08 11:08:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 109 @ 1684 updates, score 21.32) (writing took 3.1054353937506676 seconds)
2020-12-08 11:08:23 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2020-12-08 11:08:23 | INFO | train | epoch 109 | loss 4.118 | nll_loss 1.384 | symm_mse 14.603 | ppl 2.61 | wps 20440.5 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 1684 | lr 0.000210558 | gnorm 1.222 | train_wall 263 | wall 0
2020-12-08 11:08:23 | INFO | fairseq.trainer | begin training epoch 110
2020-12-08 11:08:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:08:36 | INFO | train_inner | epoch 110:     16 / 421 loss=4.093, nll_loss=1.41, symm_mse=13.931, ppl=2.66, wps=16201.5, ups=1.16, wpb=13966.4, bsz=490.4, num_updates=1700, lr=0.000212558, gnorm=1.177, train_wall=62, wall=0
2020-12-08 11:09:39 | INFO | train_inner | epoch 110:    116 / 421 loss=4.09, nll_loss=1.401, symm_mse=13.98, ppl=2.64, wps=22366.9, ups=1.6, wpb=13947.5, bsz=483.4, num_updates=1800, lr=0.000225055, gnorm=1.203, train_wall=62, wall=0
2020-12-08 11:10:42 | INFO | train_inner | epoch 110:    216 / 421 loss=4.09, nll_loss=1.431, symm_mse=13.644, ppl=2.7, wps=22076.4, ups=1.58, wpb=13962.8, bsz=481.1, num_updates=1900, lr=0.000237553, gnorm=1.208, train_wall=63, wall=0
2020-12-08 11:11:45 | INFO | train_inner | epoch 110:    316 / 421 loss=4.089, nll_loss=1.461, symm_mse=13.256, ppl=2.75, wps=22143.9, ups=1.59, wpb=13941.5, bsz=505.9, num_updates=2000, lr=0.00025005, gnorm=1.167, train_wall=63, wall=0
2020-12-08 11:12:48 | INFO | train_inner | epoch 110:    416 / 421 loss=4.088, nll_loss=1.479, symm_mse=13.06, ppl=2.79, wps=22407.4, ups=1.58, wpb=14141.8, bsz=507, num_updates=2100, lr=0.000262548, gnorm=1.163, train_wall=63, wall=0
2020-12-08 11:12:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:12:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:13:08 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_mse 0 | loss 5.314 | nll_loss 3.839 | ppl 14.31 | bleu 21.15 | wps 5796.4 | wpb 10324.2 | bsz 375 | num_updates 2105 | best_bleu 22.49
2020-12-08 11:13:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:13:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:13:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:13:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:13:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:13:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:13:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:13:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 110 @ 2105 updates, score 21.15) (writing took 3.092686116695404 seconds)
2020-12-08 11:13:11 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2020-12-08 11:13:11 | INFO | train | epoch 110 | loss 4.088 | nll_loss 1.441 | symm_mse 13.492 | ppl 2.71 | wps 20442.4 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 2105 | lr 0.000263172 | gnorm 1.186 | train_wall 264 | wall 0
2020-12-08 11:13:11 | INFO | fairseq.trainer | begin training epoch 111
2020-12-08 11:13:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:13:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:14:13 | INFO | train_inner | epoch 111:     95 / 421 loss=4.04, nll_loss=1.436, symm_mse=12.842, ppl=2.71, wps=16469.5, ups=1.17, wpb=14094.2, bsz=495.9, num_updates=2200, lr=0.000275045, gnorm=1.146, train_wall=62, wall=0
2020-12-08 11:15:17 | INFO | train_inner | epoch 111:    195 / 421 loss=4.094, nll_loss=1.494, symm_mse=12.94, ppl=2.82, wps=22135.8, ups=1.59, wpb=13953.1, bsz=482.6, num_updates=2300, lr=0.000287543, gnorm=1.168, train_wall=63, wall=0
2020-12-08 11:16:20 | INFO | train_inner | epoch 111:    295 / 421 loss=4.087, nll_loss=1.514, symm_mse=12.595, ppl=2.86, wps=22061.8, ups=1.58, wpb=13934.5, bsz=490.6, num_updates=2400, lr=0.00030004, gnorm=1.141, train_wall=63, wall=0
2020-12-08 11:17:22 | INFO | train_inner | epoch 111:    395 / 421 loss=4.09, nll_loss=1.539, symm_mse=12.346, ppl=2.91, wps=22151.4, ups=1.6, wpb=13877.2, bsz=499.6, num_updates=2500, lr=0.000312538, gnorm=1.128, train_wall=62, wall=0
2020-12-08 11:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:17:56 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_mse 0 | loss 5.288 | nll_loss 3.824 | ppl 14.16 | bleu 20.88 | wps 5668.8 | wpb 10324.2 | bsz 375 | num_updates 2526 | best_bleu 22.49
2020-12-08 11:17:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 111 @ 2526 updates, score 20.88) (writing took 2.6413627918809652 seconds)
2020-12-08 11:17:59 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2020-12-08 11:17:59 | INFO | train | epoch 111 | loss 4.083 | nll_loss 1.501 | symm_mse 12.697 | ppl 2.83 | wps 20444.6 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 2526 | lr 0.000315787 | gnorm 1.155 | train_wall 264 | wall 0
2020-12-08 11:17:59 | INFO | fairseq.trainer | begin training epoch 112
2020-12-08 11:18:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:18:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:18:48 | INFO | train_inner | epoch 112:     74 / 421 loss=4.098, nll_loss=1.528, symm_mse=12.559, ppl=2.88, wps=16336.2, ups=1.17, wpb=13974.7, bsz=493.1, num_updates=2600, lr=0.000325035, gnorm=1.161, train_wall=62, wall=0
2020-12-08 11:19:51 | INFO | train_inner | epoch 112:    174 / 421 loss=4.06, nll_loss=1.525, symm_mse=12.064, ppl=2.88, wps=22268.7, ups=1.6, wpb=13957.6, bsz=487.4, num_updates=2700, lr=0.000337533, gnorm=1.109, train_wall=62, wall=0
2020-12-08 11:20:54 | INFO | train_inner | epoch 112:    274 / 421 loss=4.141, nll_loss=1.593, symm_mse=12.381, ppl=3.02, wps=21811.7, ups=1.58, wpb=13831.8, bsz=468.1, num_updates=2800, lr=0.00035003, gnorm=1.128, train_wall=63, wall=0
2020-12-08 11:21:57 | INFO | train_inner | epoch 112:    374 / 421 loss=4.069, nll_loss=1.576, symm_mse=11.578, ppl=2.98, wps=22316.1, ups=1.59, wpb=14074.8, bsz=506.2, num_updates=2900, lr=0.000362528, gnorm=1.07, train_wall=63, wall=0
2020-12-08 11:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:22:44 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_mse 0 | loss 5.282 | nll_loss 3.831 | ppl 14.23 | bleu 20.31 | wps 5728.3 | wpb 10324.2 | bsz 375 | num_updates 2947 | best_bleu 22.49
2020-12-08 11:22:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:22:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:22:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 112 @ 2947 updates, score 20.31) (writing took 3.096891637891531 seconds)
2020-12-08 11:22:47 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2020-12-08 11:22:47 | INFO | train | epoch 112 | loss 4.087 | nll_loss 1.56 | symm_mse 12.023 | ppl 2.95 | wps 20378.7 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 2947 | lr 0.000368401 | gnorm 1.104 | train_wall 264 | wall 0
2020-12-08 11:22:47 | INFO | fairseq.trainer | begin training epoch 113
2020-12-08 11:22:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:22:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:23:23 | INFO | train_inner | epoch 113:     53 / 421 loss=4.1, nll_loss=1.594, symm_mse=11.813, ppl=3.02, wps=16171.1, ups=1.16, wpb=13931.9, bsz=497.4, num_updates=3000, lr=0.000375025, gnorm=1.113, train_wall=62, wall=0
2020-12-08 11:24:27 | INFO | train_inner | epoch 113:    153 / 421 loss=4.04, nll_loss=1.555, symm_mse=11.389, ppl=2.94, wps=22161.8, ups=1.58, wpb=14055.6, bsz=498.8, num_updates=3100, lr=0.000387523, gnorm=1.049, train_wall=63, wall=0
2020-12-08 11:25:30 | INFO | train_inner | epoch 113:    253 / 421 loss=4.071, nll_loss=1.601, symm_mse=11.319, ppl=3.03, wps=22222.8, ups=1.58, wpb=14085.1, bsz=513.5, num_updates=3200, lr=0.00040002, gnorm=1.077, train_wall=63, wall=0
2020-12-08 11:26:34 | INFO | train_inner | epoch 113:    353 / 421 loss=4.117, nll_loss=1.65, symm_mse=11.377, ppl=3.14, wps=21888.3, ups=1.57, wpb=13921.7, bsz=493.5, num_updates=3300, lr=0.000412518, gnorm=1.132, train_wall=63, wall=0
2020-12-08 11:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:27:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:27:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:27:36 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_mse 0 | loss 5.26 | nll_loss 3.814 | ppl 14.06 | bleu 21.12 | wps 4937.4 | wpb 10324.2 | bsz 375 | num_updates 3368 | best_bleu 22.49
2020-12-08 11:27:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 113 @ 3368 updates, score 21.12) (writing took 3.1604836229234934 seconds)
2020-12-08 11:27:39 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2020-12-08 11:27:39 | INFO | train | epoch 113 | loss 4.105 | nll_loss 1.621 | symm_mse 11.546 | ppl 3.08 | wps 20142.3 | ups 1.44 | wpb 13969.5 | bsz 492.6 | num_updates 3368 | lr 0.000421016 | gnorm 1.099 | train_wall 265 | wall 0
2020-12-08 11:27:39 | INFO | fairseq.trainer | begin training epoch 114
2020-12-08 11:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:28:02 | INFO | train_inner | epoch 114:     32 / 421 loss=4.19, nll_loss=1.702, symm_mse=11.766, ppl=3.25, wps=15701.2, ups=1.13, wpb=13890.9, bsz=466.4, num_updates=3400, lr=0.000425015, gnorm=1.127, train_wall=62, wall=0
2020-12-08 11:29:05 | INFO | train_inner | epoch 114:    132 / 421 loss=4.14, nll_loss=1.665, symm_mse=11.468, ppl=3.17, wps=22060.8, ups=1.59, wpb=13910.6, bsz=480.3, num_updates=3500, lr=0.000437513, gnorm=1.08, train_wall=63, wall=0
2020-12-08 11:30:09 | INFO | train_inner | epoch 114:    232 / 421 loss=4.114, nll_loss=1.671, symm_mse=11.042, ppl=3.18, wps=22109.5, ups=1.58, wpb=14016.5, bsz=491.3, num_updates=3600, lr=0.00045001, gnorm=1.056, train_wall=63, wall=0
2020-12-08 11:31:12 | INFO | train_inner | epoch 114:    332 / 421 loss=4.16, nll_loss=1.711, symm_mse=11.253, ppl=3.27, wps=22081.9, ups=1.59, wpb=13921, bsz=485.6, num_updates=3700, lr=0.000462508, gnorm=1.084, train_wall=63, wall=0
2020-12-08 11:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:32:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:32:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:32:26 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_mse 0 | loss 5.242 | nll_loss 3.784 | ppl 13.78 | bleu 21.28 | wps 5635.6 | wpb 10324.2 | bsz 375 | num_updates 3789 | best_bleu 22.49
2020-12-08 11:32:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:32:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 114 @ 3789 updates, score 21.28) (writing took 3.086148228496313 seconds)
2020-12-08 11:32:29 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2020-12-08 11:32:29 | INFO | train | epoch 114 | loss 4.119 | nll_loss 1.677 | symm_mse 11.063 | ppl 3.2 | wps 20305.3 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 3789 | lr 0.00047363 | gnorm 1.055 | train_wall 265 | wall 0
2020-12-08 11:32:29 | INFO | fairseq.trainer | begin training epoch 115
2020-12-08 11:32:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:32:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:32:39 | INFO | train_inner | epoch 115:     11 / 421 loss=4.063, nll_loss=1.671, symm_mse=10.34, ppl=3.18, wps=15998.5, ups=1.15, wpb=13965.3, bsz=522.3, num_updates=3800, lr=0.000475005, gnorm=0.996, train_wall=63, wall=0
2020-12-08 11:33:42 | INFO | train_inner | epoch 115:    111 / 421 loss=4.121, nll_loss=1.687, symm_mse=10.949, ppl=3.22, wps=22270.5, ups=1.59, wpb=13972.5, bsz=496, num_updates=3900, lr=0.000487503, gnorm=1.082, train_wall=63, wall=0
2020-12-08 11:34:45 | INFO | train_inner | epoch 115:    211 / 421 loss=4.139, nll_loss=1.719, symm_mse=10.82, ppl=3.29, wps=22043.9, ups=1.58, wpb=13989.7, bsz=482.4, num_updates=4000, lr=0.0005, gnorm=1.049, train_wall=63, wall=0
2020-12-08 11:35:49 | INFO | train_inner | epoch 115:    311 / 421 loss=4.158, nll_loss=1.749, symm_mse=10.735, ppl=3.36, wps=21892.5, ups=1.57, wpb=13903.9, bsz=495, num_updates=4100, lr=0.000493865, gnorm=1.051, train_wall=63, wall=0
2020-12-08 11:36:52 | INFO | train_inner | epoch 115:    411 / 421 loss=4.159, nll_loss=1.769, symm_mse=10.509, ppl=3.41, wps=22297.8, ups=1.58, wpb=14086.3, bsz=496.7, num_updates=4200, lr=0.00048795, gnorm=1.057, train_wall=63, wall=0
2020-12-08 11:36:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:36:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:36:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:36:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:36:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:37:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:37:15 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_mse 0 | loss 5.252 | nll_loss 3.791 | ppl 13.84 | bleu 20.76 | wps 5694.6 | wpb 10324.2 | bsz 375 | num_updates 4210 | best_bleu 22.49
2020-12-08 11:37:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:37:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:37:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:37:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:37:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:37:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 115 @ 4210 updates, score 20.76) (writing took 3.040311995893717 seconds)
2020-12-08 11:37:18 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2020-12-08 11:37:18 | INFO | train | epoch 115 | loss 4.143 | nll_loss 1.731 | symm_mse 10.744 | ppl 3.32 | wps 20321.9 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 4210 | lr 0.00048737 | gnorm 1.062 | train_wall 265 | wall 0
2020-12-08 11:37:18 | INFO | fairseq.trainer | begin training epoch 116
2020-12-08 11:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:37:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:38:18 | INFO | train_inner | epoch 116:     90 / 421 loss=4.087, nll_loss=1.697, symm_mse=10.346, ppl=3.24, wps=16213.9, ups=1.16, wpb=13976.2, bsz=492.1, num_updates=4300, lr=0.000482243, gnorm=1.027, train_wall=63, wall=0
2020-12-08 11:39:21 | INFO | train_inner | epoch 116:    190 / 421 loss=4.082, nll_loss=1.701, symm_mse=10.209, ppl=3.25, wps=22201.9, ups=1.58, wpb=14089.3, bsz=501.7, num_updates=4400, lr=0.000476731, gnorm=1.003, train_wall=63, wall=0
2020-12-08 11:40:25 | INFO | train_inner | epoch 116:    290 / 421 loss=4.169, nll_loss=1.782, symm_mse=10.474, ppl=3.44, wps=21685.1, ups=1.57, wpb=13777.3, bsz=476.5, num_updates=4500, lr=0.000471405, gnorm=1.05, train_wall=63, wall=0
2020-12-08 11:41:28 | INFO | train_inner | epoch 116:    390 / 421 loss=4.128, nll_loss=1.768, symm_mse=10.068, ppl=3.41, wps=22194.9, ups=1.58, wpb=14046, bsz=499.2, num_updates=4600, lr=0.000466252, gnorm=0.991, train_wall=63, wall=0
2020-12-08 11:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:41:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:42:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:42:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:42:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:42:06 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_mse 0 | loss 5.22 | nll_loss 3.776 | ppl 13.7 | bleu 20.84 | wps 5257.2 | wpb 10324.2 | bsz 375 | num_updates 4631 | best_bleu 22.49
2020-12-08 11:42:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:42:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:42:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:42:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:42:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 116 @ 4631 updates, score 20.84) (writing took 3.020101683214307 seconds)
2020-12-08 11:42:09 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2020-12-08 11:42:09 | INFO | train | epoch 116 | loss 4.118 | nll_loss 1.739 | symm_mse 10.259 | ppl 3.34 | wps 20239.9 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 4631 | lr 0.000464689 | gnorm 1.015 | train_wall 265 | wall 0
2020-12-08 11:42:09 | INFO | fairseq.trainer | begin training epoch 117
2020-12-08 11:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:42:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:42:55 | INFO | train_inner | epoch 117:     69 / 421 loss=4.127, nll_loss=1.748, symm_mse=10.264, ppl=3.36, wps=16027.4, ups=1.16, wpb=13864.2, bsz=478, num_updates=4700, lr=0.000461266, gnorm=1.005, train_wall=62, wall=0
2020-12-08 11:43:58 | INFO | train_inner | epoch 117:    169 / 421 loss=4.038, nll_loss=1.687, symm_mse=9.742, ppl=3.22, wps=22134.9, ups=1.57, wpb=14094.6, bsz=499.4, num_updates=4800, lr=0.000456435, gnorm=0.949, train_wall=63, wall=0
2020-12-08 11:45:02 | INFO | train_inner | epoch 117:    269 / 421 loss=4.074, nll_loss=1.727, symm_mse=9.775, ppl=3.31, wps=22067.5, ups=1.58, wpb=14002.2, bsz=504.5, num_updates=4900, lr=0.000451754, gnorm=0.983, train_wall=63, wall=0
2020-12-08 11:46:05 | INFO | train_inner | epoch 117:    369 / 421 loss=4.082, nll_loss=1.744, symm_mse=9.699, ppl=3.35, wps=22039.8, ups=1.59, wpb=13900.5, bsz=485.4, num_updates=5000, lr=0.000447214, gnorm=0.957, train_wall=63, wall=0
2020-12-08 11:46:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:46:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:46:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:46:56 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_mse 0 | loss 5.203 | nll_loss 3.737 | ppl 13.33 | bleu 21.68 | wps 5375.9 | wpb 10324.2 | bsz 375 | num_updates 5052 | best_bleu 22.49
2020-12-08 11:46:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:46:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:46:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:46:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 117 @ 5052 updates, score 21.68) (writing took 3.2030154392123222 seconds)
2020-12-08 11:46:59 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2020-12-08 11:46:59 | INFO | train | epoch 117 | loss 4.077 | nll_loss 1.727 | symm_mse 9.824 | ppl 3.31 | wps 20257.5 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 5052 | lr 0.000444906 | gnorm 0.975 | train_wall 265 | wall 0
2020-12-08 11:46:59 | INFO | fairseq.trainer | begin training epoch 118
2020-12-08 11:47:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:47:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:47:32 | INFO | train_inner | epoch 118:     48 / 421 loss=4.056, nll_loss=1.718, symm_mse=9.641, ppl=3.29, wps=16065.6, ups=1.15, wpb=13986.4, bsz=497.8, num_updates=5100, lr=0.000442807, gnorm=0.976, train_wall=62, wall=0
2020-12-08 11:48:35 | INFO | train_inner | epoch 118:    148 / 421 loss=4.05, nll_loss=1.704, symm_mse=9.725, ppl=3.26, wps=21978.8, ups=1.58, wpb=13944.8, bsz=481.2, num_updates=5200, lr=0.000438529, gnorm=0.997, train_wall=63, wall=0
2020-12-08 11:49:39 | INFO | train_inner | epoch 118:    248 / 421 loss=4.078, nll_loss=1.732, symm_mse=9.787, ppl=3.32, wps=21944.8, ups=1.58, wpb=13893.1, bsz=479.4, num_updates=5300, lr=0.000434372, gnorm=0.975, train_wall=63, wall=0
2020-12-08 11:50:42 | INFO | train_inner | epoch 118:    348 / 421 loss=4.018, nll_loss=1.713, symm_mse=9.166, ppl=3.28, wps=22037.6, ups=1.58, wpb=13960.1, bsz=505.5, num_updates=5400, lr=0.000430331, gnorm=0.888, train_wall=63, wall=0
2020-12-08 11:51:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:51:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:51:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:51:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:51:46 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_mse 0 | loss 5.188 | nll_loss 3.729 | ppl 13.26 | bleu 21.78 | wps 5831.2 | wpb 10324.2 | bsz 375 | num_updates 5473 | best_bleu 22.49
2020-12-08 11:51:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:51:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:51:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 118 @ 5473 updates, score 21.78) (writing took 3.044807804748416 seconds)
2020-12-08 11:51:49 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2020-12-08 11:51:49 | INFO | train | epoch 118 | loss 4.04 | nll_loss 1.71 | symm_mse 9.503 | ppl 3.27 | wps 20317.2 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 5473 | lr 0.000427452 | gnorm 0.941 | train_wall 265 | wall 0
2020-12-08 11:51:49 | INFO | fairseq.trainer | begin training epoch 119
2020-12-08 11:51:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:51:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:52:08 | INFO | train_inner | epoch 119:     27 / 421 loss=4.022, nll_loss=1.715, symm_mse=9.201, ppl=3.28, wps=16077.6, ups=1.16, wpb=13875.7, bsz=495.5, num_updates=5500, lr=0.000426401, gnorm=0.891, train_wall=63, wall=0
2020-12-08 11:53:12 | INFO | train_inner | epoch 119:    127 / 421 loss=3.969, nll_loss=1.646, symm_mse=9.26, ppl=3.13, wps=22210, ups=1.58, wpb=14085.1, bsz=489.4, num_updates=5600, lr=0.000422577, gnorm=0.91, train_wall=63, wall=0
2020-12-08 11:54:15 | INFO | train_inner | epoch 119:    227 / 421 loss=4.061, nll_loss=1.726, symm_mse=9.615, ppl=3.31, wps=22126, ups=1.58, wpb=13978.4, bsz=470.9, num_updates=5700, lr=0.000418854, gnorm=0.97, train_wall=63, wall=0
2020-12-08 11:55:18 | INFO | train_inner | epoch 119:    327 / 421 loss=4.002, nll_loss=1.712, symm_mse=8.947, ppl=3.28, wps=22057.4, ups=1.58, wpb=13957.6, bsz=510.1, num_updates=5800, lr=0.000415227, gnorm=0.889, train_wall=63, wall=0
2020-12-08 11:56:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 11:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 11:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 11:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 11:56:34 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_mse 0 | loss 5.173 | nll_loss 3.695 | ppl 12.95 | bleu 22.2 | wps 6089.7 | wpb 10324.2 | bsz 375 | num_updates 5894 | best_bleu 22.49
2020-12-08 11:56:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 11:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 119 @ 5894 updates, score 22.2) (writing took 3.1039830539375544 seconds)
2020-12-08 11:56:37 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2020-12-08 11:56:37 | INFO | train | epoch 119 | loss 4.004 | nll_loss 1.694 | symm_mse 9.189 | ppl 3.24 | wps 20370.2 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 5894 | lr 0.000411903 | gnorm 0.915 | train_wall 265 | wall 0
2020-12-08 11:56:37 | INFO | fairseq.trainer | begin training epoch 120
2020-12-08 11:56:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 11:56:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 11:56:44 | INFO | train_inner | epoch 120:      6 / 421 loss=3.98, nll_loss=1.686, symm_mse=8.962, ppl=3.22, wps=16248.4, ups=1.16, wpb=13979.7, bsz=503.5, num_updates=5900, lr=0.000411693, gnorm=0.916, train_wall=63, wall=0
2020-12-08 11:57:47 | INFO | train_inner | epoch 120:    106 / 421 loss=3.937, nll_loss=1.64, symm_mse=8.887, ppl=3.12, wps=22429, ups=1.6, wpb=14061, bsz=495.1, num_updates=6000, lr=0.000408248, gnorm=0.88, train_wall=62, wall=0
2020-12-08 11:58:50 | INFO | train_inner | epoch 120:    206 / 421 loss=3.964, nll_loss=1.661, symm_mse=9.025, ppl=3.16, wps=22035.4, ups=1.58, wpb=13926, bsz=485.4, num_updates=6100, lr=0.000404888, gnorm=0.91, train_wall=63, wall=0
2020-12-08 11:59:54 | INFO | train_inner | epoch 120:    306 / 421 loss=3.999, nll_loss=1.693, symm_mse=9.154, ppl=3.23, wps=21962.6, ups=1.57, wpb=13953.6, bsz=479.8, num_updates=6200, lr=0.00040161, gnorm=0.901, train_wall=63, wall=0
2020-12-08 12:00:57 | INFO | train_inner | epoch 120:    406 / 421 loss=3.987, nll_loss=1.71, symm_mse=8.754, ppl=3.27, wps=22241.2, ups=1.58, wpb=14039.7, bsz=502.4, num_updates=6300, lr=0.00039841, gnorm=0.882, train_wall=63, wall=0
2020-12-08 12:01:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:01:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:01:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:01:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:01:23 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_mse 0 | loss 5.159 | nll_loss 3.687 | ppl 12.88 | bleu 22.14 | wps 5856.3 | wpb 10324.2 | bsz 375 | num_updates 6315 | best_bleu 22.49
2020-12-08 12:01:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 120 @ 6315 updates, score 22.14) (writing took 3.0386068373918533 seconds)
2020-12-08 12:01:26 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2020-12-08 12:01:26 | INFO | train | epoch 120 | loss 3.971 | nll_loss 1.675 | symm_mse 8.953 | ppl 3.19 | wps 20362.1 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 6315 | lr 0.000397936 | gnorm 0.905 | train_wall 265 | wall 0
2020-12-08 12:01:26 | INFO | fairseq.trainer | begin training epoch 121
2020-12-08 12:01:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:01:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:01:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:02:22 | INFO | train_inner | epoch 121:     85 / 421 loss=3.931, nll_loss=1.633, symm_mse=8.884, ppl=3.1, wps=16233.3, ups=1.17, wpb=13855.1, bsz=493.4, num_updates=6400, lr=0.000395285, gnorm=0.936, train_wall=62, wall=0
2020-12-08 12:03:26 | INFO | train_inner | epoch 121:    185 / 421 loss=3.939, nll_loss=1.653, symm_mse=8.764, ppl=3.14, wps=21928.1, ups=1.58, wpb=13892.9, bsz=499.4, num_updates=6500, lr=0.000392232, gnorm=0.898, train_wall=63, wall=0
2020-12-08 12:04:29 | INFO | train_inner | epoch 121:    285 / 421 loss=3.912, nll_loss=1.643, symm_mse=8.506, ppl=3.12, wps=22273.4, ups=1.58, wpb=14133.2, bsz=503.9, num_updates=6600, lr=0.000389249, gnorm=0.856, train_wall=63, wall=0
2020-12-08 12:05:32 | INFO | train_inner | epoch 121:    385 / 421 loss=3.997, nll_loss=1.705, symm_mse=8.969, ppl=3.26, wps=21954.9, ups=1.58, wpb=13926.2, bsz=489.2, num_updates=6700, lr=0.000386334, gnorm=0.954, train_wall=63, wall=0
2020-12-08 12:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:05:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:05:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:05:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:05:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:05:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:05:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:05:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:05:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:06:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:06:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:06:13 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_mse 0 | loss 5.134 | nll_loss 3.657 | ppl 12.61 | bleu 22.46 | wps 5753.4 | wpb 10324.2 | bsz 375 | num_updates 6736 | best_bleu 22.49
2020-12-08 12:06:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:06:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:06:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:06:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:06:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:06:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:06:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:06:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 121 @ 6736 updates, score 22.46) (writing took 3.197439581155777 seconds)
2020-12-08 12:06:16 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2020-12-08 12:06:16 | INFO | train | epoch 121 | loss 3.943 | nll_loss 1.658 | symm_mse 8.762 | ppl 3.16 | wps 20312.9 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 6736 | lr 0.0003853 | gnorm 0.909 | train_wall 265 | wall 0
2020-12-08 12:06:16 | INFO | fairseq.trainer | begin training epoch 122
2020-12-08 12:06:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:06:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:06:59 | INFO | train_inner | epoch 122:     64 / 421 loss=3.921, nll_loss=1.638, symm_mse=8.685, ppl=3.11, wps=16154.5, ups=1.16, wpb=13942.7, bsz=478, num_updates=6800, lr=0.000383482, gnorm=0.916, train_wall=62, wall=0
2020-12-08 12:08:02 | INFO | train_inner | epoch 122:    164 / 421 loss=3.899, nll_loss=1.626, symm_mse=8.52, ppl=3.09, wps=22219.6, ups=1.57, wpb=14113.6, bsz=498.8, num_updates=6900, lr=0.000380693, gnorm=0.857, train_wall=63, wall=0
2020-12-08 12:09:06 | INFO | train_inner | epoch 122:    264 / 421 loss=3.884, nll_loss=1.623, symm_mse=8.361, ppl=3.08, wps=22127, ups=1.58, wpb=14028.9, bsz=511.9, num_updates=7000, lr=0.000377964, gnorm=0.878, train_wall=63, wall=0
2020-12-08 12:10:09 | INFO | train_inner | epoch 122:    364 / 421 loss=3.931, nll_loss=1.654, symm_mse=8.656, ppl=3.15, wps=22085, ups=1.57, wpb=14031.4, bsz=481.9, num_updates=7100, lr=0.000375293, gnorm=0.868, train_wall=63, wall=0
2020-12-08 12:10:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:10:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:10:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:10:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:10:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:10:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:10:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:10:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:10:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:10:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:10:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:10:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:11:03 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_mse 0 | loss 5.139 | nll_loss 3.668 | ppl 12.71 | bleu 21.86 | wps 5453.3 | wpb 10324.2 | bsz 375 | num_updates 7157 | best_bleu 22.49
2020-12-08 12:11:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:11:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:11:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:11:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:11:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:11:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:11:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:11:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 122 @ 7157 updates, score 21.86) (writing took 3.0265601202845573 seconds)
2020-12-08 12:11:06 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2020-12-08 12:11:06 | INFO | train | epoch 122 | loss 3.915 | nll_loss 1.641 | symm_mse 8.581 | ppl 3.12 | wps 20252.9 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 7157 | lr 0.000373796 | gnorm 0.876 | train_wall 265 | wall 0
2020-12-08 12:11:06 | INFO | fairseq.trainer | begin training epoch 123
2020-12-08 12:11:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:11:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:11:36 | INFO | train_inner | epoch 123:     43 / 421 loss=3.937, nll_loss=1.664, symm_mse=8.612, ppl=3.17, wps=15786, ups=1.16, wpb=13662.6, bsz=479.8, num_updates=7200, lr=0.000372678, gnorm=0.878, train_wall=62, wall=0
2020-12-08 12:12:39 | INFO | train_inner | epoch 123:    143 / 421 loss=3.888, nll_loss=1.614, symm_mse=8.495, ppl=3.06, wps=22128.3, ups=1.58, wpb=14016.2, bsz=481.8, num_updates=7300, lr=0.000370117, gnorm=0.83, train_wall=63, wall=0
2020-12-08 12:13:42 | INFO | train_inner | epoch 123:    243 / 421 loss=3.886, nll_loss=1.623, symm_mse=8.388, ppl=3.08, wps=22134.4, ups=1.58, wpb=14016.4, bsz=500.1, num_updates=7400, lr=0.000367607, gnorm=0.839, train_wall=63, wall=0
2020-12-08 12:14:45 | INFO | train_inner | epoch 123:    343 / 421 loss=3.876, nll_loss=1.626, symm_mse=8.222, ppl=3.09, wps=22211, ups=1.59, wpb=13998, bsz=505.6, num_updates=7500, lr=0.000365148, gnorm=0.827, train_wall=63, wall=0
2020-12-08 12:15:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:15:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:15:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:15:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:15:53 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_mse 0 | loss 5.121 | nll_loss 3.662 | ppl 12.66 | bleu 21.56 | wps 5508.3 | wpb 10324.2 | bsz 375 | num_updates 7578 | best_bleu 22.49
2020-12-08 12:15:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:15:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:15:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 123 @ 7578 updates, score 21.56) (writing took 3.1799403596669436 seconds)
2020-12-08 12:15:56 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2020-12-08 12:15:56 | INFO | train | epoch 123 | loss 3.888 | nll_loss 1.625 | symm_mse 8.39 | ppl 3.08 | wps 20261 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 7578 | lr 0.000363264 | gnorm 0.839 | train_wall 265 | wall 0
2020-12-08 12:15:56 | INFO | fairseq.trainer | begin training epoch 124
2020-12-08 12:15:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:15:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:16:13 | INFO | train_inner | epoch 124:     22 / 421 loss=3.893, nll_loss=1.631, symm_mse=8.402, ppl=3.1, wps=15860.2, ups=1.14, wpb=13900.7, bsz=486.9, num_updates=7600, lr=0.000362738, gnorm=0.862, train_wall=63, wall=0
2020-12-08 12:17:15 | INFO | train_inner | epoch 124:    122 / 421 loss=3.857, nll_loss=1.588, symm_mse=8.388, ppl=3.01, wps=22603.6, ups=1.61, wpb=14072.1, bsz=491.5, num_updates=7700, lr=0.000360375, gnorm=0.843, train_wall=62, wall=0
2020-12-08 12:18:19 | INFO | train_inner | epoch 124:    222 / 421 loss=3.83, nll_loss=1.583, symm_mse=8.071, ppl=3, wps=21922.7, ups=1.56, wpb=14018.2, bsz=495.9, num_updates=7800, lr=0.000358057, gnorm=0.82, train_wall=64, wall=0
2020-12-08 12:19:23 | INFO | train_inner | epoch 124:    322 / 421 loss=3.862, nll_loss=1.62, symm_mse=8.095, ppl=3.07, wps=21957.9, ups=1.58, wpb=13929.4, bsz=508.9, num_updates=7900, lr=0.000355784, gnorm=0.828, train_wall=63, wall=0
2020-12-08 12:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:20:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:20:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:20:42 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_mse 0 | loss 5.108 | nll_loss 3.633 | ppl 12.4 | bleu 22.35 | wps 5898.7 | wpb 10324.2 | bsz 375 | num_updates 7999 | best_bleu 22.49
2020-12-08 12:20:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 124 @ 7999 updates, score 22.35) (writing took 3.2581058628857136 seconds)
2020-12-08 12:20:45 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2020-12-08 12:20:45 | INFO | train | epoch 124 | loss 3.864 | nll_loss 1.609 | symm_mse 8.249 | ppl 3.05 | wps 20346 | ups 1.46 | wpb 13969.5 | bsz 492.6 | num_updates 7999 | lr 0.000353575 | gnorm 0.841 | train_wall 265 | wall 0
2020-12-08 12:20:45 | INFO | fairseq.trainer | begin training epoch 125
2020-12-08 12:20:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:20:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:20:49 | INFO | train_inner | epoch 125:      1 / 421 loss=3.915, nll_loss=1.655, symm_mse=8.424, ppl=3.15, wps=15968.4, ups=1.16, wpb=13817.5, bsz=472.6, num_updates=8000, lr=0.000353553, gnorm=0.871, train_wall=63, wall=0
2020-12-08 12:21:52 | INFO | train_inner | epoch 125:    101 / 421 loss=3.832, nll_loss=1.572, symm_mse=8.227, ppl=2.97, wps=22245.8, ups=1.6, wpb=13925.3, bsz=490.2, num_updates=8100, lr=0.000351364, gnorm=0.839, train_wall=62, wall=0
2020-12-08 12:22:55 | INFO | train_inner | epoch 125:    201 / 421 loss=3.824, nll_loss=1.58, symm_mse=8.032, ppl=2.99, wps=22097.6, ups=1.58, wpb=14015.8, bsz=504.6, num_updates=8200, lr=0.000349215, gnorm=0.822, train_wall=63, wall=0
2020-12-08 12:23:58 | INFO | train_inner | epoch 125:    301 / 421 loss=3.877, nll_loss=1.613, symm_mse=8.389, ppl=3.06, wps=22325.5, ups=1.59, wpb=14051, bsz=479.1, num_updates=8300, lr=0.000347105, gnorm=0.871, train_wall=63, wall=0
2020-12-08 12:25:02 | INFO | train_inner | epoch 125:    401 / 421 loss=3.848, nll_loss=1.614, symm_mse=7.982, ppl=3.06, wps=22093.8, ups=1.57, wpb=14029.4, bsz=494.8, num_updates=8400, lr=0.000345033, gnorm=0.826, train_wall=63, wall=0
2020-12-08 12:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:25:32 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_mse 0 | loss 5.106 | nll_loss 3.636 | ppl 12.43 | bleu 22.06 | wps 5770.8 | wpb 10324.2 | bsz 375 | num_updates 8420 | best_bleu 22.49
2020-12-08 12:25:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:25:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 125 @ 8420 updates, score 22.06) (writing took 3.225599220022559 seconds)
2020-12-08 12:25:35 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2020-12-08 12:25:35 | INFO | train | epoch 125 | loss 3.844 | nll_loss 1.595 | symm_mse 8.132 | ppl 3.02 | wps 20313.5 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 8420 | lr 0.000344623 | gnorm 0.842 | train_wall 265 | wall 0
2020-12-08 12:25:35 | INFO | fairseq.trainer | begin training epoch 126
2020-12-08 12:25:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:25:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:26:28 | INFO | train_inner | epoch 126:     80 / 421 loss=3.779, nll_loss=1.547, symm_mse=7.804, ppl=2.92, wps=16192.7, ups=1.16, wpb=13972.3, bsz=510.1, num_updates=8500, lr=0.000342997, gnorm=0.823, train_wall=62, wall=0
2020-12-08 12:27:32 | INFO | train_inner | epoch 126:    180 / 421 loss=3.794, nll_loss=1.558, symm_mse=7.876, ppl=2.95, wps=22017.3, ups=1.57, wpb=14007, bsz=501.6, num_updates=8600, lr=0.000340997, gnorm=0.803, train_wall=63, wall=0
2020-12-08 12:28:35 | INFO | train_inner | epoch 126:    280 / 421 loss=3.872, nll_loss=1.62, symm_mse=8.231, ppl=3.07, wps=21940, ups=1.58, wpb=13873.5, bsz=483, num_updates=8700, lr=0.000339032, gnorm=0.852, train_wall=63, wall=0
2020-12-08 12:29:38 | INFO | train_inner | epoch 126:    380 / 421 loss=3.846, nll_loss=1.6, symm_mse=8.132, ppl=3.03, wps=21969.1, ups=1.58, wpb=13880.5, bsz=488.6, num_updates=8800, lr=0.0003371, gnorm=0.893, train_wall=63, wall=0
2020-12-08 12:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:30:21 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_mse 0 | loss 5.101 | nll_loss 3.615 | ppl 12.25 | bleu 22.16 | wps 5783.1 | wpb 10324.2 | bsz 375 | num_updates 8841 | best_bleu 22.49
2020-12-08 12:30:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:30:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:30:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 126 @ 8841 updates, score 22.16) (writing took 3.282016344368458 seconds)
2020-12-08 12:30:24 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2020-12-08 12:30:24 | INFO | train | epoch 126 | loss 3.825 | nll_loss 1.582 | symm_mse 8.042 | ppl 2.99 | wps 20315.7 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 8841 | lr 0.000336317 | gnorm 0.843 | train_wall 265 | wall 0
2020-12-08 12:30:24 | INFO | fairseq.trainer | begin training epoch 127
2020-12-08 12:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:30:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:31:04 | INFO | train_inner | epoch 127:     59 / 421 loss=3.816, nll_loss=1.565, symm_mse=8.103, ppl=2.96, wps=16222.7, ups=1.16, wpb=13965.6, bsz=473.6, num_updates=8900, lr=0.000335201, gnorm=0.841, train_wall=62, wall=0
2020-12-08 12:32:08 | INFO | train_inner | epoch 127:    159 / 421 loss=3.796, nll_loss=1.556, symm_mse=7.925, ppl=2.94, wps=21826.8, ups=1.56, wpb=13969.7, bsz=499.4, num_updates=9000, lr=0.000333333, gnorm=0.829, train_wall=64, wall=0
2020-12-08 12:33:12 | INFO | train_inner | epoch 127:    259 / 421 loss=3.785, nll_loss=1.556, symm_mse=7.792, ppl=2.94, wps=22045.1, ups=1.58, wpb=13992.9, bsz=493.2, num_updates=9100, lr=0.000331497, gnorm=0.801, train_wall=63, wall=0
2020-12-08 12:34:15 | INFO | train_inner | epoch 127:    359 / 421 loss=3.816, nll_loss=1.581, symm_mse=7.949, ppl=2.99, wps=22344.1, ups=1.59, wpb=14053.9, bsz=492.8, num_updates=9200, lr=0.00032969, gnorm=0.834, train_wall=63, wall=0
2020-12-08 12:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:35:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:35:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:35:11 | INFO | valid | epoch 127 | valid on 'valid' subset | symm_mse 0 | loss 5.079 | nll_loss 3.601 | ppl 12.13 | bleu 22.38 | wps 5621.3 | wpb 10324.2 | bsz 375 | num_updates 9262 | best_bleu 22.49
2020-12-08 12:35:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:35:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:35:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:35:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 127 @ 9262 updates, score 22.38) (writing took 3.187927322462201 seconds)
2020-12-08 12:35:15 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2020-12-08 12:35:15 | INFO | train | epoch 127 | loss 3.804 | nll_loss 1.568 | symm_mse 7.915 | ppl 2.96 | wps 20271.1 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 9262 | lr 0.000328585 | gnorm 0.822 | train_wall 265 | wall 0
2020-12-08 12:35:15 | INFO | fairseq.trainer | begin training epoch 128
2020-12-08 12:35:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:35:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:35:41 | INFO | train_inner | epoch 128:     38 / 421 loss=3.799, nll_loss=1.566, symm_mse=7.86, ppl=2.96, wps=16166.9, ups=1.16, wpb=13972.6, bsz=501.9, num_updates=9300, lr=0.000327913, gnorm=0.822, train_wall=62, wall=0
2020-12-08 12:36:44 | INFO | train_inner | epoch 128:    138 / 421 loss=3.794, nll_loss=1.55, symm_mse=7.982, ppl=2.93, wps=21946.9, ups=1.58, wpb=13909.2, bsz=480.2, num_updates=9400, lr=0.000326164, gnorm=0.831, train_wall=63, wall=0
2020-12-08 12:37:48 | INFO | train_inner | epoch 128:    238 / 421 loss=3.775, nll_loss=1.546, symm_mse=7.758, ppl=2.92, wps=22023.2, ups=1.57, wpb=14069.5, bsz=496, num_updates=9500, lr=0.000324443, gnorm=0.795, train_wall=64, wall=0
2020-12-08 12:38:52 | INFO | train_inner | epoch 128:    338 / 421 loss=3.797, nll_loss=1.568, symm_mse=7.817, ppl=2.96, wps=21962.8, ups=1.57, wpb=13970.5, bsz=484.8, num_updates=9600, lr=0.000322749, gnorm=0.815, train_wall=63, wall=0
2020-12-08 12:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:39:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:40:01 | INFO | valid | epoch 128 | valid on 'valid' subset | symm_mse 0 | loss 5.07 | nll_loss 3.59 | ppl 12.04 | bleu 22.41 | wps 5951.8 | wpb 10324.2 | bsz 375 | num_updates 9683 | best_bleu 22.49
2020-12-08 12:40:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:40:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:40:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:40:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:40:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:40:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:40:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:40:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 128 @ 9683 updates, score 22.41) (writing took 3.164185205474496 seconds)
2020-12-08 12:40:04 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2020-12-08 12:40:04 | INFO | train | epoch 128 | loss 3.787 | nll_loss 1.556 | symm_mse 7.822 | ppl 2.94 | wps 20314.5 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 9683 | lr 0.000321362 | gnorm 0.821 | train_wall 266 | wall 0
2020-12-08 12:40:04 | INFO | fairseq.trainer | begin training epoch 129
2020-12-08 12:40:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:40:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:40:18 | INFO | train_inner | epoch 129:     17 / 421 loss=3.795, nll_loss=1.574, symm_mse=7.719, ppl=2.98, wps=16087.8, ups=1.16, wpb=13816.8, bsz=500.8, num_updates=9700, lr=0.000321081, gnorm=0.84, train_wall=63, wall=0
2020-12-08 12:41:21 | INFO | train_inner | epoch 129:    117 / 421 loss=3.741, nll_loss=1.516, symm_mse=7.654, ppl=2.86, wps=22120.3, ups=1.59, wpb=13925.2, bsz=500.7, num_updates=9800, lr=0.000319438, gnorm=0.788, train_wall=63, wall=0
2020-12-08 12:42:24 | INFO | train_inner | epoch 129:    217 / 421 loss=3.737, nll_loss=1.519, symm_mse=7.556, ppl=2.87, wps=22160.6, ups=1.58, wpb=13996.6, bsz=527.8, num_updates=9900, lr=0.000317821, gnorm=0.802, train_wall=63, wall=0
2020-12-08 12:43:28 | INFO | train_inner | epoch 129:    317 / 421 loss=3.79, nll_loss=1.554, symm_mse=7.896, ppl=2.94, wps=22204.7, ups=1.57, wpb=14160.5, bsz=474.6, num_updates=10000, lr=0.000316228, gnorm=0.822, train_wall=64, wall=0
2020-12-08 12:44:31 | INFO | train_inner | epoch 129:    417 / 421 loss=3.808, nll_loss=1.582, symm_mse=7.826, ppl=2.99, wps=21919, ups=1.57, wpb=13934.4, bsz=473.3, num_updates=10100, lr=0.000314658, gnorm=0.797, train_wall=63, wall=0
2020-12-08 12:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:44:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:44:50 | INFO | valid | epoch 129 | valid on 'valid' subset | symm_mse 0 | loss 5.068 | nll_loss 3.585 | ppl 12 | bleu 22.5 | wps 5972.5 | wpb 10324.2 | bsz 375 | num_updates 10104 | best_bleu 22.5
2020-12-08 12:44:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:44:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:44:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_best.pt (epoch 129 @ 10104 updates, score 22.5) (writing took 5.010308371856809 seconds)
2020-12-08 12:44:55 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2020-12-08 12:44:55 | INFO | train | epoch 129 | loss 3.77 | nll_loss 1.543 | symm_mse 7.741 | ppl 2.91 | wps 20202.6 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 10104 | lr 0.000314596 | gnorm 0.803 | train_wall 265 | wall 0
2020-12-08 12:44:55 | INFO | fairseq.trainer | begin training epoch 130
2020-12-08 12:44:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:44:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:45:59 | INFO | train_inner | epoch 130:     96 / 421 loss=3.72, nll_loss=1.495, symm_mse=7.595, ppl=2.82, wps=16066.6, ups=1.14, wpb=14032.1, bsz=492.2, num_updates=10200, lr=0.000313112, gnorm=0.802, train_wall=62, wall=0
2020-12-08 12:47:02 | INFO | train_inner | epoch 130:    196 / 421 loss=3.748, nll_loss=1.525, symm_mse=7.649, ppl=2.88, wps=21849.4, ups=1.58, wpb=13847.6, bsz=490, num_updates=10300, lr=0.000311588, gnorm=0.784, train_wall=63, wall=0
2020-12-08 12:48:05 | INFO | train_inner | epoch 130:    296 / 421 loss=3.792, nll_loss=1.562, symm_mse=7.834, ppl=2.95, wps=21878.8, ups=1.57, wpb=13903.2, bsz=480, num_updates=10400, lr=0.000310087, gnorm=0.821, train_wall=63, wall=0
2020-12-08 12:49:09 | INFO | train_inner | epoch 130:    396 / 421 loss=3.75, nll_loss=1.535, symm_mse=7.562, ppl=2.9, wps=22236.2, ups=1.58, wpb=14067.6, bsz=506.2, num_updates=10500, lr=0.000308607, gnorm=0.79, train_wall=63, wall=0
2020-12-08 12:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-08 12:49:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-08 12:49:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-08 12:49:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-08 12:49:42 | INFO | valid | epoch 130 | valid on 'valid' subset | symm_mse 0 | loss 5.059 | nll_loss 3.575 | ppl 11.92 | bleu 22.49 | wps 5821.5 | wpb 10324.2 | bsz 375 | num_updates 10525 | best_bleu 22.5
2020-12-08 12:49:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-08 12:49:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:49:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-one/checkpoint_last.pt (epoch 130 @ 10525 updates, score 22.49) (writing took 3.283491810783744 seconds)
2020-12-08 12:49:45 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2020-12-08 12:49:45 | INFO | train | epoch 130 | loss 3.754 | nll_loss 1.531 | symm_mse 7.66 | ppl 2.89 | wps 20309.5 | ups 1.45 | wpb 13969.5 | bsz 492.6 | num_updates 10525 | lr 0.00030824 | gnorm 0.8 | train_wall 265 | wall 0
2020-12-08 12:49:45 | INFO | fairseq.trainer | begin training epoch 131
2020-12-08 12:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-08 12:49:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-08 12:50:35 | INFO | train_inner | epoch 131:     75 / 421 loss=3.71, nll_loss=1.493, symm_mse=7.489, ppl=2.82, wps=16254.1, ups=1.16, wpb=14008.7, bsz=507.8, num_updates=10600, lr=0.000307148, gnorm=0.81, train_wall=62, wall=0
2020-12-08 12:51:38 | INFO | train_inner | epoch 131:    175 / 421 loss=3.764, nll_loss=1.53, symm_mse=7.812, ppl=2.89, wps=21969.9, ups=1.58, wpb=13867.3, bsz=480.6, num_updates=10700, lr=0.000305709, gnorm=0.822, train_wall=63, wall=0
2020-12-08 12:52:41 | INFO | train_inner | epoch 131:    275 / 421 loss=3.753, nll_loss=1.536, symm_mse=7.587, ppl=2.9, wps=22046.1, ups=1.58, wpb=13969.2, bsz=489, num_updates=10800, lr=0.00030429, gnorm=0.797, train_wall=63, wall=0
2020-12-08 12:53:45 | INFO | train_inner | epoch 131:    375 / 421 loss=3.726, nll_loss=1.517, symm_mse=7.444, ppl=2.86, wps=22238.4, ups=1.57, wpb=14140.1, bsz=493.5, num_updates=10900, lr=0.000302891, gnorm=0.778, train_wall=63, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 248 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
