nohup: ignoring input
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.0000125
lrscheduler=fixed
warmup_updates=0
max_epoch=100
r3f_lambda=0.02
save_dir=./examples/entr/bash/../checkpoints/closer_gap
2020-12-09 17:13:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:17586
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:17586
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:17586
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-09 17:13:25 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-09 17:13:29 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:17586', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[1.25e-05], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.02, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer_gap', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
2020-12-09 17:13:29 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-09 17:13:29 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-09 17:13:29 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-09 17:13:29 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-09 17:13:29 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-09 17:13:30 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-09 17:13:30 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-09 17:13:30 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-09 17:13:30 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-09 17:13:30 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-09 17:13:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-09 17:13:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-09 17:13:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-09 17:13:30 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-09 17:13:30 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-09 17:13:30 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-09 17:13:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-09 17:13:30 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-09 17:13:30 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-09 17:13:30 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-09 17:13:31 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-09 17:13:31 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-09 17:13:31 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-09 17:13:31 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-09 17:13:31 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-09 17:13:31 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-09 17:13:31 | INFO | fairseq.trainer | begin training epoch 1
2020-12-09 17:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:14:19 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=26.955, loss=3.468, nll_loss=0.76, ppl=1.69, wps=24121.6, ups=2.27, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.25e-05, gnorm=2.083, train_wall=44, wall=49
2020-12-09 17:15:04 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=24.762, loss=3.417, nll_loss=0.74, ppl=1.67, wps=23465.9, ups=2.22, wpb=10583.4, bsz=369.8, num_updates=200, lr=1.25e-05, gnorm=1.9, train_wall=45, wall=94
2020-12-09 17:15:49 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=23.286, loss=3.375, nll_loss=0.74, ppl=1.67, wps=23140.1, ups=2.24, wpb=10335, bsz=373, num_updates=300, lr=1.25e-05, gnorm=1.84, train_wall=44, wall=139
2020-12-09 17:16:33 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=22.276, loss=3.337, nll_loss=0.735, ppl=1.66, wps=23668, ups=2.24, wpb=10571.8, bsz=388.4, num_updates=400, lr=1.25e-05, gnorm=1.763, train_wall=44, wall=183
2020-12-09 17:17:18 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=22.443, loss=3.346, nll_loss=0.739, ppl=1.67, wps=23277.7, ups=2.24, wpb=10411.2, bsz=371.8, num_updates=500, lr=1.25e-05, gnorm=1.776, train_wall=45, wall=228
2020-12-09 17:17:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:18:06 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | loss 6.194 | nll_loss 4.456 | ppl 21.94 | bleu 21.81 | wps 4627.1 | wpb 7508.5 | bsz 272.7 | num_updates 561
2020-12-09 17:18:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:18:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:18:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:18:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 1 @ 561 updates, score 21.81) (writing took 2.1106847748160362 seconds)
2020-12-09 17:18:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-09 17:18:08 | INFO | train | epoch 001 | symm_kl 24.147 | loss 3.397 | nll_loss 0.743 | ppl 1.67 | wps 21520.5 | ups 2.05 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 1.25e-05 | gnorm 1.887 | train_wall 250 | wall 278
2020-12-09 17:18:08 | INFO | fairseq.trainer | begin training epoch 2
2020-12-09 17:18:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:18:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:18:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:18:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:18:28 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=24.296, loss=3.402, nll_loss=0.731, ppl=1.66, wps=14770.9, ups=1.43, wpb=10345.6, bsz=358.4, num_updates=600, lr=1.25e-05, gnorm=1.926, train_wall=44, wall=298
2020-12-09 17:19:13 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=24.013, loss=3.393, nll_loss=0.731, ppl=1.66, wps=23557.3, ups=2.24, wpb=10532.9, bsz=366.4, num_updates=700, lr=1.25e-05, gnorm=1.888, train_wall=45, wall=343
2020-12-09 17:19:58 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=21.996, loss=3.326, nll_loss=0.724, ppl=1.65, wps=23349.1, ups=2.22, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.25e-05, gnorm=1.742, train_wall=45, wall=388
2020-12-09 17:20:42 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=22.467, loss=3.341, nll_loss=0.725, ppl=1.65, wps=23567.9, ups=2.24, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.25e-05, gnorm=1.782, train_wall=45, wall=432
2020-12-09 17:21:27 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=22.272, loss=3.344, nll_loss=0.73, ppl=1.66, wps=23390.5, ups=2.23, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.25e-05, gnorm=1.785, train_wall=45, wall=477
2020-12-09 17:22:12 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=22.879, loss=3.361, nll_loss=0.735, ppl=1.66, wps=23294.2, ups=2.23, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.25e-05, gnorm=1.84, train_wall=45, wall=522
2020-12-09 17:22:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:22:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:22:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:22:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:22:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:22:42 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | loss 6.204 | nll_loss 4.462 | ppl 22.04 | bleu 21.75 | wps 4706.1 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 21.81
2020-12-09 17:22:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:22:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:22:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:22:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 2 @ 1122 updates, score 21.75) (writing took 2.755646603181958 seconds)
2020-12-09 17:22:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-09 17:22:45 | INFO | train | epoch 002 | symm_kl 22.748 | loss 3.353 | nll_loss 0.728 | ppl 1.66 | wps 21227.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.25e-05 | gnorm 1.816 | train_wall 250 | wall 555
2020-12-09 17:22:45 | INFO | fairseq.trainer | begin training epoch 3
2020-12-09 17:22:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:22:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:23:22 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=22.96, loss=3.355, nll_loss=0.722, ppl=1.65, wps=14834.2, ups=1.42, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.25e-05, gnorm=1.857, train_wall=44, wall=592
2020-12-09 17:24:07 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=21.858, loss=3.326, nll_loss=0.726, ppl=1.65, wps=23366.2, ups=2.24, wpb=10420.6, bsz=376, num_updates=1300, lr=1.25e-05, gnorm=1.785, train_wall=44, wall=637
2020-12-09 17:24:52 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=21.958, loss=3.324, nll_loss=0.722, ppl=1.65, wps=23465.3, ups=2.24, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.25e-05, gnorm=1.779, train_wall=44, wall=682
2020-12-09 17:25:36 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=22.373, loss=3.351, nll_loss=0.735, ppl=1.66, wps=23501.9, ups=2.24, wpb=10472.3, bsz=374.7, num_updates=1500, lr=1.25e-05, gnorm=1.794, train_wall=44, wall=726
2020-12-09 17:26:21 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=21.748, loss=3.321, nll_loss=0.725, ppl=1.65, wps=23637.9, ups=2.22, wpb=10650.7, bsz=373.4, num_updates=1600, lr=1.25e-05, gnorm=1.747, train_wall=45, wall=771
2020-12-09 17:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:27:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:27:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:27:19 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | loss 6.212 | nll_loss 4.469 | ppl 22.15 | bleu 21.76 | wps 4595.9 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 21.81
2020-12-09 17:27:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:27:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:27:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 3 @ 1683 updates, score 21.76) (writing took 2.724819239228964 seconds)
2020-12-09 17:27:22 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-09 17:27:22 | INFO | train | epoch 003 | symm_kl 22.236 | loss 3.336 | nll_loss 0.724 | ppl 1.65 | wps 21237 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 1.25e-05 | gnorm 1.796 | train_wall 249 | wall 832
2020-12-09 17:27:22 | INFO | fairseq.trainer | begin training epoch 4
2020-12-09 17:27:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:27:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:27:33 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=23.165, loss=3.359, nll_loss=0.716, ppl=1.64, wps=14678.9, ups=1.4, wpb=10447.8, bsz=352, num_updates=1700, lr=1.25e-05, gnorm=1.882, train_wall=44, wall=842
2020-12-09 17:28:17 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=22.648, loss=3.351, nll_loss=0.722, ppl=1.65, wps=23518.7, ups=2.25, wpb=10469.1, bsz=365.6, num_updates=1800, lr=1.25e-05, gnorm=1.834, train_wall=44, wall=887
2020-12-09 17:29:02 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=22.383, loss=3.346, nll_loss=0.728, ppl=1.66, wps=22906.8, ups=2.23, wpb=10271.1, bsz=367.4, num_updates=1900, lr=1.25e-05, gnorm=1.834, train_wall=45, wall=932
2020-12-09 17:29:47 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=22.324, loss=3.333, nll_loss=0.715, ppl=1.64, wps=23538, ups=2.23, wpb=10571.4, bsz=356.9, num_updates=2000, lr=1.25e-05, gnorm=1.814, train_wall=45, wall=977
2020-12-09 17:30:32 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=21.753, loss=3.32, nll_loss=0.721, ppl=1.65, wps=23392.9, ups=2.22, wpb=10532.7, bsz=370.6, num_updates=2100, lr=1.25e-05, gnorm=1.784, train_wall=45, wall=1022
2020-12-09 17:31:17 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=20.902, loss=3.295, nll_loss=0.723, ppl=1.65, wps=23677.7, ups=2.23, wpb=10614.4, bsz=387.6, num_updates=2200, lr=1.25e-05, gnorm=1.719, train_wall=45, wall=1067
2020-12-09 17:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:31:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:31:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:31:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:31:57 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | loss 6.212 | nll_loss 4.471 | ppl 22.18 | bleu 21.75 | wps 4609.9 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 21.81
2020-12-09 17:31:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:32:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 4 @ 2244 updates, score 21.75) (writing took 2.702035941183567 seconds)
2020-12-09 17:32:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-09 17:32:00 | INFO | train | epoch 004 | symm_kl 21.861 | loss 3.323 | nll_loss 0.721 | ppl 1.65 | wps 21186.3 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 1.25e-05 | gnorm 1.788 | train_wall 250 | wall 1110
2020-12-09 17:32:00 | INFO | fairseq.trainer | begin training epoch 5
2020-12-09 17:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:32:27 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=20.981, loss=3.287, nll_loss=0.714, ppl=1.64, wps=14801.4, ups=1.42, wpb=10433.5, bsz=373.3, num_updates=2300, lr=1.25e-05, gnorm=1.731, train_wall=44, wall=1137
2020-12-09 17:33:12 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=21.439, loss=3.312, nll_loss=0.723, ppl=1.65, wps=23434.3, ups=2.24, wpb=10447.6, bsz=372.4, num_updates=2400, lr=1.25e-05, gnorm=1.765, train_wall=44, wall=1182
2020-12-09 17:33:57 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=21.369, loss=3.309, nll_loss=0.722, ppl=1.65, wps=23334.3, ups=2.22, wpb=10524.3, bsz=363.8, num_updates=2500, lr=1.25e-05, gnorm=1.751, train_wall=45, wall=1227
2020-12-09 17:34:43 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=22.776, loss=3.359, nll_loss=0.725, ppl=1.65, wps=22541, ups=2.16, wpb=10415.7, bsz=357.5, num_updates=2600, lr=1.25e-05, gnorm=1.864, train_wall=46, wall=1273
2020-12-09 17:35:28 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=20.659, loss=3.28, nll_loss=0.715, ppl=1.64, wps=23511.8, ups=2.23, wpb=10565, bsz=383, num_updates=2700, lr=1.25e-05, gnorm=1.7, train_wall=45, wall=1318
2020-12-09 17:36:13 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=21.435, loss=3.304, nll_loss=0.714, ppl=1.64, wps=23169.1, ups=2.21, wpb=10479.6, bsz=374.7, num_updates=2800, lr=1.25e-05, gnorm=1.763, train_wall=45, wall=1363
2020-12-09 17:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:36:38 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | loss 6.22 | nll_loss 4.478 | ppl 22.29 | bleu 21.79 | wps 4112.2 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 21.81
2020-12-09 17:36:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:36:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:36:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:36:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 5 @ 2805 updates, score 21.79) (writing took 2.7573343720287085 seconds)
2020-12-09 17:36:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-09 17:36:41 | INFO | train | epoch 005 | symm_kl 21.566 | loss 3.313 | nll_loss 0.719 | ppl 1.65 | wps 20911.9 | ups 1.99 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 1.25e-05 | gnorm 1.772 | train_wall 252 | wall 1391
2020-12-09 17:36:41 | INFO | fairseq.trainer | begin training epoch 6
2020-12-09 17:36:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:36:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:37:26 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=21.218, loss=3.298, nll_loss=0.716, ppl=1.64, wps=14216.8, ups=1.38, wpb=10318.1, bsz=377.4, num_updates=2900, lr=1.25e-05, gnorm=1.79, train_wall=44, wall=1436
2020-12-09 17:38:11 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=21.571, loss=3.311, nll_loss=0.715, ppl=1.64, wps=23542.3, ups=2.2, wpb=10679.3, bsz=372.1, num_updates=3000, lr=1.25e-05, gnorm=1.764, train_wall=45, wall=1481
2020-12-09 17:38:56 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=21.44, loss=3.306, nll_loss=0.712, ppl=1.64, wps=23227.3, ups=2.22, wpb=10477.8, bsz=365.4, num_updates=3100, lr=1.25e-05, gnorm=1.774, train_wall=45, wall=1526
2020-12-09 17:39:41 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=21.829, loss=3.331, nll_loss=0.725, ppl=1.65, wps=23452.9, ups=2.23, wpb=10517.4, bsz=358, num_updates=3200, lr=1.25e-05, gnorm=1.781, train_wall=45, wall=1571
2020-12-09 17:40:26 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=20.956, loss=3.28, nll_loss=0.704, ppl=1.63, wps=23571.4, ups=2.24, wpb=10534.9, bsz=372, num_updates=3300, lr=1.25e-05, gnorm=1.729, train_wall=45, wall=1616
2020-12-09 17:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:40:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:40:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:40:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:40:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:40:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:40:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:41:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:41:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:41:16 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | loss 6.22 | nll_loss 4.475 | ppl 22.25 | bleu 21.88 | wps 4667.7 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 21.88
2020-12-09 17:41:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:41:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:41:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:41:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 6 @ 3366 updates, score 21.88) (writing took 4.583661925047636 seconds)
2020-12-09 17:41:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-09 17:41:20 | INFO | train | epoch 006 | symm_kl 21.322 | loss 3.304 | nll_loss 0.716 | ppl 1.64 | wps 21048.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 1.25e-05 | gnorm 1.761 | train_wall 250 | wall 1670
2020-12-09 17:41:20 | INFO | fairseq.trainer | begin training epoch 7
2020-12-09 17:41:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:41:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:41:38 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=20.399, loss=3.288, nll_loss=0.73, ppl=1.66, wps=14155.2, ups=1.38, wpb=10237, bsz=369, num_updates=3400, lr=1.25e-05, gnorm=1.696, train_wall=44, wall=1688
2020-12-09 17:42:23 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=21.505, loss=3.307, nll_loss=0.713, ppl=1.64, wps=23353.2, ups=2.22, wpb=10508.4, bsz=371.6, num_updates=3500, lr=1.25e-05, gnorm=1.783, train_wall=45, wall=1733
2020-12-09 17:43:08 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=21.074, loss=3.3, nll_loss=0.717, ppl=1.64, wps=23184.4, ups=2.23, wpb=10404.4, bsz=363.4, num_updates=3600, lr=1.25e-05, gnorm=1.749, train_wall=45, wall=1778
2020-12-09 17:43:53 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=20.654, loss=3.287, nll_loss=0.72, ppl=1.65, wps=23387.1, ups=2.24, wpb=10456.6, bsz=375.4, num_updates=3700, lr=1.25e-05, gnorm=1.726, train_wall=45, wall=1823
2020-12-09 17:44:38 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=21.834, loss=3.317, nll_loss=0.711, ppl=1.64, wps=23300.9, ups=2.23, wpb=10467.8, bsz=366.5, num_updates=3800, lr=1.25e-05, gnorm=1.84, train_wall=45, wall=1867
2020-12-09 17:45:23 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=21.079, loss=3.296, nll_loss=0.713, ppl=1.64, wps=23676.5, ups=2.22, wpb=10688.9, bsz=373.3, num_updates=3900, lr=1.25e-05, gnorm=1.727, train_wall=45, wall=1913
2020-12-09 17:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:45:55 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | loss 6.22 | nll_loss 4.477 | ppl 22.27 | bleu 21.88 | wps 4698.2 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 21.88
2020-12-09 17:45:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:45:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:45:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:46:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 7 @ 3927 updates, score 21.88) (writing took 4.57113597355783 seconds)
2020-12-09 17:46:00 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-09 17:46:00 | INFO | train | epoch 007 | symm_kl 21.105 | loss 3.297 | nll_loss 0.715 | ppl 1.64 | wps 21035.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 1.25e-05 | gnorm 1.754 | train_wall 250 | wall 1950
2020-12-09 17:46:00 | INFO | fairseq.trainer | begin training epoch 8
2020-12-09 17:46:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:46:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:46:35 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=20.635, loss=3.278, nll_loss=0.712, ppl=1.64, wps=14642.8, ups=1.38, wpb=10590, bsz=375, num_updates=4000, lr=1.25e-05, gnorm=1.71, train_wall=44, wall=1985
2020-12-09 17:47:20 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=22.281, loss=3.332, nll_loss=0.709, ppl=1.63, wps=23340.7, ups=2.22, wpb=10504.7, bsz=358.9, num_updates=4100, lr=1.25e-05, gnorm=1.849, train_wall=45, wall=2030
2020-12-09 17:48:05 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=20.465, loss=3.278, nll_loss=0.713, ppl=1.64, wps=23251.2, ups=2.24, wpb=10367.5, bsz=366.4, num_updates=4200, lr=1.25e-05, gnorm=1.706, train_wall=44, wall=2075
2020-12-09 17:48:50 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=19.851, loss=3.252, nll_loss=0.71, ppl=1.64, wps=23172.6, ups=2.22, wpb=10416.1, bsz=389.5, num_updates=4300, lr=1.25e-05, gnorm=1.684, train_wall=45, wall=2120
2020-12-09 17:49:35 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=20.144, loss=3.262, nll_loss=0.712, ppl=1.64, wps=23709.6, ups=2.23, wpb=10648.7, bsz=379.6, num_updates=4400, lr=1.25e-05, gnorm=1.666, train_wall=45, wall=2164
2020-12-09 17:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:50:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:50:35 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | loss 6.225 | nll_loss 4.48 | ppl 22.32 | bleu 21.92 | wps 4556.3 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 21.92
2020-12-09 17:50:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 8 @ 4488 updates, score 21.92) (writing took 4.641429387032986 seconds)
2020-12-09 17:50:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-09 17:50:40 | INFO | train | epoch 008 | symm_kl 20.929 | loss 3.291 | nll_loss 0.713 | ppl 1.64 | wps 21029.7 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 1.25e-05 | gnorm 1.742 | train_wall 250 | wall 2229
2020-12-09 17:50:40 | INFO | fairseq.trainer | begin training epoch 9
2020-12-09 17:50:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:50:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:50:48 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=21.702, loss=3.324, nll_loss=0.721, ppl=1.65, wps=14085.6, ups=1.36, wpb=10320.4, bsz=352.8, num_updates=4500, lr=1.25e-05, gnorm=1.796, train_wall=44, wall=2238
2020-12-09 17:51:32 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=20.919, loss=3.281, nll_loss=0.702, ppl=1.63, wps=23779.5, ups=2.26, wpb=10532.6, bsz=374.2, num_updates=4600, lr=1.25e-05, gnorm=1.736, train_wall=44, wall=2282
2020-12-09 17:52:17 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=22.368, loss=3.347, nll_loss=0.72, ppl=1.65, wps=23460.9, ups=2.23, wpb=10528, bsz=345, num_updates=4700, lr=1.25e-05, gnorm=1.827, train_wall=45, wall=2327
2020-12-09 17:53:02 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=19.872, loss=3.254, nll_loss=0.711, ppl=1.64, wps=23385.4, ups=2.23, wpb=10473.1, bsz=377.1, num_updates=4800, lr=1.25e-05, gnorm=1.671, train_wall=45, wall=2372
2020-12-09 17:53:47 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=20.52, loss=3.275, nll_loss=0.709, ppl=1.63, wps=23371.9, ups=2.23, wpb=10483.1, bsz=368.3, num_updates=4900, lr=1.25e-05, gnorm=1.745, train_wall=45, wall=2416
2020-12-09 17:54:32 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=19.956, loss=3.255, nll_loss=0.71, ppl=1.64, wps=23257.5, ups=2.21, wpb=10514.7, bsz=388.6, num_updates=5000, lr=1.25e-05, gnorm=1.697, train_wall=45, wall=2462
2020-12-09 17:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:54:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:54:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:54:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:54:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:54:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:54:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:54:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:54:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:54:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:54:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:54:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:54:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:54:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:54:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:55:14 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | loss 6.23 | nll_loss 4.484 | ppl 22.37 | bleu 21.83 | wps 4848.9 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 21.92
2020-12-09 17:55:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:55:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 9 @ 5049 updates, score 21.83) (writing took 2.7499793246388435 seconds)
2020-12-09 17:55:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-09 17:55:16 | INFO | train | epoch 009 | symm_kl 20.748 | loss 3.285 | nll_loss 0.712 | ppl 1.64 | wps 21240.9 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 1.25e-05 | gnorm 1.738 | train_wall 250 | wall 2506
2020-12-09 17:55:16 | INFO | fairseq.trainer | begin training epoch 10
2020-12-09 17:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:55:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:55:42 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=20.851, loss=3.298, nll_loss=0.72, ppl=1.65, wps=14808.3, ups=1.43, wpb=10353, bsz=356, num_updates=5100, lr=1.25e-05, gnorm=1.755, train_wall=44, wall=2532
2020-12-09 17:56:27 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=20.528, loss=3.281, nll_loss=0.714, ppl=1.64, wps=23626.2, ups=2.23, wpb=10577.3, bsz=365.6, num_updates=5200, lr=1.25e-05, gnorm=1.703, train_wall=45, wall=2576
2020-12-09 17:57:11 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=21.002, loss=3.295, nll_loss=0.71, ppl=1.64, wps=23374.8, ups=2.23, wpb=10501.8, bsz=363.6, num_updates=5300, lr=1.25e-05, gnorm=1.764, train_wall=45, wall=2621
2020-12-09 17:57:56 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=20.794, loss=3.287, nll_loss=0.713, ppl=1.64, wps=23215.6, ups=2.22, wpb=10450.1, bsz=375.6, num_updates=5400, lr=1.25e-05, gnorm=1.758, train_wall=45, wall=2666
2020-12-09 17:58:41 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=20.15, loss=3.259, nll_loss=0.706, ppl=1.63, wps=23289.5, ups=2.22, wpb=10472.1, bsz=373.6, num_updates=5500, lr=1.25e-05, gnorm=1.718, train_wall=45, wall=2711
2020-12-09 17:59:26 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=20.139, loss=3.264, nll_loss=0.71, ppl=1.64, wps=23361.6, ups=2.22, wpb=10516.7, bsz=381.2, num_updates=5600, lr=1.25e-05, gnorm=1.692, train_wall=45, wall=2756
2020-12-09 17:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 17:59:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:59:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:59:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 17:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 17:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 17:59:51 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | loss 6.232 | nll_loss 4.487 | ppl 22.42 | bleu 21.92 | wps 4801.7 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 21.92
2020-12-09 17:59:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 17:59:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:59:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 17:59:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 10 @ 5610 updates, score 21.92) (writing took 4.644291486591101 seconds)
2020-12-09 17:59:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-12-09 17:59:56 | INFO | train | epoch 010 | symm_kl 20.577 | loss 3.279 | nll_loss 0.711 | ppl 1.64 | wps 21062.6 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 1.25e-05 | gnorm 1.734 | train_wall 250 | wall 2786
2020-12-09 17:59:56 | INFO | fairseq.trainer | begin training epoch 11
2020-12-09 17:59:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 17:59:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:00:38 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=21.311, loss=3.304, nll_loss=0.708, ppl=1.63, wps=14481.3, ups=1.4, wpb=10358.6, bsz=351.1, num_updates=5700, lr=1.25e-05, gnorm=1.785, train_wall=43, wall=2828
2020-12-09 18:01:23 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=19.739, loss=3.244, nll_loss=0.705, ppl=1.63, wps=23526.8, ups=2.23, wpb=10564, bsz=383.6, num_updates=5800, lr=1.25e-05, gnorm=1.674, train_wall=45, wall=2873
2020-12-09 18:02:08 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=21.299, loss=3.306, nll_loss=0.715, ppl=1.64, wps=23214.9, ups=2.23, wpb=10400.1, bsz=355.4, num_updates=5900, lr=1.25e-05, gnorm=1.779, train_wall=45, wall=2918
2020-12-09 18:02:53 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=20.979, loss=3.299, nll_loss=0.719, ppl=1.65, wps=23043, ups=2.22, wpb=10394.2, bsz=372.6, num_updates=6000, lr=1.25e-05, gnorm=1.785, train_wall=45, wall=2963
2020-12-09 18:03:38 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=19.829, loss=3.249, nll_loss=0.703, ppl=1.63, wps=23599.9, ups=2.22, wpb=10652.7, bsz=380.5, num_updates=6100, lr=1.25e-05, gnorm=1.682, train_wall=45, wall=3008
2020-12-09 18:04:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:04:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:04:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:04:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:04:30 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | loss 6.232 | nll_loss 4.488 | ppl 22.43 | bleu 21.81 | wps 4665.6 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 21.92
2020-12-09 18:04:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:04:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 11 @ 6171 updates, score 21.81) (writing took 2.1985623985528946 seconds)
2020-12-09 18:04:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-12-09 18:04:32 | INFO | train | epoch 011 | symm_kl 20.495 | loss 3.276 | nll_loss 0.71 | ppl 1.64 | wps 21242.1 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 1.25e-05 | gnorm 1.729 | train_wall 250 | wall 3062
2020-12-09 18:04:32 | INFO | fairseq.trainer | begin training epoch 12
2020-12-09 18:04:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:04:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:04:48 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=20.158, loss=3.263, nll_loss=0.707, ppl=1.63, wps=14928.6, ups=1.43, wpb=10473.3, bsz=364.9, num_updates=6200, lr=1.25e-05, gnorm=1.697, train_wall=44, wall=3078
2020-12-09 18:05:33 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=20.042, loss=3.261, nll_loss=0.709, ppl=1.63, wps=23275.8, ups=2.24, wpb=10400.1, bsz=369, num_updates=6300, lr=1.25e-05, gnorm=1.685, train_wall=45, wall=3123
2020-12-09 18:06:18 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=20.91, loss=3.296, nll_loss=0.716, ppl=1.64, wps=23067.5, ups=2.21, wpb=10444.8, bsz=372, num_updates=6400, lr=1.25e-05, gnorm=1.771, train_wall=45, wall=3168
2020-12-09 18:07:03 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=19.625, loss=3.241, nll_loss=0.702, ppl=1.63, wps=23543.8, ups=2.21, wpb=10631.9, bsz=382.4, num_updates=6500, lr=1.25e-05, gnorm=1.687, train_wall=45, wall=3213
2020-12-09 18:07:48 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=21.2, loss=3.307, nll_loss=0.718, ppl=1.64, wps=23309.5, ups=2.21, wpb=10531, bsz=361.8, num_updates=6600, lr=1.25e-05, gnorm=1.771, train_wall=45, wall=3258
2020-12-09 18:08:33 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=19.962, loss=3.254, nll_loss=0.705, ppl=1.63, wps=23430.5, ups=2.23, wpb=10493.3, bsz=369.8, num_updates=6700, lr=1.25e-05, gnorm=1.675, train_wall=45, wall=3303
2020-12-09 18:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:08:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:08:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:08:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:08:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:08:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:08:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:09:08 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | loss 6.229 | nll_loss 4.484 | ppl 22.38 | bleu 21.71 | wps 4640.1 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 21.92
2020-12-09 18:09:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:09:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:09:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:09:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:09:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:09:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 12 @ 6732 updates, score 21.71) (writing took 2.767734969034791 seconds)
2020-12-09 18:09:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-12-09 18:09:11 | INFO | train | epoch 012 | symm_kl 20.332 | loss 3.271 | nll_loss 0.709 | ppl 1.63 | wps 21124.5 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 1.25e-05 | gnorm 1.716 | train_wall 251 | wall 3341
2020-12-09 18:09:11 | INFO | fairseq.trainer | begin training epoch 13
2020-12-09 18:09:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:09:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:09:44 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=21.382, loss=3.3, nll_loss=0.704, ppl=1.63, wps=14735.7, ups=1.42, wpb=10372.9, bsz=360.1, num_updates=6800, lr=1.25e-05, gnorm=1.813, train_wall=44, wall=3373
2020-12-09 18:10:29 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=21.001, loss=3.297, nll_loss=0.708, ppl=1.63, wps=23499.7, ups=2.22, wpb=10571.4, bsz=349, num_updates=6900, lr=1.25e-05, gnorm=1.745, train_wall=45, wall=3418
2020-12-09 18:11:14 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=19.946, loss=3.258, nll_loss=0.709, ppl=1.64, wps=23355.7, ups=2.22, wpb=10544.3, bsz=369.9, num_updates=7000, lr=1.25e-05, gnorm=1.686, train_wall=45, wall=3464
2020-12-09 18:11:59 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=19.012, loss=3.222, nll_loss=0.703, ppl=1.63, wps=23179.1, ups=2.21, wpb=10490.1, bsz=389.7, num_updates=7100, lr=1.25e-05, gnorm=1.628, train_wall=45, wall=3509
2020-12-09 18:12:44 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=19.908, loss=3.258, nll_loss=0.709, ppl=1.63, wps=23393.1, ups=2.23, wpb=10510.3, bsz=369.8, num_updates=7200, lr=1.25e-05, gnorm=1.681, train_wall=45, wall=3554
2020-12-09 18:13:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:13:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:13:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:13:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:13:46 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | loss 6.229 | nll_loss 4.486 | ppl 22.4 | bleu 21.9 | wps 4702 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 21.92
2020-12-09 18:13:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:13:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 13 @ 7293 updates, score 21.9) (writing took 2.7328818682581186 seconds)
2020-12-09 18:13:49 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-12-09 18:13:49 | INFO | train | epoch 013 | symm_kl 20.233 | loss 3.267 | nll_loss 0.708 | ppl 1.63 | wps 21166.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 1.25e-05 | gnorm 1.713 | train_wall 251 | wall 3619
2020-12-09 18:13:49 | INFO | fairseq.trainer | begin training epoch 14
2020-12-09 18:13:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:13:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:13:55 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=20.261, loss=3.269, nll_loss=0.712, ppl=1.64, wps=14538.7, ups=1.41, wpb=10327.2, bsz=373.9, num_updates=7300, lr=1.25e-05, gnorm=1.734, train_wall=45, wall=3625
2020-12-09 18:14:39 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=20.776, loss=3.288, nll_loss=0.709, ppl=1.63, wps=23921.3, ups=2.26, wpb=10590, bsz=367, num_updates=7400, lr=1.25e-05, gnorm=1.731, train_wall=44, wall=3669
2020-12-09 18:15:25 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=19.813, loss=3.248, nll_loss=0.701, ppl=1.63, wps=23311.3, ups=2.2, wpb=10574.3, bsz=374, num_updates=7500, lr=1.25e-05, gnorm=1.693, train_wall=45, wall=3714
2020-12-09 18:16:09 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=20.686, loss=3.281, nll_loss=0.707, ppl=1.63, wps=23163.2, ups=2.23, wpb=10386.9, bsz=357.6, num_updates=7600, lr=1.25e-05, gnorm=1.776, train_wall=45, wall=3759
2020-12-09 18:16:54 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=20.345, loss=3.276, nll_loss=0.714, ppl=1.64, wps=23155.4, ups=2.24, wpb=10338.9, bsz=367.2, num_updates=7700, lr=1.25e-05, gnorm=1.732, train_wall=44, wall=3804
2020-12-09 18:17:39 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=19.798, loss=3.252, nll_loss=0.705, ppl=1.63, wps=23324.6, ups=2.2, wpb=10594.9, bsz=368.6, num_updates=7800, lr=1.25e-05, gnorm=1.668, train_wall=45, wall=3849
2020-12-09 18:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:18:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:18:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:18:23 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | loss 6.234 | nll_loss 4.492 | ppl 22.5 | bleu 21.71 | wps 4977.8 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 21.92
2020-12-09 18:18:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:18:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 14 @ 7854 updates, score 21.71) (writing took 2.743558978661895 seconds)
2020-12-09 18:18:26 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-12-09 18:18:26 | INFO | train | epoch 014 | symm_kl 20.091 | loss 3.262 | nll_loss 0.707 | ppl 1.63 | wps 21217.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 1.25e-05 | gnorm 1.71 | train_wall 251 | wall 3896
2020-12-09 18:18:26 | INFO | fairseq.trainer | begin training epoch 15
2020-12-09 18:18:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:18:49 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=19.239, loss=3.24, nll_loss=0.713, ppl=1.64, wps=14901.4, ups=1.44, wpb=10380.4, bsz=382.1, num_updates=7900, lr=1.25e-05, gnorm=1.667, train_wall=44, wall=3919
2020-12-09 18:19:34 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=20.262, loss=3.272, nll_loss=0.71, ppl=1.64, wps=23453.5, ups=2.24, wpb=10471, bsz=365.8, num_updates=8000, lr=1.25e-05, gnorm=1.716, train_wall=44, wall=3964
2020-12-09 18:20:19 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=19.603, loss=3.237, nll_loss=0.698, ppl=1.62, wps=23314.7, ups=2.22, wpb=10485.3, bsz=382.9, num_updates=8100, lr=1.25e-05, gnorm=1.717, train_wall=45, wall=4009
2020-12-09 18:21:04 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=20.116, loss=3.261, nll_loss=0.703, ppl=1.63, wps=23382.6, ups=2.22, wpb=10526.2, bsz=362.5, num_updates=8200, lr=1.25e-05, gnorm=1.692, train_wall=45, wall=4054
2020-12-09 18:21:49 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=19.094, loss=3.232, nll_loss=0.708, ppl=1.63, wps=23482.4, ups=2.23, wpb=10527.6, bsz=374.3, num_updates=8300, lr=1.25e-05, gnorm=1.634, train_wall=45, wall=4098
2020-12-09 18:22:33 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=21.054, loss=3.298, nll_loss=0.714, ppl=1.64, wps=23461.2, ups=2.24, wpb=10468.7, bsz=364.2, num_updates=8400, lr=1.25e-05, gnorm=1.796, train_wall=44, wall=4143
2020-12-09 18:22:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:22:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:22:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:22:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:22:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:22:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:22:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:22:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:23:01 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | loss 6.236 | nll_loss 4.492 | ppl 22.5 | bleu 21.88 | wps 4531.2 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 21.92
2020-12-09 18:23:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:23:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:23:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:23:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:23:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:23:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 15 @ 8415 updates, score 21.88) (writing took 2.9386207088828087 seconds)
2020-12-09 18:23:04 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-12-09 18:23:04 | INFO | train | epoch 015 | symm_kl 20.016 | loss 3.259 | nll_loss 0.706 | ppl 1.63 | wps 21148 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 1.25e-05 | gnorm 1.71 | train_wall 250 | wall 4174
2020-12-09 18:23:04 | INFO | fairseq.trainer | begin training epoch 16
2020-12-09 18:23:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:23:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:23:45 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=19.528, loss=3.239, nll_loss=0.702, ppl=1.63, wps=14561.7, ups=1.4, wpb=10408.1, bsz=377.6, num_updates=8500, lr=1.25e-05, gnorm=1.675, train_wall=44, wall=4215
2020-12-09 18:24:30 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=19.926, loss=3.257, nll_loss=0.706, ppl=1.63, wps=23073.2, ups=2.21, wpb=10433.6, bsz=372.2, num_updates=8600, lr=1.25e-05, gnorm=1.717, train_wall=45, wall=4260
2020-12-09 18:25:15 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=20.124, loss=3.264, nll_loss=0.707, ppl=1.63, wps=23465.2, ups=2.23, wpb=10533.1, bsz=379.8, num_updates=8700, lr=1.25e-05, gnorm=1.729, train_wall=45, wall=4305
2020-12-09 18:26:00 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=19.279, loss=3.226, nll_loss=0.697, ppl=1.62, wps=23323.8, ups=2.21, wpb=10551.2, bsz=375.2, num_updates=8800, lr=1.25e-05, gnorm=1.633, train_wall=45, wall=4350
2020-12-09 18:26:45 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=20.084, loss=3.263, nll_loss=0.704, ppl=1.63, wps=23278.4, ups=2.22, wpb=10494.4, bsz=360.2, num_updates=8900, lr=1.25e-05, gnorm=1.692, train_wall=45, wall=4395
2020-12-09 18:27:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:27:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:27:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:27:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:27:40 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | loss 6.234 | nll_loss 4.489 | ppl 22.46 | bleu 21.84 | wps 4555.3 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 21.92
2020-12-09 18:27:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:27:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 16 @ 8976 updates, score 21.84) (writing took 2.7686314042657614 seconds)
2020-12-09 18:27:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-12-09 18:27:43 | INFO | train | epoch 016 | symm_kl 19.913 | loss 3.256 | nll_loss 0.706 | ppl 1.63 | wps 21086.7 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 1.25e-05 | gnorm 1.699 | train_wall 251 | wall 4453
2020-12-09 18:27:43 | INFO | fairseq.trainer | begin training epoch 17
2020-12-09 18:27:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:27:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:27:56 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=19.739, loss=3.257, nll_loss=0.712, ppl=1.64, wps=14653.9, ups=1.4, wpb=10459.4, bsz=358.3, num_updates=9000, lr=1.25e-05, gnorm=1.687, train_wall=44, wall=4466
2020-12-09 18:28:41 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=19.847, loss=3.252, nll_loss=0.705, ppl=1.63, wps=23329.7, ups=2.22, wpb=10489.1, bsz=374.8, num_updates=9100, lr=1.25e-05, gnorm=1.702, train_wall=45, wall=4511
2020-12-09 18:29:27 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=20.223, loss=3.252, nll_loss=0.69, ppl=1.61, wps=23461.6, ups=2.21, wpb=10592.8, bsz=369.3, num_updates=9200, lr=1.25e-05, gnorm=1.746, train_wall=45, wall=4556
2020-12-09 18:30:11 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=20.114, loss=3.258, nll_loss=0.699, ppl=1.62, wps=23469.3, ups=2.23, wpb=10507.6, bsz=365.5, num_updates=9300, lr=1.25e-05, gnorm=1.699, train_wall=45, wall=4601
2020-12-09 18:30:56 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=18.976, loss=3.231, nll_loss=0.711, ppl=1.64, wps=23221.8, ups=2.23, wpb=10397.2, bsz=380.2, num_updates=9400, lr=1.25e-05, gnorm=1.647, train_wall=45, wall=4646
2020-12-09 18:31:41 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=20.481, loss=3.289, nll_loss=0.721, ppl=1.65, wps=23507.2, ups=2.24, wpb=10491.6, bsz=362.4, num_updates=9500, lr=1.25e-05, gnorm=1.724, train_wall=44, wall=4691
2020-12-09 18:31:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:32:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:32:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:32:18 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | loss 6.236 | nll_loss 4.492 | ppl 22.5 | bleu 21.74 | wps 4737.1 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 21.92
2020-12-09 18:32:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:32:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:32:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 17 @ 9537 updates, score 21.74) (writing took 2.722132407128811 seconds)
2020-12-09 18:32:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-12-09 18:32:20 | INFO | train | epoch 017 | symm_kl 19.844 | loss 3.254 | nll_loss 0.706 | ppl 1.63 | wps 21199.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 1.25e-05 | gnorm 1.699 | train_wall 250 | wall 4730
2020-12-09 18:32:20 | INFO | fairseq.trainer | begin training epoch 18
2020-12-09 18:32:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:32:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:32:51 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=20.052, loss=3.261, nll_loss=0.702, ppl=1.63, wps=14808.7, ups=1.42, wpb=10417.2, bsz=358.2, num_updates=9600, lr=1.25e-05, gnorm=1.724, train_wall=44, wall=4761
2020-12-09 18:33:36 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=20.122, loss=3.261, nll_loss=0.704, ppl=1.63, wps=23180.5, ups=2.21, wpb=10492.8, bsz=370, num_updates=9700, lr=1.25e-05, gnorm=1.717, train_wall=45, wall=4806
2020-12-09 18:34:22 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=20.073, loss=3.257, nll_loss=0.699, ppl=1.62, wps=23069.2, ups=2.21, wpb=10456, bsz=363.1, num_updates=9800, lr=1.25e-05, gnorm=1.728, train_wall=45, wall=4852
2020-12-09 18:35:07 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=19.214, loss=3.234, nll_loss=0.706, ppl=1.63, wps=23151.9, ups=2.23, wpb=10380.2, bsz=372.9, num_updates=9900, lr=1.25e-05, gnorm=1.668, train_wall=45, wall=4896
2020-12-09 18:35:52 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=19.027, loss=3.227, nll_loss=0.705, ppl=1.63, wps=23568.1, ups=2.22, wpb=10622, bsz=379.6, num_updates=10000, lr=1.25e-05, gnorm=1.612, train_wall=45, wall=4941
2020-12-09 18:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:36:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:36:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:36:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:36:58 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | loss 6.233 | nll_loss 4.49 | ppl 22.48 | bleu 21.83 | wps 4296.4 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 21.92
2020-12-09 18:36:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:37:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 18 @ 10098 updates, score 21.83) (writing took 2.7175529841333628 seconds)
2020-12-09 18:37:00 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-12-09 18:37:00 | INFO | train | epoch 018 | symm_kl 19.721 | loss 3.249 | nll_loss 0.704 | ppl 1.63 | wps 21008.8 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 1.25e-05 | gnorm 1.691 | train_wall 251 | wall 5010
2020-12-09 18:37:00 | INFO | fairseq.trainer | begin training epoch 19
2020-12-09 18:37:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:37:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:37:04 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=20.006, loss=3.266, nll_loss=0.712, ppl=1.64, wps=14336.8, ups=1.38, wpb=10417.9, bsz=365.4, num_updates=10100, lr=1.25e-05, gnorm=1.722, train_wall=45, wall=5014
2020-12-09 18:37:49 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=19.253, loss=3.232, nll_loss=0.7, ppl=1.62, wps=23542.4, ups=2.26, wpb=10422.9, bsz=373.4, num_updates=10200, lr=1.25e-05, gnorm=1.665, train_wall=44, wall=5058
2020-12-09 18:38:34 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=18.583, loss=3.209, nll_loss=0.701, ppl=1.63, wps=23410.5, ups=2.2, wpb=10632.5, bsz=380, num_updates=10300, lr=1.25e-05, gnorm=1.592, train_wall=45, wall=5104
2020-12-09 18:39:19 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=20.358, loss=3.27, nll_loss=0.702, ppl=1.63, wps=23420, ups=2.23, wpb=10486.3, bsz=361.5, num_updates=10400, lr=1.25e-05, gnorm=1.768, train_wall=45, wall=5149
2020-12-09 18:40:04 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=20.007, loss=3.265, nll_loss=0.709, ppl=1.64, wps=23250.4, ups=2.22, wpb=10482.8, bsz=369.7, num_updates=10500, lr=1.25e-05, gnorm=1.711, train_wall=45, wall=5194
2020-12-09 18:40:49 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=19.822, loss=3.247, nll_loss=0.698, ppl=1.62, wps=23177.4, ups=2.22, wpb=10451.7, bsz=365.4, num_updates=10600, lr=1.25e-05, gnorm=1.723, train_wall=45, wall=5239
2020-12-09 18:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:41:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:41:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:41:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:41:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:41:36 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | loss 6.235 | nll_loss 4.491 | ppl 22.49 | bleu 21.8 | wps 4643.3 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 21.92
2020-12-09 18:41:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:41:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:41:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:41:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 19 @ 10659 updates, score 21.8) (writing took 2.7996008154004812 seconds)
2020-12-09 18:41:39 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-12-09 18:41:39 | INFO | train | epoch 019 | symm_kl 19.622 | loss 3.246 | nll_loss 0.703 | ppl 1.63 | wps 21132.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 1.25e-05 | gnorm 1.698 | train_wall 251 | wall 5288
2020-12-09 18:41:39 | INFO | fairseq.trainer | begin training epoch 20
2020-12-09 18:41:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:41:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:42:00 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=19.656, loss=3.246, nll_loss=0.704, ppl=1.63, wps=14840.9, ups=1.42, wpb=10477.3, bsz=372.4, num_updates=10700, lr=1.25e-05, gnorm=1.718, train_wall=44, wall=5309
2020-12-09 18:42:44 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=19.734, loss=3.255, nll_loss=0.707, ppl=1.63, wps=23645.6, ups=2.24, wpb=10538.1, bsz=365.8, num_updates=10800, lr=1.25e-05, gnorm=1.68, train_wall=44, wall=5354
2020-12-09 18:43:29 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=18.988, loss=3.226, nll_loss=0.703, ppl=1.63, wps=23237.1, ups=2.21, wpb=10496, bsz=377, num_updates=10900, lr=1.25e-05, gnorm=1.631, train_wall=45, wall=5399
2020-12-09 18:44:14 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=20.437, loss=3.27, nll_loss=0.7, ppl=1.62, wps=23345.9, ups=2.22, wpb=10521.2, bsz=361.4, num_updates=11000, lr=1.25e-05, gnorm=1.755, train_wall=45, wall=5444
2020-12-09 18:44:59 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=19.216, loss=3.235, nll_loss=0.705, ppl=1.63, wps=23205.7, ups=2.22, wpb=10445.5, bsz=372.4, num_updates=11100, lr=1.25e-05, gnorm=1.683, train_wall=45, wall=5489
2020-12-09 18:45:44 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=19.999, loss=3.26, nll_loss=0.707, ppl=1.63, wps=23230.6, ups=2.23, wpb=10410.2, bsz=367.4, num_updates=11200, lr=1.25e-05, gnorm=1.745, train_wall=45, wall=5534
2020-12-09 18:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:46:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:46:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:46:14 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | loss 6.239 | nll_loss 4.496 | ppl 22.57 | bleu 21.74 | wps 4654.5 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 21.92
2020-12-09 18:46:14 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:46:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:46:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:46:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:46:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:46:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 20 @ 11220 updates, score 21.74) (writing took 2.7825285606086254 seconds)
2020-12-09 18:46:17 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-12-09 18:46:17 | INFO | train | epoch 020 | symm_kl 19.605 | loss 3.246 | nll_loss 0.703 | ppl 1.63 | wps 21155.1 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 1.25e-05 | gnorm 1.696 | train_wall 251 | wall 5566
2020-12-09 18:46:17 | INFO | fairseq.trainer | begin training epoch 21
2020-12-09 18:46:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:46:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:46:55 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=19.43, loss=3.243, nll_loss=0.708, ppl=1.63, wps=14568.7, ups=1.41, wpb=10315.2, bsz=376.1, num_updates=11300, lr=1.25e-05, gnorm=1.715, train_wall=44, wall=5605
2020-12-09 18:47:40 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=19.691, loss=3.249, nll_loss=0.704, ppl=1.63, wps=23404.6, ups=2.22, wpb=10526.7, bsz=359.6, num_updates=11400, lr=1.25e-05, gnorm=1.681, train_wall=45, wall=5650
2020-12-09 18:48:25 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=19.436, loss=3.234, nll_loss=0.697, ppl=1.62, wps=23328.6, ups=2.22, wpb=10514.2, bsz=377.1, num_updates=11500, lr=1.25e-05, gnorm=1.704, train_wall=45, wall=5695
2020-12-09 18:49:10 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=19.075, loss=3.219, nll_loss=0.693, ppl=1.62, wps=23455.3, ups=2.21, wpb=10592.6, bsz=375.9, num_updates=11600, lr=1.25e-05, gnorm=1.652, train_wall=45, wall=5740
2020-12-09 18:49:55 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=19.215, loss=3.238, nll_loss=0.707, ppl=1.63, wps=23266.2, ups=2.22, wpb=10501.6, bsz=373.3, num_updates=11700, lr=1.25e-05, gnorm=1.65, train_wall=45, wall=5785
2020-12-09 18:50:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:50:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:50:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:50:52 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | loss 6.245 | nll_loss 4.501 | ppl 22.65 | bleu 21.64 | wps 4709.1 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 21.92
2020-12-09 18:50:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:50:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:50:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 21 @ 11781 updates, score 21.64) (writing took 2.741713035851717 seconds)
2020-12-09 18:50:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-12-09 18:50:55 | INFO | train | epoch 021 | symm_kl 19.513 | loss 3.242 | nll_loss 0.703 | ppl 1.63 | wps 21158.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 1.25e-05 | gnorm 1.689 | train_wall 251 | wall 5844
2020-12-09 18:50:55 | INFO | fairseq.trainer | begin training epoch 22
2020-12-09 18:50:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:50:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:51:06 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=20.047, loss=3.267, nll_loss=0.707, ppl=1.63, wps=14850.4, ups=1.42, wpb=10469.7, bsz=353, num_updates=11800, lr=1.25e-05, gnorm=1.719, train_wall=44, wall=5856
2020-12-09 18:51:51 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=19.264, loss=3.23, nll_loss=0.698, ppl=1.62, wps=23590.3, ups=2.23, wpb=10589.6, bsz=376.8, num_updates=11900, lr=1.25e-05, gnorm=1.665, train_wall=45, wall=5901
2020-12-09 18:52:36 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=18.827, loss=3.213, nll_loss=0.697, ppl=1.62, wps=23195.7, ups=2.21, wpb=10493.2, bsz=374.8, num_updates=12000, lr=1.25e-05, gnorm=1.621, train_wall=45, wall=5946
2020-12-09 18:53:21 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=19.775, loss=3.258, nll_loss=0.711, ppl=1.64, wps=23197.1, ups=2.23, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1.25e-05, gnorm=1.698, train_wall=45, wall=5991
2020-12-09 18:54:06 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=19.668, loss=3.238, nll_loss=0.69, ppl=1.61, wps=23301.4, ups=2.21, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1.25e-05, gnorm=1.716, train_wall=45, wall=6036
2020-12-09 18:54:51 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=19.41, loss=3.246, nll_loss=0.71, ppl=1.64, wps=23390, ups=2.24, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1.25e-05, gnorm=1.696, train_wall=44, wall=6081
2020-12-09 18:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:55:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:55:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:55:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:55:31 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | loss 6.238 | nll_loss 4.495 | ppl 22.55 | bleu 21.96 | wps 4518.5 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 21.96
2020-12-09 18:55:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 18:55:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 22 @ 12342 updates, score 21.96) (writing took 4.502743257209659 seconds)
2020-12-09 18:55:35 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-12-09 18:55:35 | INFO | train | epoch 022 | symm_kl 19.438 | loss 3.24 | nll_loss 0.702 | ppl 1.63 | wps 20959.4 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.25e-05 | gnorm 1.682 | train_wall 251 | wall 6125
2020-12-09 18:55:35 | INFO | fairseq.trainer | begin training epoch 23
2020-12-09 18:55:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:56:04 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=20.821, loss=3.296, nll_loss=0.711, ppl=1.64, wps=14282.8, ups=1.37, wpb=10402.8, bsz=349, num_updates=12400, lr=1.25e-05, gnorm=1.79, train_wall=44, wall=6153
2020-12-09 18:56:49 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=18.372, loss=3.205, nll_loss=0.703, ppl=1.63, wps=23325.9, ups=2.22, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1.25e-05, gnorm=1.592, train_wall=45, wall=6199
2020-12-09 18:57:33 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=20.683, loss=3.286, nll_loss=0.706, ppl=1.63, wps=23285.2, ups=2.23, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1.25e-05, gnorm=1.789, train_wall=45, wall=6243
2020-12-09 18:58:19 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=19.168, loss=3.228, nll_loss=0.699, ppl=1.62, wps=23391.9, ups=2.21, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1.25e-05, gnorm=1.656, train_wall=45, wall=6289
2020-12-09 18:59:03 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=18.684, loss=3.215, nll_loss=0.702, ppl=1.63, wps=23237.1, ups=2.23, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1.25e-05, gnorm=1.635, train_wall=45, wall=6333
2020-12-09 18:59:48 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=18.924, loss=3.214, nll_loss=0.693, ppl=1.62, wps=23452.2, ups=2.23, wpb=10521.5, bsz=381, num_updates=12900, lr=1.25e-05, gnorm=1.654, train_wall=45, wall=6378
2020-12-09 18:59:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 18:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 18:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 18:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 18:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 18:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 18:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:00:10 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | loss 6.238 | nll_loss 4.497 | ppl 22.58 | bleu 21.83 | wps 4853.4 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 21.96
2020-12-09 19:00:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:00:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 23 @ 12903 updates, score 21.83) (writing took 2.756601257249713 seconds)
2020-12-09 19:00:12 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-12-09 19:00:12 | INFO | train | epoch 023 | symm_kl 19.384 | loss 3.237 | nll_loss 0.701 | ppl 1.63 | wps 21220.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.25e-05 | gnorm 1.682 | train_wall 250 | wall 6402
2020-12-09 19:00:12 | INFO | fairseq.trainer | begin training epoch 24
2020-12-09 19:00:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:00:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:00:59 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=19.127, loss=3.229, nll_loss=0.7, ppl=1.62, wps=14995.4, ups=1.42, wpb=10536, bsz=373.3, num_updates=13000, lr=1.25e-05, gnorm=1.646, train_wall=44, wall=6449
2020-12-09 19:01:44 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=19.792, loss=3.248, nll_loss=0.696, ppl=1.62, wps=22948.7, ups=2.2, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1.25e-05, gnorm=1.706, train_wall=45, wall=6494
2020-12-09 19:02:29 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=19.048, loss=3.225, nll_loss=0.698, ppl=1.62, wps=23341.1, ups=2.22, wpb=10536, bsz=369, num_updates=13200, lr=1.25e-05, gnorm=1.662, train_wall=45, wall=6539
2020-12-09 19:03:14 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=19.371, loss=3.241, nll_loss=0.707, ppl=1.63, wps=23185.5, ups=2.25, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1.25e-05, gnorm=1.672, train_wall=44, wall=6584
2020-12-09 19:03:59 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=19.384, loss=3.243, nll_loss=0.709, ppl=1.64, wps=23334.9, ups=2.23, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1.25e-05, gnorm=1.684, train_wall=45, wall=6628
2020-12-09 19:04:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:04:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:04:48 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | loss 6.237 | nll_loss 4.494 | ppl 22.54 | bleu 21.74 | wps 4542.1 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 21.96
2020-12-09 19:04:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:04:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 24 @ 13464 updates, score 21.74) (writing took 2.7371969912201166 seconds)
2020-12-09 19:04:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-12-09 19:04:51 | INFO | train | epoch 024 | symm_kl 19.291 | loss 3.234 | nll_loss 0.701 | ppl 1.63 | wps 21094.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.25e-05 | gnorm 1.67 | train_wall 251 | wall 6681
2020-12-09 19:04:51 | INFO | fairseq.trainer | begin training epoch 25
2020-12-09 19:04:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:04:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:05:10 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=19.304, loss=3.228, nll_loss=0.691, ppl=1.61, wps=14797.2, ups=1.4, wpb=10574, bsz=365.8, num_updates=13500, lr=1.25e-05, gnorm=1.668, train_wall=44, wall=6700
2020-12-09 19:05:55 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=19.438, loss=3.233, nll_loss=0.692, ppl=1.62, wps=23376.3, ups=2.21, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1.25e-05, gnorm=1.681, train_wall=45, wall=6745
2020-12-09 19:06:40 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=18.754, loss=3.219, nll_loss=0.703, ppl=1.63, wps=22992.7, ups=2.22, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1.25e-05, gnorm=1.649, train_wall=45, wall=6790
2020-12-09 19:07:25 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=19.613, loss=3.247, nll_loss=0.701, ppl=1.63, wps=23331.5, ups=2.22, wpb=10508.8, bsz=363, num_updates=13800, lr=1.25e-05, gnorm=1.711, train_wall=45, wall=6835
2020-12-09 19:08:10 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=18.496, loss=3.214, nll_loss=0.707, ppl=1.63, wps=23664.4, ups=2.23, wpb=10632, bsz=381.2, num_updates=13900, lr=1.25e-05, gnorm=1.603, train_wall=45, wall=6880
2020-12-09 19:08:55 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=18.938, loss=3.222, nll_loss=0.702, ppl=1.63, wps=23086, ups=2.22, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1.25e-05, gnorm=1.662, train_wall=45, wall=6925
2020-12-09 19:09:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:09:27 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | loss 6.249 | nll_loss 4.504 | ppl 22.69 | bleu 21.77 | wps 4692.5 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 21.96
2020-12-09 19:09:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:09:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:09:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:09:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 25 @ 14025 updates, score 21.77) (writing took 2.7623254992067814 seconds)
2020-12-09 19:09:30 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-12-09 19:09:30 | INFO | train | epoch 025 | symm_kl 19.215 | loss 3.232 | nll_loss 0.7 | ppl 1.62 | wps 21116.3 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.25e-05 | gnorm 1.673 | train_wall 251 | wall 6959
2020-12-09 19:09:30 | INFO | fairseq.trainer | begin training epoch 26
2020-12-09 19:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:09:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:10:06 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=18.265, loss=3.191, nll_loss=0.692, ppl=1.62, wps=14739.7, ups=1.42, wpb=10395, bsz=387.4, num_updates=14100, lr=1.25e-05, gnorm=1.618, train_wall=44, wall=6996
2020-12-09 19:10:51 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=20.203, loss=3.261, nll_loss=0.695, ppl=1.62, wps=23106.1, ups=2.22, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1.25e-05, gnorm=1.752, train_wall=45, wall=7041
2020-12-09 19:11:36 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=18.363, loss=3.202, nll_loss=0.699, ppl=1.62, wps=23663.7, ups=2.22, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1.25e-05, gnorm=1.608, train_wall=45, wall=7086
2020-12-09 19:12:21 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=19.715, loss=3.253, nll_loss=0.705, ppl=1.63, wps=23253.9, ups=2.21, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1.25e-05, gnorm=1.724, train_wall=45, wall=7131
2020-12-09 19:13:07 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=19.632, loss=3.248, nll_loss=0.701, ppl=1.63, wps=23270.9, ups=2.21, wpb=10526, bsz=359.1, num_updates=14500, lr=1.25e-05, gnorm=1.702, train_wall=45, wall=7176
2020-12-09 19:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:13:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:13:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:13:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:14:05 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | loss 6.252 | nll_loss 4.509 | ppl 22.77 | bleu 21.85 | wps 4746.1 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 21.96
2020-12-09 19:14:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:14:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 26 @ 14586 updates, score 21.85) (writing took 2.761586107313633 seconds)
2020-12-09 19:14:08 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-12-09 19:14:08 | INFO | train | epoch 026 | symm_kl 19.173 | loss 3.23 | nll_loss 0.7 | ppl 1.62 | wps 21134.6 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.25e-05 | gnorm 1.677 | train_wall 251 | wall 7238
2020-12-09 19:14:08 | INFO | fairseq.trainer | begin training epoch 27
2020-12-09 19:14:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:14:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:14:17 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=19.267, loss=3.241, nll_loss=0.708, ppl=1.63, wps=14562.6, ups=1.42, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1.25e-05, gnorm=1.689, train_wall=44, wall=7247
2020-12-09 19:15:02 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=19.46, loss=3.244, nll_loss=0.704, ppl=1.63, wps=23478.4, ups=2.23, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1.25e-05, gnorm=1.703, train_wall=45, wall=7292
2020-12-09 19:15:47 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=19.521, loss=3.242, nll_loss=0.699, ppl=1.62, wps=23415.2, ups=2.22, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1.25e-05, gnorm=1.699, train_wall=45, wall=7337
2020-12-09 19:16:32 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=18.519, loss=3.207, nll_loss=0.699, ppl=1.62, wps=23405.2, ups=2.23, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1.25e-05, gnorm=1.625, train_wall=45, wall=7382
2020-12-09 19:17:17 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=18.765, loss=3.213, nll_loss=0.696, ppl=1.62, wps=23400.5, ups=2.22, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1.25e-05, gnorm=1.645, train_wall=45, wall=7427
2020-12-09 19:18:02 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=19.557, loss=3.243, nll_loss=0.7, ppl=1.62, wps=23152.8, ups=2.22, wpb=10447, bsz=358.8, num_updates=15100, lr=1.25e-05, gnorm=1.711, train_wall=45, wall=7472
2020-12-09 19:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:18:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:18:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:18:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:18:44 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | loss 6.243 | nll_loss 4.501 | ppl 22.65 | bleu 21.82 | wps 4743.4 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 21.96
2020-12-09 19:18:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:18:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:18:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:18:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 27 @ 15147 updates, score 21.82) (writing took 2.72097135707736 seconds)
2020-12-09 19:18:46 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-12-09 19:18:46 | INFO | train | epoch 027 | symm_kl 19.13 | loss 3.23 | nll_loss 0.7 | ppl 1.62 | wps 21129.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.25e-05 | gnorm 1.681 | train_wall 251 | wall 7516
2020-12-09 19:18:46 | INFO | fairseq.trainer | begin training epoch 28
2020-12-09 19:18:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:18:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:19:12 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=18.357, loss=3.195, nll_loss=0.691, ppl=1.61, wps=14621.8, ups=1.42, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1.25e-05, gnorm=1.658, train_wall=44, wall=7542
2020-12-09 19:19:58 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=19.638, loss=3.246, nll_loss=0.699, ppl=1.62, wps=23176.9, ups=2.21, wpb=10482.5, bsz=354, num_updates=15300, lr=1.25e-05, gnorm=1.697, train_wall=45, wall=7588
2020-12-09 19:20:43 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=19.067, loss=3.224, nll_loss=0.695, ppl=1.62, wps=23445.9, ups=2.22, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1.25e-05, gnorm=1.652, train_wall=45, wall=7633
2020-12-09 19:21:28 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=19.609, loss=3.243, nll_loss=0.697, ppl=1.62, wps=23381.4, ups=2.22, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1.25e-05, gnorm=1.709, train_wall=45, wall=7678
2020-12-09 19:22:12 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=18.663, loss=3.223, nll_loss=0.711, ppl=1.64, wps=23322.5, ups=2.23, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1.25e-05, gnorm=1.624, train_wall=45, wall=7722
2020-12-09 19:22:58 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=18.885, loss=3.23, nll_loss=0.71, ppl=1.64, wps=23429.7, ups=2.22, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1.25e-05, gnorm=1.635, train_wall=45, wall=7767
2020-12-09 19:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:23:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:23:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:23:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:23:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:23:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:23:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:23:21 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | loss 6.24 | nll_loss 4.498 | ppl 22.59 | bleu 21.84 | wps 4961.4 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 21.96
2020-12-09 19:23:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:23:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:23:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:23:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 28 @ 15708 updates, score 21.84) (writing took 2.756704492494464 seconds)
2020-12-09 19:23:23 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-12-09 19:23:23 | INFO | train | epoch 028 | symm_kl 19.047 | loss 3.227 | nll_loss 0.7 | ppl 1.62 | wps 21227.1 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.25e-05 | gnorm 1.66 | train_wall 251 | wall 7793
2020-12-09 19:23:23 | INFO | fairseq.trainer | begin training epoch 29
2020-12-09 19:23:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:23:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:24:07 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=18.652, loss=3.212, nll_loss=0.697, ppl=1.62, wps=15019.1, ups=1.43, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1.25e-05, gnorm=1.652, train_wall=44, wall=7837
2020-12-09 19:24:53 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=19.295, loss=3.231, nll_loss=0.696, ppl=1.62, wps=23269.2, ups=2.21, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1.25e-05, gnorm=1.684, train_wall=45, wall=7883
2020-12-09 19:25:38 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=19, loss=3.231, nll_loss=0.703, ppl=1.63, wps=23388.3, ups=2.22, wpb=10552.2, bsz=366, num_updates=16000, lr=1.25e-05, gnorm=1.641, train_wall=45, wall=7928
2020-12-09 19:26:23 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=18.74, loss=3.211, nll_loss=0.696, ppl=1.62, wps=23057.9, ups=2.22, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1.25e-05, gnorm=1.659, train_wall=45, wall=7973
2020-12-09 19:27:08 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=19.173, loss=3.232, nll_loss=0.7, ppl=1.63, wps=22894.5, ups=2.21, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1.25e-05, gnorm=1.675, train_wall=45, wall=8018
2020-12-09 19:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:27:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:27:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:27:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:28:01 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | loss 6.244 | nll_loss 4.5 | ppl 22.63 | bleu 21.67 | wps 4270.6 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 21.96
2020-12-09 19:28:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:28:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:28:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:28:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:28:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:28:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 29 @ 16269 updates, score 21.67) (writing took 2.7404040042310953 seconds)
2020-12-09 19:28:04 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-12-09 19:28:04 | INFO | train | epoch 029 | symm_kl 18.995 | loss 3.225 | nll_loss 0.699 | ppl 1.62 | wps 20939.6 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.25e-05 | gnorm 1.66 | train_wall 252 | wall 8074
2020-12-09 19:28:04 | INFO | fairseq.trainer | begin training epoch 30
2020-12-09 19:28:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:28:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:28:21 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=18.717, loss=3.216, nll_loss=0.701, ppl=1.63, wps=14426.3, ups=1.38, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1.25e-05, gnorm=1.648, train_wall=44, wall=8091
2020-12-09 19:29:06 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=19.052, loss=3.224, nll_loss=0.696, ppl=1.62, wps=23552.2, ups=2.22, wpb=10585.6, bsz=366, num_updates=16400, lr=1.25e-05, gnorm=1.652, train_wall=45, wall=8135
2020-12-09 19:29:51 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=18.965, loss=3.222, nll_loss=0.698, ppl=1.62, wps=23064.6, ups=2.21, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1.25e-05, gnorm=1.664, train_wall=45, wall=8181
2020-12-09 19:30:36 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=19.435, loss=3.242, nll_loss=0.701, ppl=1.63, wps=23203.3, ups=2.23, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1.25e-05, gnorm=1.704, train_wall=45, wall=8226
2020-12-09 19:31:21 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=18.771, loss=3.215, nll_loss=0.697, ppl=1.62, wps=23523.6, ups=2.22, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1.25e-05, gnorm=1.645, train_wall=45, wall=8271
2020-12-09 19:32:05 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=18.337, loss=3.208, nll_loss=0.703, ppl=1.63, wps=23476.4, ups=2.23, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1.25e-05, gnorm=1.584, train_wall=45, wall=8315
2020-12-09 19:32:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:32:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:32:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:32:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:32:39 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | loss 6.243 | nll_loss 4.5 | ppl 22.62 | bleu 21.86 | wps 4878.9 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 21.96
2020-12-09 19:32:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:32:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:32:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 30 @ 16830 updates, score 21.86) (writing took 2.681761981919408 seconds)
2020-12-09 19:32:42 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-12-09 19:32:42 | INFO | train | epoch 030 | symm_kl 18.928 | loss 3.222 | nll_loss 0.699 | ppl 1.62 | wps 21194.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.25e-05 | gnorm 1.657 | train_wall 251 | wall 8352
2020-12-09 19:32:42 | INFO | fairseq.trainer | begin training epoch 31
2020-12-09 19:32:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:32:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:33:15 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=19.745, loss=3.251, nll_loss=0.699, ppl=1.62, wps=14880.2, ups=1.43, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1.25e-05, gnorm=1.745, train_wall=44, wall=8385
2020-12-09 19:34:00 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=18.259, loss=3.197, nll_loss=0.696, ppl=1.62, wps=23530.3, ups=2.22, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1.25e-05, gnorm=1.603, train_wall=45, wall=8430
2020-12-09 19:34:46 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=18.889, loss=3.218, nll_loss=0.696, ppl=1.62, wps=23215.1, ups=2.21, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1.25e-05, gnorm=1.662, train_wall=45, wall=8475
2020-12-09 19:35:30 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=20.014, loss=3.264, nll_loss=0.702, ppl=1.63, wps=23302.7, ups=2.25, wpb=10374.8, bsz=340, num_updates=17200, lr=1.25e-05, gnorm=1.726, train_wall=44, wall=8520
2020-12-09 19:36:15 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=18.521, loss=3.205, nll_loss=0.696, ppl=1.62, wps=23543.1, ups=2.24, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1.25e-05, gnorm=1.637, train_wall=45, wall=8565
2020-12-09 19:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-09 19:36:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:36:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:36:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:36:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:36:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:36:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-09 19:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-09 19:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-09 19:37:16 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | loss 6.242 | nll_loss 4.501 | ppl 22.64 | bleu 21.81 | wps 4738.4 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 21.96
2020-12-09 19:37:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-09 19:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:37:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 31 @ 17391 updates, score 21.81) (writing took 2.729080645367503 seconds)
2020-12-09 19:37:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-12-09 19:37:19 | INFO | train | epoch 031 | symm_kl 18.902 | loss 3.221 | nll_loss 0.698 | ppl 1.62 | wps 21208.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.25e-05 | gnorm 1.659 | train_wall 250 | wall 8629
2020-12-09 19:37:19 | INFO | fairseq.trainer | begin training epoch 32
2020-12-09 19:37:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-09 19:37:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-09 19:37:26 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=18.563, loss=3.214, nll_loss=0.702, ppl=1.63, wps=14576.3, ups=1.41, wpb=10367.8, bsz=370, num_updates=17400, lr=1.25e-05, gnorm=1.647, train_wall=45, wall=8636
2020-12-09 19:38:10 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=18.897, loss=3.224, nll_loss=0.698, ppl=1.62, wps=23712, ups=2.27, wpb=10453, bsz=358.9, num_updates=17500, lr=1.25e-05, gnorm=1.637, train_wall=44, wall=8680
2020-12-09 19:38:55 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=17.856, loss=3.178, nll_loss=0.688, ppl=1.61, wps=23866.5, ups=2.23, wpb=10691.6, bsz=379.4, num_updates=17600, lr=1.25e-05, gnorm=1.544, train_wall=45, wall=8725
2020-12-09 19:39:40 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=19.083, loss=3.225, nll_loss=0.695, ppl=1.62, wps=23375.3, ups=2.22, wpb=10541.6, bsz=365.9, num_updates=17700, lr=1.25e-05, gnorm=1.679, train_wall=45, wall=8770
2020-12-09 19:40:25 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=20.324, loss=3.271, nll_loss=0.701, ppl=1.63, wps=23165, ups=2.23, wpb=10411, bsz=347.3, num_updates=17800, lr=1.25e-05, gnorm=1.79, train_wall=45, wall=8815
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 222 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
