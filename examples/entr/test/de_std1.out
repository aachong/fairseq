nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/de_std
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --de-std'
2021-01-08 20:49:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:44 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19219
2021-01-08 20:49:44 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19219
2021-01-08 20:49:44 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:19219
2021-01-08 20:49:44 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-08 20:49:45 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-08 20:49:45 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-08 20:49:48 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=True, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19219', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/de_std', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-08 20:49:48 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-08 20:49:48 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-08 20:49:48 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-08 20:49:48 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-08 20:49:48 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-08 20:49:49 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-08 20:49:49 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-08 20:49:49 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-08 20:49:49 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-08 20:49:49 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-08 20:49:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-08 20:49:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-08 20:49:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-08 20:49:49 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 20:49:49 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 20:49:49 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 20:49:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-08 20:49:49 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-08 20:49:49 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-08 20:49:49 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-08 20:49:50 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-08 20:49:50 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-08 20:49:50 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-08 20:49:50 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-08 20:49:50 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-08 20:49:50 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-08 20:49:50 | INFO | fairseq.trainer | begin training epoch 1
2021-01-08 20:49:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:49:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:49:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:50:46 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=1.06, self_kl=0, self_cv=0, loss=4.188, nll_loss=0.855, ppl=1.81, wps=20238.5, ups=1.91, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.43e-06, gnorm=1.264, train_wall=53, wall=57
2021-01-08 20:51:41 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=0.985, self_kl=0, self_cv=0, loss=4.112, nll_loss=0.948, ppl=1.93, wps=19527.6, ups=1.85, wpb=10583.4, bsz=369.8, num_updates=200, lr=2.76e-06, gnorm=0.931, train_wall=54, wall=111
2021-01-08 20:52:35 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=0.911, self_kl=0, self_cv=0, loss=4.044, nll_loss=1.034, ppl=2.05, wps=19075.8, ups=1.85, wpb=10335, bsz=373, num_updates=300, lr=4.09e-06, gnorm=0.719, train_wall=54, wall=166
2021-01-08 20:53:29 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=0.875, self_kl=0, self_cv=0, loss=3.994, nll_loss=1.042, ppl=2.06, wps=19535.8, ups=1.85, wpb=10571.8, bsz=388.4, num_updates=400, lr=5.42e-06, gnorm=0.647, train_wall=54, wall=220
2021-01-08 20:54:24 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=0.853, self_kl=0, self_cv=0, loss=3.973, nll_loss=1.055, ppl=2.08, wps=19029.8, ups=1.83, wpb=10411.2, bsz=371.8, num_updates=500, lr=6.75e-06, gnorm=0.634, train_wall=55, wall=274
2021-01-08 20:54:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 20:54:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:54:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:54:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:54:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:54:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:54:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:55:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:55:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:55:18 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.44 | nll_loss 3.98 | ppl 15.78 | bleu 22.11 | wps 4559 | wpb 7508.5 | bsz 272.7 | num_updates 561
2021-01-08 20:55:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 20:55:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:55:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:55:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.11) (writing took 2.155001763254404 seconds)
2021-01-08 20:55:20 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-08 20:55:20 | INFO | train | epoch 001 | symm_kl 0.928 | self_kl 0 | self_cv 0 | loss 4.054 | nll_loss 0.995 | ppl 1.99 | wps 18043.3 | ups 1.72 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 7.5613e-06 | gnorm 0.818 | train_wall 302 | wall 331
2021-01-08 20:55:20 | INFO | fairseq.trainer | begin training epoch 2
2021-01-08 20:55:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:55:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:55:43 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=0.842, self_kl=0, self_cv=0, loss=3.959, nll_loss=1.057, ppl=2.08, wps=12977.6, ups=1.25, wpb=10345.6, bsz=358.4, num_updates=600, lr=8.08e-06, gnorm=0.634, train_wall=53, wall=354
2021-01-08 20:56:37 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=0.827, self_kl=0, self_cv=0, loss=3.943, nll_loss=1.064, ppl=2.09, wps=19576.6, ups=1.86, wpb=10532.9, bsz=366.4, num_updates=700, lr=9.41e-06, gnorm=0.62, train_wall=54, wall=408
2021-01-08 20:57:32 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=0.81, self_kl=0, self_cv=0, loss=3.911, nll_loss=1.056, ppl=2.08, wps=19299.6, ups=1.84, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.074e-05, gnorm=0.605, train_wall=54, wall=462
2021-01-08 20:58:26 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=0.797, self_kl=0, self_cv=0, loss=3.896, nll_loss=1.06, ppl=2.08, wps=19426.1, ups=1.84, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.207e-05, gnorm=0.603, train_wall=54, wall=517
2021-01-08 20:59:20 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=0.792, self_kl=0, self_cv=0, loss=3.9, nll_loss=1.071, ppl=2.1, wps=19342.6, ups=1.85, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.34e-05, gnorm=0.603, train_wall=54, wall=571
2021-01-08 21:00:14 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=0.78, self_kl=0, self_cv=0, loss=3.885, nll_loss=1.075, ppl=2.11, wps=19401.6, ups=1.86, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.473e-05, gnorm=0.605, train_wall=54, wall=625
2021-01-08 21:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:00:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:00:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:00:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:00:47 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.376 | nll_loss 3.918 | ppl 15.11 | bleu 22.04 | wps 4594.9 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.11
2021-01-08 21:00:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:00:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 2 @ 1122 updates, score 22.04) (writing took 2.7929097898304462 seconds)
2021-01-08 21:00:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-08 21:00:50 | INFO | train | epoch 002 | symm_kl 0.802 | self_kl 0 | self_cv 0 | loss 3.907 | nll_loss 1.064 | ppl 2.09 | wps 17836 | ups 1.7 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.50226e-05 | gnorm 0.609 | train_wall 302 | wall 660
2021-01-08 21:00:50 | INFO | fairseq.trainer | begin training epoch 3
2021-01-08 21:00:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:00:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:01:34 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=0.769, self_kl=0, self_cv=0, loss=3.857, nll_loss=1.061, ppl=2.09, wps=13051.5, ups=1.25, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.606e-05, gnorm=0.606, train_wall=53, wall=705
2021-01-08 21:02:28 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=0.762, self_kl=0, self_cv=0, loss=3.855, nll_loss=1.069, ppl=2.1, wps=19211.9, ups=1.84, wpb=10420.6, bsz=376, num_updates=1300, lr=1.739e-05, gnorm=0.599, train_wall=54, wall=759
2021-01-08 21:03:22 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=0.752, self_kl=0, self_cv=0, loss=3.836, nll_loss=1.064, ppl=2.09, wps=19288.8, ups=1.84, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.872e-05, gnorm=0.593, train_wall=54, wall=813
2021-01-08 21:04:17 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=0.748, self_kl=0, self_cv=0, loss=3.847, nll_loss=1.083, ppl=2.12, wps=19018.9, ups=1.82, wpb=10472.3, bsz=374.7, num_updates=1500, lr=2.005e-05, gnorm=0.601, train_wall=55, wall=868
2021-01-08 21:05:12 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=0.738, self_kl=0, self_cv=0, loss=3.822, nll_loss=1.073, ppl=2.1, wps=19468.1, ups=1.83, wpb=10650.7, bsz=373.4, num_updates=1600, lr=2.138e-05, gnorm=0.585, train_wall=55, wall=923
2021-01-08 21:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:05:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:05:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:05:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:06:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:06:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:06:18 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.352 | nll_loss 3.894 | ppl 14.86 | bleu 22.12 | wps 4661.4 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.12
2021-01-08 21:06:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:06:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:06:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:06:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.12) (writing took 4.577386260032654 seconds)
2021-01-08 21:06:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-08 21:06:23 | INFO | train | epoch 003 | symm_kl 0.75 | self_kl 0 | self_cv 0 | loss 3.838 | nll_loss 1.07 | ppl 2.1 | wps 17656.9 | ups 1.68 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 2.24839e-05 | gnorm 0.597 | train_wall 303 | wall 993
2021-01-08 21:06:23 | INFO | fairseq.trainer | begin training epoch 4
2021-01-08 21:06:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:06:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:06:35 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=0.733, self_kl=0, self_cv=0, loss=3.81, nll_loss=1.067, ppl=2.09, wps=12656.1, ups=1.21, wpb=10447.8, bsz=352, num_updates=1700, lr=2.271e-05, gnorm=0.602, train_wall=54, wall=1005
2021-01-08 21:07:28 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=0.728, self_kl=0, self_cv=0, loss=3.807, nll_loss=1.071, ppl=2.1, wps=19592.7, ups=1.87, wpb=10469.1, bsz=365.6, num_updates=1800, lr=2.404e-05, gnorm=0.598, train_wall=53, wall=1059
2021-01-08 21:08:22 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=0.721, self_kl=0, self_cv=0, loss=3.804, nll_loss=1.078, ppl=2.11, wps=19068.4, ups=1.86, wpb=10271.1, bsz=367.4, num_updates=1900, lr=2.537e-05, gnorm=0.608, train_wall=54, wall=1113
2021-01-08 21:09:16 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=0.714, self_kl=0, self_cv=0, loss=3.785, nll_loss=1.068, ppl=2.1, wps=19642.9, ups=1.86, wpb=10571.4, bsz=356.9, num_updates=2000, lr=2.67e-05, gnorm=0.59, train_wall=54, wall=1167
2021-01-08 21:10:10 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=0.707, self_kl=0, self_cv=0, loss=3.781, nll_loss=1.076, ppl=2.11, wps=19590.4, ups=1.86, wpb=10532.7, bsz=370.6, num_updates=2100, lr=2.803e-05, gnorm=0.59, train_wall=54, wall=1220
2021-01-08 21:11:04 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=0.701, self_kl=0, self_cv=0, loss=3.774, nll_loss=1.078, ppl=2.11, wps=19608.8, ups=1.85, wpb=10614.4, bsz=387.6, num_updates=2200, lr=2.936e-05, gnorm=0.587, train_wall=54, wall=1274
2021-01-08 21:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:11:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:11:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:11:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:11:48 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.346 | nll_loss 3.889 | ppl 14.82 | bleu 22.31 | wps 4571.2 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.31
2021-01-08 21:11:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:11:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:11:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:11:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_best.pt (epoch 4 @ 2244 updates, score 22.31) (writing took 4.539460152387619 seconds)
2021-01-08 21:11:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-08 21:11:53 | INFO | train | epoch 004 | symm_kl 0.713 | self_kl 0 | self_cv 0 | loss 3.787 | nll_loss 1.073 | ppl 2.1 | wps 17810.5 | ups 1.7 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 2.99452e-05 | gnorm 0.594 | train_wall 300 | wall 1324
2021-01-08 21:11:53 | INFO | fairseq.trainer | begin training epoch 5
2021-01-08 21:11:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:11:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:12:25 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=0.692, self_kl=0, self_cv=0, loss=3.75, nll_loss=1.065, ppl=2.09, wps=12761.7, ups=1.22, wpb=10433.5, bsz=373.3, num_updates=2300, lr=3.069e-05, gnorm=0.588, train_wall=53, wall=1356
2021-01-08 21:13:20 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=0.691, self_kl=0, self_cv=0, loss=3.757, nll_loss=1.076, ppl=2.11, wps=19302.9, ups=1.85, wpb=10447.6, bsz=372.4, num_updates=2400, lr=3.202e-05, gnorm=0.596, train_wall=54, wall=1410
2021-01-08 21:14:14 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=0.687, self_kl=0, self_cv=0, loss=3.754, nll_loss=1.077, ppl=2.11, wps=19378.9, ups=1.84, wpb=10524.3, bsz=363.8, num_updates=2500, lr=3.335e-05, gnorm=0.593, train_wall=54, wall=1465
2021-01-08 21:15:08 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=0.684, self_kl=0, self_cv=0, loss=3.762, nll_loss=1.09, ppl=2.13, wps=19162.7, ups=1.84, wpb=10415.7, bsz=357.5, num_updates=2600, lr=3.468e-05, gnorm=0.599, train_wall=54, wall=1519
2021-01-08 21:16:03 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=0.674, self_kl=0, self_cv=0, loss=3.733, nll_loss=1.075, ppl=2.11, wps=19382.9, ups=1.83, wpb=10565, bsz=383, num_updates=2700, lr=3.601e-05, gnorm=0.59, train_wall=54, wall=1574
2021-01-08 21:16:57 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=0.671, self_kl=0, self_cv=0, loss=3.732, nll_loss=1.08, ppl=2.11, wps=19263.1, ups=1.84, wpb=10479.6, bsz=374.7, num_updates=2800, lr=3.734e-05, gnorm=0.592, train_wall=54, wall=1628
2021-01-08 21:17:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:17:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:17:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:17:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:17:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:17:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:17:21 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.339 | nll_loss 3.882 | ppl 14.75 | bleu 22.28 | wps 4621 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.31
2021-01-08 21:17:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:17:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:17:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:17:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 5 @ 2805 updates, score 22.28) (writing took 2.785413870587945 seconds)
2021-01-08 21:17:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-08 21:17:23 | INFO | train | epoch 005 | symm_kl 0.683 | self_kl 0 | self_cv 0 | loss 3.748 | nll_loss 1.078 | ppl 2.11 | wps 17799 | ups 1.7 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 3.74065e-05 | gnorm 0.593 | train_wall 303 | wall 1654
2021-01-08 21:17:23 | INFO | fairseq.trainer | begin training epoch 6
2021-01-08 21:17:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:17:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:18:17 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=0.665, self_kl=0, self_cv=0, loss=3.714, nll_loss=1.068, ppl=2.1, wps=12863.5, ups=1.25, wpb=10318.1, bsz=377.4, num_updates=2900, lr=3.867e-05, gnorm=0.598, train_wall=53, wall=1708
2021-01-08 21:19:12 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=0.664, self_kl=0, self_cv=0, loss=3.718, nll_loss=1.075, ppl=2.11, wps=19535.1, ups=1.83, wpb=10679.3, bsz=372.1, num_updates=3000, lr=4e-05, gnorm=0.583, train_wall=54, wall=1763
2021-01-08 21:20:06 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=0.659, self_kl=0, self_cv=0, loss=3.712, nll_loss=1.075, ppl=2.11, wps=19260.2, ups=1.84, wpb=10477.8, bsz=365.4, num_updates=3100, lr=3.93496e-05, gnorm=0.596, train_wall=54, wall=1817
2021-01-08 21:21:01 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=0.66, self_kl=0, self_cv=0, loss=3.731, nll_loss=1.096, ppl=2.14, wps=19425.5, ups=1.85, wpb=10517.4, bsz=358, num_updates=3200, lr=3.87298e-05, gnorm=0.601, train_wall=54, wall=1871
2021-01-08 21:21:55 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=0.648, self_kl=0, self_cv=0, loss=3.69, nll_loss=1.071, ppl=2.1, wps=19434.3, ups=1.84, wpb=10534.9, bsz=372, num_updates=3300, lr=3.81385e-05, gnorm=0.589, train_wall=54, wall=1926
2021-01-08 21:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:22:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:22:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:22:51 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.34 | nll_loss 3.884 | ppl 14.77 | bleu 22.14 | wps 4675.3 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.31
2021-01-08 21:22:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:22:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:22:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:22:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 6 @ 3366 updates, score 22.14) (writing took 2.7535803578794003 seconds)
2021-01-08 21:22:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-08 21:22:54 | INFO | train | epoch 006 | symm_kl 0.658 | self_kl 0 | self_cv 0 | loss 3.714 | nll_loss 1.08 | ppl 2.11 | wps 17775.3 | ups 1.7 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 3.77627e-05 | gnorm 0.595 | train_wall 303 | wall 1985
2021-01-08 21:22:54 | INFO | fairseq.trainer | begin training epoch 7
2021-01-08 21:22:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:22:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:23:15 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=0.646, self_kl=0, self_cv=0, loss=3.712, nll_loss=1.096, ppl=2.14, wps=12753.3, ups=1.25, wpb=10237, bsz=369, num_updates=3400, lr=3.75735e-05, gnorm=0.606, train_wall=54, wall=2006
2021-01-08 21:24:10 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=0.645, self_kl=0, self_cv=0, loss=3.686, nll_loss=1.071, ppl=2.1, wps=19291, ups=1.84, wpb=10508.4, bsz=371.6, num_updates=3500, lr=3.70328e-05, gnorm=0.59, train_wall=54, wall=2060
2021-01-08 21:25:04 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=0.642, self_kl=0, self_cv=0, loss=3.689, nll_loss=1.079, ppl=2.11, wps=19126.4, ups=1.84, wpb=10404.4, bsz=363.4, num_updates=3600, lr=3.65148e-05, gnorm=0.598, train_wall=54, wall=2115
2021-01-08 21:25:58 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=0.641, self_kl=0, self_cv=0, loss=3.692, nll_loss=1.084, ppl=2.12, wps=19246, ups=1.84, wpb=10456.6, bsz=375.4, num_updates=3700, lr=3.6018e-05, gnorm=0.596, train_wall=54, wall=2169
2021-01-08 21:26:53 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=0.637, self_kl=0, self_cv=0, loss=3.682, nll_loss=1.078, ppl=2.11, wps=19170.2, ups=1.83, wpb=10467.8, bsz=366.5, num_updates=3800, lr=3.55409e-05, gnorm=0.602, train_wall=54, wall=2224
2021-01-08 21:27:48 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=0.634, self_kl=0, self_cv=0, loss=3.681, nll_loss=1.084, ppl=2.12, wps=19495.2, ups=1.82, wpb=10688.9, bsz=373.3, num_updates=3900, lr=3.50823e-05, gnorm=0.585, train_wall=55, wall=2278
2021-01-08 21:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:28:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:28:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:28:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:28:25 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.336 | nll_loss 3.886 | ppl 14.78 | bleu 22.1 | wps 4120.8 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.31
2021-01-08 21:28:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:28:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:28:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 7 @ 3927 updates, score 22.1) (writing took 2.794677807018161 seconds)
2021-01-08 21:28:28 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-08 21:28:28 | INFO | train | epoch 007 | symm_kl 0.639 | self_kl 0 | self_cv 0 | loss 3.685 | nll_loss 1.079 | ppl 2.11 | wps 17609.2 | ups 1.68 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 3.49615e-05 | gnorm 0.594 | train_wall 304 | wall 2319
2021-01-08 21:28:28 | INFO | fairseq.trainer | begin training epoch 8
2021-01-08 21:28:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:28:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:29:10 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=0.629, self_kl=0, self_cv=0, loss=3.661, nll_loss=1.069, ppl=2.1, wps=12839.1, ups=1.21, wpb=10590, bsz=375, num_updates=4000, lr=3.4641e-05, gnorm=0.587, train_wall=53, wall=2361
2021-01-08 21:30:05 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=0.632, self_kl=0, self_cv=0, loss=3.669, nll_loss=1.072, ppl=2.1, wps=19178.8, ups=1.83, wpb=10504.7, bsz=358.9, num_updates=4100, lr=3.4216e-05, gnorm=0.593, train_wall=55, wall=2416
2021-01-08 21:30:59 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=0.625, self_kl=0, self_cv=0, loss=3.66, nll_loss=1.074, ppl=2.11, wps=19168.7, ups=1.85, wpb=10367.5, bsz=366.4, num_updates=4200, lr=3.38062e-05, gnorm=0.593, train_wall=54, wall=2470
2021-01-08 21:31:53 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=0.622, self_kl=0, self_cv=0, loss=3.653, nll_loss=1.071, ppl=2.1, wps=19165.1, ups=1.84, wpb=10416.1, bsz=389.5, num_updates=4300, lr=3.34108e-05, gnorm=0.59, train_wall=54, wall=2524
2021-01-08 21:32:48 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=0.619, self_kl=0, self_cv=0, loss=3.653, nll_loss=1.076, ppl=2.11, wps=19418.9, ups=1.82, wpb=10648.7, bsz=379.6, num_updates=4400, lr=3.30289e-05, gnorm=0.585, train_wall=55, wall=2579
2021-01-08 21:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:33:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:33:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:33:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:33:57 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.342 | nll_loss 3.891 | ppl 14.84 | bleu 22.17 | wps 4564.7 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.31
2021-01-08 21:33:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:33:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:33:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:34:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:34:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:34:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 8 @ 4488 updates, score 22.17) (writing took 2.764846760779619 seconds)
2021-01-08 21:34:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-08 21:34:00 | INFO | train | epoch 008 | symm_kl 0.625 | self_kl 0 | self_cv 0 | loss 3.662 | nll_loss 1.076 | ppl 2.11 | wps 17718.3 | ups 1.69 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 3.27035e-05 | gnorm 0.592 | train_wall 304 | wall 2651
2021-01-08 21:34:00 | INFO | fairseq.trainer | begin training epoch 9
2021-01-08 21:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:34:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:34:10 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=0.623, self_kl=0, self_cv=0, loss=3.673, nll_loss=1.092, ppl=2.13, wps=12700, ups=1.23, wpb=10320.4, bsz=352.8, num_updates=4500, lr=3.26599e-05, gnorm=0.605, train_wall=54, wall=2660
2021-01-08 21:35:03 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=0.617, self_kl=0, self_cv=0, loss=3.635, nll_loss=1.058, ppl=2.08, wps=19718.1, ups=1.87, wpb=10532.6, bsz=374.2, num_updates=4600, lr=3.23029e-05, gnorm=0.582, train_wall=53, wall=2714
2021-01-08 21:35:58 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=0.621, self_kl=0, self_cv=0, loss=3.664, nll_loss=1.084, ppl=2.12, wps=19203.7, ups=1.82, wpb=10528, bsz=345, num_updates=4700, lr=3.19574e-05, gnorm=0.595, train_wall=55, wall=2769
2021-01-08 21:36:52 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=0.611, self_kl=0, self_cv=0, loss=3.636, nll_loss=1.071, ppl=2.1, wps=19157.1, ups=1.83, wpb=10473.1, bsz=377.1, num_updates=4800, lr=3.16228e-05, gnorm=0.588, train_wall=54, wall=2823
2021-01-08 21:37:47 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=0.612, self_kl=0, self_cv=0, loss=3.64, nll_loss=1.072, ppl=2.1, wps=19139.2, ups=1.83, wpb=10483.1, bsz=368.3, num_updates=4900, lr=3.12984e-05, gnorm=0.591, train_wall=55, wall=2878
2021-01-08 21:38:42 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=0.608, self_kl=0, self_cv=0, loss=3.634, nll_loss=1.074, ppl=2.11, wps=19210.3, ups=1.83, wpb=10514.7, bsz=388.6, num_updates=5000, lr=3.09839e-05, gnorm=0.586, train_wall=55, wall=2933
2021-01-08 21:39:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:39:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:39:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:39:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:39:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:39:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:39:29 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.335 | nll_loss 3.888 | ppl 14.8 | bleu 22.13 | wps 4639.4 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.31
2021-01-08 21:39:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:39:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:39:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:39:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 9 @ 5049 updates, score 22.13) (writing took 2.8158313147723675 seconds)
2021-01-08 21:39:32 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-08 21:39:32 | INFO | train | epoch 009 | symm_kl 0.614 | self_kl 0 | self_cv 0 | loss 3.644 | nll_loss 1.074 | ppl 2.11 | wps 17729.5 | ups 1.69 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 3.08332e-05 | gnorm 0.59 | train_wall 304 | wall 2983
2021-01-08 21:39:32 | INFO | fairseq.trainer | begin training epoch 10
2021-01-08 21:39:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:39:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:40:02 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=0.611, self_kl=0, self_cv=0, loss=3.648, nll_loss=1.083, ppl=2.12, wps=12975.3, ups=1.25, wpb=10353, bsz=356, num_updates=5100, lr=3.06786e-05, gnorm=0.598, train_wall=53, wall=3012
2021-01-08 21:40:56 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=0.609, self_kl=0, self_cv=0, loss=3.634, nll_loss=1.072, ppl=2.1, wps=19316.7, ups=1.83, wpb=10577.3, bsz=365.6, num_updates=5200, lr=3.03822e-05, gnorm=0.583, train_wall=55, wall=3067
2021-01-08 21:41:51 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=0.609, self_kl=0, self_cv=0, loss=3.635, nll_loss=1.072, ppl=2.1, wps=19298.1, ups=1.84, wpb=10501.8, bsz=363.6, num_updates=5300, lr=3.00942e-05, gnorm=0.588, train_wall=54, wall=3122
2021-01-08 21:42:46 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=0.604, self_kl=0, self_cv=0, loss=3.63, nll_loss=1.076, ppl=2.11, wps=19082.8, ups=1.83, wpb=10450.1, bsz=375.6, num_updates=5400, lr=2.98142e-05, gnorm=0.592, train_wall=55, wall=3176
2021-01-08 21:43:40 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=0.599, self_kl=0, self_cv=0, loss=3.614, nll_loss=1.066, ppl=2.09, wps=19108, ups=1.82, wpb=10472.1, bsz=373.6, num_updates=5500, lr=2.9542e-05, gnorm=0.589, train_wall=55, wall=3231
2021-01-08 21:44:35 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=0.601, self_kl=0, self_cv=0, loss=3.625, nll_loss=1.075, ppl=2.11, wps=19241.2, ups=1.83, wpb=10516.7, bsz=381.2, num_updates=5600, lr=2.9277e-05, gnorm=0.587, train_wall=54, wall=3286
2021-01-08 21:44:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:44:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:44:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:44:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:44:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:44:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:45:01 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.339 | nll_loss 3.889 | ppl 14.82 | bleu 22.03 | wps 4603.8 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.31
2021-01-08 21:45:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:45:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:45:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:45:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:45:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:45:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.03) (writing took 2.8612321745604277 seconds)
2021-01-08 21:45:04 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-08 21:45:04 | INFO | train | epoch 010 | symm_kl 0.605 | self_kl 0 | self_cv 0 | loss 3.628 | nll_loss 1.072 | ppl 2.1 | wps 17685.3 | ups 1.69 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 2.92509e-05 | gnorm 0.589 | train_wall 304 | wall 3315
2021-01-08 21:45:04 | INFO | fairseq.trainer | begin training epoch 11
2021-01-08 21:45:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:45:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:45:55 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=0.604, self_kl=0, self_cv=0, loss=3.623, nll_loss=1.068, ppl=2.1, wps=12903.9, ups=1.25, wpb=10358.6, bsz=351.1, num_updates=5700, lr=2.90191e-05, gnorm=0.597, train_wall=53, wall=3366
2021-01-08 21:46:50 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=0.595, self_kl=0, self_cv=0, loss=3.6, nll_loss=1.057, ppl=2.08, wps=19185, ups=1.82, wpb=10564, bsz=383.6, num_updates=5800, lr=2.87678e-05, gnorm=0.577, train_wall=55, wall=3421
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 96 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/de_std
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --de-std'
2021-01-08 21:50:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:50:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:50:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:50:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:50:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:50:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:50:10 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19202
2021-01-08 21:50:10 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19202
2021-01-08 21:50:11 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-08 21:50:11 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-08 21:50:14 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=True, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19202', distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=2, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/de_std', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-08 21:50:15 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-08 21:50:15 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-08 21:50:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-08 21:50:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-08 21:50:15 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-08 21:50:16 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-08 21:50:16 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-08 21:50:16 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-08 21:50:16 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-08 21:50:16 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-08 21:50:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-08 21:50:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-08 21:50:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2021-01-08 21:50:16 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 21:50:16 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 21:50:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************
2021-01-08 21:50:16 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)
2021-01-08 21:50:16 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-08 21:50:16 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-08 21:50:17 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 11 @ 5610 updates)
2021-01-08 21:50:17 | INFO | fairseq.trainer | loading train data for epoch 11
2021-01-08 21:50:17 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-08 21:50:17 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-08 21:50:17 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-08 21:50:17 | INFO | fairseq.trainer | begin training epoch 11
2021-01-08 21:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:51:05 | INFO | train_inner | epoch 011:     90 / 841 symm_kl=0.604, self_kl=0, self_cv=0, loss=3.629, nll_loss=1.073, ppl=2.1, wps=9626.3, ups=1.34, wpb=7197.4, bsz=240.2, num_updates=5700, lr=2.90191e-05, gnorm=0.719, train_wall=50, wall=0
2021-01-08 21:51:56 | INFO | train_inner | epoch 011:    190 / 841 symm_kl=0.599, self_kl=0, self_cv=0, loss=3.611, nll_loss=1.063, ppl=2.09, wps=13672, ups=1.97, wpb=6948, bsz=242.3, num_updates=5800, lr=2.87678e-05, gnorm=0.726, train_wall=51, wall=0
2021-01-08 21:52:49 | INFO | train_inner | epoch 011:    290 / 841 symm_kl=0.595, self_kl=0, self_cv=0, loss=3.599, nll_loss=1.057, ppl=2.08, wps=13496.7, ups=1.89, wpb=7129.6, bsz=262.8, num_updates=5900, lr=2.8523e-05, gnorm=0.702, train_wall=53, wall=0
2021-01-08 21:53:41 | INFO | train_inner | epoch 011:    390 / 841 symm_kl=0.596, self_kl=0, self_cv=0, loss=3.616, nll_loss=1.073, ppl=2.1, wps=13189.7, ups=1.9, wpb=6927.2, bsz=239.6, num_updates=6000, lr=2.82843e-05, gnorm=0.73, train_wall=52, wall=0
2021-01-08 21:54:34 | INFO | train_inner | epoch 011:    490 / 841 symm_kl=0.6, self_kl=0, self_cv=0, loss=3.649, nll_loss=1.103, ppl=2.15, wps=13258.8, ups=1.91, wpb=6944.5, bsz=241.7, num_updates=6100, lr=2.80515e-05, gnorm=0.748, train_wall=52, wall=0
2021-01-08 21:55:26 | INFO | train_inner | epoch 011:    590 / 841 symm_kl=0.596, self_kl=0, self_cv=0, loss=3.623, nll_loss=1.08, ppl=2.11, wps=13119.9, ups=1.9, wpb=6908.8, bsz=250.6, num_updates=6200, lr=2.78243e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-08 21:56:20 | INFO | train_inner | epoch 011:    690 / 841 symm_kl=0.594, self_kl=0, self_cv=0, loss=3.608, nll_loss=1.067, ppl=2.1, wps=13374.8, ups=1.88, wpb=7114.6, bsz=252.8, num_updates=6300, lr=2.76026e-05, gnorm=0.709, train_wall=53, wall=0
2021-01-08 21:57:12 | INFO | train_inner | epoch 011:    790 / 841 symm_kl=0.594, self_kl=0, self_cv=0, loss=3.621, nll_loss=1.082, ppl=2.12, wps=13342.4, ups=1.89, wpb=7050.2, bsz=245.7, num_updates=6400, lr=2.73861e-05, gnorm=0.729, train_wall=53, wall=0
2021-01-08 21:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 21:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:57:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 21:57:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 21:57:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 21:58:07 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.346 | nll_loss 3.9 | ppl 14.93 | bleu 22.07 | wps 3358.5 | wpb 5162.1 | bsz 187.5 | num_updates 6451 | best_bleu 22.31
2021-01-08 21:58:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 21:58:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:58:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 11 @ 6451 updates, score 22.07) (writing took 2.738438280299306 seconds)
2021-01-08 21:58:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-08 21:58:10 | INFO | train | epoch 011 | symm_kl 0.601 | self_kl 0 | self_cv 0 | loss 3.623 | nll_loss 1.073 | ppl 2.1 | wps 14654.6 | ups 1.75 | wpb 8389.7 | bsz 295.8 | num_updates 6451 | lr 2.72777e-05 | gnorm 0.67 | train_wall 742 | wall 0
2021-01-08 21:58:10 | INFO | fairseq.trainer | begin training epoch 12
2021-01-08 21:58:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 21:58:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 21:58:38 | INFO | train_inner | epoch 012:     49 / 841 symm_kl=0.59, self_kl=0, self_cv=0, loss=3.592, nll_loss=1.056, ppl=2.08, wps=8210.7, ups=1.17, wpb=7025.8, bsz=249, num_updates=6500, lr=2.71746e-05, gnorm=0.706, train_wall=52, wall=0
2021-01-08 21:59:30 | INFO | train_inner | epoch 012:    149 / 841 symm_kl=0.591, self_kl=0, self_cv=0, loss=3.605, nll_loss=1.069, ppl=2.1, wps=13193.6, ups=1.91, wpb=6910.7, bsz=244.7, num_updates=6600, lr=2.6968e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 22:00:23 | INFO | train_inner | epoch 012:    249 / 841 symm_kl=0.588, self_kl=0, self_cv=0, loss=3.599, nll_loss=1.068, ppl=2.1, wps=13162, ups=1.91, wpb=6896.9, bsz=251.2, num_updates=6700, lr=2.6766e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-08 22:01:15 | INFO | train_inner | epoch 012:    349 / 841 symm_kl=0.595, self_kl=0, self_cv=0, loss=3.623, nll_loss=1.082, ppl=2.12, wps=13497.6, ups=1.92, wpb=7035.7, bsz=242.4, num_updates=6800, lr=2.65684e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 22:02:08 | INFO | train_inner | epoch 012:    449 / 841 symm_kl=0.586, self_kl=0, self_cv=0, loss=3.586, nll_loss=1.057, ppl=2.08, wps=13511.9, ups=1.89, wpb=7144.7, bsz=264.3, num_updates=6900, lr=2.63752e-05, gnorm=0.706, train_wall=53, wall=0
2021-01-08 22:03:00 | INFO | train_inner | epoch 012:    549 / 841 symm_kl=0.589, self_kl=0, self_cv=0, loss=3.608, nll_loss=1.076, ppl=2.11, wps=13398.9, ups=1.9, wpb=7050.6, bsz=237.9, num_updates=7000, lr=2.61861e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 22:03:53 | INFO | train_inner | epoch 012:    649 / 841 symm_kl=0.59, self_kl=0, self_cv=0, loss=3.629, nll_loss=1.097, ppl=2.14, wps=13207.8, ups=1.91, wpb=6914, bsz=244.3, num_updates=7100, lr=2.60011e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-08 22:04:45 | INFO | train_inner | epoch 012:    749 / 841 symm_kl=0.584, self_kl=0, self_cv=0, loss=3.592, nll_loss=1.067, ppl=2.09, wps=13316.4, ups=1.9, wpb=7004.5, bsz=244.8, num_updates=7200, lr=2.58199e-05, gnorm=0.721, train_wall=52, wall=0
2021-01-08 22:05:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:05:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:05:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:05:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:05:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:05:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:05:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:06:02 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.346 | nll_loss 3.899 | ppl 14.91 | bleu 22.24 | wps 3289.4 | wpb 5162.1 | bsz 187.5 | num_updates 7292 | best_bleu 22.31
2021-01-08 22:06:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:06:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:06:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:06:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 12 @ 7292 updates, score 22.24) (writing took 2.7441884502768517 seconds)
2021-01-08 22:06:05 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-08 22:06:05 | INFO | train | epoch 012 | symm_kl 0.589 | self_kl 0 | self_cv 0 | loss 3.605 | nll_loss 1.073 | ppl 2.1 | wps 12384.2 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 7292 | lr 2.56565e-05 | gnorm 0.723 | train_wall 439 | wall 0
2021-01-08 22:06:05 | INFO | fairseq.trainer | begin training epoch 13
2021-01-08 22:06:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:06:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:06:12 | INFO | train_inner | epoch 013:      8 / 841 symm_kl=0.586, self_kl=0, self_cv=0, loss=3.607, nll_loss=1.079, ppl=2.11, wps=8034.2, ups=1.16, wpb=6955.2, bsz=241.5, num_updates=7300, lr=2.56424e-05, gnorm=0.73, train_wall=52, wall=0
2021-01-08 22:07:03 | INFO | train_inner | epoch 013:    108 / 841 symm_kl=0.588, self_kl=0, self_cv=0, loss=3.593, nll_loss=1.061, ppl=2.09, wps=13593.7, ups=1.95, wpb=6980.5, bsz=239.2, num_updates=7400, lr=2.54686e-05, gnorm=0.725, train_wall=51, wall=0
2021-01-08 22:07:56 | INFO | train_inner | epoch 013:    208 / 841 symm_kl=0.589, self_kl=0, self_cv=0, loss=3.602, nll_loss=1.069, ppl=2.1, wps=13341.7, ups=1.89, wpb=7060.9, bsz=232.7, num_updates=7500, lr=2.52982e-05, gnorm=0.718, train_wall=53, wall=0
2021-01-08 22:08:49 | INFO | train_inner | epoch 013:    308 / 841 symm_kl=0.583, self_kl=0, self_cv=0, loss=3.592, nll_loss=1.068, ppl=2.1, wps=13463.9, ups=1.91, wpb=7056.6, bsz=244.3, num_updates=7600, lr=2.51312e-05, gnorm=0.717, train_wall=52, wall=0
2021-01-08 22:09:41 | INFO | train_inner | epoch 013:    408 / 841 symm_kl=0.582, self_kl=0, self_cv=0, loss=3.597, nll_loss=1.074, ppl=2.11, wps=13373, ups=1.91, wpb=7016.6, bsz=245, num_updates=7700, lr=2.49675e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 22:10:33 | INFO | train_inner | epoch 013:    508 / 841 symm_kl=0.578, self_kl=0, self_cv=0, loss=3.578, nll_loss=1.061, ppl=2.09, wps=13298.3, ups=1.91, wpb=6965.9, bsz=261.7, num_updates=7800, lr=2.48069e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 22:11:26 | INFO | train_inner | epoch 013:    608 / 841 symm_kl=0.582, self_kl=0, self_cv=0, loss=3.598, nll_loss=1.077, ppl=2.11, wps=13368.6, ups=1.92, wpb=6958.8, bsz=254.2, num_updates=7900, lr=2.46494e-05, gnorm=0.724, train_wall=52, wall=0
2021-01-08 22:12:18 | INFO | train_inner | epoch 013:    708 / 841 symm_kl=0.58, self_kl=0, self_cv=0, loss=3.589, nll_loss=1.07, ppl=2.1, wps=13372.6, ups=1.9, wpb=7024.6, bsz=241.7, num_updates=8000, lr=2.44949e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 22:13:10 | INFO | train_inner | epoch 013:    808 / 841 symm_kl=0.58, self_kl=0, self_cv=0, loss=3.596, nll_loss=1.078, ppl=2.11, wps=13227.1, ups=1.92, wpb=6906.2, bsz=248.5, num_updates=8100, lr=2.43432e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-08 22:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:13:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:13:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:13:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:13:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:13:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:13:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:13:56 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.346 | nll_loss 3.904 | ppl 14.97 | bleu 21.97 | wps 3305.4 | wpb 5162.1 | bsz 187.5 | num_updates 8133 | best_bleu 22.31
2021-01-08 22:13:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:13:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:13:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:13:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 13 @ 8133 updates, score 21.97) (writing took 2.735624285414815 seconds)
2021-01-08 22:13:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-08 22:13:59 | INFO | train | epoch 013 | symm_kl 0.582 | self_kl 0 | self_cv 0 | loss 3.593 | nll_loss 1.07 | ppl 2.1 | wps 12410.4 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 8133 | lr 2.42938e-05 | gnorm 0.724 | train_wall 438 | wall 0
2021-01-08 22:13:59 | INFO | fairseq.trainer | begin training epoch 14
2021-01-08 22:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:14:36 | INFO | train_inner | epoch 014:     67 / 841 symm_kl=0.58, self_kl=0, self_cv=0, loss=3.585, nll_loss=1.065, ppl=2.09, wps=8172.6, ups=1.17, wpb=7009.9, bsz=251.5, num_updates=8200, lr=2.41943e-05, gnorm=0.714, train_wall=52, wall=0
2021-01-08 22:15:28 | INFO | train_inner | epoch 014:    167 / 841 symm_kl=0.579, self_kl=0, self_cv=0, loss=3.586, nll_loss=1.068, ppl=2.1, wps=13578.8, ups=1.91, wpb=7106, bsz=244.3, num_updates=8300, lr=2.40481e-05, gnorm=0.713, train_wall=52, wall=0
2021-01-08 22:16:21 | INFO | train_inner | epoch 014:    267 / 841 symm_kl=0.577, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.06, ppl=2.08, wps=13367.9, ups=1.9, wpb=7023.7, bsz=243.1, num_updates=8400, lr=2.39046e-05, gnorm=0.716, train_wall=52, wall=0
2021-01-08 22:17:13 | INFO | train_inner | epoch 014:    367 / 841 symm_kl=0.577, self_kl=0, self_cv=0, loss=3.585, nll_loss=1.07, ppl=2.1, wps=13352.9, ups=1.93, wpb=6934.5, bsz=251, num_updates=8500, lr=2.37635e-05, gnorm=0.723, train_wall=52, wall=0
2021-01-08 22:18:05 | INFO | train_inner | epoch 014:    467 / 841 symm_kl=0.578, self_kl=0, self_cv=0, loss=3.59, nll_loss=1.075, ppl=2.11, wps=13335.1, ups=1.92, wpb=6954, bsz=235.9, num_updates=8600, lr=2.3625e-05, gnorm=0.727, train_wall=52, wall=0
2021-01-08 22:18:57 | INFO | train_inner | epoch 014:    567 / 841 symm_kl=0.577, self_kl=0, self_cv=0, loss=3.592, nll_loss=1.077, ppl=2.11, wps=13307.5, ups=1.92, wpb=6931.6, bsz=251.3, num_updates=8700, lr=2.34888e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-08 22:19:50 | INFO | train_inner | epoch 014:    667 / 841 symm_kl=0.577, self_kl=0, self_cv=0, loss=3.591, nll_loss=1.076, ppl=2.11, wps=13256.8, ups=1.9, wpb=6973.4, bsz=239, num_updates=8800, lr=2.3355e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-08 22:20:42 | INFO | train_inner | epoch 014:    767 / 841 symm_kl=0.572, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.066, ppl=2.09, wps=13421.3, ups=1.91, wpb=7010.3, bsz=250.1, num_updates=8900, lr=2.32234e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 22:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:21:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:21:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:21:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:21:49 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.347 | nll_loss 3.903 | ppl 14.95 | bleu 22.22 | wps 3342.2 | wpb 5162.1 | bsz 187.5 | num_updates 8974 | best_bleu 22.31
2021-01-08 22:21:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:21:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:21:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:21:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 14 @ 8974 updates, score 22.22) (writing took 2.8516471218317747 seconds)
2021-01-08 22:21:52 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-08 22:21:52 | INFO | train | epoch 014 | symm_kl 0.577 | self_kl 0 | self_cv 0 | loss 3.584 | nll_loss 1.069 | ppl 2.1 | wps 12429.6 | ups 1.78 | wpb 6993.1 | bsz 246.6 | num_updates 8974 | lr 2.31274e-05 | gnorm 0.724 | train_wall 438 | wall 0
2021-01-08 22:21:52 | INFO | fairseq.trainer | begin training epoch 15
2021-01-08 22:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:22:08 | INFO | train_inner | epoch 015:     26 / 841 symm_kl=0.574, self_kl=0, self_cv=0, loss=3.585, nll_loss=1.076, ppl=2.11, wps=8145, ups=1.16, wpb=7020.6, bsz=257.8, num_updates=9000, lr=2.3094e-05, gnorm=0.723, train_wall=52, wall=0
2021-01-08 22:23:00 | INFO | train_inner | epoch 015:    126 / 841 symm_kl=0.574, self_kl=0, self_cv=0, loss=3.577, nll_loss=1.066, ppl=2.09, wps=13322.2, ups=1.93, wpb=6894.3, bsz=249.4, num_updates=9100, lr=2.29668e-05, gnorm=0.73, train_wall=52, wall=0
2021-01-08 22:23:52 | INFO | train_inner | epoch 015:    226 / 841 symm_kl=0.576, self_kl=0, self_cv=0, loss=3.585, nll_loss=1.072, ppl=2.1, wps=13459, ups=1.92, wpb=6991.8, bsz=239.4, num_updates=9200, lr=2.28416e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 22:24:45 | INFO | train_inner | epoch 015:    326 / 841 symm_kl=0.57, self_kl=0, self_cv=0, loss=3.562, nll_loss=1.057, ppl=2.08, wps=13270.4, ups=1.89, wpb=7023.8, bsz=257.8, num_updates=9300, lr=2.27185e-05, gnorm=0.717, train_wall=53, wall=0
2021-01-08 22:25:37 | INFO | train_inner | epoch 015:    426 / 841 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.557, nll_loss=1.053, ppl=2.07, wps=13360.9, ups=1.91, wpb=7009.1, bsz=248.5, num_updates=9400, lr=2.25973e-05, gnorm=0.721, train_wall=52, wall=0
2021-01-08 22:26:30 | INFO | train_inner | epoch 015:    526 / 841 symm_kl=0.572, self_kl=0, self_cv=0, loss=3.581, nll_loss=1.074, ppl=2.1, wps=13333.8, ups=1.91, wpb=6995.4, bsz=237.4, num_updates=9500, lr=2.24781e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-08 22:27:23 | INFO | train_inner | epoch 015:    626 / 841 symm_kl=0.57, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.071, ppl=2.1, wps=13136.7, ups=1.89, wpb=6949.9, bsz=253.3, num_updates=9600, lr=2.23607e-05, gnorm=0.728, train_wall=53, wall=0
2021-01-08 22:28:15 | INFO | train_inner | epoch 015:    726 / 841 symm_kl=0.57, self_kl=0, self_cv=0, loss=3.578, nll_loss=1.074, ppl=2.11, wps=13280.2, ups=1.89, wpb=7021.7, bsz=244.5, num_updates=9700, lr=2.22451e-05, gnorm=0.723, train_wall=53, wall=0
2021-01-08 22:29:08 | INFO | train_inner | epoch 015:    826 / 841 symm_kl=0.571, self_kl=0, self_cv=0, loss=3.583, nll_loss=1.079, ppl=2.11, wps=13289.3, ups=1.89, wpb=7021.9, bsz=245.1, num_updates=9800, lr=2.21313e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-08 22:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:29:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:29:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:29:45 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.349 | nll_loss 3.907 | ppl 15 | bleu 22.09 | wps 3252 | wpb 5162.1 | bsz 187.5 | num_updates 9815 | best_bleu 22.31
2021-01-08 22:29:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:29:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:29:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:29:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 15 @ 9815 updates, score 22.09) (writing took 2.7987553738057613 seconds)
2021-01-08 22:29:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-08 22:29:48 | INFO | train | epoch 015 | symm_kl 0.572 | self_kl 0 | self_cv 0 | loss 3.575 | nll_loss 1.068 | ppl 2.1 | wps 12355.9 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 9815 | lr 2.21144e-05 | gnorm 0.724 | train_wall 440 | wall 0
2021-01-08 22:29:48 | INFO | fairseq.trainer | begin training epoch 16
2021-01-08 22:29:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:30:35 | INFO | train_inner | epoch 016:     85 / 841 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.56, nll_loss=1.056, ppl=2.08, wps=8032.7, ups=1.16, wpb=6954.5, bsz=255.8, num_updates=9900, lr=2.20193e-05, gnorm=0.713, train_wall=52, wall=0
2021-01-08 22:31:27 | INFO | train_inner | epoch 016:    185 / 841 symm_kl=0.567, self_kl=0, self_cv=0, loss=3.562, nll_loss=1.06, ppl=2.09, wps=13409.3, ups=1.92, wpb=6993, bsz=239.7, num_updates=10000, lr=2.19089e-05, gnorm=0.721, train_wall=52, wall=0
2021-01-08 22:32:20 | INFO | train_inner | epoch 016:    285 / 841 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.569, nll_loss=1.065, ppl=2.09, wps=13206.1, ups=1.9, wpb=6955.7, bsz=259.4, num_updates=10100, lr=2.18002e-05, gnorm=0.728, train_wall=52, wall=0
2021-01-08 22:33:12 | INFO | train_inner | epoch 016:    385 / 841 symm_kl=0.57, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.071, ppl=2.1, wps=13260, ups=1.9, wpb=6971.9, bsz=242.2, num_updates=10200, lr=2.1693e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-08 22:34:05 | INFO | train_inner | epoch 016:    485 / 841 symm_kl=0.563, self_kl=0, self_cv=0, loss=3.55, nll_loss=1.054, ppl=2.08, wps=13240.9, ups=1.89, wpb=7005.8, bsz=255.9, num_updates=10300, lr=2.15875e-05, gnorm=0.717, train_wall=53, wall=0
2021-01-08 22:34:57 | INFO | train_inner | epoch 016:    585 / 841 symm_kl=0.566, self_kl=0, self_cv=0, loss=3.561, nll_loss=1.062, ppl=2.09, wps=13554.4, ups=1.91, wpb=7087.6, bsz=248.5, num_updates=10400, lr=2.14834e-05, gnorm=0.713, train_wall=52, wall=0
2021-01-08 22:35:50 | INFO | train_inner | epoch 016:    685 / 841 symm_kl=0.57, self_kl=0, self_cv=0, loss=3.577, nll_loss=1.073, ppl=2.1, wps=13303.5, ups=1.91, wpb=6981.5, bsz=238.3, num_updates=10500, lr=2.13809e-05, gnorm=0.728, train_wall=52, wall=0
2021-01-08 22:36:43 | INFO | train_inner | epoch 016:    785 / 841 symm_kl=0.567, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.076, ppl=2.11, wps=13212, ups=1.9, wpb=6947.6, bsz=236.6, num_updates=10600, lr=2.12798e-05, gnorm=0.733, train_wall=52, wall=0
2021-01-08 22:37:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:37:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:37:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:37:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:37:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:37:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:37:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:37:40 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.357 | nll_loss 3.917 | ppl 15.11 | bleu 21.93 | wps 3293.5 | wpb 5162.1 | bsz 187.5 | num_updates 10656 | best_bleu 22.31
2021-01-08 22:37:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:37:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:37:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:37:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 16 @ 10656 updates, score 21.93) (writing took 2.7559548560529947 seconds)
2021-01-08 22:37:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-08 22:37:43 | INFO | train | epoch 016 | symm_kl 0.567 | self_kl 0 | self_cv 0 | loss 3.567 | nll_loss 1.066 | ppl 2.09 | wps 12381 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 10656 | lr 2.12238e-05 | gnorm 0.724 | train_wall 439 | wall 0
2021-01-08 22:37:43 | INFO | fairseq.trainer | begin training epoch 17
2021-01-08 22:37:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:37:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:38:08 | INFO | train_inner | epoch 017:     44 / 841 symm_kl=0.562, self_kl=0, self_cv=0, loss=3.555, nll_loss=1.062, ppl=2.09, wps=8189.5, ups=1.16, wpb=7033.7, bsz=246.8, num_updates=10700, lr=2.11801e-05, gnorm=0.723, train_wall=51, wall=0
2021-01-08 22:39:01 | INFO | train_inner | epoch 017:    144 / 841 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.578, nll_loss=1.076, ppl=2.11, wps=13241.2, ups=1.92, wpb=6911.4, bsz=235.9, num_updates=10800, lr=2.10819e-05, gnorm=0.733, train_wall=52, wall=0
2021-01-08 22:39:53 | INFO | train_inner | epoch 017:    244 / 841 symm_kl=0.557, self_kl=0, self_cv=0, loss=3.514, nll_loss=1.026, ppl=2.04, wps=13565.7, ups=1.89, wpb=7170.6, bsz=253.3, num_updates=10900, lr=2.09849e-05, gnorm=0.701, train_wall=53, wall=0
2021-01-08 22:40:46 | INFO | train_inner | epoch 017:    344 / 841 symm_kl=0.563, self_kl=0, self_cv=0, loss=3.554, nll_loss=1.058, ppl=2.08, wps=13392.6, ups=1.91, wpb=7019.9, bsz=250.1, num_updates=11000, lr=2.08893e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 22:41:38 | INFO | train_inner | epoch 017:    444 / 841 symm_kl=0.561, self_kl=0, self_cv=0, loss=3.544, nll_loss=1.051, ppl=2.07, wps=13320.6, ups=1.92, wpb=6926.6, bsz=244.6, num_updates=11100, lr=2.0795e-05, gnorm=0.723, train_wall=52, wall=0
2021-01-08 22:42:30 | INFO | train_inner | epoch 017:    544 / 841 symm_kl=0.564, self_kl=0, self_cv=0, loss=3.562, nll_loss=1.067, ppl=2.1, wps=13554.1, ups=1.92, wpb=7073.1, bsz=254.1, num_updates=11200, lr=2.0702e-05, gnorm=0.714, train_wall=52, wall=0
2021-01-08 22:43:22 | INFO | train_inner | epoch 017:    644 / 841 symm_kl=0.562, self_kl=0, self_cv=0, loss=3.565, nll_loss=1.071, ppl=2.1, wps=13210.8, ups=1.91, wpb=6919.8, bsz=253, num_updates=11300, lr=2.06102e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-08 22:44:15 | INFO | train_inner | epoch 017:    744 / 841 symm_kl=0.564, self_kl=0, self_cv=0, loss=3.581, nll_loss=1.086, ppl=2.12, wps=13256.3, ups=1.91, wpb=6953.2, bsz=236.6, num_updates=11400, lr=2.05196e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-08 22:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:45:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:45:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:45:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:45:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:45:34 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.353 | nll_loss 3.913 | ppl 15.06 | bleu 22.06 | wps 3274.8 | wpb 5162.1 | bsz 187.5 | num_updates 11497 | best_bleu 22.31
2021-01-08 22:45:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:45:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:45:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:45:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 17 @ 11497 updates, score 22.06) (writing took 2.747969828546047 seconds)
2021-01-08 22:45:37 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-08 22:45:37 | INFO | train | epoch 017 | symm_kl 0.563 | self_kl 0 | self_cv 0 | loss 3.56 | nll_loss 1.065 | ppl 2.09 | wps 12410.7 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 11497 | lr 2.04328e-05 | gnorm 0.725 | train_wall 438 | wall 0
2021-01-08 22:45:37 | INFO | fairseq.trainer | begin training epoch 18
2021-01-08 22:45:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:45:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:45:41 | INFO | train_inner | epoch 018:      3 / 841 symm_kl=0.566, self_kl=0, self_cv=0, loss=3.59, nll_loss=1.094, ppl=2.14, wps=8053.6, ups=1.16, wpb=6956.1, bsz=237.9, num_updates=11500, lr=2.04302e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-08 22:46:33 | INFO | train_inner | epoch 018:    103 / 841 symm_kl=0.562, self_kl=0, self_cv=0, loss=3.548, nll_loss=1.053, ppl=2.08, wps=13487, ups=1.93, wpb=6991.7, bsz=244.5, num_updates=11600, lr=2.03419e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 22:47:26 | INFO | train_inner | epoch 018:    203 / 841 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.541, nll_loss=1.051, ppl=2.07, wps=13265.6, ups=1.89, wpb=7025.4, bsz=253.5, num_updates=11700, lr=2.02548e-05, gnorm=0.714, train_wall=53, wall=0
2021-01-08 22:48:19 | INFO | train_inner | epoch 018:    303 / 841 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.544, nll_loss=1.055, ppl=2.08, wps=13472.7, ups=1.91, wpb=7069.6, bsz=237.3, num_updates=11800, lr=2.01688e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 22:49:11 | INFO | train_inner | epoch 018:    403 / 841 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.55, nll_loss=1.061, ppl=2.09, wps=13112.3, ups=1.9, wpb=6903.4, bsz=248.2, num_updates=11900, lr=2.00839e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-08 22:50:04 | INFO | train_inner | epoch 018:    503 / 841 symm_kl=0.56, self_kl=0, self_cv=0, loss=3.562, nll_loss=1.072, ppl=2.1, wps=13098.5, ups=1.91, wpb=6862.7, bsz=238.1, num_updates=12000, lr=2e-05, gnorm=0.742, train_wall=52, wall=0
2021-01-08 22:50:56 | INFO | train_inner | epoch 018:    603 / 841 symm_kl=0.556, self_kl=0, self_cv=0, loss=3.541, nll_loss=1.057, ppl=2.08, wps=13391.2, ups=1.91, wpb=7020.9, bsz=265.5, num_updates=12100, lr=1.99172e-05, gnorm=0.716, train_wall=52, wall=0
2021-01-08 22:51:49 | INFO | train_inner | epoch 018:    703 / 841 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.556, nll_loss=1.069, ppl=2.1, wps=13295.6, ups=1.89, wpb=7053.2, bsz=245.3, num_updates=12200, lr=1.98354e-05, gnorm=0.717, train_wall=53, wall=0
2021-01-08 22:52:42 | INFO | train_inner | epoch 018:    803 / 841 symm_kl=0.561, self_kl=0, self_cv=0, loss=3.569, nll_loss=1.079, ppl=2.11, wps=13310.6, ups=1.89, wpb=7053.9, bsz=244.4, num_updates=12300, lr=1.97546e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-08 22:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 22:53:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:53:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:53:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:53:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:53:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 22:53:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 22:53:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 22:53:30 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.355 | nll_loss 3.915 | ppl 15.09 | bleu 22.16 | wps 3303.9 | wpb 5162.1 | bsz 187.5 | num_updates 12338 | best_bleu 22.31
2021-01-08 22:53:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 22:53:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:53:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:53:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 18 @ 12338 updates, score 22.16) (writing took 2.948919018730521 seconds)
2021-01-08 22:53:33 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-08 22:53:33 | INFO | train | epoch 018 | symm_kl 0.559 | self_kl 0 | self_cv 0 | loss 3.552 | nll_loss 1.063 | ppl 2.09 | wps 12336.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 12338 | lr 1.97241e-05 | gnorm 0.724 | train_wall 441 | wall 0
2021-01-08 22:53:33 | INFO | fairseq.trainer | begin training epoch 19
2021-01-08 22:53:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 22:53:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 22:54:08 | INFO | train_inner | epoch 019:     62 / 841 symm_kl=0.558, self_kl=0, self_cv=0, loss=3.55, nll_loss=1.062, ppl=2.09, wps=8007.1, ups=1.16, wpb=6896.6, bsz=243.5, num_updates=12400, lr=1.96748e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-08 22:55:01 | INFO | train_inner | epoch 019:    162 / 841 symm_kl=0.557, self_kl=0, self_cv=0, loss=3.548, nll_loss=1.062, ppl=2.09, wps=13225.5, ups=1.89, wpb=6989.1, bsz=251.3, num_updates=12500, lr=1.95959e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-08 22:55:55 | INFO | train_inner | epoch 019:    262 / 841 symm_kl=0.554, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.045, ppl=2.06, wps=13200.4, ups=1.86, wpb=7097.7, bsz=253.8, num_updates=12600, lr=1.9518e-05, gnorm=0.702, train_wall=54, wall=0
2021-01-08 22:56:48 | INFO | train_inner | epoch 019:    362 / 841 symm_kl=0.556, self_kl=0, self_cv=0, loss=3.538, nll_loss=1.054, ppl=2.08, wps=13409.1, ups=1.89, wpb=7093.6, bsz=244.6, num_updates=12700, lr=1.9441e-05, gnorm=0.715, train_wall=53, wall=0
2021-01-08 22:57:41 | INFO | train_inner | epoch 019:    462 / 841 symm_kl=0.558, self_kl=0, self_cv=0, loss=3.561, nll_loss=1.075, ppl=2.11, wps=12956, ups=1.88, wpb=6905.2, bsz=244.6, num_updates=12800, lr=1.93649e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-08 22:58:34 | INFO | train_inner | epoch 019:    562 / 841 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.556, nll_loss=1.068, ppl=2.1, wps=13126.1, ups=1.88, wpb=6998.4, bsz=247, num_updates=12900, lr=1.92897e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-08 22:59:28 | INFO | train_inner | epoch 019:    662 / 841 symm_kl=0.556, self_kl=0, self_cv=0, loss=3.557, nll_loss=1.074, ppl=2.1, wps=12862.5, ups=1.87, wpb=6873.8, bsz=235.4, num_updates=13000, lr=1.92154e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-08 23:00:21 | INFO | train_inner | epoch 019:    762 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.526, nll_loss=1.049, ppl=2.07, wps=13213.5, ups=1.87, wpb=7068.4, bsz=249.8, num_updates=13100, lr=1.91419e-05, gnorm=0.708, train_wall=53, wall=0
2021-01-08 23:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:01:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:01:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:01:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:01:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:01:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:01:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:01:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:01:31 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.359 | nll_loss 3.917 | ppl 15.1 | bleu 21.87 | wps 3263.1 | wpb 5162.1 | bsz 187.5 | num_updates 13179 | best_bleu 22.31
2021-01-08 23:01:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:01:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:01:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 19 @ 13179 updates, score 21.87) (writing took 2.74056582711637 seconds)
2021-01-08 23:01:34 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-08 23:01:34 | INFO | train | epoch 019 | symm_kl 0.556 | self_kl 0 | self_cv 0 | loss 3.547 | nll_loss 1.063 | ppl 2.09 | wps 12228.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 13179 | lr 1.90844e-05 | gnorm 0.725 | train_wall 445 | wall 0
2021-01-08 23:01:34 | INFO | fairseq.trainer | begin training epoch 20
2021-01-08 23:01:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:01:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:01:48 | INFO | train_inner | epoch 020:     21 / 841 symm_kl=0.556, self_kl=0, self_cv=0, loss=3.563, nll_loss=1.081, ppl=2.12, wps=8045.3, ups=1.15, wpb=6994, bsz=248.3, num_updates=13200, lr=1.90693e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-08 23:02:41 | INFO | train_inner | epoch 020:    121 / 841 symm_kl=0.554, self_kl=0, self_cv=0, loss=3.531, nll_loss=1.049, ppl=2.07, wps=13374.2, ups=1.91, wpb=7017.9, bsz=242.3, num_updates=13300, lr=1.89974e-05, gnorm=0.715, train_wall=52, wall=0
2021-01-08 23:03:34 | INFO | train_inner | epoch 020:    221 / 841 symm_kl=0.557, self_kl=0, self_cv=0, loss=3.549, nll_loss=1.064, ppl=2.09, wps=13184.2, ups=1.88, wpb=7020.6, bsz=251.6, num_updates=13400, lr=1.89264e-05, gnorm=0.72, train_wall=53, wall=0
2021-01-08 23:04:27 | INFO | train_inner | epoch 020:    321 / 841 symm_kl=0.552, self_kl=0, self_cv=0, loss=3.539, nll_loss=1.061, ppl=2.09, wps=13263.9, ups=1.89, wpb=7025, bsz=246.7, num_updates=13500, lr=1.88562e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-08 23:05:20 | INFO | train_inner | epoch 020:    421 / 841 symm_kl=0.553, self_kl=0, self_cv=0, loss=3.537, nll_loss=1.056, ppl=2.08, wps=13064.3, ups=1.89, wpb=6915.6, bsz=235.6, num_updates=13600, lr=1.87867e-05, gnorm=0.728, train_wall=53, wall=0
2021-01-08 23:06:13 | INFO | train_inner | epoch 020:    521 / 841 symm_kl=0.554, self_kl=0, self_cv=0, loss=3.54, nll_loss=1.06, ppl=2.08, wps=13466.4, ups=1.89, wpb=7112.7, bsz=255.1, num_updates=13700, lr=1.8718e-05, gnorm=0.715, train_wall=53, wall=0
2021-01-08 23:07:06 | INFO | train_inner | epoch 020:    621 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.551, nll_loss=1.076, ppl=2.11, wps=13256.7, ups=1.89, wpb=7010.6, bsz=248.2, num_updates=13800, lr=1.86501e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-08 23:07:58 | INFO | train_inner | epoch 020:    721 / 841 symm_kl=0.552, self_kl=0, self_cv=0, loss=3.541, nll_loss=1.062, ppl=2.09, wps=13102, ups=1.9, wpb=6896.3, bsz=245.3, num_updates=13900, lr=1.85829e-05, gnorm=0.741, train_wall=52, wall=0
2021-01-08 23:08:51 | INFO | train_inner | epoch 020:    821 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.542, nll_loss=1.066, ppl=2.09, wps=13205.3, ups=1.9, wpb=6959.7, bsz=244.1, num_updates=14000, lr=1.85164e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-08 23:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:09:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:09:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:09:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:09:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:09:30 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.359 | nll_loss 3.918 | ppl 15.12 | bleu 22.11 | wps 3281.4 | wpb 5162.1 | bsz 187.5 | num_updates 14020 | best_bleu 22.31
2021-01-08 23:09:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:09:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:09:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:09:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 20 @ 14020 updates, score 22.11) (writing took 2.747231127694249 seconds)
2021-01-08 23:09:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-08 23:09:32 | INFO | train | epoch 020 | symm_kl 0.553 | self_kl 0 | self_cv 0 | loss 3.541 | nll_loss 1.061 | ppl 2.09 | wps 12298.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 14020 | lr 1.85032e-05 | gnorm 0.726 | train_wall 442 | wall 0
2021-01-08 23:09:32 | INFO | fairseq.trainer | begin training epoch 21
2021-01-08 23:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:10:17 | INFO | train_inner | epoch 021:     80 / 841 symm_kl=0.55, self_kl=0, self_cv=0, loss=3.538, nll_loss=1.062, ppl=2.09, wps=7925, ups=1.16, wpb=6830.2, bsz=245.2, num_updates=14100, lr=1.84506e-05, gnorm=0.745, train_wall=52, wall=0
2021-01-08 23:11:10 | INFO | train_inner | epoch 021:    180 / 841 symm_kl=0.549, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.052, ppl=2.07, wps=13201.3, ups=1.9, wpb=6962.5, bsz=257.5, num_updates=14200, lr=1.83855e-05, gnorm=0.72, train_wall=53, wall=0
2021-01-08 23:12:03 | INFO | train_inner | epoch 021:    280 / 841 symm_kl=0.552, self_kl=0, self_cv=0, loss=3.54, nll_loss=1.062, ppl=2.09, wps=13334, ups=1.9, wpb=7033, bsz=234.1, num_updates=14300, lr=1.83211e-05, gnorm=0.72, train_wall=53, wall=0
2021-01-08 23:12:56 | INFO | train_inner | epoch 021:    380 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.53, nll_loss=1.052, ppl=2.07, wps=13232.9, ups=1.88, wpb=7032.3, bsz=245.6, num_updates=14400, lr=1.82574e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-08 23:13:49 | INFO | train_inner | epoch 021:    480 / 841 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.034, ppl=2.05, wps=13286.9, ups=1.88, wpb=7050, bsz=266.2, num_updates=14500, lr=1.81944e-05, gnorm=0.703, train_wall=53, wall=0
2021-01-08 23:14:41 | INFO | train_inner | epoch 021:    580 / 841 symm_kl=0.55, self_kl=0, self_cv=0, loss=3.538, nll_loss=1.062, ppl=2.09, wps=13309.3, ups=1.9, wpb=7018.4, bsz=243.8, num_updates=14600, lr=1.81319e-05, gnorm=0.719, train_wall=53, wall=0
2021-01-08 23:15:34 | INFO | train_inner | epoch 021:    680 / 841 symm_kl=0.55, self_kl=0, self_cv=0, loss=3.542, nll_loss=1.068, ppl=2.1, wps=13219.7, ups=1.89, wpb=6979, bsz=241.3, num_updates=14700, lr=1.80702e-05, gnorm=0.729, train_wall=53, wall=0
2021-01-08 23:16:27 | INFO | train_inner | epoch 021:    780 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.547, nll_loss=1.071, ppl=2.1, wps=13335.1, ups=1.89, wpb=7039.6, bsz=251.2, num_updates=14800, lr=1.8009e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-08 23:16:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:17:27 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.362 | nll_loss 3.924 | ppl 15.18 | bleu 22.16 | wps 3307 | wpb 5162.1 | bsz 187.5 | num_updates 14861 | best_bleu 22.31
2021-01-08 23:17:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:17:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 21 @ 14861 updates, score 22.16) (writing took 2.8142589330673218 seconds)
2021-01-08 23:17:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-08 23:17:30 | INFO | train | epoch 021 | symm_kl 0.55 | self_kl 0 | self_cv 0 | loss 3.535 | nll_loss 1.059 | ppl 2.08 | wps 12323.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 14861 | lr 1.7972e-05 | gnorm 0.725 | train_wall 441 | wall 0
2021-01-08 23:17:30 | INFO | fairseq.trainer | begin training epoch 22
2021-01-08 23:17:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:17:53 | INFO | train_inner | epoch 022:     39 / 841 symm_kl=0.553, self_kl=0, self_cv=0, loss=3.543, nll_loss=1.064, ppl=2.09, wps=8222.5, ups=1.17, wpb=7042.5, bsz=243.5, num_updates=14900, lr=1.79485e-05, gnorm=0.719, train_wall=51, wall=0
2021-01-08 23:18:45 | INFO | train_inner | epoch 022:    139 / 841 symm_kl=0.548, self_kl=0, self_cv=0, loss=3.524, nll_loss=1.05, ppl=2.07, wps=13328.3, ups=1.9, wpb=7008.8, bsz=235, num_updates=15000, lr=1.78885e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 23:19:38 | INFO | train_inner | epoch 022:    239 / 841 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.51, nll_loss=1.043, ppl=2.06, wps=13286.7, ups=1.88, wpb=7062.3, bsz=268.4, num_updates=15100, lr=1.78292e-05, gnorm=0.715, train_wall=53, wall=0
2021-01-08 23:20:31 | INFO | train_inner | epoch 022:    339 / 841 symm_kl=0.547, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.055, ppl=2.08, wps=13196.7, ups=1.89, wpb=6964.2, bsz=238, num_updates=15200, lr=1.77705e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-08 23:21:24 | INFO | train_inner | epoch 022:    439 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.552, nll_loss=1.076, ppl=2.11, wps=13115.3, ups=1.89, wpb=6924.1, bsz=244.3, num_updates=15300, lr=1.77123e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-08 23:22:17 | INFO | train_inner | epoch 022:    539 / 841 symm_kl=0.547, self_kl=0, self_cv=0, loss=3.528, nll_loss=1.057, ppl=2.08, wps=13257.3, ups=1.91, wpb=6957.6, bsz=249.4, num_updates=15400, lr=1.76547e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-08 23:23:09 | INFO | train_inner | epoch 022:    639 / 841 symm_kl=0.546, self_kl=0, self_cv=0, loss=3.518, nll_loss=1.046, ppl=2.07, wps=13379.7, ups=1.9, wpb=7039.5, bsz=238.3, num_updates=15500, lr=1.75977e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 23:24:01 | INFO | train_inner | epoch 022:    739 / 841 symm_kl=0.548, self_kl=0, self_cv=0, loss=3.543, nll_loss=1.072, ppl=2.1, wps=13486.4, ups=1.92, wpb=7036.7, bsz=247.1, num_updates=15600, lr=1.75412e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-08 23:24:54 | INFO | train_inner | epoch 022:    839 / 841 symm_kl=0.546, self_kl=0, self_cv=0, loss=3.544, nll_loss=1.076, ppl=2.11, wps=13227.3, ups=1.92, wpb=6905.9, bsz=245.4, num_updates=15700, lr=1.74852e-05, gnorm=0.747, train_wall=52, wall=0
2021-01-08 23:24:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:24:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:25:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:25:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:25:22 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.362 | nll_loss 3.926 | ppl 15.2 | bleu 22.07 | wps 3373.6 | wpb 5162.1 | bsz 187.5 | num_updates 15702 | best_bleu 22.31
2021-01-08 23:25:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:25:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 22 @ 15702 updates, score 22.07) (writing took 2.8243619296699762 seconds)
2021-01-08 23:25:25 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-08 23:25:25 | INFO | train | epoch 022 | symm_kl 0.547 | self_kl 0 | self_cv 0 | loss 3.53 | nll_loss 1.059 | ppl 2.08 | wps 12374.2 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 15702 | lr 1.74841e-05 | gnorm 0.727 | train_wall 440 | wall 0
2021-01-08 23:25:25 | INFO | fairseq.trainer | begin training epoch 23
2021-01-08 23:25:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:26:19 | INFO | train_inner | epoch 023:     98 / 841 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.542, nll_loss=1.064, ppl=2.09, wps=8163.9, ups=1.17, wpb=6968.5, bsz=232.6, num_updates=15800, lr=1.74298e-05, gnorm=0.734, train_wall=52, wall=0
2021-01-08 23:27:12 | INFO | train_inner | epoch 023:    198 / 841 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.526, nll_loss=1.058, ppl=2.08, wps=13259.7, ups=1.88, wpb=7036.6, bsz=248.8, num_updates=15900, lr=1.73749e-05, gnorm=0.714, train_wall=53, wall=0
2021-01-08 23:28:04 | INFO | train_inner | epoch 023:    298 / 841 symm_kl=0.545, self_kl=0, self_cv=0, loss=3.525, nll_loss=1.055, ppl=2.08, wps=13173, ups=1.92, wpb=6877.8, bsz=239.6, num_updates=16000, lr=1.73205e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-08 23:28:57 | INFO | train_inner | epoch 023:    398 / 841 symm_kl=0.545, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.059, ppl=2.08, wps=13446.9, ups=1.9, wpb=7066.4, bsz=240.5, num_updates=16100, lr=1.72666e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 23:29:49 | INFO | train_inner | epoch 023:    498 / 841 symm_kl=0.547, self_kl=0, self_cv=0, loss=3.536, nll_loss=1.066, ppl=2.09, wps=13396.5, ups=1.91, wpb=7001.1, bsz=243.4, num_updates=16200, lr=1.72133e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-08 23:30:41 | INFO | train_inner | epoch 023:    598 / 841 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.516, nll_loss=1.051, ppl=2.07, wps=13482.9, ups=1.91, wpb=7073.9, bsz=263.6, num_updates=16300, lr=1.71604e-05, gnorm=0.717, train_wall=52, wall=0
2021-01-08 23:31:34 | INFO | train_inner | epoch 023:    698 / 841 symm_kl=0.546, self_kl=0, self_cv=0, loss=3.528, nll_loss=1.059, ppl=2.08, wps=13220, ups=1.91, wpb=6909.8, bsz=238.2, num_updates=16400, lr=1.7108e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-08 23:32:26 | INFO | train_inner | epoch 023:    798 / 841 symm_kl=0.54, self_kl=0, self_cv=0, loss=3.506, nll_loss=1.045, ppl=2.06, wps=13370.4, ups=1.9, wpb=7041.3, bsz=262.4, num_updates=16500, lr=1.70561e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 23:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:32:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:32:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:32:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:32:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:32:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:32:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:32:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:33:17 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.36 | nll_loss 3.926 | ppl 15.2 | bleu 21.97 | wps 3258.9 | wpb 5162.1 | bsz 187.5 | num_updates 16543 | best_bleu 22.31
2021-01-08 23:33:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:33:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:33:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 23 @ 16543 updates, score 21.97) (writing took 2.8146550096571445 seconds)
2021-01-08 23:33:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-08 23:33:20 | INFO | train | epoch 023 | symm_kl 0.545 | self_kl 0 | self_cv 0 | loss 3.525 | nll_loss 1.057 | ppl 2.08 | wps 12379.6 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 16543 | lr 1.70339e-05 | gnorm 0.727 | train_wall 439 | wall 0
2021-01-08 23:33:20 | INFO | fairseq.trainer | begin training epoch 24
2021-01-08 23:33:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:33:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:33:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:33:53 | INFO | train_inner | epoch 024:     57 / 841 symm_kl=0.542, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.054, ppl=2.08, wps=8092.2, ups=1.16, wpb=6994.2, bsz=250.1, num_updates=16600, lr=1.70046e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 23:34:46 | INFO | train_inner | epoch 024:    157 / 841 symm_kl=0.546, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.058, ppl=2.08, wps=13261.7, ups=1.88, wpb=7048.5, bsz=244.6, num_updates=16700, lr=1.69536e-05, gnorm=0.729, train_wall=53, wall=0
2021-01-08 23:35:39 | INFO | train_inner | epoch 024:    257 / 841 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.051, ppl=2.07, wps=13143, ups=1.88, wpb=6988.9, bsz=243.4, num_updates=16800, lr=1.69031e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-08 23:36:32 | INFO | train_inner | epoch 024:    357 / 841 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.522, nll_loss=1.055, ppl=2.08, wps=13095.3, ups=1.88, wpb=6970.8, bsz=248.8, num_updates=16900, lr=1.6853e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-08 23:37:26 | INFO | train_inner | epoch 024:    457 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.513, nll_loss=1.049, ppl=2.07, wps=13088.2, ups=1.87, wpb=6988.4, bsz=244.2, num_updates=17000, lr=1.68034e-05, gnorm=0.719, train_wall=53, wall=0
2021-01-08 23:38:19 | INFO | train_inner | epoch 024:    557 / 841 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.534, nll_loss=1.07, ppl=2.1, wps=13038.9, ups=1.89, wpb=6907.2, bsz=240.6, num_updates=17100, lr=1.67542e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-08 23:39:12 | INFO | train_inner | epoch 024:    657 / 841 symm_kl=0.542, self_kl=0, self_cv=0, loss=3.528, nll_loss=1.065, ppl=2.09, wps=12919.3, ups=1.88, wpb=6862.7, bsz=245.6, num_updates=17200, lr=1.67054e-05, gnorm=0.745, train_wall=53, wall=0
2021-01-08 23:40:05 | INFO | train_inner | epoch 024:    757 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.531, nll_loss=1.07, ppl=2.1, wps=13361.3, ups=1.9, wpb=7044.1, bsz=254.7, num_updates=17300, lr=1.6657e-05, gnorm=0.729, train_wall=53, wall=0
2021-01-08 23:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:40:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:40:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:40:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:41:17 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.36 | nll_loss 3.925 | ppl 15.19 | bleu 22.11 | wps 3336.4 | wpb 5162.1 | bsz 187.5 | num_updates 17384 | best_bleu 22.31
2021-01-08 23:41:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:41:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:41:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:41:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 24 @ 17384 updates, score 22.11) (writing took 2.7674978971481323 seconds)
2021-01-08 23:41:20 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-08 23:41:20 | INFO | train | epoch 024 | symm_kl 0.542 | self_kl 0 | self_cv 0 | loss 3.521 | nll_loss 1.057 | ppl 2.08 | wps 12246.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 17384 | lr 1.66167e-05 | gnorm 0.728 | train_wall 445 | wall 0
2021-01-08 23:41:20 | INFO | fairseq.trainer | begin training epoch 25
2021-01-08 23:41:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:41:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:41:32 | INFO | train_inner | epoch 025:     16 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.502, nll_loss=1.044, ppl=2.06, wps=8122.4, ups=1.15, wpb=7079.8, bsz=253.3, num_updates=17400, lr=1.66091e-05, gnorm=0.714, train_wall=53, wall=0
2021-01-08 23:42:24 | INFO | train_inner | epoch 025:    116 / 841 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.508, nll_loss=1.041, ppl=2.06, wps=13395.1, ups=1.91, wpb=7025.1, bsz=236.7, num_updates=17500, lr=1.65616e-05, gnorm=0.722, train_wall=52, wall=0
2021-01-08 23:43:18 | INFO | train_inner | epoch 025:    216 / 841 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.521, nll_loss=1.056, ppl=2.08, wps=13231.7, ups=1.87, wpb=7057.8, bsz=239.8, num_updates=17600, lr=1.65145e-05, gnorm=0.72, train_wall=53, wall=0
2021-01-08 23:44:10 | INFO | train_inner | epoch 025:    316 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.522, nll_loss=1.058, ppl=2.08, wps=13216.4, ups=1.9, wpb=6942.4, bsz=247, num_updates=17700, lr=1.64677e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-08 23:45:03 | INFO | train_inner | epoch 025:    416 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.045, ppl=2.06, wps=13121.6, ups=1.89, wpb=6951.2, bsz=250.8, num_updates=17800, lr=1.64214e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-08 23:45:56 | INFO | train_inner | epoch 025:    516 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.524, nll_loss=1.062, ppl=2.09, wps=13331.8, ups=1.9, wpb=7010.3, bsz=239.7, num_updates=17900, lr=1.63755e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-08 23:46:49 | INFO | train_inner | epoch 025:    616 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.528, nll_loss=1.067, ppl=2.09, wps=13416.1, ups=1.89, wpb=7100.8, bsz=254.1, num_updates=18000, lr=1.63299e-05, gnorm=0.723, train_wall=53, wall=0
2021-01-08 23:47:41 | INFO | train_inner | epoch 025:    716 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.516, nll_loss=1.058, ppl=2.08, wps=13347.5, ups=1.9, wpb=7010.8, bsz=251.8, num_updates=18100, lr=1.62848e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-08 23:48:34 | INFO | train_inner | epoch 025:    816 / 841 symm_kl=0.537, self_kl=0, self_cv=0, loss=3.512, nll_loss=1.056, ppl=2.08, wps=13048.9, ups=1.9, wpb=6885.6, bsz=252, num_updates=18200, lr=1.624e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-08 23:48:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:48:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:48:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:48:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:48:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:48:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:48:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:48:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:48:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:49:14 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.367 | nll_loss 3.929 | ppl 15.24 | bleu 21.95 | wps 3351.6 | wpb 5162.1 | bsz 187.5 | num_updates 18225 | best_bleu 22.31
2021-01-08 23:49:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:49:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:49:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:49:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 25 @ 18225 updates, score 21.95) (writing took 2.746009713038802 seconds)
2021-01-08 23:49:17 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-08 23:49:17 | INFO | train | epoch 025 | symm_kl 0.54 | self_kl 0 | self_cv 0 | loss 3.517 | nll_loss 1.056 | ppl 2.08 | wps 12334.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 18225 | lr 1.62288e-05 | gnorm 0.727 | train_wall 441 | wall 0
2021-01-08 23:49:17 | INFO | fairseq.trainer | begin training epoch 26
2021-01-08 23:49:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:49:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:49:59 | INFO | train_inner | epoch 026:     75 / 841 symm_kl=0.534, self_kl=0, self_cv=0, loss=3.494, nll_loss=1.041, ppl=2.06, wps=8081.1, ups=1.17, wpb=6922.2, bsz=261.9, num_updates=18300, lr=1.61955e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-08 23:50:52 | INFO | train_inner | epoch 026:    175 / 841 symm_kl=0.539, self_kl=0, self_cv=0, loss=3.5, nll_loss=1.039, ppl=2.06, wps=13306.8, ups=1.9, wpb=6994, bsz=245, num_updates=18400, lr=1.61515e-05, gnorm=0.727, train_wall=52, wall=0
2021-01-08 23:51:44 | INFO | train_inner | epoch 026:    275 / 841 symm_kl=0.54, self_kl=0, self_cv=0, loss=3.518, nll_loss=1.056, ppl=2.08, wps=13259.1, ups=1.91, wpb=6934.7, bsz=234, num_updates=18500, lr=1.61077e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-08 23:52:37 | INFO | train_inner | epoch 026:    375 / 841 symm_kl=0.536, self_kl=0, self_cv=0, loss=3.498, nll_loss=1.042, ppl=2.06, wps=13455.8, ups=1.9, wpb=7099.1, bsz=267.1, num_updates=18600, lr=1.60644e-05, gnorm=0.709, train_wall=53, wall=0
2021-01-08 23:53:30 | INFO | train_inner | epoch 026:    475 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.061, ppl=2.09, wps=13298.3, ups=1.89, wpb=7021.5, bsz=251.7, num_updates=18700, lr=1.60214e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-08 23:54:23 | INFO | train_inner | epoch 026:    575 / 841 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.527, nll_loss=1.065, ppl=2.09, wps=13363.9, ups=1.89, wpb=7082.6, bsz=240.9, num_updates=18800, lr=1.59787e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-08 23:55:16 | INFO | train_inner | epoch 026:    675 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.518, nll_loss=1.06, ppl=2.08, wps=13255.4, ups=1.89, wpb=7013.7, bsz=245.2, num_updates=18900, lr=1.59364e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-08 23:56:09 | INFO | train_inner | epoch 026:    775 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.062, ppl=2.09, wps=13177.7, ups=1.9, wpb=6948, bsz=237.9, num_updates=19000, lr=1.58944e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-08 23:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 23:56:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:56:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:56:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:56:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:56:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 23:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 23:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 23:57:11 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.367 | nll_loss 3.93 | ppl 15.24 | bleu 22.02 | wps 3340.5 | wpb 5162.1 | bsz 187.5 | num_updates 19066 | best_bleu 22.31
2021-01-08 23:57:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 23:57:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:57:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:57:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 26 @ 19066 updates, score 22.02) (writing took 2.7858347799628973 seconds)
2021-01-08 23:57:14 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-08 23:57:14 | INFO | train | epoch 026 | symm_kl 0.538 | self_kl 0 | self_cv 0 | loss 3.513 | nll_loss 1.054 | ppl 2.08 | wps 12340.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 19066 | lr 1.58669e-05 | gnorm 0.729 | train_wall 441 | wall 0
2021-01-08 23:57:14 | INFO | fairseq.trainer | begin training epoch 27
2021-01-08 23:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 23:57:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 23:57:34 | INFO | train_inner | epoch 027:     34 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.526, nll_loss=1.069, ppl=2.1, wps=7993.4, ups=1.16, wpb=6867.4, bsz=233.5, num_updates=19100, lr=1.58527e-05, gnorm=0.746, train_wall=52, wall=0
2021-01-08 23:58:27 | INFO | train_inner | epoch 027:    134 / 841 symm_kl=0.539, self_kl=0, self_cv=0, loss=3.515, nll_loss=1.056, ppl=2.08, wps=13385.5, ups=1.89, wpb=7097.4, bsz=252.8, num_updates=19200, lr=1.58114e-05, gnorm=0.721, train_wall=53, wall=0
2021-01-08 23:59:20 | INFO | train_inner | epoch 027:    234 / 841 symm_kl=0.54, self_kl=0, self_cv=0, loss=3.521, nll_loss=1.059, ppl=2.08, wps=13165.1, ups=1.9, wpb=6941.9, bsz=227.5, num_updates=19300, lr=1.57704e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 00:00:14 | INFO | train_inner | epoch 027:    334 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.512, nll_loss=1.053, ppl=2.07, wps=13130.6, ups=1.87, wpb=7006.4, bsz=245.8, num_updates=19400, lr=1.57297e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 00:01:06 | INFO | train_inner | epoch 027:    434 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.492, nll_loss=1.044, ppl=2.06, wps=13282.4, ups=1.89, wpb=7028.6, bsz=264.2, num_updates=19500, lr=1.56893e-05, gnorm=0.721, train_wall=53, wall=0
2021-01-09 00:02:00 | INFO | train_inner | epoch 027:    534 / 841 symm_kl=0.536, self_kl=0, self_cv=0, loss=3.497, nll_loss=1.041, ppl=2.06, wps=13302.8, ups=1.88, wpb=7084.5, bsz=239.9, num_updates=19600, lr=1.56492e-05, gnorm=0.719, train_wall=53, wall=0
2021-01-09 00:02:53 | INFO | train_inner | epoch 027:    634 / 841 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.508, nll_loss=1.058, ppl=2.08, wps=13094.1, ups=1.9, wpb=6907.8, bsz=252.4, num_updates=19700, lr=1.56094e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 00:03:45 | INFO | train_inner | epoch 027:    734 / 841 symm_kl=0.535, self_kl=0, self_cv=0, loss=3.505, nll_loss=1.051, ppl=2.07, wps=13177.1, ups=1.89, wpb=6968.9, bsz=242.7, num_updates=19800, lr=1.557e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 00:04:38 | INFO | train_inner | epoch 027:    834 / 841 symm_kl=0.537, self_kl=0, self_cv=0, loss=3.525, nll_loss=1.069, ppl=2.1, wps=13193.1, ups=1.9, wpb=6953.8, bsz=247.1, num_updates=19900, lr=1.55308e-05, gnorm=0.744, train_wall=53, wall=0
2021-01-09 00:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:04:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:05:09 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.368 | nll_loss 3.935 | ppl 15.29 | bleu 22.01 | wps 3363.2 | wpb 5162.1 | bsz 187.5 | num_updates 19907 | best_bleu 22.31
2021-01-09 00:05:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:05:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 27 @ 19907 updates, score 22.01) (writing took 2.734806487336755 seconds)
2021-01-09 00:05:12 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-09 00:05:12 | INFO | train | epoch 027 | symm_kl 0.536 | self_kl 0 | self_cv 0 | loss 3.509 | nll_loss 1.054 | ppl 2.08 | wps 12292.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 19907 | lr 1.55281e-05 | gnorm 0.731 | train_wall 443 | wall 0
2021-01-09 00:05:12 | INFO | fairseq.trainer | begin training epoch 28
2021-01-09 00:05:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:05:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:06:03 | INFO | train_inner | epoch 028:     93 / 841 symm_kl=0.529, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.018, ppl=2.03, wps=8120.9, ups=1.17, wpb=6927.4, bsz=259.4, num_updates=20000, lr=1.54919e-05, gnorm=0.72, train_wall=52, wall=0
2021-01-09 00:06:56 | INFO | train_inner | epoch 028:    193 / 841 symm_kl=0.539, self_kl=0, self_cv=0, loss=3.52, nll_loss=1.06, ppl=2.09, wps=13140.7, ups=1.89, wpb=6940.1, bsz=232.2, num_updates=20100, lr=1.54533e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 00:07:49 | INFO | train_inner | epoch 028:    293 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.491, nll_loss=1.043, ppl=2.06, wps=13226.9, ups=1.88, wpb=7021.5, bsz=236.2, num_updates=20200, lr=1.5415e-05, gnorm=0.723, train_wall=53, wall=0
2021-01-09 00:08:42 | INFO | train_inner | epoch 028:    393 / 841 symm_kl=0.536, self_kl=0, self_cv=0, loss=3.502, nll_loss=1.047, ppl=2.07, wps=13373.6, ups=1.89, wpb=7071.4, bsz=247, num_updates=20300, lr=1.5377e-05, gnorm=0.717, train_wall=53, wall=0
2021-01-09 00:09:35 | INFO | train_inner | epoch 028:    493 / 841 symm_kl=0.534, self_kl=0, self_cv=0, loss=3.5, nll_loss=1.047, ppl=2.07, wps=13239.2, ups=1.9, wpb=6983.7, bsz=247.7, num_updates=20400, lr=1.53393e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:10:27 | INFO | train_inner | epoch 028:    593 / 841 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.539, nll_loss=1.085, ppl=2.12, wps=13267.9, ups=1.9, wpb=6974.9, bsz=235.5, num_updates=20500, lr=1.53018e-05, gnorm=0.74, train_wall=52, wall=0
2021-01-09 00:11:21 | INFO | train_inner | epoch 028:    693 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.494, nll_loss=1.046, ppl=2.07, wps=13226.2, ups=1.88, wpb=7020, bsz=264.1, num_updates=20600, lr=1.52647e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 00:12:14 | INFO | train_inner | epoch 028:    793 / 841 symm_kl=0.535, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.068, ppl=2.1, wps=13199.2, ups=1.89, wpb=6999.3, bsz=251.7, num_updates=20700, lr=1.52277e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 00:12:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:12:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:12:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:12:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:12:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:12:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:13:06 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.369 | nll_loss 3.935 | ppl 15.29 | bleu 22.03 | wps 3403.5 | wpb 5162.1 | bsz 187.5 | num_updates 20748 | best_bleu 22.31
2021-01-09 00:13:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:13:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:13:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:13:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 28 @ 20748 updates, score 22.03) (writing took 2.7397618778049946 seconds)
2021-01-09 00:13:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-09 00:13:09 | INFO | train | epoch 028 | symm_kl 0.534 | self_kl 0 | self_cv 0 | loss 3.505 | nll_loss 1.053 | ppl 2.07 | wps 12332 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 20748 | lr 1.52101e-05 | gnorm 0.729 | train_wall 442 | wall 0
2021-01-09 00:13:09 | INFO | fairseq.trainer | begin training epoch 29
2021-01-09 00:13:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:13:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:13:39 | INFO | train_inner | epoch 029:     52 / 841 symm_kl=0.534, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.051, ppl=2.07, wps=8132.2, ups=1.17, wpb=6951.8, bsz=240.7, num_updates=20800, lr=1.51911e-05, gnorm=0.733, train_wall=52, wall=0
2021-01-09 00:14:32 | INFO | train_inner | epoch 029:    152 / 841 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.497, nll_loss=1.047, ppl=2.07, wps=13553.1, ups=1.9, wpb=7128.6, bsz=257.6, num_updates=20900, lr=1.51547e-05, gnorm=0.71, train_wall=52, wall=0
2021-01-09 00:15:24 | INFO | train_inner | epoch 029:    252 / 841 symm_kl=0.534, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.051, ppl=2.07, wps=13373.3, ups=1.9, wpb=7025.7, bsz=239.9, num_updates=21000, lr=1.51186e-05, gnorm=0.736, train_wall=52, wall=0
2021-01-09 00:16:17 | INFO | train_inner | epoch 029:    352 / 841 symm_kl=0.534, self_kl=0, self_cv=0, loss=3.502, nll_loss=1.049, ppl=2.07, wps=13255.5, ups=1.88, wpb=7032.2, bsz=241.8, num_updates=21100, lr=1.50827e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 00:17:10 | INFO | train_inner | epoch 029:    452 / 841 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.501, nll_loss=1.049, ppl=2.07, wps=13418.7, ups=1.91, wpb=7018.6, bsz=248.1, num_updates=21200, lr=1.50471e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-09 00:18:02 | INFO | train_inner | epoch 029:    552 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.507, nll_loss=1.06, ppl=2.09, wps=13051.9, ups=1.9, wpb=6886.3, bsz=246.6, num_updates=21300, lr=1.50117e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 00:18:55 | INFO | train_inner | epoch 029:    652 / 841 symm_kl=0.53, self_kl=0, self_cv=0, loss=3.496, nll_loss=1.048, ppl=2.07, wps=13154.6, ups=1.9, wpb=6932.5, bsz=251.5, num_updates=21400, lr=1.49766e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 00:19:48 | INFO | train_inner | epoch 029:    752 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.499, nll_loss=1.049, ppl=2.07, wps=13085.5, ups=1.89, wpb=6930.2, bsz=250.4, num_updates=21500, lr=1.49417e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 00:20:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:20:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:20:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:20:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:20:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:20:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:21:03 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.371 | nll_loss 3.938 | ppl 15.33 | bleu 21.96 | wps 3333.2 | wpb 5162.1 | bsz 187.5 | num_updates 21589 | best_bleu 22.31
2021-01-09 00:21:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:21:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:21:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 29 @ 21589 updates, score 21.96) (writing took 2.733704386278987 seconds)
2021-01-09 00:21:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-09 00:21:06 | INFO | train | epoch 029 | symm_kl 0.532 | self_kl 0 | self_cv 0 | loss 3.502 | nll_loss 1.051 | ppl 2.07 | wps 12340.6 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 21589 | lr 1.49109e-05 | gnorm 0.73 | train_wall 441 | wall 0
2021-01-09 00:21:06 | INFO | fairseq.trainer | begin training epoch 30
2021-01-09 00:21:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:21:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:21:14 | INFO | train_inner | epoch 030:     11 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.51, nll_loss=1.061, ppl=2.09, wps=8130.7, ups=1.16, wpb=7026.8, bsz=245.9, num_updates=21600, lr=1.49071e-05, gnorm=0.727, train_wall=52, wall=0
2021-01-09 00:22:06 | INFO | train_inner | epoch 030:    111 / 841 symm_kl=0.53, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.035, ppl=2.05, wps=13507.6, ups=1.92, wpb=7025.7, bsz=248.8, num_updates=21700, lr=1.48727e-05, gnorm=0.719, train_wall=52, wall=0
2021-01-09 00:22:59 | INFO | train_inner | epoch 030:    211 / 841 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.502, nll_loss=1.052, ppl=2.07, wps=13397.8, ups=1.91, wpb=7000.1, bsz=241, num_updates=21800, lr=1.48386e-05, gnorm=0.729, train_wall=52, wall=0
2021-01-09 00:23:51 | INFO | train_inner | epoch 030:    311 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.486, nll_loss=1.045, ppl=2.06, wps=13139.5, ups=1.9, wpb=6902.2, bsz=246.9, num_updates=21900, lr=1.48047e-05, gnorm=0.727, train_wall=52, wall=0
2021-01-09 00:24:44 | INFO | train_inner | epoch 030:    411 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.055, ppl=2.08, wps=13432, ups=1.9, wpb=7069.9, bsz=244.4, num_updates=22000, lr=1.4771e-05, gnorm=0.729, train_wall=52, wall=0
2021-01-09 00:25:36 | INFO | train_inner | epoch 030:    511 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.496, nll_loss=1.047, ppl=2.07, wps=13102.8, ups=1.9, wpb=6891.9, bsz=250.8, num_updates=22100, lr=1.47375e-05, gnorm=0.745, train_wall=52, wall=0
2021-01-09 00:26:29 | INFO | train_inner | epoch 030:    611 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.509, nll_loss=1.061, ppl=2.09, wps=13352.7, ups=1.9, wpb=7041.6, bsz=250, num_updates=22200, lr=1.47043e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:27:23 | INFO | train_inner | epoch 030:    711 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.496, nll_loss=1.047, ppl=2.07, wps=13252.7, ups=1.87, wpb=7085.7, bsz=244.1, num_updates=22300, lr=1.46713e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-09 00:28:15 | INFO | train_inner | epoch 030:    811 / 841 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.516, nll_loss=1.067, ppl=2.09, wps=13261.2, ups=1.91, wpb=6943.8, bsz=237.1, num_updates=22400, lr=1.46385e-05, gnorm=0.743, train_wall=52, wall=0
2021-01-09 00:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:28:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:28:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:28:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:28:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:28:59 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.374 | nll_loss 3.94 | ppl 15.34 | bleu 21.93 | wps 3378.8 | wpb 5162.1 | bsz 187.5 | num_updates 22430 | best_bleu 22.31
2021-01-09 00:28:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:29:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:29:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:29:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 30 @ 22430 updates, score 21.93) (writing took 2.823744088411331 seconds)
2021-01-09 00:29:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-09 00:29:02 | INFO | train | epoch 030 | symm_kl 0.531 | self_kl 0 | self_cv 0 | loss 3.499 | nll_loss 1.051 | ppl 2.07 | wps 12355 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 22430 | lr 1.46287e-05 | gnorm 0.731 | train_wall 441 | wall 0
2021-01-09 00:29:02 | INFO | fairseq.trainer | begin training epoch 31
2021-01-09 00:29:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:29:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:29:41 | INFO | train_inner | epoch 031:     70 / 841 symm_kl=0.532, self_kl=0, self_cv=0, loss=3.498, nll_loss=1.048, ppl=2.07, wps=8103.7, ups=1.16, wpb=6968.6, bsz=239.6, num_updates=22500, lr=1.46059e-05, gnorm=0.736, train_wall=52, wall=0
2021-01-09 00:30:34 | INFO | train_inner | epoch 031:    170 / 841 symm_kl=0.53, self_kl=0, self_cv=0, loss=3.497, nll_loss=1.05, ppl=2.07, wps=13329.7, ups=1.89, wpb=7069.6, bsz=256.6, num_updates=22600, lr=1.45736e-05, gnorm=0.721, train_wall=53, wall=0
2021-01-09 00:31:27 | INFO | train_inner | epoch 031:    270 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.48, nll_loss=1.038, ppl=2.05, wps=13214.1, ups=1.89, wpb=7008.9, bsz=257.6, num_updates=22700, lr=1.45414e-05, gnorm=0.72, train_wall=53, wall=0
2021-01-09 00:32:20 | INFO | train_inner | epoch 031:    370 / 841 symm_kl=0.529, self_kl=0, self_cv=0, loss=3.493, nll_loss=1.046, ppl=2.06, wps=13213.5, ups=1.89, wpb=6973.3, bsz=240.8, num_updates=22800, lr=1.45095e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 00:33:12 | INFO | train_inner | epoch 031:    470 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.056, ppl=2.08, wps=13204.2, ups=1.9, wpb=6942.9, bsz=240.3, num_updates=22900, lr=1.44778e-05, gnorm=0.736, train_wall=52, wall=0
2021-01-09 00:34:05 | INFO | train_inner | epoch 031:    570 / 841 symm_kl=0.529, self_kl=0, self_cv=0, loss=3.499, nll_loss=1.054, ppl=2.08, wps=13153.7, ups=1.89, wpb=6965.1, bsz=232, num_updates=23000, lr=1.44463e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 00:34:58 | INFO | train_inner | epoch 031:    670 / 841 symm_kl=0.527, self_kl=0, self_cv=0, loss=3.49, nll_loss=1.047, ppl=2.07, wps=13247.8, ups=1.89, wpb=7023.6, bsz=260.4, num_updates=23100, lr=1.4415e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:35:52 | INFO | train_inner | epoch 031:    770 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.49, nll_loss=1.047, ppl=2.07, wps=13186.5, ups=1.87, wpb=7042.2, bsz=239.5, num_updates=23200, lr=1.43839e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 00:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:36:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:36:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:36:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:36:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:36:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:36:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:36:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:36:57 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.375 | nll_loss 3.94 | ppl 15.34 | bleu 21.95 | wps 3337.5 | wpb 5162.1 | bsz 187.5 | num_updates 23271 | best_bleu 22.31
2021-01-09 00:36:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:37:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 31 @ 23271 updates, score 21.95) (writing took 2.7895688712596893 seconds)
2021-01-09 00:37:00 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-09 00:37:00 | INFO | train | epoch 031 | symm_kl 0.529 | self_kl 0 | self_cv 0 | loss 3.496 | nll_loss 1.05 | ppl 2.07 | wps 12294.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 23271 | lr 1.43619e-05 | gnorm 0.73 | train_wall 443 | wall 0
2021-01-09 00:37:00 | INFO | fairseq.trainer | begin training epoch 32
2021-01-09 00:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:37:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:37:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:37:18 | INFO | train_inner | epoch 032:     29 / 841 symm_kl=0.53, self_kl=0, self_cv=0, loss=3.511, nll_loss=1.065, ppl=2.09, wps=7979.6, ups=1.16, wpb=6855.6, bsz=257.2, num_updates=23300, lr=1.4353e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 00:38:10 | INFO | train_inner | epoch 032:    129 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.492, nll_loss=1.048, ppl=2.07, wps=13178.7, ups=1.9, wpb=6949.2, bsz=240.2, num_updates=23400, lr=1.43223e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 00:39:03 | INFO | train_inner | epoch 032:    229 / 841 symm_kl=0.529, self_kl=0, self_cv=0, loss=3.497, nll_loss=1.053, ppl=2.08, wps=13388, ups=1.9, wpb=7041.8, bsz=249.3, num_updates=23500, lr=1.42918e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-09 00:39:56 | INFO | train_inner | epoch 032:    329 / 841 symm_kl=0.527, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.033, ppl=2.05, wps=13559.7, ups=1.89, wpb=7164.7, bsz=243.3, num_updates=23600, lr=1.42615e-05, gnorm=0.717, train_wall=53, wall=0
2021-01-09 00:40:49 | INFO | train_inner | epoch 032:    429 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.485, nll_loss=1.043, ppl=2.06, wps=13258, ups=1.9, wpb=6986.5, bsz=239.9, num_updates=23700, lr=1.42314e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:41:41 | INFO | train_inner | epoch 032:    529 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.496, nll_loss=1.053, ppl=2.07, wps=13272.9, ups=1.91, wpb=6944.3, bsz=244, num_updates=23800, lr=1.42014e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-09 00:42:33 | INFO | train_inner | epoch 032:    629 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.506, nll_loss=1.059, ppl=2.08, wps=13330.2, ups=1.91, wpb=6983, bsz=227.9, num_updates=23900, lr=1.41717e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 00:43:26 | INFO | train_inner | epoch 032:    729 / 841 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.045, ppl=2.06, wps=13186, ups=1.89, wpb=6962.5, bsz=262.6, num_updates=24000, lr=1.41421e-05, gnorm=0.728, train_wall=53, wall=0
2021-01-09 00:44:19 | INFO | train_inner | epoch 032:    829 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.498, nll_loss=1.059, ppl=2.08, wps=13219.3, ups=1.89, wpb=6997.6, bsz=263.9, num_updates=24100, lr=1.41128e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-09 00:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:44:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:44:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:44:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:44:53 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.377 | nll_loss 3.942 | ppl 15.37 | bleu 22.08 | wps 3287.8 | wpb 5162.1 | bsz 187.5 | num_updates 24112 | best_bleu 22.31
2021-01-09 00:44:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:44:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:44:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:44:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 32 @ 24112 updates, score 22.08) (writing took 2.8382079377770424 seconds)
2021-01-09 00:44:56 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-09 00:44:56 | INFO | train | epoch 032 | symm_kl 0.528 | self_kl 0 | self_cv 0 | loss 3.493 | nll_loss 1.05 | ppl 2.07 | wps 12347 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 24112 | lr 1.41093e-05 | gnorm 0.732 | train_wall 441 | wall 0
2021-01-09 00:44:56 | INFO | fairseq.trainer | begin training epoch 33
2021-01-09 00:44:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:44:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:45:45 | INFO | train_inner | epoch 033:     88 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.501, nll_loss=1.058, ppl=2.08, wps=8077.3, ups=1.17, wpb=6922.2, bsz=250.5, num_updates=24200, lr=1.40836e-05, gnorm=0.74, train_wall=51, wall=0
2021-01-09 00:46:38 | INFO | train_inner | epoch 033:    188 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.48, nll_loss=1.039, ppl=2.05, wps=13340.6, ups=1.89, wpb=7057.5, bsz=252.1, num_updates=24300, lr=1.40546e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-09 00:47:30 | INFO | train_inner | epoch 033:    288 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.483, nll_loss=1.039, ppl=2.05, wps=13341.3, ups=1.9, wpb=7028.8, bsz=253.6, num_updates=24400, lr=1.40257e-05, gnorm=0.724, train_wall=52, wall=0
2021-01-09 00:48:23 | INFO | train_inner | epoch 033:    388 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.485, nll_loss=1.046, ppl=2.06, wps=13303.8, ups=1.89, wpb=7022.7, bsz=254.5, num_updates=24500, lr=1.39971e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-09 00:49:16 | INFO | train_inner | epoch 033:    488 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.49, nll_loss=1.049, ppl=2.07, wps=13287.2, ups=1.89, wpb=7019.7, bsz=244.8, num_updates=24600, lr=1.39686e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:50:09 | INFO | train_inner | epoch 033:    588 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.5, nll_loss=1.056, ppl=2.08, wps=13015, ups=1.9, wpb=6857.3, bsz=225.8, num_updates=24700, lr=1.39403e-05, gnorm=0.752, train_wall=53, wall=0
2021-01-09 00:51:01 | INFO | train_inner | epoch 033:    688 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.494, nll_loss=1.053, ppl=2.08, wps=13424.6, ups=1.9, wpb=7059.3, bsz=248.2, num_updates=24800, lr=1.39122e-05, gnorm=0.734, train_wall=52, wall=0
2021-01-09 00:51:54 | INFO | train_inner | epoch 033:    788 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.5, nll_loss=1.061, ppl=2.09, wps=13032, ups=1.89, wpb=6891.7, bsz=238.3, num_updates=24900, lr=1.38842e-05, gnorm=0.748, train_wall=53, wall=0
2021-01-09 00:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 00:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:52:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:52:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 00:52:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 00:52:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 00:52:50 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.375 | nll_loss 3.943 | ppl 15.38 | bleu 21.97 | wps 3365.7 | wpb 5162.1 | bsz 187.5 | num_updates 24953 | best_bleu 22.31
2021-01-09 00:52:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 00:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:52:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:52:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 33 @ 24953 updates, score 21.97) (writing took 2.7387593165040016 seconds)
2021-01-09 00:52:52 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-09 00:52:52 | INFO | train | epoch 033 | symm_kl 0.526 | self_kl 0 | self_cv 0 | loss 3.49 | nll_loss 1.049 | ppl 2.07 | wps 12354 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 24953 | lr 1.38694e-05 | gnorm 0.733 | train_wall 441 | wall 0
2021-01-09 00:52:52 | INFO | fairseq.trainer | begin training epoch 34
2021-01-09 00:52:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 00:52:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 00:53:19 | INFO | train_inner | epoch 034:     47 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.49, nll_loss=1.05, ppl=2.07, wps=8304.3, ups=1.17, wpb=7086.7, bsz=249, num_updates=25000, lr=1.38564e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-09 00:54:13 | INFO | train_inner | epoch 034:    147 / 841 symm_kl=0.523, self_kl=0, self_cv=0, loss=3.48, nll_loss=1.042, ppl=2.06, wps=13196.7, ups=1.88, wpb=7005.5, bsz=259.4, num_updates=25100, lr=1.38288e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-09 00:55:05 | INFO | train_inner | epoch 034:    247 / 841 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.51, nll_loss=1.064, ppl=2.09, wps=13280.6, ups=1.91, wpb=6956, bsz=242.1, num_updates=25200, lr=1.38013e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 00:55:57 | INFO | train_inner | epoch 034:    347 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.486, nll_loss=1.046, ppl=2.06, wps=13342.3, ups=1.91, wpb=6993.1, bsz=247.7, num_updates=25300, lr=1.3774e-05, gnorm=0.728, train_wall=52, wall=0
2021-01-09 00:56:50 | INFO | train_inner | epoch 034:    447 / 841 symm_kl=0.523, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.048, ppl=2.07, wps=13238.2, ups=1.89, wpb=6993.4, bsz=250.7, num_updates=25400, lr=1.37469e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 00:57:43 | INFO | train_inner | epoch 034:    547 / 841 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.037, ppl=2.05, wps=13100.3, ups=1.91, wpb=6868.7, bsz=246.2, num_updates=25500, lr=1.37199e-05, gnorm=0.743, train_wall=52, wall=0
2021-01-09 00:58:36 | INFO | train_inner | epoch 034:    647 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.037, ppl=2.05, wps=13361.7, ups=1.87, wpb=7133.4, bsz=268.8, num_updates=25600, lr=1.36931e-05, gnorm=0.715, train_wall=53, wall=0
2021-01-09 00:59:29 | INFO | train_inner | epoch 034:    747 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.039, ppl=2.05, wps=13149.8, ups=1.88, wpb=7008.9, bsz=233.7, num_updates=25700, lr=1.36664e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 01:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:00:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:00:50 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.376 | nll_loss 3.944 | ppl 15.4 | bleu 21.9 | wps 2976.2 | wpb 5162.1 | bsz 187.5 | num_updates 25794 | best_bleu 22.31
2021-01-09 01:00:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:00:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:00:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:00:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 34 @ 25794 updates, score 21.9) (writing took 2.7422172222286463 seconds)
2021-01-09 01:00:52 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-09 01:00:52 | INFO | train | epoch 034 | symm_kl 0.525 | self_kl 0 | self_cv 0 | loss 3.487 | nll_loss 1.048 | ppl 2.07 | wps 12251.6 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 25794 | lr 1.36415e-05 | gnorm 0.732 | train_wall 442 | wall 0
2021-01-09 01:00:52 | INFO | fairseq.trainer | begin training epoch 35
2021-01-09 01:00:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:00:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:00:59 | INFO | train_inner | epoch 035:      6 / 841 symm_kl=0.528, self_kl=0, self_cv=0, loss=3.51, nll_loss=1.068, ppl=2.1, wps=7759.3, ups=1.12, wpb=6939.7, bsz=225.9, num_updates=25800, lr=1.36399e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 01:01:51 | INFO | train_inner | epoch 035:    106 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.032, ppl=2.04, wps=13418.6, ups=1.92, wpb=7002.7, bsz=246.1, num_updates=25900, lr=1.36135e-05, gnorm=0.725, train_wall=52, wall=0
2021-01-09 01:02:44 | INFO | train_inner | epoch 035:    206 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.039, ppl=2.05, wps=13208.9, ups=1.88, wpb=7017.3, bsz=246.9, num_updates=26000, lr=1.35873e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-09 01:03:37 | INFO | train_inner | epoch 035:    306 / 841 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.044, ppl=2.06, wps=13193.5, ups=1.89, wpb=6991, bsz=254.2, num_updates=26100, lr=1.35613e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 01:04:30 | INFO | train_inner | epoch 035:    406 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.033, ppl=2.05, wps=13256.9, ups=1.9, wpb=6982.5, bsz=248, num_updates=26200, lr=1.35354e-05, gnorm=0.73, train_wall=52, wall=0
2021-01-09 01:05:23 | INFO | train_inner | epoch 035:    506 / 841 symm_kl=0.527, self_kl=0, self_cv=0, loss=3.499, nll_loss=1.057, ppl=2.08, wps=13309.8, ups=1.89, wpb=7033.9, bsz=240, num_updates=26300, lr=1.35096e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 01:06:16 | INFO | train_inner | epoch 035:    606 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.493, nll_loss=1.055, ppl=2.08, wps=13311.8, ups=1.88, wpb=7071.2, bsz=248.2, num_updates=26400, lr=1.3484e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 01:07:08 | INFO | train_inner | epoch 035:    706 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.048, ppl=2.07, wps=13264.2, ups=1.9, wpb=6985.1, bsz=255, num_updates=26500, lr=1.34585e-05, gnorm=0.74, train_wall=52, wall=0
2021-01-09 01:08:01 | INFO | train_inner | epoch 035:    806 / 841 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.064, ppl=2.09, wps=13090.8, ups=1.9, wpb=6888.8, bsz=236.8, num_updates=26600, lr=1.34332e-05, gnorm=0.752, train_wall=52, wall=0
2021-01-09 01:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:08:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:08:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:08:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:08:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:08:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:08:46 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.377 | nll_loss 3.944 | ppl 15.39 | bleu 22.08 | wps 3410.5 | wpb 5162.1 | bsz 187.5 | num_updates 26635 | best_bleu 22.31
2021-01-09 01:08:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:08:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 35 @ 26635 updates, score 22.08) (writing took 2.722022633999586 seconds)
2021-01-09 01:08:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-09 01:08:49 | INFO | train | epoch 035 | symm_kl 0.523 | self_kl 0 | self_cv 0 | loss 3.484 | nll_loss 1.047 | ppl 2.07 | wps 12334 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 26635 | lr 1.34244e-05 | gnorm 0.734 | train_wall 442 | wall 0
2021-01-09 01:08:49 | INFO | fairseq.trainer | begin training epoch 36
2021-01-09 01:08:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:08:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:09:26 | INFO | train_inner | epoch 036:     65 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.044, ppl=2.06, wps=8163.4, ups=1.18, wpb=6925.9, bsz=241.8, num_updates=26700, lr=1.3408e-05, gnorm=0.737, train_wall=51, wall=0
2021-01-09 01:10:18 | INFO | train_inner | epoch 036:    165 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.49, nll_loss=1.051, ppl=2.07, wps=13046.1, ups=1.91, wpb=6841.9, bsz=239, num_updates=26800, lr=1.3383e-05, gnorm=0.75, train_wall=52, wall=0
2021-01-09 01:11:11 | INFO | train_inner | epoch 036:    265 / 841 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.038, ppl=2.05, wps=13331.9, ups=1.9, wpb=7033, bsz=237.7, num_updates=26900, lr=1.33581e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-09 01:12:04 | INFO | train_inner | epoch 036:    365 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.492, nll_loss=1.054, ppl=2.08, wps=13358.7, ups=1.9, wpb=7030.5, bsz=248.8, num_updates=27000, lr=1.33333e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-09 01:12:56 | INFO | train_inner | epoch 036:    465 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.041, ppl=2.06, wps=13430.2, ups=1.9, wpb=7050.6, bsz=263.2, num_updates=27100, lr=1.33087e-05, gnorm=0.726, train_wall=52, wall=0
2021-01-09 01:13:49 | INFO | train_inner | epoch 036:    565 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.048, ppl=2.07, wps=13334.9, ups=1.89, wpb=7050.8, bsz=249.6, num_updates=27200, lr=1.32842e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-09 01:14:42 | INFO | train_inner | epoch 036:    665 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.049, ppl=2.07, wps=13427.3, ups=1.89, wpb=7117.2, bsz=247.4, num_updates=27300, lr=1.32599e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-09 01:15:35 | INFO | train_inner | epoch 036:    765 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.032, ppl=2.04, wps=13411.2, ups=1.9, wpb=7066.8, bsz=253.9, num_updates=27400, lr=1.32357e-05, gnorm=0.718, train_wall=53, wall=0
2021-01-09 01:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:16:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:16:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:16:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:16:42 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.382 | nll_loss 3.949 | ppl 15.45 | bleu 21.95 | wps 3345.9 | wpb 5162.1 | bsz 187.5 | num_updates 27476 | best_bleu 22.31
2021-01-09 01:16:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:16:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:16:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:16:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 36 @ 27476 updates, score 21.95) (writing took 2.806174272671342 seconds)
2021-01-09 01:16:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-09 01:16:45 | INFO | train | epoch 036 | symm_kl 0.522 | self_kl 0 | self_cv 0 | loss 3.482 | nll_loss 1.047 | ppl 2.07 | wps 12353.4 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 27476 | lr 1.32173e-05 | gnorm 0.733 | train_wall 441 | wall 0
2021-01-09 01:16:45 | INFO | fairseq.trainer | begin training epoch 37
2021-01-09 01:16:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:16:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:17:01 | INFO | train_inner | epoch 037:     24 / 841 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.506, nll_loss=1.069, ppl=2.1, wps=7932, ups=1.16, wpb=6816.7, bsz=230.2, num_updates=27500, lr=1.32116e-05, gnorm=0.754, train_wall=52, wall=0
2021-01-09 01:17:53 | INFO | train_inner | epoch 037:    124 / 841 symm_kl=0.523, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.045, ppl=2.06, wps=13249, ups=1.92, wpb=6916.3, bsz=236.9, num_updates=27600, lr=1.31876e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 01:18:45 | INFO | train_inner | epoch 037:    224 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.038, ppl=2.05, wps=13294.4, ups=1.9, wpb=6987.3, bsz=253.4, num_updates=27700, lr=1.31638e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-09 01:19:37 | INFO | train_inner | epoch 037:    324 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.048, ppl=2.07, wps=13462.4, ups=1.92, wpb=7007.8, bsz=236.6, num_updates=27800, lr=1.31401e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-09 01:20:30 | INFO | train_inner | epoch 037:    424 / 841 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.038, ppl=2.05, wps=13352.4, ups=1.9, wpb=7009.3, bsz=247, num_updates=27900, lr=1.31165e-05, gnorm=0.727, train_wall=52, wall=0
2021-01-09 01:21:23 | INFO | train_inner | epoch 037:    524 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.037, ppl=2.05, wps=13186.1, ups=1.9, wpb=6944, bsz=258.3, num_updates=28000, lr=1.30931e-05, gnorm=0.734, train_wall=52, wall=0
2021-01-09 01:22:16 | INFO | train_inner | epoch 037:    624 / 841 symm_kl=0.524, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.044, ppl=2.06, wps=13387, ups=1.88, wpb=7104.6, bsz=244.5, num_updates=28100, lr=1.30698e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-09 01:23:08 | INFO | train_inner | epoch 037:    724 / 841 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.066, ppl=2.09, wps=13418.1, ups=1.9, wpb=7060.6, bsz=247.7, num_updates=28200, lr=1.30466e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-09 01:24:01 | INFO | train_inner | epoch 037:    824 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.041, ppl=2.06, wps=13217.3, ups=1.89, wpb=6981.5, bsz=257.8, num_updates=28300, lr=1.30235e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 01:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:24:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:24:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:24:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:24:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:24:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:24:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:24:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:24:38 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.385 | nll_loss 3.955 | ppl 15.5 | bleu 21.99 | wps 3263.7 | wpb 5162.1 | bsz 187.5 | num_updates 28317 | best_bleu 22.31
2021-01-09 01:24:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:24:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:24:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:24:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 37 @ 28317 updates, score 21.99) (writing took 2.7482720613479614 seconds)
2021-01-09 01:24:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-09 01:24:41 | INFO | train | epoch 037 | symm_kl 0.521 | self_kl 0 | self_cv 0 | loss 3.479 | nll_loss 1.045 | ppl 2.06 | wps 12362.2 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 28317 | lr 1.30196e-05 | gnorm 0.735 | train_wall 440 | wall 0
2021-01-09 01:24:41 | INFO | fairseq.trainer | begin training epoch 38
2021-01-09 01:24:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:24:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:25:27 | INFO | train_inner | epoch 038:     83 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.043, ppl=2.06, wps=7861.3, ups=1.16, wpb=6751.6, bsz=240.6, num_updates=28400, lr=1.30005e-05, gnorm=0.745, train_wall=51, wall=0
2021-01-09 01:26:20 | INFO | train_inner | epoch 038:    183 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.475, nll_loss=1.044, ppl=2.06, wps=12999.3, ups=1.88, wpb=6909.3, bsz=250.5, num_updates=28500, lr=1.29777e-05, gnorm=0.744, train_wall=53, wall=0
2021-01-09 01:27:13 | INFO | train_inner | epoch 038:    283 / 841 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.042, ppl=2.06, wps=13315.9, ups=1.88, wpb=7077.9, bsz=254.4, num_updates=28600, lr=1.2955e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 01:28:06 | INFO | train_inner | epoch 038:    383 / 841 symm_kl=0.521, self_kl=0, self_cv=0, loss=3.488, nll_loss=1.054, ppl=2.08, wps=13297.5, ups=1.9, wpb=7003.9, bsz=240.2, num_updates=28700, lr=1.29324e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-09 01:28:59 | INFO | train_inner | epoch 038:    483 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.462, nll_loss=1.035, ppl=2.05, wps=13270.5, ups=1.89, wpb=7019.7, bsz=258.6, num_updates=28800, lr=1.29099e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 01:29:52 | INFO | train_inner | epoch 038:    583 / 841 symm_kl=0.521, self_kl=0, self_cv=0, loss=3.483, nll_loss=1.05, ppl=2.07, wps=13409.1, ups=1.89, wpb=7096.4, bsz=233.7, num_updates=28900, lr=1.28876e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 01:30:45 | INFO | train_inner | epoch 038:    683 / 841 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.489, nll_loss=1.058, ppl=2.08, wps=13116.8, ups=1.88, wpb=6971.4, bsz=249.4, num_updates=29000, lr=1.28654e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 01:31:38 | INFO | train_inner | epoch 038:    783 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.038, ppl=2.05, wps=13335.3, ups=1.89, wpb=7064.3, bsz=244.2, num_updates=29100, lr=1.28432e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 01:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:32:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:32:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:32:36 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.382 | nll_loss 3.95 | ppl 15.46 | bleu 22 | wps 3416.3 | wpb 5162.1 | bsz 187.5 | num_updates 29158 | best_bleu 22.31
2021-01-09 01:32:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:32:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:32:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:32:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 38 @ 29158 updates, score 22.0) (writing took 2.802433719858527 seconds)
2021-01-09 01:32:39 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-09 01:32:39 | INFO | train | epoch 038 | symm_kl 0.519 | self_kl 0 | self_cv 0 | loss 3.476 | nll_loss 1.045 | ppl 2.06 | wps 12307.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 29158 | lr 1.28304e-05 | gnorm 0.734 | train_wall 443 | wall 0
2021-01-09 01:32:39 | INFO | fairseq.trainer | begin training epoch 39
2021-01-09 01:32:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:33:03 | INFO | train_inner | epoch 039:     42 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.482, nll_loss=1.047, ppl=2.07, wps=8060.5, ups=1.17, wpb=6884.6, bsz=244.2, num_updates=29200, lr=1.28212e-05, gnorm=0.749, train_wall=52, wall=0
2021-01-09 01:33:56 | INFO | train_inner | epoch 039:    142 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.037, ppl=2.05, wps=13330.8, ups=1.9, wpb=7009.6, bsz=243.4, num_updates=29300, lr=1.27993e-05, gnorm=0.729, train_wall=52, wall=0
2021-01-09 01:34:49 | INFO | train_inner | epoch 039:    242 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.036, ppl=2.05, wps=13299, ups=1.89, wpb=7040.2, bsz=253.7, num_updates=29400, lr=1.27775e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-09 01:35:42 | INFO | train_inner | epoch 039:    342 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.456, nll_loss=1.028, ppl=2.04, wps=13374.8, ups=1.89, wpb=7077.6, bsz=251.8, num_updates=29500, lr=1.27559e-05, gnorm=0.719, train_wall=53, wall=0
2021-01-09 01:36:35 | INFO | train_inner | epoch 039:    442 / 841 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.046, ppl=2.06, wps=13223.5, ups=1.89, wpb=7012.8, bsz=241.4, num_updates=29600, lr=1.27343e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-09 01:37:28 | INFO | train_inner | epoch 039:    542 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.048, ppl=2.07, wps=13336.8, ups=1.88, wpb=7077.2, bsz=246.2, num_updates=29700, lr=1.27128e-05, gnorm=0.729, train_wall=53, wall=0
2021-01-09 01:38:21 | INFO | train_inner | epoch 039:    642 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.491, nll_loss=1.063, ppl=2.09, wps=13141.6, ups=1.9, wpb=6923, bsz=247.6, num_updates=29800, lr=1.26915e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 01:39:13 | INFO | train_inner | epoch 039:    742 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.472, nll_loss=1.042, ppl=2.06, wps=13224.9, ups=1.9, wpb=6975.1, bsz=249.2, num_updates=29900, lr=1.26702e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 01:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:40:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:40:32 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.385 | nll_loss 3.955 | ppl 15.51 | bleu 21.88 | wps 3401.3 | wpb 5162.1 | bsz 187.5 | num_updates 29999 | best_bleu 22.31
2021-01-09 01:40:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:40:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:40:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:40:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 39 @ 29999 updates, score 21.88) (writing took 2.7275256030261517 seconds)
2021-01-09 01:40:35 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-09 01:40:35 | INFO | train | epoch 039 | symm_kl 0.518 | self_kl 0 | self_cv 0 | loss 3.474 | nll_loss 1.045 | ppl 2.06 | wps 12346.6 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 29999 | lr 1.26493e-05 | gnorm 0.735 | train_wall 441 | wall 0
2021-01-09 01:40:35 | INFO | fairseq.trainer | begin training epoch 40
2021-01-09 01:40:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:40:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:40:39 | INFO | train_inner | epoch 040:      1 / 841 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.479, nll_loss=1.052, ppl=2.07, wps=8046.4, ups=1.17, wpb=6901.7, bsz=239.3, num_updates=30000, lr=1.26491e-05, gnorm=0.75, train_wall=52, wall=0
2021-01-09 01:41:31 | INFO | train_inner | epoch 040:    101 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.039, ppl=2.05, wps=13578.6, ups=1.93, wpb=7036.4, bsz=223.3, num_updates=30100, lr=1.26281e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-09 01:42:24 | INFO | train_inner | epoch 040:    201 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.047, ppl=2.07, wps=13001.3, ups=1.9, wpb=6845.4, bsz=248, num_updates=30200, lr=1.26072e-05, gnorm=0.75, train_wall=52, wall=0
2021-01-09 01:43:16 | INFO | train_inner | epoch 040:    301 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.473, nll_loss=1.047, ppl=2.07, wps=13173.1, ups=1.9, wpb=6945.4, bsz=255.1, num_updates=30300, lr=1.25863e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 01:44:10 | INFO | train_inner | epoch 040:    401 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.458, nll_loss=1.032, ppl=2.04, wps=13265.4, ups=1.88, wpb=7070.3, bsz=249.4, num_updates=30400, lr=1.25656e-05, gnorm=0.722, train_wall=53, wall=0
2021-01-09 01:45:03 | INFO | train_inner | epoch 040:    501 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.461, nll_loss=1.036, ppl=2.05, wps=13133.6, ups=1.86, wpb=7062.2, bsz=247.4, num_updates=30500, lr=1.2545e-05, gnorm=0.727, train_wall=54, wall=0
2021-01-09 01:45:56 | INFO | train_inner | epoch 040:    601 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.473, nll_loss=1.044, ppl=2.06, wps=13278.5, ups=1.9, wpb=7006.3, bsz=239.4, num_updates=30600, lr=1.25245e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 01:46:49 | INFO | train_inner | epoch 040:    701 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.489, nll_loss=1.061, ppl=2.09, wps=13145.9, ups=1.88, wpb=6995.1, bsz=257.9, num_updates=30700, lr=1.25041e-05, gnorm=0.745, train_wall=53, wall=0
2021-01-09 01:47:43 | INFO | train_inner | epoch 040:    801 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.042, ppl=2.06, wps=13152.6, ups=1.87, wpb=7039.3, bsz=254, num_updates=30800, lr=1.24838e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 01:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:48:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:48:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:48:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:48:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:48:32 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.385 | nll_loss 3.954 | ppl 15.5 | bleu 22 | wps 3387.5 | wpb 5162.1 | bsz 187.5 | num_updates 30840 | best_bleu 22.31
2021-01-09 01:48:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:48:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:48:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:48:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 40 @ 30840 updates, score 22.0) (writing took 2.72427299618721 seconds)
2021-01-09 01:48:34 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-09 01:48:34 | INFO | train | epoch 040 | symm_kl 0.517 | self_kl 0 | self_cv 0 | loss 3.472 | nll_loss 1.043 | ppl 2.06 | wps 12274 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 30840 | lr 1.24757e-05 | gnorm 0.735 | train_wall 444 | wall 0
2021-01-09 01:48:34 | INFO | fairseq.trainer | begin training epoch 41
2021-01-09 01:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:49:09 | INFO | train_inner | epoch 041:     60 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.04, ppl=2.06, wps=8068.6, ups=1.17, wpb=6923.1, bsz=250.5, num_updates=30900, lr=1.24635e-05, gnorm=0.733, train_wall=52, wall=0
2021-01-09 01:50:02 | INFO | train_inner | epoch 041:    160 / 841 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.048, ppl=2.07, wps=13199.4, ups=1.89, wpb=6979.5, bsz=240.9, num_updates=31000, lr=1.24434e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 01:50:55 | INFO | train_inner | epoch 041:    260 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.475, nll_loss=1.044, ppl=2.06, wps=13087.8, ups=1.87, wpb=6984.7, bsz=228.7, num_updates=31100, lr=1.24234e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 01:51:48 | INFO | train_inner | epoch 041:    360 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.013, ppl=2.02, wps=13358.6, ups=1.88, wpb=7114.4, bsz=257.7, num_updates=31200, lr=1.24035e-05, gnorm=0.713, train_wall=53, wall=0
2021-01-09 01:52:41 | INFO | train_inner | epoch 041:    460 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.472, nll_loss=1.05, ppl=2.07, wps=13283.2, ups=1.9, wpb=6974.5, bsz=273.4, num_updates=31300, lr=1.23836e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-09 01:53:33 | INFO | train_inner | epoch 041:    560 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.044, ppl=2.06, wps=13287.5, ups=1.9, wpb=6981.9, bsz=246.2, num_updates=31400, lr=1.23639e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-09 01:54:26 | INFO | train_inner | epoch 041:    660 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.494, nll_loss=1.064, ppl=2.09, wps=13084.1, ups=1.88, wpb=6960.1, bsz=248.4, num_updates=31500, lr=1.23443e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 01:55:19 | INFO | train_inner | epoch 041:    760 / 841 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.046, ppl=2.06, wps=13005, ups=1.88, wpb=6902.8, bsz=223.7, num_updates=31600, lr=1.23247e-05, gnorm=0.756, train_wall=53, wall=0
2021-01-09 01:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 01:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 01:56:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 01:56:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 01:56:32 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.389 | nll_loss 3.958 | ppl 15.54 | bleu 21.92 | wps 3125.9 | wpb 5162.1 | bsz 187.5 | num_updates 31681 | best_bleu 22.31
2021-01-09 01:56:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 01:56:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:56:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 41 @ 31681 updates, score 21.92) (writing took 2.7467970475554466 seconds)
2021-01-09 01:56:35 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-09 01:56:35 | INFO | train | epoch 041 | symm_kl 0.516 | self_kl 0 | self_cv 0 | loss 3.47 | nll_loss 1.043 | ppl 2.06 | wps 12246.7 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 31681 | lr 1.2309e-05 | gnorm 0.735 | train_wall 443 | wall 0
2021-01-09 01:56:35 | INFO | fairseq.trainer | begin training epoch 42
2021-01-09 01:56:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 01:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 01:56:47 | INFO | train_inner | epoch 042:     19 / 841 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.043, ppl=2.06, wps=7979.7, ups=1.14, wpb=7013.8, bsz=242.9, num_updates=31700, lr=1.23053e-05, gnorm=0.738, train_wall=52, wall=0
2021-01-09 01:57:40 | INFO | train_inner | epoch 042:    119 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.02, ppl=2.03, wps=13550.6, ups=1.92, wpb=7073.2, bsz=241.4, num_updates=31800, lr=1.22859e-05, gnorm=0.719, train_wall=52, wall=0
2021-01-09 01:58:32 | INFO | train_inner | epoch 042:    219 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.045, ppl=2.06, wps=13065.6, ups=1.9, wpb=6869.5, bsz=241.4, num_updates=31900, lr=1.22666e-05, gnorm=0.747, train_wall=52, wall=0
2021-01-09 01:59:25 | INFO | train_inner | epoch 042:    319 / 841 symm_kl=0.522, self_kl=0, self_cv=0, loss=3.493, nll_loss=1.059, ppl=2.08, wps=13310.8, ups=1.89, wpb=7040, bsz=240.5, num_updates=32000, lr=1.22474e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 02:00:18 | INFO | train_inner | epoch 042:    419 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.027, ppl=2.04, wps=13315.1, ups=1.89, wpb=7043.8, bsz=264.1, num_updates=32100, lr=1.22284e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 02:01:11 | INFO | train_inner | epoch 042:    519 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.044, ppl=2.06, wps=13314.4, ups=1.89, wpb=7051.3, bsz=242.2, num_updates=32200, lr=1.22094e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 02:02:04 | INFO | train_inner | epoch 042:    619 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.043, ppl=2.06, wps=13317.5, ups=1.89, wpb=7040.7, bsz=259.2, num_updates=32300, lr=1.21904e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 02:02:57 | INFO | train_inner | epoch 042:    719 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.042, ppl=2.06, wps=13105.3, ups=1.89, wpb=6934.4, bsz=244.5, num_updates=32400, lr=1.21716e-05, gnorm=0.746, train_wall=53, wall=0
2021-01-09 02:03:49 | INFO | train_inner | epoch 042:    819 / 841 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.481, nll_loss=1.054, ppl=2.08, wps=13256.5, ups=1.91, wpb=6950.8, bsz=239.9, num_updates=32500, lr=1.21529e-05, gnorm=0.745, train_wall=52, wall=0
2021-01-09 02:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:04:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:04:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:04:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:04:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:04:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:04:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:04:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:04:29 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.389 | nll_loss 3.957 | ppl 15.54 | bleu 22.02 | wps 3293.6 | wpb 5162.1 | bsz 187.5 | num_updates 32522 | best_bleu 22.31
2021-01-09 02:04:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:04:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:04:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 42 @ 32522 updates, score 22.02) (writing took 2.7985679265111685 seconds)
2021-01-09 02:04:32 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-09 02:04:32 | INFO | train | epoch 042 | symm_kl 0.515 | self_kl 0 | self_cv 0 | loss 3.468 | nll_loss 1.043 | ppl 2.06 | wps 12318.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 32522 | lr 1.21488e-05 | gnorm 0.738 | train_wall 442 | wall 0
2021-01-09 02:04:32 | INFO | fairseq.trainer | begin training epoch 43
2021-01-09 02:04:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:04:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:05:16 | INFO | train_inner | epoch 043:     78 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.458, nll_loss=1.035, ppl=2.05, wps=7993.2, ups=1.16, wpb=6918.1, bsz=244.6, num_updates=32600, lr=1.21342e-05, gnorm=0.742, train_wall=52, wall=0
2021-01-09 02:06:09 | INFO | train_inner | epoch 043:    178 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.449, nll_loss=1.028, ppl=2.04, wps=13208.6, ups=1.88, wpb=7010.5, bsz=254.6, num_updates=32700, lr=1.21157e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-09 02:07:01 | INFO | train_inner | epoch 043:    278 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.487, nll_loss=1.058, ppl=2.08, wps=13295.9, ups=1.9, wpb=6989, bsz=228.7, num_updates=32800, lr=1.20972e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 02:07:54 | INFO | train_inner | epoch 043:    378 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.04, ppl=2.06, wps=13125.3, ups=1.89, wpb=6932.2, bsz=250.8, num_updates=32900, lr=1.20788e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 02:08:48 | INFO | train_inner | epoch 043:    478 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.485, nll_loss=1.062, ppl=2.09, wps=13110.9, ups=1.87, wpb=7018.2, bsz=238.2, num_updates=33000, lr=1.20605e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 02:09:41 | INFO | train_inner | epoch 043:    578 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.031, ppl=2.04, wps=13202.9, ups=1.89, wpb=7001.5, bsz=257.5, num_updates=33100, lr=1.20422e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:10:34 | INFO | train_inner | epoch 043:    678 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.044, ppl=2.06, wps=13151.7, ups=1.88, wpb=7013.9, bsz=254.3, num_updates=33200, lr=1.20241e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 02:11:27 | INFO | train_inner | epoch 043:    778 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.462, nll_loss=1.038, ppl=2.05, wps=13273.1, ups=1.89, wpb=7017.4, bsz=244.8, num_updates=33300, lr=1.2006e-05, gnorm=0.737, train_wall=53, wall=0
2021-01-09 02:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:12:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:12:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:12:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:12:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:12:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:12:28 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.391 | nll_loss 3.962 | ppl 15.58 | bleu 21.95 | wps 3304 | wpb 5162.1 | bsz 187.5 | num_updates 33363 | best_bleu 22.31
2021-01-09 02:12:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:12:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:12:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:12:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 43 @ 33363 updates, score 21.95) (writing took 2.793250847607851 seconds)
2021-01-09 02:12:31 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-09 02:12:31 | INFO | train | epoch 043 | symm_kl 0.514 | self_kl 0 | self_cv 0 | loss 3.465 | nll_loss 1.041 | ppl 2.06 | wps 12277.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 33363 | lr 1.19947e-05 | gnorm 0.738 | train_wall 443 | wall 0
2021-01-09 02:12:31 | INFO | fairseq.trainer | begin training epoch 44
2021-01-09 02:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:12:53 | INFO | train_inner | epoch 044:     37 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.463, nll_loss=1.04, ppl=2.06, wps=8086.3, ups=1.16, wpb=6971.9, bsz=251.4, num_updates=33400, lr=1.1988e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 02:13:46 | INFO | train_inner | epoch 044:    137 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.462, nll_loss=1.036, ppl=2.05, wps=13216.9, ups=1.88, wpb=7027.8, bsz=247.4, num_updates=33500, lr=1.19701e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 02:14:39 | INFO | train_inner | epoch 044:    237 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.021, ppl=2.03, wps=13254.6, ups=1.89, wpb=7000.9, bsz=251.1, num_updates=33600, lr=1.19523e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 02:15:32 | INFO | train_inner | epoch 044:    337 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.033, ppl=2.05, wps=13241.1, ups=1.88, wpb=7025.2, bsz=242.6, num_updates=33700, lr=1.19345e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:16:25 | INFO | train_inner | epoch 044:    437 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.035, ppl=2.05, wps=13109.1, ups=1.88, wpb=6976.7, bsz=252.1, num_updates=33800, lr=1.19169e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:17:18 | INFO | train_inner | epoch 044:    537 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.022, ppl=2.03, wps=13251, ups=1.9, wpb=6973.9, bsz=253.2, num_updates=33900, lr=1.18993e-05, gnorm=0.73, train_wall=52, wall=0
2021-01-09 02:18:11 | INFO | train_inner | epoch 044:    637 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.493, nll_loss=1.07, ppl=2.1, wps=13287.3, ups=1.89, wpb=7039.6, bsz=253.7, num_updates=34000, lr=1.18818e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-09 02:19:04 | INFO | train_inner | epoch 044:    737 / 841 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.487, nll_loss=1.057, ppl=2.08, wps=13188.6, ups=1.9, wpb=6935.2, bsz=228.2, num_updates=34100, lr=1.18643e-05, gnorm=0.752, train_wall=52, wall=0
2021-01-09 02:19:56 | INFO | train_inner | epoch 044:    837 / 841 symm_kl=0.516, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.059, ppl=2.08, wps=13458.9, ups=1.9, wpb=7066.6, bsz=245.2, num_updates=34200, lr=1.1847e-05, gnorm=0.741, train_wall=52, wall=0
2021-01-09 02:19:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:19:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:19:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:20:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:20:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:20:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:20:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:20:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:20:26 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.387 | nll_loss 3.956 | ppl 15.52 | bleu 22.1 | wps 3376.7 | wpb 5162.1 | bsz 187.5 | num_updates 34204 | best_bleu 22.31
2021-01-09 02:20:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:20:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 44 @ 34204 updates, score 22.1) (writing took 2.7413591258227825 seconds)
2021-01-09 02:20:28 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-09 02:20:28 | INFO | train | epoch 044 | symm_kl 0.513 | self_kl 0 | self_cv 0 | loss 3.463 | nll_loss 1.041 | ppl 2.06 | wps 12320.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 34204 | lr 1.18463e-05 | gnorm 0.736 | train_wall 442 | wall 0
2021-01-09 02:20:28 | INFO | fairseq.trainer | begin training epoch 45
2021-01-09 02:20:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:20:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:21:21 | INFO | train_inner | epoch 045:     96 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.45, nll_loss=1.033, ppl=2.05, wps=8041.9, ups=1.18, wpb=6828.8, bsz=238.7, num_updates=34300, lr=1.18297e-05, gnorm=0.747, train_wall=51, wall=0
2021-01-09 02:22:14 | INFO | train_inner | epoch 045:    196 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.04, ppl=2.06, wps=13276.2, ups=1.9, wpb=7005.3, bsz=255.7, num_updates=34400, lr=1.18125e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:23:07 | INFO | train_inner | epoch 045:    296 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.033, ppl=2.05, wps=13211.6, ups=1.89, wpb=6979.4, bsz=250, num_updates=34500, lr=1.17954e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 02:23:59 | INFO | train_inner | epoch 045:    396 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.053, ppl=2.08, wps=13401.3, ups=1.9, wpb=7056.1, bsz=246.2, num_updates=34600, lr=1.17783e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-09 02:24:52 | INFO | train_inner | epoch 045:    496 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.463, nll_loss=1.043, ppl=2.06, wps=13218.4, ups=1.9, wpb=6965.3, bsz=245.1, num_updates=34700, lr=1.17613e-05, gnorm=0.742, train_wall=53, wall=0
2021-01-09 02:25:44 | INFO | train_inner | epoch 045:    596 / 841 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.462, nll_loss=1.036, ppl=2.05, wps=13301, ups=1.9, wpb=6986.9, bsz=244, num_updates=34800, lr=1.17444e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-09 02:26:37 | INFO | train_inner | epoch 045:    696 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.041, ppl=2.06, wps=13379.2, ups=1.9, wpb=7049.9, bsz=244.7, num_updates=34900, lr=1.17276e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 02:27:30 | INFO | train_inner | epoch 045:    796 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.037, ppl=2.05, wps=13377.6, ups=1.89, wpb=7070.3, bsz=244.4, num_updates=35000, lr=1.17108e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 02:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:28:21 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.39 | nll_loss 3.962 | ppl 15.59 | bleu 22.03 | wps 3377.2 | wpb 5162.1 | bsz 187.5 | num_updates 35045 | best_bleu 22.31
2021-01-09 02:28:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:28:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:28:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:28:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 45 @ 35045 updates, score 22.03) (writing took 2.7546276319772005 seconds)
2021-01-09 02:28:24 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-09 02:28:24 | INFO | train | epoch 045 | symm_kl 0.512 | self_kl 0 | self_cv 0 | loss 3.461 | nll_loss 1.04 | ppl 2.06 | wps 12365.9 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 35045 | lr 1.17033e-05 | gnorm 0.738 | train_wall 440 | wall 0
2021-01-09 02:28:24 | INFO | fairseq.trainer | begin training epoch 46
2021-01-09 02:28:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:28:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:28:55 | INFO | train_inner | epoch 046:     55 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.039, ppl=2.06, wps=8130, ups=1.17, wpb=6942.7, bsz=245.6, num_updates=35100, lr=1.16941e-05, gnorm=0.748, train_wall=52, wall=0
2021-01-09 02:29:48 | INFO | train_inner | epoch 046:    155 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.028, ppl=2.04, wps=13311.5, ups=1.89, wpb=7046.4, bsz=245, num_updates=35200, lr=1.16775e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:30:41 | INFO | train_inner | epoch 046:    255 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.054, ppl=2.08, wps=12904.5, ups=1.88, wpb=6855, bsz=254.6, num_updates=35300, lr=1.16609e-05, gnorm=0.747, train_wall=53, wall=0
2021-01-09 02:31:35 | INFO | train_inner | epoch 046:    355 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.024, ppl=2.03, wps=13208.2, ups=1.88, wpb=7031.7, bsz=244, num_updates=35400, lr=1.16445e-05, gnorm=0.727, train_wall=53, wall=0
2021-01-09 02:32:28 | INFO | train_inner | epoch 046:    455 / 841 symm_kl=0.514, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.047, ppl=2.07, wps=13277.8, ups=1.88, wpb=7071.8, bsz=247.8, num_updates=35500, lr=1.1628e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 02:33:21 | INFO | train_inner | epoch 046:    555 / 841 symm_kl=0.512, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.043, ppl=2.06, wps=13198.9, ups=1.87, wpb=7057, bsz=250.6, num_updates=35600, lr=1.16117e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 02:34:15 | INFO | train_inner | epoch 046:    655 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.45, nll_loss=1.03, ppl=2.04, wps=13127.7, ups=1.88, wpb=6969.4, bsz=240.2, num_updates=35700, lr=1.15954e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-09 02:35:07 | INFO | train_inner | epoch 046:    755 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.047, ppl=2.07, wps=13287.2, ups=1.89, wpb=7023.6, bsz=255.4, num_updates=35800, lr=1.15792e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-09 02:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:35:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:35:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:35:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:35:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:36:21 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.392 | nll_loss 3.963 | ppl 15.59 | bleu 22.13 | wps 3369.9 | wpb 5162.1 | bsz 187.5 | num_updates 35886 | best_bleu 22.31
2021-01-09 02:36:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:36:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:36:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 46 @ 35886 updates, score 22.13) (writing took 2.7900088876485825 seconds)
2021-01-09 02:36:23 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-09 02:36:23 | INFO | train | epoch 046 | symm_kl 0.511 | self_kl 0 | self_cv 0 | loss 3.459 | nll_loss 1.04 | ppl 2.06 | wps 12266.2 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 35886 | lr 1.15653e-05 | gnorm 0.738 | train_wall 444 | wall 0
2021-01-09 02:36:23 | INFO | fairseq.trainer | begin training epoch 47
2021-01-09 02:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:36:34 | INFO | train_inner | epoch 047:     14 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.463, nll_loss=1.045, ppl=2.06, wps=7987.6, ups=1.16, wpb=6907.6, bsz=238.6, num_updates=35900, lr=1.15631e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 02:37:27 | INFO | train_inner | epoch 047:    114 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.039, ppl=2.05, wps=13484.7, ups=1.89, wpb=7125.2, bsz=248.4, num_updates=36000, lr=1.1547e-05, gnorm=0.724, train_wall=53, wall=0
2021-01-09 02:38:20 | INFO | train_inner | epoch 047:    214 / 841 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.041, ppl=2.06, wps=13038.5, ups=1.88, wpb=6937.1, bsz=225.4, num_updates=36100, lr=1.1531e-05, gnorm=0.748, train_wall=53, wall=0
2021-01-09 02:39:13 | INFO | train_inner | epoch 047:    314 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.038, ppl=2.05, wps=13145.5, ups=1.88, wpb=7003.2, bsz=250.4, num_updates=36200, lr=1.15151e-05, gnorm=0.734, train_wall=53, wall=0
2021-01-09 02:40:06 | INFO | train_inner | epoch 047:    414 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.045, ppl=2.06, wps=13277.8, ups=1.89, wpb=7009.4, bsz=243.7, num_updates=36300, lr=1.14992e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 02:41:00 | INFO | train_inner | epoch 047:    514 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.452, nll_loss=1.038, ppl=2.05, wps=12965.8, ups=1.86, wpb=6987.9, bsz=251.4, num_updates=36400, lr=1.14834e-05, gnorm=0.739, train_wall=54, wall=0
2021-01-09 02:41:53 | INFO | train_inner | epoch 047:    614 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.45, nll_loss=1.033, ppl=2.05, wps=13199, ups=1.89, wpb=6969.2, bsz=249.2, num_updates=36500, lr=1.14676e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-09 02:42:45 | INFO | train_inner | epoch 047:    714 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.041, ppl=2.06, wps=13245.5, ups=1.89, wpb=6997.7, bsz=257.1, num_updates=36600, lr=1.1452e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 02:43:38 | INFO | train_inner | epoch 047:    814 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.042, ppl=2.06, wps=13038.2, ups=1.89, wpb=6906.9, bsz=246.6, num_updates=36700, lr=1.14364e-05, gnorm=0.749, train_wall=53, wall=0
2021-01-09 02:43:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:44:21 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.394 | nll_loss 3.967 | ppl 15.64 | bleu 21.9 | wps 3351.2 | wpb 5162.1 | bsz 187.5 | num_updates 36727 | best_bleu 22.31
2021-01-09 02:44:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:44:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:44:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 47 @ 36727 updates, score 21.9) (writing took 2.746307471767068 seconds)
2021-01-09 02:44:23 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-09 02:44:23 | INFO | train | epoch 047 | symm_kl 0.51 | self_kl 0 | self_cv 0 | loss 3.458 | nll_loss 1.04 | ppl 2.06 | wps 12257 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 36727 | lr 1.14321e-05 | gnorm 0.739 | train_wall 445 | wall 0
2021-01-09 02:44:23 | INFO | fairseq.trainer | begin training epoch 48
2021-01-09 02:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:45:04 | INFO | train_inner | epoch 048:     73 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.452, nll_loss=1.034, ppl=2.05, wps=8151.4, ups=1.17, wpb=6990.7, bsz=247.8, num_updates=36800, lr=1.14208e-05, gnorm=0.742, train_wall=52, wall=0
2021-01-09 02:45:57 | INFO | train_inner | epoch 048:    173 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.046, ppl=2.06, wps=13084.7, ups=1.88, wpb=6962.7, bsz=247.8, num_updates=36900, lr=1.14053e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 02:46:50 | INFO | train_inner | epoch 048:    273 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.044, ppl=2.06, wps=13389.7, ups=1.9, wpb=7061.1, bsz=252.6, num_updates=37000, lr=1.13899e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 02:47:43 | INFO | train_inner | epoch 048:    373 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.042, ppl=2.06, wps=13154.3, ups=1.89, wpb=6972.1, bsz=229.8, num_updates=37100, lr=1.13745e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 02:48:36 | INFO | train_inner | epoch 048:    473 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.461, nll_loss=1.043, ppl=2.06, wps=13163.1, ups=1.89, wpb=6960.7, bsz=233.2, num_updates=37200, lr=1.13592e-05, gnorm=0.742, train_wall=53, wall=0
2021-01-09 02:49:29 | INFO | train_inner | epoch 048:    573 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.043, ppl=2.06, wps=13187.7, ups=1.9, wpb=6954.3, bsz=256.4, num_updates=37300, lr=1.1344e-05, gnorm=0.744, train_wall=53, wall=0
2021-01-09 02:50:22 | INFO | train_inner | epoch 048:    673 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.027, ppl=2.04, wps=13389.2, ups=1.88, wpb=7120.2, bsz=253.4, num_updates=37400, lr=1.13288e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 02:51:15 | INFO | train_inner | epoch 048:    773 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.029, ppl=2.04, wps=13133.9, ups=1.88, wpb=6985.8, bsz=248.7, num_updates=37500, lr=1.13137e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 02:51:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:51:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:51:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:51:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:51:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:51:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:52:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:52:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:52:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:52:18 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.395 | nll_loss 3.968 | ppl 15.64 | bleu 22.06 | wps 3366.5 | wpb 5162.1 | bsz 187.5 | num_updates 37568 | best_bleu 22.31
2021-01-09 02:52:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 02:52:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:52:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:52:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 48 @ 37568 updates, score 22.06) (writing took 2.733352681621909 seconds)
2021-01-09 02:52:21 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-09 02:52:21 | INFO | train | epoch 048 | symm_kl 0.509 | self_kl 0 | self_cv 0 | loss 3.456 | nll_loss 1.039 | ppl 2.05 | wps 12307.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 37568 | lr 1.13035e-05 | gnorm 0.738 | train_wall 443 | wall 0
2021-01-09 02:52:21 | INFO | fairseq.trainer | begin training epoch 49
2021-01-09 02:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:52:41 | INFO | train_inner | epoch 049:     32 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.461, nll_loss=1.048, ppl=2.07, wps=8109.4, ups=1.17, wpb=6943, bsz=256.8, num_updates=37600, lr=1.12987e-05, gnorm=0.747, train_wall=52, wall=0
2021-01-09 02:53:33 | INFO | train_inner | epoch 049:    132 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.026, ppl=2.04, wps=13334.5, ups=1.91, wpb=6976.3, bsz=244.5, num_updates=37700, lr=1.12837e-05, gnorm=0.734, train_wall=52, wall=0
2021-01-09 02:54:26 | INFO | train_inner | epoch 049:    232 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.453, nll_loss=1.036, ppl=2.05, wps=13105.4, ups=1.88, wpb=6959, bsz=249, num_updates=37800, lr=1.12687e-05, gnorm=0.745, train_wall=53, wall=0
2021-01-09 02:55:19 | INFO | train_inner | epoch 049:    332 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.046, ppl=2.07, wps=13281.6, ups=1.9, wpb=6997.6, bsz=244.2, num_updates=37900, lr=1.12538e-05, gnorm=0.751, train_wall=52, wall=0
2021-01-09 02:56:12 | INFO | train_inner | epoch 049:    432 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.053, ppl=2.07, wps=13161.9, ups=1.9, wpb=6942.1, bsz=240.1, num_updates=38000, lr=1.1239e-05, gnorm=0.75, train_wall=53, wall=0
2021-01-09 02:57:04 | INFO | train_inner | epoch 049:    532 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.034, ppl=2.05, wps=13205.5, ups=1.91, wpb=6928.1, bsz=247.8, num_updates=38100, lr=1.12243e-05, gnorm=0.741, train_wall=52, wall=0
2021-01-09 02:57:57 | INFO | train_inner | epoch 049:    632 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.024, ppl=2.03, wps=13200, ups=1.88, wpb=7006.7, bsz=243.4, num_updates=38200, lr=1.12096e-05, gnorm=0.73, train_wall=53, wall=0
2021-01-09 02:58:50 | INFO | train_inner | epoch 049:    732 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.041, ppl=2.06, wps=13437.1, ups=1.9, wpb=7071.3, bsz=244.9, num_updates=38300, lr=1.11949e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-09 02:59:43 | INFO | train_inner | epoch 049:    832 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.046, ppl=2.06, wps=13431.7, ups=1.89, wpb=7088.3, bsz=250.8, num_updates=38400, lr=1.11803e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 02:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 02:59:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:59:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 02:59:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:59:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 02:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 02:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 02:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 02:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:00:17 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.393 | nll_loss 3.964 | ppl 15.6 | bleu 22.12 | wps 3073.6 | wpb 5162.1 | bsz 187.5 | num_updates 38409 | best_bleu 22.31
2021-01-09 03:00:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:00:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:00:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:00:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 49 @ 38409 updates, score 22.12) (writing took 2.7574742566794157 seconds)
2021-01-09 03:00:20 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-09 03:00:20 | INFO | train | epoch 049 | symm_kl 0.508 | self_kl 0 | self_cv 0 | loss 3.454 | nll_loss 1.038 | ppl 2.05 | wps 12288.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 38409 | lr 1.1179e-05 | gnorm 0.74 | train_wall 441 | wall 0
2021-01-09 03:00:20 | INFO | fairseq.trainer | begin training epoch 50
2021-01-09 03:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:00:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:01:10 | INFO | train_inner | epoch 050:     91 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.456, nll_loss=1.041, ppl=2.06, wps=7987.6, ups=1.14, wpb=6979.7, bsz=243.8, num_updates=38500, lr=1.11658e-05, gnorm=0.743, train_wall=51, wall=0
2021-01-09 03:02:02 | INFO | train_inner | epoch 050:    191 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.031, ppl=2.04, wps=13330.3, ups=1.91, wpb=6994.5, bsz=238.6, num_updates=38600, lr=1.11513e-05, gnorm=0.737, train_wall=52, wall=0
2021-01-09 03:02:55 | INFO | train_inner | epoch 050:    291 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.036, ppl=2.05, wps=13207.7, ups=1.9, wpb=6941.3, bsz=237.9, num_updates=38700, lr=1.11369e-05, gnorm=0.751, train_wall=52, wall=0
2021-01-09 03:03:47 | INFO | train_inner | epoch 050:    391 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.038, ppl=2.05, wps=13284.4, ups=1.92, wpb=6913.7, bsz=239.3, num_updates=38800, lr=1.11226e-05, gnorm=0.752, train_wall=52, wall=0
2021-01-09 03:04:40 | INFO | train_inner | epoch 050:    491 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.029, ppl=2.04, wps=13306.1, ups=1.87, wpb=7114.2, bsz=265.7, num_updates=38900, lr=1.11083e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 03:05:33 | INFO | train_inner | epoch 050:    591 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.029, ppl=2.04, wps=13361, ups=1.9, wpb=7029.6, bsz=252.2, num_updates=39000, lr=1.1094e-05, gnorm=0.735, train_wall=52, wall=0
2021-01-09 03:06:26 | INFO | train_inner | epoch 050:    691 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.458, nll_loss=1.044, ppl=2.06, wps=13227.2, ups=1.9, wpb=6964.4, bsz=253.4, num_updates=39100, lr=1.10798e-05, gnorm=0.747, train_wall=52, wall=0
2021-01-09 03:07:18 | INFO | train_inner | epoch 050:    791 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.042, ppl=2.06, wps=13392.8, ups=1.91, wpb=7006.8, bsz=248.8, num_updates=39200, lr=1.10657e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 03:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:07:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:07:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:07:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:07:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:07:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:08:12 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.393 | nll_loss 3.965 | ppl 15.61 | bleu 22.07 | wps 3369 | wpb 5162.1 | bsz 187.5 | num_updates 39250 | best_bleu 22.31
2021-01-09 03:08:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:08:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:08:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:08:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 50 @ 39250 updates, score 22.07) (writing took 2.7580969221889973 seconds)
2021-01-09 03:08:15 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-09 03:08:15 | INFO | train | epoch 050 | symm_kl 0.507 | self_kl 0 | self_cv 0 | loss 3.452 | nll_loss 1.038 | ppl 2.05 | wps 12384.8 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 39250 | lr 1.10586e-05 | gnorm 0.742 | train_wall 440 | wall 0
2021-01-09 03:08:15 | INFO | fairseq.trainer | begin training epoch 51
2021-01-09 03:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:08:43 | INFO | train_inner | epoch 051:     50 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.058, ppl=2.08, wps=8106.1, ups=1.17, wpb=6909.2, bsz=234.2, num_updates=39300, lr=1.10516e-05, gnorm=0.757, train_wall=51, wall=0
2021-01-09 03:09:36 | INFO | train_inner | epoch 051:    150 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.03, ppl=2.04, wps=13284.8, ups=1.9, wpb=7004, bsz=247.4, num_updates=39400, lr=1.10375e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 03:10:29 | INFO | train_inner | epoch 051:    250 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.037, ppl=2.05, wps=13172.2, ups=1.88, wpb=7009.8, bsz=251.9, num_updates=39500, lr=1.10236e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-09 03:11:22 | INFO | train_inner | epoch 051:    350 / 841 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.461, nll_loss=1.044, ppl=2.06, wps=13323.3, ups=1.91, wpb=6978.1, bsz=239.6, num_updates=39600, lr=1.10096e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 03:12:14 | INFO | train_inner | epoch 051:    450 / 841 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.048, ppl=2.07, wps=13235, ups=1.9, wpb=6962.3, bsz=257.7, num_updates=39700, lr=1.09958e-05, gnorm=0.754, train_wall=52, wall=0
2021-01-09 03:13:07 | INFO | train_inner | epoch 051:    550 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.456, nll_loss=1.043, ppl=2.06, wps=13128.2, ups=1.88, wpb=6975.3, bsz=243.5, num_updates=39800, lr=1.09819e-05, gnorm=0.75, train_wall=53, wall=0
2021-01-09 03:14:01 | INFO | train_inner | epoch 051:    650 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.026, ppl=2.04, wps=13103.9, ups=1.87, wpb=7011.5, bsz=238.6, num_updates=39900, lr=1.09682e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 03:14:54 | INFO | train_inner | epoch 051:    750 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.432, nll_loss=1.025, ppl=2.03, wps=13242.6, ups=1.89, wpb=7017.5, bsz=256.5, num_updates=40000, lr=1.09545e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-09 03:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:15:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:15:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:15:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:15:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:15:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:16:10 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.394 | nll_loss 3.966 | ppl 15.62 | bleu 21.96 | wps 3362.2 | wpb 5162.1 | bsz 187.5 | num_updates 40091 | best_bleu 22.31
2021-01-09 03:16:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:16:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:16:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:16:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 51 @ 40091 updates, score 21.96) (writing took 2.7449879720807076 seconds)
2021-01-09 03:16:12 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-09 03:16:12 | INFO | train | epoch 051 | symm_kl 0.507 | self_kl 0 | self_cv 0 | loss 3.451 | nll_loss 1.037 | ppl 2.05 | wps 12307.8 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 40091 | lr 1.0942e-05 | gnorm 0.742 | train_wall 443 | wall 0
2021-01-09 03:16:12 | INFO | fairseq.trainer | begin training epoch 52
2021-01-09 03:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:16:20 | INFO | train_inner | epoch 052:      9 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.45, nll_loss=1.036, ppl=2.05, wps=8137.1, ups=1.16, wpb=7034.9, bsz=242, num_updates=40100, lr=1.09408e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 03:17:12 | INFO | train_inner | epoch 052:    109 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.033, ppl=2.05, wps=13532.9, ups=1.92, wpb=7030.1, bsz=240.3, num_updates=40200, lr=1.09272e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-09 03:18:05 | INFO | train_inner | epoch 052:    209 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.028, ppl=2.04, wps=13373.4, ups=1.89, wpb=7090.2, bsz=261.3, num_updates=40300, lr=1.09136e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 03:18:58 | INFO | train_inner | epoch 052:    309 / 841 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.049, ppl=2.07, wps=13356.6, ups=1.91, wpb=6986.5, bsz=234.3, num_updates=40400, lr=1.09001e-05, gnorm=0.748, train_wall=52, wall=0
2021-01-09 03:19:50 | INFO | train_inner | epoch 052:    409 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.057, ppl=2.08, wps=13211.2, ups=1.91, wpb=6913.3, bsz=229.6, num_updates=40500, lr=1.08866e-05, gnorm=0.763, train_wall=52, wall=0
2021-01-09 03:20:43 | INFO | train_inner | epoch 052:    509 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.021, ppl=2.03, wps=13211.6, ups=1.89, wpb=7002, bsz=245.4, num_updates=40600, lr=1.08732e-05, gnorm=0.732, train_wall=53, wall=0
2021-01-09 03:21:36 | INFO | train_inner | epoch 052:    609 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.034, ppl=2.05, wps=13115.6, ups=1.89, wpb=6934.2, bsz=245.4, num_updates=40700, lr=1.08598e-05, gnorm=0.737, train_wall=53, wall=0
2021-01-09 03:22:28 | INFO | train_inner | epoch 052:    709 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.053, ppl=2.07, wps=13113.4, ups=1.9, wpb=6891.6, bsz=249.7, num_updates=40800, lr=1.08465e-05, gnorm=0.759, train_wall=52, wall=0
2021-01-09 03:23:21 | INFO | train_inner | epoch 052:    809 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.024, ppl=2.03, wps=13509.8, ups=1.91, wpb=7072.1, bsz=263, num_updates=40900, lr=1.08333e-05, gnorm=0.728, train_wall=52, wall=0
2021-01-09 03:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:23:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:23:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:23:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:24:05 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.394 | nll_loss 3.967 | ppl 15.64 | bleu 21.97 | wps 3379.7 | wpb 5162.1 | bsz 187.5 | num_updates 40932 | best_bleu 22.31
2021-01-09 03:24:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:24:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:24:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:24:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 52 @ 40932 updates, score 21.97) (writing took 2.793936485424638 seconds)
2021-01-09 03:24:08 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-09 03:24:08 | INFO | train | epoch 052 | symm_kl 0.506 | self_kl 0 | self_cv 0 | loss 3.449 | nll_loss 1.037 | ppl 2.05 | wps 12367.4 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 40932 | lr 1.0829e-05 | gnorm 0.742 | train_wall 440 | wall 0
2021-01-09 03:24:08 | INFO | fairseq.trainer | begin training epoch 53
2021-01-09 03:24:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:24:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:24:46 | INFO | train_inner | epoch 053:     68 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.028, ppl=2.04, wps=8217, ups=1.17, wpb=7026.4, bsz=246.6, num_updates=41000, lr=1.082e-05, gnorm=0.74, train_wall=52, wall=0
2021-01-09 03:25:39 | INFO | train_inner | epoch 053:    168 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.034, ppl=2.05, wps=13359.6, ups=1.89, wpb=7062.2, bsz=247, num_updates=41100, lr=1.08069e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-09 03:26:32 | INFO | train_inner | epoch 053:    268 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.041, ppl=2.06, wps=13242.8, ups=1.89, wpb=7002.4, bsz=241.1, num_updates=41200, lr=1.07937e-05, gnorm=0.747, train_wall=53, wall=0
2021-01-09 03:27:24 | INFO | train_inner | epoch 053:    368 / 841 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.032, ppl=2.05, wps=13249.1, ups=1.92, wpb=6915.4, bsz=241.7, num_updates=41300, lr=1.07807e-05, gnorm=0.743, train_wall=52, wall=0
2021-01-09 03:28:17 | INFO | train_inner | epoch 053:    468 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.021, ppl=2.03, wps=13216.6, ups=1.88, wpb=7046.2, bsz=251.3, num_updates=41400, lr=1.07676e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 03:29:11 | INFO | train_inner | epoch 053:    568 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.031, ppl=2.04, wps=13150.5, ups=1.88, wpb=7011.1, bsz=250.3, num_updates=41500, lr=1.07547e-05, gnorm=0.738, train_wall=53, wall=0
2021-01-09 03:30:04 | INFO | train_inner | epoch 053:    668 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.035, ppl=2.05, wps=13062, ups=1.89, wpb=6921.8, bsz=258.8, num_updates=41600, lr=1.07417e-05, gnorm=0.754, train_wall=53, wall=0
2021-01-09 03:30:57 | INFO | train_inner | epoch 053:    768 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.465, nll_loss=1.051, ppl=2.07, wps=13302.6, ups=1.89, wpb=7022.4, bsz=245.9, num_updates=41700, lr=1.07288e-05, gnorm=0.75, train_wall=53, wall=0
2021-01-09 03:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:32:06 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.401 | nll_loss 3.972 | ppl 15.69 | bleu 22 | wps 2961.2 | wpb 5162.1 | bsz 187.5 | num_updates 41773 | best_bleu 22.31
2021-01-09 03:32:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:32:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:32:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:32:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 53 @ 41773 updates, score 22.0) (writing took 2.7634832970798016 seconds)
2021-01-09 03:32:09 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-09 03:32:09 | INFO | train | epoch 053 | symm_kl 0.505 | self_kl 0 | self_cv 0 | loss 3.447 | nll_loss 1.037 | ppl 2.05 | wps 12238.2 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 41773 | lr 1.07195e-05 | gnorm 0.743 | train_wall 442 | wall 0
2021-01-09 03:32:09 | INFO | fairseq.trainer | begin training epoch 54
2021-01-09 03:32:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:32:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:32:26 | INFO | train_inner | epoch 054:     27 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.046, ppl=2.06, wps=7816.4, ups=1.12, wpb=6967.1, bsz=240.1, num_updates=41800, lr=1.0716e-05, gnorm=0.751, train_wall=52, wall=0
2021-01-09 03:33:18 | INFO | train_inner | epoch 054:    127 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.027, ppl=2.04, wps=13440.4, ups=1.92, wpb=7011.5, bsz=248.6, num_updates=41900, lr=1.07032e-05, gnorm=0.741, train_wall=52, wall=0
2021-01-09 03:34:10 | INFO | train_inner | epoch 054:    227 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.452, nll_loss=1.039, ppl=2.05, wps=13300.5, ups=1.9, wpb=6983, bsz=235.6, num_updates=42000, lr=1.06904e-05, gnorm=0.751, train_wall=52, wall=0
2021-01-09 03:35:04 | INFO | train_inner | epoch 054:    327 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.453, nll_loss=1.043, ppl=2.06, wps=13270.1, ups=1.88, wpb=7053.2, bsz=248, num_updates=42100, lr=1.06777e-05, gnorm=0.736, train_wall=53, wall=0
2021-01-09 03:35:56 | INFO | train_inner | epoch 054:    427 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.449, nll_loss=1.041, ppl=2.06, wps=13226.6, ups=1.89, wpb=6998.7, bsz=253, num_updates=42200, lr=1.06651e-05, gnorm=0.74, train_wall=53, wall=0
2021-01-09 03:36:49 | INFO | train_inner | epoch 054:    527 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.019, ppl=2.03, wps=13238.7, ups=1.9, wpb=6986.1, bsz=249.9, num_updates=42300, lr=1.06525e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 03:37:42 | INFO | train_inner | epoch 054:    627 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.033, ppl=2.05, wps=13530.8, ups=1.91, wpb=7097.7, bsz=261.8, num_updates=42400, lr=1.06399e-05, gnorm=0.731, train_wall=52, wall=0
2021-01-09 03:38:34 | INFO | train_inner | epoch 054:    727 / 841 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.046, ppl=2.06, wps=13290.7, ups=1.91, wpb=6950.3, bsz=234.8, num_updates=42500, lr=1.06274e-05, gnorm=0.759, train_wall=52, wall=0
2021-01-09 03:39:26 | INFO | train_inner | epoch 054:    827 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.035, ppl=2.05, wps=13086.5, ups=1.91, wpb=6852.4, bsz=243.1, num_updates=42600, lr=1.06149e-05, gnorm=0.752, train_wall=52, wall=0
2021-01-09 03:39:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:39:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:39:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:39:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:40:02 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.403 | nll_loss 3.975 | ppl 15.72 | bleu 21.95 | wps 3319 | wpb 5162.1 | bsz 187.5 | num_updates 42614 | best_bleu 22.31
2021-01-09 03:40:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:40:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:40:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:40:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 54 @ 42614 updates, score 21.95) (writing took 2.7361538130789995 seconds)
2021-01-09 03:40:04 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-09 03:40:04 | INFO | train | epoch 054 | symm_kl 0.504 | self_kl 0 | self_cv 0 | loss 3.445 | nll_loss 1.035 | ppl 2.05 | wps 12361.6 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 42614 | lr 1.06132e-05 | gnorm 0.743 | train_wall 440 | wall 0
2021-01-09 03:40:04 | INFO | fairseq.trainer | begin training epoch 55
2021-01-09 03:40:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:40:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:40:52 | INFO | train_inner | epoch 055:     86 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.04, ppl=2.06, wps=8200.9, ups=1.17, wpb=7019.3, bsz=247.1, num_updates=42700, lr=1.06025e-05, gnorm=0.741, train_wall=51, wall=0
2021-01-09 03:41:45 | INFO | train_inner | epoch 055:    186 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.035, ppl=2.05, wps=13219.8, ups=1.89, wpb=7009.9, bsz=255.7, num_updates=42800, lr=1.05901e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 03:42:38 | INFO | train_inner | epoch 055:    286 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.033, ppl=2.05, wps=13276.5, ups=1.89, wpb=7042.5, bsz=255.4, num_updates=42900, lr=1.05777e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-09 03:43:30 | INFO | train_inner | epoch 055:    386 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.034, ppl=2.05, wps=13226.4, ups=1.91, wpb=6941.1, bsz=248.1, num_updates=43000, lr=1.05654e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 03:44:24 | INFO | train_inner | epoch 055:    486 / 841 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.456, nll_loss=1.041, ppl=2.06, wps=13167.2, ups=1.88, wpb=6991.4, bsz=232.4, num_updates=43100, lr=1.05531e-05, gnorm=0.751, train_wall=53, wall=0
2021-01-09 03:45:17 | INFO | train_inner | epoch 055:    586 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.025, ppl=2.03, wps=13297.6, ups=1.88, wpb=7091.5, bsz=249.9, num_updates=43200, lr=1.05409e-05, gnorm=0.726, train_wall=53, wall=0
2021-01-09 03:46:09 | INFO | train_inner | epoch 055:    686 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.037, ppl=2.05, wps=13066.5, ups=1.9, wpb=6859.4, bsz=230.3, num_updates=43300, lr=1.05287e-05, gnorm=0.755, train_wall=52, wall=0
2021-01-09 03:47:02 | INFO | train_inner | epoch 055:    786 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.031, ppl=2.04, wps=13413.1, ups=1.9, wpb=7049.6, bsz=255.1, num_updates=43400, lr=1.05166e-05, gnorm=0.74, train_wall=52, wall=0
2021-01-09 03:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:47:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:47:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:47:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:47:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:47:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:47:58 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.401 | nll_loss 3.974 | ppl 15.71 | bleu 22.05 | wps 3400.6 | wpb 5162.1 | bsz 187.5 | num_updates 43455 | best_bleu 22.31
2021-01-09 03:47:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:47:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:48:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:48:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 55 @ 43455 updates, score 22.05) (writing took 2.7990891449153423 seconds)
2021-01-09 03:48:01 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-09 03:48:01 | INFO | train | epoch 055 | symm_kl 0.503 | self_kl 0 | self_cv 0 | loss 3.444 | nll_loss 1.035 | ppl 2.05 | wps 12330.4 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 43455 | lr 1.051e-05 | gnorm 0.742 | train_wall 442 | wall 0
2021-01-09 03:48:01 | INFO | fairseq.trainer | begin training epoch 56
2021-01-09 03:48:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:48:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:48:27 | INFO | train_inner | epoch 056:     45 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.046, ppl=2.06, wps=7997.2, ups=1.17, wpb=6831.9, bsz=235, num_updates=43500, lr=1.05045e-05, gnorm=0.773, train_wall=52, wall=0
2021-01-09 03:49:20 | INFO | train_inner | epoch 056:    145 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.03, ppl=2.04, wps=13405.7, ups=1.9, wpb=7041.8, bsz=254.3, num_updates=43600, lr=1.04925e-05, gnorm=0.729, train_wall=52, wall=0
2021-01-09 03:50:13 | INFO | train_inner | epoch 056:    245 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.031, ppl=2.04, wps=13362.4, ups=1.88, wpb=7105, bsz=246.1, num_updates=43700, lr=1.04804e-05, gnorm=0.728, train_wall=53, wall=0
2021-01-09 03:51:06 | INFO | train_inner | epoch 056:    345 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.432, nll_loss=1.029, ppl=2.04, wps=13263.2, ups=1.87, wpb=7074.3, bsz=285.4, num_updates=43800, lr=1.04685e-05, gnorm=0.725, train_wall=53, wall=0
2021-01-09 03:51:59 | INFO | train_inner | epoch 056:    445 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.036, ppl=2.05, wps=13155.8, ups=1.9, wpb=6926.8, bsz=236.8, num_updates=43900, lr=1.04565e-05, gnorm=0.756, train_wall=52, wall=0
2021-01-09 03:52:52 | INFO | train_inner | epoch 056:    545 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.032, ppl=2.04, wps=13059.8, ups=1.89, wpb=6918.4, bsz=228.9, num_updates=44000, lr=1.04447e-05, gnorm=0.755, train_wall=53, wall=0
2021-01-09 03:53:45 | INFO | train_inner | epoch 056:    645 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.038, ppl=2.05, wps=13097.6, ups=1.89, wpb=6934.3, bsz=249.2, num_updates=44100, lr=1.04328e-05, gnorm=0.757, train_wall=53, wall=0
2021-01-09 03:54:38 | INFO | train_inner | epoch 056:    745 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.022, ppl=2.03, wps=13275.6, ups=1.9, wpb=7004.9, bsz=239, num_updates=44200, lr=1.0421e-05, gnorm=0.745, train_wall=53, wall=0
2021-01-09 03:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 03:55:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:55:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:55:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:55:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:55:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 03:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 03:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 03:55:56 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.4 | nll_loss 3.974 | ppl 15.71 | bleu 22.04 | wps 3326.5 | wpb 5162.1 | bsz 187.5 | num_updates 44296 | best_bleu 22.31
2021-01-09 03:55:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 03:55:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:55:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:55:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 56 @ 44296 updates, score 22.04) (writing took 2.803836476057768 seconds)
2021-01-09 03:55:59 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-09 03:55:59 | INFO | train | epoch 056 | symm_kl 0.503 | self_kl 0 | self_cv 0 | loss 3.443 | nll_loss 1.035 | ppl 2.05 | wps 12311.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 44296 | lr 1.04097e-05 | gnorm 0.745 | train_wall 442 | wall 0
2021-01-09 03:55:59 | INFO | fairseq.trainer | begin training epoch 57
2021-01-09 03:56:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 03:56:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 03:56:04 | INFO | train_inner | epoch 057:      4 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.055, ppl=2.08, wps=8124.2, ups=1.16, wpb=7031.8, bsz=246.8, num_updates=44300, lr=1.04092e-05, gnorm=0.749, train_wall=52, wall=0
2021-01-09 03:56:56 | INFO | train_inner | epoch 057:    104 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.035, ppl=2.05, wps=13355.1, ups=1.92, wpb=6961.6, bsz=253.3, num_updates=44400, lr=1.03975e-05, gnorm=0.745, train_wall=52, wall=0
2021-01-09 03:57:49 | INFO | train_inner | epoch 057:    204 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.032, ppl=2.04, wps=13099.9, ups=1.91, wpb=6871.3, bsz=243.3, num_updates=44500, lr=1.03858e-05, gnorm=0.75, train_wall=52, wall=0
2021-01-09 03:58:42 | INFO | train_inner | epoch 057:    304 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.044, ppl=2.06, wps=13393.1, ups=1.88, wpb=7107.2, bsz=247.8, num_updates=44600, lr=1.03742e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 03:59:35 | INFO | train_inner | epoch 057:    404 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.031, ppl=2.04, wps=13158.2, ups=1.9, wpb=6933.8, bsz=233.7, num_updates=44700, lr=1.03626e-05, gnorm=0.752, train_wall=53, wall=0
2021-01-09 04:00:27 | INFO | train_inner | epoch 057:    504 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.034, ppl=2.05, wps=13263.3, ups=1.9, wpb=6976.7, bsz=253.1, num_updates=44800, lr=1.0351e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 04:01:21 | INFO | train_inner | epoch 057:    604 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.018, ppl=2.02, wps=13201.2, ups=1.88, wpb=7034.2, bsz=244.1, num_updates=44900, lr=1.03395e-05, gnorm=0.731, train_wall=53, wall=0
2021-01-09 04:02:14 | INFO | train_inner | epoch 057:    704 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.041, ppl=2.06, wps=13381.6, ups=1.88, wpb=7099.9, bsz=258.9, num_updates=45000, lr=1.0328e-05, gnorm=0.737, train_wall=53, wall=0
2021-01-09 04:03:07 | INFO | train_inner | epoch 057:    804 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.032, ppl=2.05, wps=13113.8, ups=1.88, wpb=6966.4, bsz=229.4, num_updates=45100, lr=1.03165e-05, gnorm=0.75, train_wall=53, wall=0
2021-01-09 04:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:03:54 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.403 | nll_loss 3.976 | ppl 15.74 | bleu 22.07 | wps 3320.1 | wpb 5162.1 | bsz 187.5 | num_updates 45137 | best_bleu 22.31
2021-01-09 04:03:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:03:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:03:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:03:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 57 @ 45137 updates, score 22.07) (writing took 2.7290086690336466 seconds)
2021-01-09 04:03:57 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-09 04:03:57 | INFO | train | epoch 057 | symm_kl 0.502 | self_kl 0 | self_cv 0 | loss 3.441 | nll_loss 1.034 | ppl 2.05 | wps 12306.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 45137 | lr 1.03123e-05 | gnorm 0.744 | train_wall 442 | wall 0
2021-01-09 04:03:57 | INFO | fairseq.trainer | begin training epoch 58
2021-01-09 04:03:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:03:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:04:32 | INFO | train_inner | epoch 058:     63 / 841 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.029, ppl=2.04, wps=8258.9, ups=1.17, wpb=7078.4, bsz=246, num_updates=45200, lr=1.03051e-05, gnorm=0.732, train_wall=52, wall=0
2021-01-09 04:05:26 | INFO | train_inner | epoch 058:    163 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.022, ppl=2.03, wps=13275.1, ups=1.88, wpb=7064.6, bsz=258.4, num_updates=45300, lr=1.02937e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 04:06:19 | INFO | train_inner | epoch 058:    263 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.449, nll_loss=1.042, ppl=2.06, wps=12919, ups=1.88, wpb=6878.6, bsz=258.2, num_updates=45400, lr=1.02824e-05, gnorm=0.757, train_wall=53, wall=0
2021-01-09 04:07:12 | INFO | train_inner | epoch 058:    363 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.041, ppl=2.06, wps=13006.2, ups=1.88, wpb=6932.6, bsz=244.3, num_updates=45500, lr=1.02711e-05, gnorm=0.757, train_wall=53, wall=0
2021-01-09 04:08:06 | INFO | train_inner | epoch 058:    463 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.024, ppl=2.03, wps=13231.6, ups=1.87, wpb=7069.8, bsz=233, num_updates=45600, lr=1.02598e-05, gnorm=0.739, train_wall=53, wall=0
2021-01-09 04:08:59 | INFO | train_inner | epoch 058:    563 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.04, ppl=2.06, wps=13209.7, ups=1.89, wpb=7006.4, bsz=254.6, num_updates=45700, lr=1.02486e-05, gnorm=0.743, train_wall=53, wall=0
2021-01-09 04:09:51 | INFO | train_inner | epoch 058:    663 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.038, ppl=2.05, wps=13038.8, ups=1.89, wpb=6886.3, bsz=233, num_updates=45800, lr=1.02374e-05, gnorm=0.762, train_wall=53, wall=0
2021-01-09 04:10:44 | INFO | train_inner | epoch 058:    763 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.037, ppl=2.05, wps=13355.9, ups=1.91, wpb=7005, bsz=237.4, num_updates=45900, lr=1.02262e-05, gnorm=0.74, train_wall=52, wall=0
2021-01-09 04:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:11:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:11:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:11:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:11:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:11:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:11:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:11:53 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.402 | nll_loss 3.976 | ppl 15.74 | bleu 21.96 | wps 3311.3 | wpb 5162.1 | bsz 187.5 | num_updates 45978 | best_bleu 22.31
2021-01-09 04:11:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:11:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:11:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:11:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 58 @ 45978 updates, score 21.96) (writing took 2.7542320881038904 seconds)
2021-01-09 04:11:56 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-09 04:11:56 | INFO | train | epoch 058 | symm_kl 0.501 | self_kl 0 | self_cv 0 | loss 3.439 | nll_loss 1.033 | ppl 2.05 | wps 12273.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 45978 | lr 1.02175e-05 | gnorm 0.745 | train_wall 444 | wall 0
2021-01-09 04:11:56 | INFO | fairseq.trainer | begin training epoch 59
2021-01-09 04:11:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:11:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:12:10 | INFO | train_inner | epoch 059:     22 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.035, ppl=2.05, wps=8059.2, ups=1.16, wpb=6953.3, bsz=252.5, num_updates=46000, lr=1.02151e-05, gnorm=0.747, train_wall=52, wall=0
2021-01-09 04:13:02 | INFO | train_inner | epoch 059:    122 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.037, ppl=2.05, wps=13493.7, ups=1.92, wpb=7032.4, bsz=242.2, num_updates=46100, lr=1.0204e-05, gnorm=0.745, train_wall=52, wall=0
2021-01-09 04:13:55 | INFO | train_inner | epoch 059:    222 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.02, ppl=2.03, wps=13325.9, ups=1.9, wpb=7002.1, bsz=252.5, num_updates=46200, lr=1.01929e-05, gnorm=0.739, train_wall=52, wall=0
2021-01-09 04:14:48 | INFO | train_inner | epoch 059:    322 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.03, ppl=2.04, wps=13105, ups=1.9, wpb=6913.8, bsz=237.7, num_updates=46300, lr=1.01819e-05, gnorm=0.752, train_wall=53, wall=0
2021-01-09 04:15:41 | INFO | train_inner | epoch 059:    422 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.028, ppl=2.04, wps=13305.4, ups=1.89, wpb=7037.9, bsz=247, num_updates=46400, lr=1.0171e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 04:16:33 | INFO | train_inner | epoch 059:    522 / 841 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.458, nll_loss=1.049, ppl=2.07, wps=13299.5, ups=1.9, wpb=7002.7, bsz=233.1, num_updates=46500, lr=1.016e-05, gnorm=0.753, train_wall=52, wall=0
2021-01-09 04:17:26 | INFO | train_inner | epoch 059:    622 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.007, ppl=2.01, wps=13195.7, ups=1.89, wpb=6984.2, bsz=254.1, num_updates=46600, lr=1.01491e-05, gnorm=0.733, train_wall=53, wall=0
2021-01-09 04:18:19 | INFO | train_inner | epoch 059:    722 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.045, ppl=2.06, wps=13400.2, ups=1.9, wpb=7068.8, bsz=260.7, num_updates=46700, lr=1.01382e-05, gnorm=0.741, train_wall=53, wall=0
2021-01-09 04:19:12 | INFO | train_inner | epoch 059:    822 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.042, ppl=2.06, wps=13250.5, ups=1.88, wpb=7040.4, bsz=252.9, num_updates=46800, lr=1.01274e-05, gnorm=0.744, train_wall=53, wall=0
2021-01-09 04:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:19:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:19:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:19:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:19:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:19:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:19:49 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.402 | nll_loss 3.975 | ppl 15.73 | bleu 22.1 | wps 3385.1 | wpb 5162.1 | bsz 187.5 | num_updates 46819 | best_bleu 22.31
2021-01-09 04:19:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:19:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 59 @ 46819 updates, score 22.1) (writing took 2.727694883942604 seconds)
2021-01-09 04:19:52 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-09 04:19:52 | INFO | train | epoch 059 | symm_kl 0.501 | self_kl 0 | self_cv 0 | loss 3.438 | nll_loss 1.033 | ppl 2.05 | wps 12353 | ups 1.77 | wpb 6993.1 | bsz 246.6 | num_updates 46819 | lr 1.01253e-05 | gnorm 0.745 | train_wall 441 | wall 0
2021-01-09 04:19:52 | INFO | fairseq.trainer | begin training epoch 60
2021-01-09 04:19:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:19:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:20:37 | INFO | train_inner | epoch 060:     81 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.037, ppl=2.05, wps=8114.8, ups=1.18, wpb=6902.4, bsz=239.1, num_updates=46900, lr=1.01166e-05, gnorm=0.749, train_wall=51, wall=0
2021-01-09 04:21:30 | INFO | train_inner | epoch 060:    181 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.024, ppl=2.03, wps=13252.4, ups=1.89, wpb=7014.9, bsz=251, num_updates=47000, lr=1.01058e-05, gnorm=0.735, train_wall=53, wall=0
2021-01-09 04:22:23 | INFO | train_inner | epoch 060:    281 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.038, ppl=2.05, wps=13136, ups=1.89, wpb=6957.3, bsz=248.6, num_updates=47100, lr=1.00951e-05, gnorm=0.752, train_wall=53, wall=0
2021-01-09 04:23:15 | INFO | train_inner | epoch 060:    381 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.031, ppl=2.04, wps=13346.4, ups=1.9, wpb=7010.3, bsz=251.6, num_updates=47200, lr=1.00844e-05, gnorm=0.744, train_wall=52, wall=0
2021-01-09 04:24:09 | INFO | train_inner | epoch 060:    481 / 841 symm_kl=0.503, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.035, ppl=2.05, wps=13035.5, ups=1.87, wpb=6963.9, bsz=234.6, num_updates=47300, lr=1.00737e-05, gnorm=0.751, train_wall=53, wall=0
2021-01-09 04:25:01 | INFO | train_inner | epoch 060:    581 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.04, ppl=2.06, wps=13410.1, ups=1.9, wpb=7051.3, bsz=243.1, num_updates=47400, lr=1.00631e-05, gnorm=0.741, train_wall=52, wall=0
2021-01-09 04:25:54 | INFO | train_inner | epoch 060:    681 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.028, ppl=2.04, wps=13260.2, ups=1.9, wpb=6971.7, bsz=258.5, num_updates=47500, lr=1.00525e-05, gnorm=0.742, train_wall=52, wall=0
2021-01-09 04:26:47 | INFO | train_inner | epoch 060:    781 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.043, ppl=2.06, wps=13225.5, ups=1.9, wpb=6975.6, bsz=245.2, num_updates=47600, lr=1.00419e-05, gnorm=0.751, train_wall=53, wall=0
2021-01-09 04:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:27:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:27:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:27:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:27:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:27:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:27:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:27:46 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.401 | nll_loss 3.975 | ppl 15.72 | bleu 21.98 | wps 3349.6 | wpb 5162.1 | bsz 187.5 | num_updates 47660 | best_bleu 22.31
2021-01-09 04:27:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:27:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:27:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:27:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 60 @ 47660 updates, score 21.98) (writing took 2.74879989027977 seconds)
2021-01-09 04:27:49 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-09 04:27:49 | INFO | train | epoch 060 | symm_kl 0.5 | self_kl 0 | self_cv 0 | loss 3.436 | nll_loss 1.033 | ppl 2.05 | wps 12337.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 47660 | lr 1.00356e-05 | gnorm 0.745 | train_wall 441 | wall 0
2021-01-09 04:27:49 | INFO | fairseq.trainer | begin training epoch 61
2021-01-09 04:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:27:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:28:13 | INFO | train_inner | epoch 061:     40 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.02, ppl=2.03, wps=8188.8, ups=1.17, wpb=7025.1, bsz=244.7, num_updates=47700, lr=1.00314e-05, gnorm=0.746, train_wall=52, wall=0
2021-01-09 04:29:05 | INFO | train_inner | epoch 061:    140 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.432, nll_loss=1.03, ppl=2.04, wps=13341, ups=1.91, wpb=6981.4, bsz=252.5, num_updates=47800, lr=1.00209e-05, gnorm=0.743, train_wall=52, wall=0
2021-01-09 04:29:58 | INFO | train_inner | epoch 061:    240 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.032, ppl=2.05, wps=13098, ups=1.89, wpb=6927.1, bsz=252.2, num_updates=47900, lr=1.00104e-05, gnorm=0.747, train_wall=53, wall=0
2021-01-09 04:30:51 | INFO | train_inner | epoch 061:    340 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.036, ppl=2.05, wps=13233.8, ups=1.89, wpb=7001.7, bsz=247.5, num_updates=48000, lr=1e-05, gnorm=0.749, train_wall=53, wall=0
2021-01-09 04:31:43 | INFO | train_inner | epoch 061:    440 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.038, ppl=2.05, wps=13143, ups=1.9, wpb=6928.8, bsz=243, num_updates=48100, lr=9.9896e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 04:32:37 | INFO | train_inner | epoch 061:    540 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.035, ppl=2.05, wps=13187.9, ups=1.88, wpb=7021.6, bsz=239.4, num_updates=48200, lr=9.97923e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 04:33:30 | INFO | train_inner | epoch 061:    640 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.021, ppl=2.03, wps=13271.8, ups=1.87, wpb=7110.4, bsz=244.1, num_updates=48300, lr=9.9689e-06, gnorm=0.736, train_wall=53, wall=0
2021-01-09 04:34:23 | INFO | train_inner | epoch 061:    740 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.033, ppl=2.05, wps=13237, ups=1.88, wpb=7027.5, bsz=258.3, num_updates=48400, lr=9.95859e-06, gnorm=0.741, train_wall=53, wall=0
2021-01-09 04:35:16 | INFO | train_inner | epoch 061:    840 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.038, ppl=2.05, wps=13238.4, ups=1.9, wpb=6957.1, bsz=238.6, num_updates=48500, lr=9.94832e-06, gnorm=0.754, train_wall=52, wall=0
2021-01-09 04:35:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:35:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:35:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:35:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:35:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:35:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:35:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:35:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:35:44 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.402 | nll_loss 3.975 | ppl 15.72 | bleu 22.1 | wps 3332.5 | wpb 5162.1 | bsz 187.5 | num_updates 48501 | best_bleu 22.31
2021-01-09 04:35:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:35:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:35:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:35:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 61 @ 48501 updates, score 22.1) (writing took 2.74739932641387 seconds)
2021-01-09 04:35:47 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-09 04:35:47 | INFO | train | epoch 061 | symm_kl 0.499 | self_kl 0 | self_cv 0 | loss 3.435 | nll_loss 1.032 | ppl 2.05 | wps 12301.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 48501 | lr 9.94822e-06 | gnorm 0.746 | train_wall 443 | wall 0
2021-01-09 04:35:47 | INFO | fairseq.trainer | begin training epoch 62
2021-01-09 04:35:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:36:41 | INFO | train_inner | epoch 062:     99 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.432, nll_loss=1.031, ppl=2.04, wps=8172.8, ups=1.17, wpb=6985.8, bsz=247.8, num_updates=48600, lr=9.93808e-06, gnorm=0.748, train_wall=51, wall=0
2021-01-09 04:37:34 | INFO | train_inner | epoch 062:    199 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.021, ppl=2.03, wps=13079.1, ups=1.89, wpb=6917.5, bsz=246.5, num_updates=48700, lr=9.92787e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 04:38:27 | INFO | train_inner | epoch 062:    299 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.028, ppl=2.04, wps=13457.4, ups=1.9, wpb=7077.4, bsz=247.4, num_updates=48800, lr=9.91769e-06, gnorm=0.736, train_wall=52, wall=0
2021-01-09 04:39:20 | INFO | train_inner | epoch 062:    399 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.033, ppl=2.05, wps=13277.2, ups=1.89, wpb=7017.6, bsz=244.9, num_updates=48900, lr=9.90755e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 04:40:12 | INFO | train_inner | epoch 062:    499 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.039, ppl=2.06, wps=13257.2, ups=1.9, wpb=6977.5, bsz=253.8, num_updates=49000, lr=9.89743e-06, gnorm=0.742, train_wall=52, wall=0
2021-01-09 04:41:06 | INFO | train_inner | epoch 062:    599 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.025, ppl=2.03, wps=13123.1, ups=1.86, wpb=7047.9, bsz=246.3, num_updates=49100, lr=9.88735e-06, gnorm=0.742, train_wall=54, wall=0
2021-01-09 04:41:59 | INFO | train_inner | epoch 062:    699 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.041, ppl=2.06, wps=13013.9, ups=1.89, wpb=6902.7, bsz=244.4, num_updates=49200, lr=9.8773e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 04:42:52 | INFO | train_inner | epoch 062:    799 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.034, ppl=2.05, wps=13206.6, ups=1.89, wpb=6989.1, bsz=241.7, num_updates=49300, lr=9.86727e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 04:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:43:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:43:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:43:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:43:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:43:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:43:45 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.406 | nll_loss 3.979 | ppl 15.77 | bleu 22.07 | wps 2918.6 | wpb 5162.1 | bsz 187.5 | num_updates 49342 | best_bleu 22.31
2021-01-09 04:43:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:43:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:43:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:43:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 62 @ 49342 updates, score 22.07) (writing took 2.7536745332181454 seconds)
2021-01-09 04:43:48 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-09 04:43:48 | INFO | train | epoch 062 | symm_kl 0.499 | self_kl 0 | self_cv 0 | loss 3.434 | nll_loss 1.032 | ppl 2.04 | wps 12225.9 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 49342 | lr 9.86307e-06 | gnorm 0.746 | train_wall 442 | wall 0
2021-01-09 04:43:48 | INFO | fairseq.trainer | begin training epoch 63
2021-01-09 04:43:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:43:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:44:21 | INFO | train_inner | epoch 063:     58 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.026, ppl=2.04, wps=7855.8, ups=1.12, wpb=6983.7, bsz=237.5, num_updates=49400, lr=9.85728e-06, gnorm=0.751, train_wall=52, wall=0
2021-01-09 04:45:14 | INFO | train_inner | epoch 063:    158 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.03, ppl=2.04, wps=13420.3, ups=1.89, wpb=7118.2, bsz=242.6, num_updates=49500, lr=9.84732e-06, gnorm=0.737, train_wall=53, wall=0
2021-01-09 04:46:07 | INFO | train_inner | epoch 063:    258 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.028, ppl=2.04, wps=13396.2, ups=1.89, wpb=7074.4, bsz=249.4, num_updates=49600, lr=9.83739e-06, gnorm=0.739, train_wall=53, wall=0
2021-01-09 04:46:59 | INFO | train_inner | epoch 063:    358 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.034, ppl=2.05, wps=13309.3, ups=1.92, wpb=6943.8, bsz=248, num_updates=49700, lr=9.82749e-06, gnorm=0.749, train_wall=52, wall=0
2021-01-09 04:47:52 | INFO | train_inner | epoch 063:    458 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.025, ppl=2.03, wps=13210.6, ups=1.88, wpb=7010.9, bsz=254.9, num_updates=49800, lr=9.81761e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 04:48:45 | INFO | train_inner | epoch 063:    558 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.035, ppl=2.05, wps=13086.6, ups=1.9, wpb=6874.4, bsz=245.8, num_updates=49900, lr=9.80777e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 04:49:38 | INFO | train_inner | epoch 063:    658 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.032, ppl=2.05, wps=13177.7, ups=1.89, wpb=6981.3, bsz=247.8, num_updates=50000, lr=9.79796e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 04:50:30 | INFO | train_inner | epoch 063:    758 / 841 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.449, nll_loss=1.047, ppl=2.07, wps=13129, ups=1.89, wpb=6945.8, bsz=252.9, num_updates=50100, lr=9.78818e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 04:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:51:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:51:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:51:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:51:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:51:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:51:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:51:44 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.406 | nll_loss 3.979 | ppl 15.77 | bleu 22.1 | wps 3131.7 | wpb 5162.1 | bsz 187.5 | num_updates 50183 | best_bleu 22.31
2021-01-09 04:51:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:51:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:51:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:51:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 63 @ 50183 updates, score 22.1) (writing took 2.7178822234272957 seconds)
2021-01-09 04:51:47 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-09 04:51:47 | INFO | train | epoch 063 | symm_kl 0.498 | self_kl 0 | self_cv 0 | loss 3.432 | nll_loss 1.031 | ppl 2.04 | wps 12288.6 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 50183 | lr 9.78008e-06 | gnorm 0.748 | train_wall 442 | wall 0
2021-01-09 04:51:47 | INFO | fairseq.trainer | begin training epoch 64
2021-01-09 04:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:51:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:51:59 | INFO | train_inner | epoch 064:     17 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.013, ppl=2.02, wps=7984.1, ups=1.13, wpb=7040.9, bsz=247.8, num_updates=50200, lr=9.77842e-06, gnorm=0.732, train_wall=52, wall=0
2021-01-09 04:52:51 | INFO | train_inner | epoch 064:    117 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.032, ppl=2.04, wps=13380.9, ups=1.91, wpb=6987.6, bsz=242.8, num_updates=50300, lr=9.7687e-06, gnorm=0.743, train_wall=52, wall=0
2021-01-09 04:53:44 | INFO | train_inner | epoch 064:    217 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.023, ppl=2.03, wps=13153.5, ups=1.87, wpb=7031, bsz=247, num_updates=50400, lr=9.759e-06, gnorm=0.737, train_wall=53, wall=0
2021-01-09 04:54:37 | INFO | train_inner | epoch 064:    317 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.033, ppl=2.05, wps=13159.6, ups=1.89, wpb=6963.7, bsz=243.5, num_updates=50500, lr=9.74933e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 04:55:30 | INFO | train_inner | epoch 064:    417 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.021, ppl=2.03, wps=13200.7, ups=1.88, wpb=7031, bsz=247.8, num_updates=50600, lr=9.7397e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 04:56:24 | INFO | train_inner | epoch 064:    517 / 841 symm_kl=0.502, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.049, ppl=2.07, wps=13198, ups=1.88, wpb=7002.1, bsz=238, num_updates=50700, lr=9.73009e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 04:57:17 | INFO | train_inner | epoch 064:    617 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.034, ppl=2.05, wps=13139.4, ups=1.87, wpb=7016, bsz=247, num_updates=50800, lr=9.7205e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 04:58:10 | INFO | train_inner | epoch 064:    717 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.033, ppl=2.05, wps=13196.6, ups=1.89, wpb=6986.3, bsz=246.4, num_updates=50900, lr=9.71095e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 04:59:03 | INFO | train_inner | epoch 064:    817 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.033, ppl=2.05, wps=13132.7, ups=1.89, wpb=6938.8, bsz=249, num_updates=51000, lr=9.70143e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 04:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 04:59:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:59:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:59:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:59:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:59:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 04:59:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 04:59:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 04:59:43 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.409 | nll_loss 3.984 | ppl 15.82 | bleu 21.92 | wps 3350.7 | wpb 5162.1 | bsz 187.5 | num_updates 51024 | best_bleu 22.31
2021-01-09 04:59:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 04:59:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:59:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 04:59:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 64 @ 51024 updates, score 21.92) (writing took 2.7362399976700544 seconds)
2021-01-09 04:59:46 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-09 04:59:46 | INFO | train | epoch 064 | symm_kl 0.497 | self_kl 0 | self_cv 0 | loss 3.431 | nll_loss 1.031 | ppl 2.04 | wps 12275.6 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 51024 | lr 9.69914e-06 | gnorm 0.746 | train_wall 444 | wall 0
2021-01-09 04:59:46 | INFO | fairseq.trainer | begin training epoch 65
2021-01-09 04:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 04:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:00:28 | INFO | train_inner | epoch 065:     76 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.035, ppl=2.05, wps=8108.4, ups=1.17, wpb=6942.3, bsz=243.8, num_updates=51100, lr=9.69193e-06, gnorm=0.751, train_wall=52, wall=0
2021-01-09 05:01:21 | INFO | train_inner | epoch 065:    176 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.022, ppl=2.03, wps=13173.2, ups=1.88, wpb=6994.2, bsz=244.3, num_updates=51200, lr=9.68246e-06, gnorm=0.743, train_wall=53, wall=0
2021-01-09 05:02:14 | INFO | train_inner | epoch 065:    276 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.028, ppl=2.04, wps=13157.5, ups=1.89, wpb=6977.3, bsz=240.3, num_updates=51300, lr=9.67302e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 05:03:07 | INFO | train_inner | epoch 065:    376 / 841 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.453, nll_loss=1.048, ppl=2.07, wps=13092.6, ups=1.89, wpb=6932, bsz=246.7, num_updates=51400, lr=9.6636e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 05:04:00 | INFO | train_inner | epoch 065:    476 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.021, ppl=2.03, wps=13134.8, ups=1.89, wpb=6957.5, bsz=253.9, num_updates=51500, lr=9.65422e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 05:04:54 | INFO | train_inner | epoch 065:    576 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.027, ppl=2.04, wps=13163.2, ups=1.87, wpb=7053, bsz=262.9, num_updates=51600, lr=9.64486e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 05:05:47 | INFO | train_inner | epoch 065:    676 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.031, ppl=2.04, wps=13157.4, ups=1.87, wpb=7018.7, bsz=239.4, num_updates=51700, lr=9.63552e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 05:06:41 | INFO | train_inner | epoch 065:    776 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.022, ppl=2.03, wps=13128.1, ups=1.87, wpb=7029, bsz=243, num_updates=51800, lr=9.62622e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 05:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:07:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:07:43 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.405 | nll_loss 3.981 | ppl 15.79 | bleu 21.97 | wps 3350.2 | wpb 5162.1 | bsz 187.5 | num_updates 51865 | best_bleu 22.31
2021-01-09 05:07:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:07:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:07:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:07:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 65 @ 51865 updates, score 21.97) (writing took 2.7277137394994497 seconds)
2021-01-09 05:07:46 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-09 05:07:46 | INFO | train | epoch 065 | symm_kl 0.497 | self_kl 0 | self_cv 0 | loss 3.43 | nll_loss 1.03 | ppl 2.04 | wps 12242.7 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 51865 | lr 9.62019e-06 | gnorm 0.747 | train_wall 445 | wall 0
2021-01-09 05:07:46 | INFO | fairseq.trainer | begin training epoch 66
2021-01-09 05:07:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:07:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:08:07 | INFO | train_inner | epoch 066:     35 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.046, ppl=2.07, wps=8091.1, ups=1.16, wpb=6971.8, bsz=247.1, num_updates=51900, lr=9.61694e-06, gnorm=0.753, train_wall=52, wall=0
2021-01-09 05:09:00 | INFO | train_inner | epoch 066:    135 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.03, ppl=2.04, wps=13228.6, ups=1.9, wpb=6970.6, bsz=261.8, num_updates=52000, lr=9.60769e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 05:09:53 | INFO | train_inner | epoch 066:    235 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.008, ppl=2.01, wps=13117.2, ups=1.87, wpb=7027.6, bsz=239, num_updates=52100, lr=9.59846e-06, gnorm=0.737, train_wall=53, wall=0
2021-01-09 05:10:46 | INFO | train_inner | epoch 066:    335 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.033, ppl=2.05, wps=13056.1, ups=1.88, wpb=6937.4, bsz=253, num_updates=52200, lr=9.58927e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 05:11:40 | INFO | train_inner | epoch 066:    435 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.015, ppl=2.02, wps=13195.1, ups=1.86, wpb=7077, bsz=249.8, num_updates=52300, lr=9.58009e-06, gnorm=0.735, train_wall=53, wall=0
2021-01-09 05:12:33 | INFO | train_inner | epoch 066:    535 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.019, ppl=2.03, wps=13371.7, ups=1.88, wpb=7113.8, bsz=237.6, num_updates=52400, lr=9.57095e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 05:13:26 | INFO | train_inner | epoch 066:    635 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.039, ppl=2.06, wps=13143.8, ups=1.89, wpb=6970.5, bsz=241.4, num_updates=52500, lr=9.56183e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 05:14:19 | INFO | train_inner | epoch 066:    735 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.038, ppl=2.05, wps=13028.6, ups=1.88, wpb=6931.3, bsz=251.5, num_updates=52600, lr=9.55274e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 05:15:13 | INFO | train_inner | epoch 066:    835 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.048, ppl=2.07, wps=13137.8, ups=1.88, wpb=6979.3, bsz=238.7, num_updates=52700, lr=9.54367e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 05:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:15:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:15:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:15:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:15:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:15:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:15:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:15:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:15:43 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.411 | nll_loss 3.984 | ppl 15.82 | bleu 21.96 | wps 3359 | wpb 5162.1 | bsz 187.5 | num_updates 52706 | best_bleu 22.31
2021-01-09 05:15:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:15:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:15:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:15:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 66 @ 52706 updates, score 21.96) (writing took 2.7800334095954895 seconds)
2021-01-09 05:15:46 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-09 05:15:46 | INFO | train | epoch 066 | symm_kl 0.496 | self_kl 0 | self_cv 0 | loss 3.428 | nll_loss 1.03 | ppl 2.04 | wps 12252.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 52706 | lr 9.54312e-06 | gnorm 0.747 | train_wall 445 | wall 0
2021-01-09 05:15:46 | INFO | fairseq.trainer | begin training epoch 67
2021-01-09 05:15:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:15:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:16:38 | INFO | train_inner | epoch 067:     94 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.038, ppl=2.05, wps=8112.8, ups=1.16, wpb=6964.5, bsz=243.3, num_updates=52800, lr=9.53463e-06, gnorm=0.754, train_wall=52, wall=0
2021-01-09 05:17:32 | INFO | train_inner | epoch 067:    194 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.026, ppl=2.04, wps=13162.4, ups=1.87, wpb=7053.6, bsz=242.5, num_updates=52900, lr=9.52561e-06, gnorm=0.74, train_wall=53, wall=0
2021-01-09 05:18:26 | INFO | train_inner | epoch 067:    294 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.041, ppl=2.06, wps=13078.3, ups=1.87, wpb=7005.4, bsz=252.7, num_updates=53000, lr=9.51662e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 05:19:19 | INFO | train_inner | epoch 067:    394 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.027, ppl=2.04, wps=13221.3, ups=1.88, wpb=7041.6, bsz=248.8, num_updates=53100, lr=9.50765e-06, gnorm=0.736, train_wall=53, wall=0
2021-01-09 05:20:12 | INFO | train_inner | epoch 067:    494 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.02, ppl=2.03, wps=12928.8, ups=1.88, wpb=6891.2, bsz=250.3, num_updates=53200, lr=9.49871e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 05:21:05 | INFO | train_inner | epoch 067:    594 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.018, ppl=2.02, wps=13348.4, ups=1.89, wpb=7079.5, bsz=237.4, num_updates=53300, lr=9.4898e-06, gnorm=0.736, train_wall=53, wall=0
2021-01-09 05:21:58 | INFO | train_inner | epoch 067:    694 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.034, ppl=2.05, wps=13309.9, ups=1.9, wpb=6994.8, bsz=253.9, num_updates=53400, lr=9.48091e-06, gnorm=0.751, train_wall=52, wall=0
2021-01-09 05:22:51 | INFO | train_inner | epoch 067:    794 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.036, ppl=2.05, wps=13029.8, ups=1.89, wpb=6885.9, bsz=246.1, num_updates=53500, lr=9.47204e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 05:23:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:23:43 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.41 | nll_loss 3.985 | ppl 15.83 | bleu 22.02 | wps 3330.5 | wpb 5162.1 | bsz 187.5 | num_updates 53547 | best_bleu 22.31
2021-01-09 05:23:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:23:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:23:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 67 @ 53547 updates, score 22.02) (writing took 2.7708417419344187 seconds)
2021-01-09 05:23:46 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-09 05:23:46 | INFO | train | epoch 067 | symm_kl 0.496 | self_kl 0 | self_cv 0 | loss 3.427 | nll_loss 1.029 | ppl 2.04 | wps 12255.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 53547 | lr 9.46789e-06 | gnorm 0.748 | train_wall 444 | wall 0
2021-01-09 05:23:46 | INFO | fairseq.trainer | begin training epoch 68
2021-01-09 05:23:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:23:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:23:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:24:17 | INFO | train_inner | epoch 068:     53 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.02, ppl=2.03, wps=8123.7, ups=1.16, wpb=7007.1, bsz=247, num_updates=53600, lr=9.4632e-06, gnorm=0.743, train_wall=52, wall=0
2021-01-09 05:25:10 | INFO | train_inner | epoch 068:    153 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.028, ppl=2.04, wps=12942.8, ups=1.87, wpb=6904.5, bsz=248, num_updates=53700, lr=9.45439e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 05:26:03 | INFO | train_inner | epoch 068:    253 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.021, ppl=2.03, wps=13220.1, ups=1.88, wpb=7017.6, bsz=259.4, num_updates=53800, lr=9.4456e-06, gnorm=0.74, train_wall=53, wall=0
2021-01-09 05:26:56 | INFO | train_inner | epoch 068:    353 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.02, ppl=2.03, wps=13153.2, ups=1.88, wpb=6983.4, bsz=245, num_updates=53900, lr=9.43683e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 05:27:50 | INFO | train_inner | epoch 068:    453 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.038, ppl=2.05, wps=13122.6, ups=1.87, wpb=7008.2, bsz=252.1, num_updates=54000, lr=9.42809e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 05:28:43 | INFO | train_inner | epoch 068:    553 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.034, ppl=2.05, wps=13208.3, ups=1.89, wpb=6993.4, bsz=241, num_updates=54100, lr=9.41937e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 05:29:36 | INFO | train_inner | epoch 068:    653 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.031, ppl=2.04, wps=13294.5, ups=1.88, wpb=7055.8, bsz=249.1, num_updates=54200, lr=9.41068e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 05:30:29 | INFO | train_inner | epoch 068:    753 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.039, ppl=2.06, wps=13165.2, ups=1.89, wpb=6982.1, bsz=236.2, num_updates=54300, lr=9.40201e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 05:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:31:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:31:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:31:43 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.41 | nll_loss 3.985 | ppl 15.83 | bleu 22 | wps 3343.3 | wpb 5162.1 | bsz 187.5 | num_updates 54388 | best_bleu 22.31
2021-01-09 05:31:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:31:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:31:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:31:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 68 @ 54388 updates, score 22.0) (writing took 2.8671703785657883 seconds)
2021-01-09 05:31:46 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-09 05:31:46 | INFO | train | epoch 068 | symm_kl 0.495 | self_kl 0 | self_cv 0 | loss 3.426 | nll_loss 1.029 | ppl 2.04 | wps 12254.6 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 54388 | lr 9.3944e-06 | gnorm 0.748 | train_wall 445 | wall 0
2021-01-09 05:31:46 | INFO | fairseq.trainer | begin training epoch 69
2021-01-09 05:31:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:31:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:31:55 | INFO | train_inner | epoch 069:     12 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.025, ppl=2.04, wps=8070.7, ups=1.16, wpb=6978.6, bsz=237, num_updates=54400, lr=9.39336e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 05:32:47 | INFO | train_inner | epoch 069:    112 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.038, ppl=2.05, wps=13401.3, ups=1.92, wpb=6972, bsz=246.2, num_updates=54500, lr=9.38474e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 05:33:41 | INFO | train_inner | epoch 069:    212 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.019, ppl=2.03, wps=12959.1, ups=1.87, wpb=6937.2, bsz=252.2, num_updates=54600, lr=9.37614e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 05:34:34 | INFO | train_inner | epoch 069:    312 / 841 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.035, ppl=2.05, wps=13301.6, ups=1.89, wpb=7040.2, bsz=241.8, num_updates=54700, lr=9.36757e-06, gnorm=0.742, train_wall=53, wall=0
2021-01-09 05:35:27 | INFO | train_inner | epoch 069:    412 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.003, ppl=2, wps=13368.4, ups=1.88, wpb=7112.1, bsz=261.1, num_updates=54800, lr=9.35902e-06, gnorm=0.728, train_wall=53, wall=0
2021-01-09 05:36:20 | INFO | train_inner | epoch 069:    512 / 841 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.04, ppl=2.06, wps=13314.1, ups=1.88, wpb=7063.4, bsz=238, num_updates=54900, lr=9.35049e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 05:37:14 | INFO | train_inner | epoch 069:    612 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.024, ppl=2.03, wps=13090.7, ups=1.86, wpb=7023.5, bsz=251.9, num_updates=55000, lr=9.34199e-06, gnorm=0.743, train_wall=53, wall=0
2021-01-09 05:38:07 | INFO | train_inner | epoch 069:    712 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.038, ppl=2.05, wps=13053.9, ups=1.89, wpb=6897.3, bsz=237.2, num_updates=55100, lr=9.33351e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 05:38:59 | INFO | train_inner | epoch 069:    812 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.03, ppl=2.04, wps=13108.4, ups=1.89, wpb=6927.8, bsz=247.9, num_updates=55200, lr=9.32505e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 05:39:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:39:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:39:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:39:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:39:42 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.408 | nll_loss 3.983 | ppl 15.81 | bleu 22.04 | wps 3379 | wpb 5162.1 | bsz 187.5 | num_updates 55229 | best_bleu 22.31
2021-01-09 05:39:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:39:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:39:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:39:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 69 @ 55229 updates, score 22.04) (writing took 2.771868050098419 seconds)
2021-01-09 05:39:45 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-09 05:39:45 | INFO | train | epoch 069 | symm_kl 0.494 | self_kl 0 | self_cv 0 | loss 3.424 | nll_loss 1.029 | ppl 2.04 | wps 12282.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 55229 | lr 9.3226e-06 | gnorm 0.749 | train_wall 444 | wall 0
2021-01-09 05:39:45 | INFO | fairseq.trainer | begin training epoch 70
2021-01-09 05:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:40:24 | INFO | train_inner | epoch 070:     71 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.017, ppl=2.02, wps=8236.6, ups=1.18, wpb=6995.9, bsz=246.4, num_updates=55300, lr=9.31661e-06, gnorm=0.745, train_wall=51, wall=0
2021-01-09 05:41:16 | INFO | train_inner | epoch 070:    171 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.036, ppl=2.05, wps=13479.9, ups=1.92, wpb=7021.9, bsz=251.8, num_updates=55400, lr=9.3082e-06, gnorm=0.75, train_wall=52, wall=0
2021-01-09 05:42:09 | INFO | train_inner | epoch 070:    271 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.025, ppl=2.03, wps=13206.3, ups=1.89, wpb=7002.1, bsz=243.2, num_updates=55500, lr=9.29981e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 05:43:02 | INFO | train_inner | epoch 070:    371 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.032, ppl=2.04, wps=13259.6, ups=1.9, wpb=6987.8, bsz=247.3, num_updates=55600, lr=9.29144e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 05:43:55 | INFO | train_inner | epoch 070:    471 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.029, ppl=2.04, wps=13264, ups=1.89, wpb=7000.1, bsz=241.9, num_updates=55700, lr=9.2831e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 05:44:48 | INFO | train_inner | epoch 070:    571 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.021, ppl=2.03, wps=13249.1, ups=1.89, wpb=7014.7, bsz=241.8, num_updates=55800, lr=9.27478e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 05:45:41 | INFO | train_inner | epoch 070:    671 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.034, ppl=2.05, wps=13188.2, ups=1.88, wpb=6999.8, bsz=255.8, num_updates=55900, lr=9.26648e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 05:46:34 | INFO | train_inner | epoch 070:    771 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.026, ppl=2.04, wps=13097.1, ups=1.87, wpb=6985.7, bsz=248.3, num_updates=56000, lr=9.2582e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 05:47:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:47:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:47:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:47:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:47:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:47:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:47:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:47:39 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.409 | nll_loss 3.985 | ppl 15.83 | bleu 22.03 | wps 3368 | wpb 5162.1 | bsz 187.5 | num_updates 56070 | best_bleu 22.31
2021-01-09 05:47:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:47:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:47:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 70 @ 56070 updates, score 22.03) (writing took 2.763962771743536 seconds)
2021-01-09 05:47:42 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-09 05:47:42 | INFO | train | epoch 070 | symm_kl 0.494 | self_kl 0 | self_cv 0 | loss 3.424 | nll_loss 1.029 | ppl 2.04 | wps 12316.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 56070 | lr 9.25242e-06 | gnorm 0.75 | train_wall 442 | wall 0
2021-01-09 05:47:42 | INFO | fairseq.trainer | begin training epoch 71
2021-01-09 05:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:48:01 | INFO | train_inner | epoch 071:     30 / 841 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.053, ppl=2.07, wps=7964.3, ups=1.16, wpb=6881.3, bsz=231.8, num_updates=56100, lr=9.24995e-06, gnorm=0.774, train_wall=53, wall=0
2021-01-09 05:48:53 | INFO | train_inner | epoch 071:    130 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.031, ppl=2.04, wps=13418.3, ups=1.9, wpb=7047.1, bsz=253.8, num_updates=56200, lr=9.24171e-06, gnorm=0.737, train_wall=52, wall=0
2021-01-09 05:49:46 | INFO | train_inner | epoch 071:    230 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.031, ppl=2.04, wps=13252.9, ups=1.88, wpb=7042.6, bsz=237.8, num_updates=56300, lr=9.2335e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 05:50:39 | INFO | train_inner | epoch 071:    330 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.023, ppl=2.03, wps=13153.6, ups=1.9, wpb=6932.6, bsz=245.6, num_updates=56400, lr=9.22531e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 05:51:32 | INFO | train_inner | epoch 071:    430 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.038, ppl=2.05, wps=13110.9, ups=1.89, wpb=6922.6, bsz=251.4, num_updates=56500, lr=9.21714e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 05:52:25 | INFO | train_inner | epoch 071:    530 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.028, ppl=2.04, wps=13382.1, ups=1.89, wpb=7081.8, bsz=244, num_updates=56600, lr=9.209e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 05:53:18 | INFO | train_inner | epoch 071:    630 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.031, ppl=2.04, wps=13339.4, ups=1.89, wpb=7048.8, bsz=251.4, num_updates=56700, lr=9.20087e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 05:54:11 | INFO | train_inner | epoch 071:    730 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.009, ppl=2.01, wps=13090.7, ups=1.89, wpb=6938.2, bsz=257.1, num_updates=56800, lr=9.19277e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 05:55:03 | INFO | train_inner | epoch 071:    830 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.025, ppl=2.03, wps=13169, ups=1.89, wpb=6959.2, bsz=244.6, num_updates=56900, lr=9.18469e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 05:55:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 05:55:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:55:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:55:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:55:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 05:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 05:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 05:55:37 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.409 | nll_loss 3.985 | ppl 15.84 | bleu 21.98 | wps 3356.6 | wpb 5162.1 | bsz 187.5 | num_updates 56911 | best_bleu 22.31
2021-01-09 05:55:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 05:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:55:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 71 @ 56911 updates, score 21.98) (writing took 2.774215156212449 seconds)
2021-01-09 05:55:40 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-09 05:55:40 | INFO | train | epoch 071 | symm_kl 0.493 | self_kl 0 | self_cv 0 | loss 3.422 | nll_loss 1.028 | ppl 2.04 | wps 12313.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 56911 | lr 9.1838e-06 | gnorm 0.751 | train_wall 442 | wall 0
2021-01-09 05:55:40 | INFO | fairseq.trainer | begin training epoch 72
2021-01-09 05:55:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 05:55:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 05:56:29 | INFO | train_inner | epoch 072:     89 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.029, ppl=2.04, wps=8071.1, ups=1.17, wpb=6887.3, bsz=251.4, num_updates=57000, lr=9.17663e-06, gnorm=0.756, train_wall=51, wall=0
2021-01-09 05:57:22 | INFO | train_inner | epoch 072:    189 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.028, ppl=2.04, wps=13064.9, ups=1.88, wpb=6937.3, bsz=240.8, num_updates=57100, lr=9.16859e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 05:58:15 | INFO | train_inner | epoch 072:    289 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.027, ppl=2.04, wps=13429.5, ups=1.88, wpb=7147.2, bsz=255, num_updates=57200, lr=9.16057e-06, gnorm=0.735, train_wall=53, wall=0
2021-01-09 05:59:08 | INFO | train_inner | epoch 072:    389 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.047, ppl=2.07, wps=13240.9, ups=1.9, wpb=6974.2, bsz=239, num_updates=57300, lr=9.15258e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 06:00:01 | INFO | train_inner | epoch 072:    489 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.023, ppl=2.03, wps=13279.4, ups=1.88, wpb=7060.7, bsz=236, num_updates=57400, lr=9.1446e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 06:00:54 | INFO | train_inner | epoch 072:    589 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.009, ppl=2.01, wps=13329.6, ups=1.9, wpb=7032.1, bsz=260.1, num_updates=57500, lr=9.13664e-06, gnorm=0.741, train_wall=53, wall=0
2021-01-09 06:01:47 | INFO | train_inner | epoch 072:    689 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.032, ppl=2.05, wps=13006.5, ups=1.89, wpb=6871.5, bsz=251.1, num_updates=57600, lr=9.12871e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 06:02:39 | INFO | train_inner | epoch 072:    789 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.027, ppl=2.04, wps=13351.2, ups=1.89, wpb=7047.1, bsz=238.8, num_updates=57700, lr=9.1208e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 06:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:03:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:03:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:03:34 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.412 | nll_loss 3.986 | ppl 15.84 | bleu 21.99 | wps 3336.1 | wpb 5162.1 | bsz 187.5 | num_updates 57752 | best_bleu 22.31
2021-01-09 06:03:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:03:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:03:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:03:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 72 @ 57752 updates, score 21.99) (writing took 2.742609655484557 seconds)
2021-01-09 06:03:37 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-09 06:03:37 | INFO | train | epoch 072 | symm_kl 0.493 | self_kl 0 | self_cv 0 | loss 3.421 | nll_loss 1.027 | ppl 2.04 | wps 12322.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 57752 | lr 9.11669e-06 | gnorm 0.75 | train_wall 442 | wall 0
2021-01-09 06:03:37 | INFO | fairseq.trainer | begin training epoch 73
2021-01-09 06:03:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:03:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:04:05 | INFO | train_inner | epoch 073:     48 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.018, ppl=2.03, wps=8190.5, ups=1.17, wpb=7009.4, bsz=239.8, num_updates=57800, lr=9.1129e-06, gnorm=0.745, train_wall=52, wall=0
2021-01-09 06:04:58 | INFO | train_inner | epoch 073:    148 / 841 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.035, ppl=2.05, wps=13093.7, ups=1.88, wpb=6970.3, bsz=245.8, num_updates=57900, lr=9.10503e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 06:05:51 | INFO | train_inner | epoch 073:    248 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.014, ppl=2.02, wps=13177.7, ups=1.91, wpb=6916.7, bsz=242, num_updates=58000, lr=9.09718e-06, gnorm=0.753, train_wall=52, wall=0
2021-01-09 06:06:43 | INFO | train_inner | epoch 073:    348 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.027, ppl=2.04, wps=13015.4, ups=1.89, wpb=6877.5, bsz=241.2, num_updates=58100, lr=9.08934e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 06:07:36 | INFO | train_inner | epoch 073:    448 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.031, ppl=2.04, wps=13326.8, ups=1.9, wpb=7010.8, bsz=247.5, num_updates=58200, lr=9.08153e-06, gnorm=0.75, train_wall=52, wall=0
2021-01-09 06:08:29 | INFO | train_inner | epoch 073:    548 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.006, ppl=2.01, wps=13249.2, ups=1.87, wpb=7072.4, bsz=265.2, num_updates=58300, lr=9.07374e-06, gnorm=0.735, train_wall=53, wall=0
2021-01-09 06:09:23 | INFO | train_inner | epoch 073:    648 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.025, ppl=2.03, wps=13185.9, ups=1.88, wpb=7029.5, bsz=256.2, num_updates=58400, lr=9.06597e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 06:10:16 | INFO | train_inner | epoch 073:    748 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.049, ppl=2.07, wps=13219, ups=1.89, wpb=6976.2, bsz=239.1, num_updates=58500, lr=9.05822e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 06:11:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:11:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:11:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:11:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:11:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:11:32 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.412 | nll_loss 3.987 | ppl 15.86 | bleu 21.98 | wps 3347.4 | wpb 5162.1 | bsz 187.5 | num_updates 58593 | best_bleu 22.31
2021-01-09 06:11:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:11:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:11:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:11:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 73 @ 58593 updates, score 21.98) (writing took 2.7978009562939405 seconds)
2021-01-09 06:11:35 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-09 06:11:35 | INFO | train | epoch 073 | symm_kl 0.492 | self_kl 0 | self_cv 0 | loss 3.42 | nll_loss 1.027 | ppl 2.04 | wps 12299.8 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 58593 | lr 9.05102e-06 | gnorm 0.751 | train_wall 443 | wall 0
2021-01-09 06:11:35 | INFO | fairseq.trainer | begin training epoch 74
2021-01-09 06:11:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:11:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:11:42 | INFO | train_inner | epoch 074:      7 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.037, ppl=2.05, wps=8128.1, ups=1.16, wpb=7028.7, bsz=235.1, num_updates=58600, lr=9.05048e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 06:12:34 | INFO | train_inner | epoch 074:    107 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.031, ppl=2.04, wps=13475.3, ups=1.93, wpb=6999.6, bsz=247.8, num_updates=58700, lr=9.04277e-06, gnorm=0.754, train_wall=52, wall=0
2021-01-09 06:13:27 | INFO | train_inner | epoch 074:    207 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.018, ppl=2.03, wps=13410.3, ups=1.88, wpb=7115.3, bsz=251, num_updates=58800, lr=9.03508e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 06:14:20 | INFO | train_inner | epoch 074:    307 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.008, ppl=2.01, wps=13237.2, ups=1.88, wpb=7040, bsz=254.3, num_updates=58900, lr=9.02741e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 06:15:13 | INFO | train_inner | epoch 074:    407 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.04, ppl=2.06, wps=13392.6, ups=1.9, wpb=7044.3, bsz=245.8, num_updates=59000, lr=9.01975e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 06:16:06 | INFO | train_inner | epoch 074:    507 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.033, ppl=2.05, wps=13228.3, ups=1.89, wpb=6992.9, bsz=236.7, num_updates=59100, lr=9.01212e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 06:16:58 | INFO | train_inner | epoch 074:    607 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.021, ppl=2.03, wps=13116.3, ups=1.89, wpb=6927.4, bsz=251.4, num_updates=59200, lr=9.0045e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 06:17:51 | INFO | train_inner | epoch 074:    707 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.038, ppl=2.05, wps=13047.5, ups=1.9, wpb=6861.4, bsz=241.2, num_updates=59300, lr=8.99691e-06, gnorm=0.768, train_wall=52, wall=0
2021-01-09 06:18:44 | INFO | train_inner | epoch 074:    807 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.026, ppl=2.04, wps=13029.1, ups=1.87, wpb=6957.9, bsz=248.5, num_updates=59400, lr=8.98933e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 06:19:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:19:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:19:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:19:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:19:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:19:30 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.417 | nll_loss 3.993 | ppl 15.92 | bleu 21.9 | wps 3363.4 | wpb 5162.1 | bsz 187.5 | num_updates 59434 | best_bleu 22.31
2021-01-09 06:19:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:19:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 74 @ 59434 updates, score 21.9) (writing took 2.7797882724553347 seconds)
2021-01-09 06:19:33 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-09 06:19:33 | INFO | train | epoch 074 | symm_kl 0.492 | self_kl 0 | self_cv 0 | loss 3.419 | nll_loss 1.027 | ppl 2.04 | wps 12313.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 59434 | lr 8.98676e-06 | gnorm 0.753 | train_wall 442 | wall 0
2021-01-09 06:19:33 | INFO | fairseq.trainer | begin training epoch 75
2021-01-09 06:19:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:19:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:20:10 | INFO | train_inner | epoch 075:     66 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.009, ppl=2.01, wps=8223, ups=1.17, wpb=7029.4, bsz=251.4, num_updates=59500, lr=8.98177e-06, gnorm=0.738, train_wall=52, wall=0
2021-01-09 06:21:03 | INFO | train_inner | epoch 075:    166 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.011, ppl=2.02, wps=13141.6, ups=1.88, wpb=6975.7, bsz=252.1, num_updates=59600, lr=8.97424e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 06:21:56 | INFO | train_inner | epoch 075:    266 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.025, ppl=2.03, wps=13395.5, ups=1.88, wpb=7132.8, bsz=248.8, num_updates=59700, lr=8.96672e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 06:22:49 | INFO | train_inner | epoch 075:    366 / 841 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.039, ppl=2.05, wps=13314.2, ups=1.88, wpb=7080, bsz=247.3, num_updates=59800, lr=8.95922e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 06:23:42 | INFO | train_inner | epoch 075:    466 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.031, ppl=2.04, wps=13120.4, ups=1.89, wpb=6937.2, bsz=236.8, num_updates=59900, lr=8.95173e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 06:24:35 | INFO | train_inner | epoch 075:    566 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.027, ppl=2.04, wps=13083.9, ups=1.91, wpb=6843.9, bsz=239.8, num_updates=60000, lr=8.94427e-06, gnorm=0.766, train_wall=52, wall=0
2021-01-09 06:25:28 | INFO | train_inner | epoch 075:    666 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.022, ppl=2.03, wps=13169.5, ups=1.89, wpb=6969.8, bsz=252.6, num_updates=60100, lr=8.93683e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 06:26:20 | INFO | train_inner | epoch 075:    766 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.034, ppl=2.05, wps=13200.7, ups=1.9, wpb=6965.3, bsz=243.4, num_updates=60200, lr=8.9294e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 06:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:27:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:27:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:27:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:27:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:27:28 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.41 | nll_loss 3.988 | ppl 15.86 | bleu 21.92 | wps 3374.1 | wpb 5162.1 | bsz 187.5 | num_updates 60275 | best_bleu 22.31
2021-01-09 06:27:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:27:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:27:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 75 @ 60275 updates, score 21.92) (writing took 2.79053970053792 seconds)
2021-01-09 06:27:31 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-09 06:27:31 | INFO | train | epoch 075 | symm_kl 0.491 | self_kl 0 | self_cv 0 | loss 3.417 | nll_loss 1.026 | ppl 2.04 | wps 12308.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 60275 | lr 8.92384e-06 | gnorm 0.752 | train_wall 443 | wall 0
2021-01-09 06:27:31 | INFO | fairseq.trainer | begin training epoch 76
2021-01-09 06:27:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:27:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:27:47 | INFO | train_inner | epoch 076:     25 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.024, ppl=2.03, wps=8098.4, ups=1.16, wpb=6997.4, bsz=251.6, num_updates=60300, lr=8.92199e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 06:28:39 | INFO | train_inner | epoch 076:    125 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.014, ppl=2.02, wps=13288.9, ups=1.9, wpb=7010.9, bsz=266.4, num_updates=60400, lr=8.91461e-06, gnorm=0.738, train_wall=53, wall=0
2021-01-09 06:29:32 | INFO | train_inner | epoch 076:    225 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.027, ppl=2.04, wps=13379.1, ups=1.89, wpb=7086.8, bsz=244.3, num_updates=60500, lr=8.90724e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 06:30:25 | INFO | train_inner | epoch 076:    325 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.03, ppl=2.04, wps=13238.1, ups=1.89, wpb=7012.2, bsz=264.1, num_updates=60600, lr=8.89988e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 06:31:18 | INFO | train_inner | epoch 076:    425 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.024, ppl=2.03, wps=13251.1, ups=1.91, wpb=6936.5, bsz=241, num_updates=60700, lr=8.89255e-06, gnorm=0.758, train_wall=52, wall=0
2021-01-09 06:32:11 | INFO | train_inner | epoch 076:    525 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.021, ppl=2.03, wps=13247.5, ups=1.89, wpb=7027.3, bsz=230.2, num_updates=60800, lr=8.88523e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 06:33:04 | INFO | train_inner | epoch 076:    625 / 841 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.033, ppl=2.05, wps=13170.6, ups=1.9, wpb=6947.3, bsz=234.2, num_updates=60900, lr=8.87794e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 06:33:57 | INFO | train_inner | epoch 076:    725 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.03, ppl=2.04, wps=13162.4, ups=1.88, wpb=6988.1, bsz=243.9, num_updates=61000, lr=8.87066e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 06:34:50 | INFO | train_inner | epoch 076:    825 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.038, ppl=2.05, wps=13151.1, ups=1.89, wpb=6965.3, bsz=243.3, num_updates=61100, lr=8.86339e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 06:34:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:34:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:34:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:35:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:35:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:35:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:35:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:35:26 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.415 | nll_loss 3.992 | ppl 15.91 | bleu 21.98 | wps 3318.1 | wpb 5162.1 | bsz 187.5 | num_updates 61116 | best_bleu 22.31
2021-01-09 06:35:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:35:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:35:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:35:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 76 @ 61116 updates, score 21.98) (writing took 2.7973811626434326 seconds)
2021-01-09 06:35:29 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-09 06:35:29 | INFO | train | epoch 076 | symm_kl 0.491 | self_kl 0 | self_cv 0 | loss 3.416 | nll_loss 1.026 | ppl 2.04 | wps 12298.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 61116 | lr 8.86223e-06 | gnorm 0.752 | train_wall 443 | wall 0
2021-01-09 06:35:29 | INFO | fairseq.trainer | begin training epoch 77
2021-01-09 06:35:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:35:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:36:15 | INFO | train_inner | epoch 077:     84 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.003, ppl=2, wps=8059.3, ups=1.17, wpb=6902.4, bsz=253.5, num_updates=61200, lr=8.85615e-06, gnorm=0.747, train_wall=52, wall=0
2021-01-09 06:37:09 | INFO | train_inner | epoch 077:    184 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.018, ppl=2.02, wps=13201.7, ups=1.88, wpb=7034.7, bsz=241.5, num_updates=61300, lr=8.84892e-06, gnorm=0.742, train_wall=53, wall=0
2021-01-09 06:38:01 | INFO | train_inner | epoch 077:    284 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.021, ppl=2.03, wps=13313, ups=1.9, wpb=7023.8, bsz=249, num_updates=61400, lr=8.84171e-06, gnorm=0.739, train_wall=53, wall=0
2021-01-09 06:38:54 | INFO | train_inner | epoch 077:    384 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.026, ppl=2.04, wps=13299.4, ups=1.89, wpb=7044, bsz=241.3, num_updates=61500, lr=8.83452e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 06:39:47 | INFO | train_inner | epoch 077:    484 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.032, ppl=2.05, wps=13282.5, ups=1.9, wpb=6992.1, bsz=251.6, num_updates=61600, lr=8.82735e-06, gnorm=0.747, train_wall=52, wall=0
2021-01-09 06:40:40 | INFO | train_inner | epoch 077:    584 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.028, ppl=2.04, wps=13102.5, ups=1.89, wpb=6931.1, bsz=228.5, num_updates=61700, lr=8.82019e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 06:41:33 | INFO | train_inner | epoch 077:    684 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.03, ppl=2.04, wps=13332.3, ups=1.89, wpb=7048.8, bsz=246.5, num_updates=61800, lr=8.81305e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 06:42:26 | INFO | train_inner | epoch 077:    784 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.045, ppl=2.06, wps=13270.5, ups=1.89, wpb=7034.2, bsz=259.6, num_updates=61900, lr=8.80593e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 06:42:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:42:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:42:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:42:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:42:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:43:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:43:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:43:23 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.415 | nll_loss 3.991 | ppl 15.9 | bleu 21.93 | wps 3371.3 | wpb 5162.1 | bsz 187.5 | num_updates 61957 | best_bleu 22.31
2021-01-09 06:43:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:43:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:43:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:43:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 77 @ 61957 updates, score 21.93) (writing took 2.757630342617631 seconds)
2021-01-09 06:43:26 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-09 06:43:26 | INFO | train | epoch 077 | symm_kl 0.49 | self_kl 0 | self_cv 0 | loss 3.415 | nll_loss 1.025 | ppl 2.04 | wps 12330.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 61957 | lr 8.80188e-06 | gnorm 0.751 | train_wall 442 | wall 0
2021-01-09 06:43:26 | INFO | fairseq.trainer | begin training epoch 78
2021-01-09 06:43:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:43:51 | INFO | train_inner | epoch 078:     43 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.027, ppl=2.04, wps=8014.6, ups=1.18, wpb=6805.6, bsz=239.4, num_updates=62000, lr=8.79883e-06, gnorm=0.772, train_wall=51, wall=0
2021-01-09 06:44:43 | INFO | train_inner | epoch 078:    143 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.034, ppl=2.05, wps=13191.4, ups=1.89, wpb=6971.3, bsz=250.7, num_updates=62100, lr=8.79174e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 06:45:37 | INFO | train_inner | epoch 078:    243 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.018, ppl=2.03, wps=13313.7, ups=1.89, wpb=7062.2, bsz=246.1, num_updates=62200, lr=8.78467e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 06:46:30 | INFO | train_inner | epoch 078:    343 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.023, ppl=2.03, wps=13270.3, ups=1.89, wpb=7038.2, bsz=236.4, num_updates=62300, lr=8.77762e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 06:47:23 | INFO | train_inner | epoch 078:    443 / 841 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.02, ppl=2.03, wps=13186.1, ups=1.88, wpb=7017.1, bsz=253.3, num_updates=62400, lr=8.77058e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 06:48:16 | INFO | train_inner | epoch 078:    543 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.018, ppl=2.03, wps=13237.6, ups=1.87, wpb=7077.7, bsz=253, num_updates=62500, lr=8.76356e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 06:49:09 | INFO | train_inner | epoch 078:    643 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.03, ppl=2.04, wps=13227.3, ups=1.9, wpb=6952.1, bsz=251.8, num_updates=62600, lr=8.75656e-06, gnorm=0.762, train_wall=52, wall=0
2021-01-09 06:50:02 | INFO | train_inner | epoch 078:    743 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.035, ppl=2.05, wps=13170.5, ups=1.89, wpb=6981.4, bsz=245.8, num_updates=62700, lr=8.74957e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 06:50:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:50:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:50:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:50:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:50:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:50:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:50:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:50:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:50:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:50:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:50:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:51:21 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.415 | nll_loss 3.993 | ppl 15.92 | bleu 21.92 | wps 3345.8 | wpb 5162.1 | bsz 187.5 | num_updates 62798 | best_bleu 22.31
2021-01-09 06:51:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:51:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:51:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 78 @ 62798 updates, score 21.92) (writing took 2.7176847867667675 seconds)
2021-01-09 06:51:24 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-09 06:51:24 | INFO | train | epoch 078 | symm_kl 0.49 | self_kl 0 | self_cv 0 | loss 3.415 | nll_loss 1.025 | ppl 2.04 | wps 12299.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 62798 | lr 8.74274e-06 | gnorm 0.754 | train_wall 443 | wall 0
2021-01-09 06:51:24 | INFO | fairseq.trainer | begin training epoch 79
2021-01-09 06:51:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:51:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:51:28 | INFO | train_inner | epoch 079:      2 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.015, ppl=2.02, wps=8050.4, ups=1.16, wpb=6960.2, bsz=242.8, num_updates=62800, lr=8.7426e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 06:52:20 | INFO | train_inner | epoch 079:    102 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.026, ppl=2.04, wps=13419.8, ups=1.94, wpb=6930, bsz=233.3, num_updates=62900, lr=8.73565e-06, gnorm=0.759, train_wall=51, wall=0
2021-01-09 06:53:13 | INFO | train_inner | epoch 079:    202 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.028, ppl=2.04, wps=13053.9, ups=1.87, wpb=6965.5, bsz=249.5, num_updates=63000, lr=8.72872e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 06:54:07 | INFO | train_inner | epoch 079:    302 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.022, ppl=2.03, wps=13165.1, ups=1.87, wpb=7022.7, bsz=245.8, num_updates=63100, lr=8.7218e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 06:55:00 | INFO | train_inner | epoch 079:    402 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.009, ppl=2.01, wps=13279.7, ups=1.89, wpb=7035.6, bsz=252.4, num_updates=63200, lr=8.71489e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 06:55:52 | INFO | train_inner | epoch 079:    502 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.034, ppl=2.05, wps=13377.6, ups=1.89, wpb=7073.9, bsz=257.7, num_updates=63300, lr=8.70801e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 06:56:45 | INFO | train_inner | epoch 079:    602 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.024, ppl=2.03, wps=13169.8, ups=1.91, wpb=6890.8, bsz=231.4, num_updates=63400, lr=8.70114e-06, gnorm=0.77, train_wall=52, wall=0
2021-01-09 06:57:38 | INFO | train_inner | epoch 079:    702 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.03, ppl=2.04, wps=13220.3, ups=1.9, wpb=6971.7, bsz=245.1, num_updates=63500, lr=8.69428e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 06:58:31 | INFO | train_inner | epoch 079:    802 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.029, ppl=2.04, wps=13220.3, ups=1.87, wpb=7057.6, bsz=254.2, num_updates=63600, lr=8.68744e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 06:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 06:58:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:58:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:58:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:58:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:59:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 06:59:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 06:59:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 06:59:19 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.414 | nll_loss 3.992 | ppl 15.91 | bleu 21.95 | wps 3372.9 | wpb 5162.1 | bsz 187.5 | num_updates 63639 | best_bleu 22.31
2021-01-09 06:59:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 06:59:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:59:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:59:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 79 @ 63639 updates, score 21.95) (writing took 2.7343919724226 seconds)
2021-01-09 06:59:22 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-09 06:59:22 | INFO | train | epoch 079 | symm_kl 0.489 | self_kl 0 | self_cv 0 | loss 3.414 | nll_loss 1.025 | ppl 2.03 | wps 12300.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 63639 | lr 8.68478e-06 | gnorm 0.754 | train_wall 443 | wall 0
2021-01-09 06:59:22 | INFO | fairseq.trainer | begin training epoch 80
2021-01-09 06:59:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 06:59:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 06:59:57 | INFO | train_inner | epoch 080:     61 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.012, ppl=2.02, wps=8092.5, ups=1.17, wpb=6940.2, bsz=251.7, num_updates=63700, lr=8.68062e-06, gnorm=0.747, train_wall=52, wall=0
2021-01-09 07:00:50 | INFO | train_inner | epoch 080:    161 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.031, ppl=2.04, wps=13219.1, ups=1.88, wpb=7036.2, bsz=243.8, num_updates=63800, lr=8.67382e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 07:01:43 | INFO | train_inner | epoch 080:    261 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.023, ppl=2.03, wps=13038.5, ups=1.88, wpb=6919.2, bsz=250, num_updates=63900, lr=8.66703e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 07:02:36 | INFO | train_inner | epoch 080:    361 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.016, ppl=2.02, wps=13186, ups=1.88, wpb=7032.4, bsz=247.8, num_updates=64000, lr=8.66025e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 07:03:29 | INFO | train_inner | epoch 080:    461 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.025, ppl=2.04, wps=13467.7, ups=1.89, wpb=7130.9, bsz=256.8, num_updates=64100, lr=8.6535e-06, gnorm=0.741, train_wall=53, wall=0
2021-01-09 07:04:22 | INFO | train_inner | epoch 080:    561 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.036, ppl=2.05, wps=13349, ups=1.89, wpb=7071.1, bsz=236, num_updates=64200, lr=8.64675e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 07:05:15 | INFO | train_inner | epoch 080:    661 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.024, ppl=2.03, wps=12970.1, ups=1.89, wpb=6870, bsz=239.5, num_updates=64300, lr=8.64003e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 07:06:08 | INFO | train_inner | epoch 080:    761 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.017, ppl=2.02, wps=13190.5, ups=1.88, wpb=7028.3, bsz=250.5, num_updates=64400, lr=8.63332e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 07:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:07:18 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.413 | nll_loss 3.99 | ppl 15.89 | bleu 22.01 | wps 3338.7 | wpb 5162.1 | bsz 187.5 | num_updates 64480 | best_bleu 22.31
2021-01-09 07:07:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:07:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:07:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:07:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 80 @ 64480 updates, score 22.01) (writing took 2.723260136321187 seconds)
2021-01-09 07:07:21 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-09 07:07:21 | INFO | train | epoch 080 | symm_kl 0.489 | self_kl 0 | self_cv 0 | loss 3.412 | nll_loss 1.024 | ppl 2.03 | wps 12277.9 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 64480 | lr 8.62796e-06 | gnorm 0.753 | train_wall 444 | wall 0
2021-01-09 07:07:21 | INFO | fairseq.trainer | begin training epoch 81
2021-01-09 07:07:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:07:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:07:34 | INFO | train_inner | epoch 081:     20 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.029, ppl=2.04, wps=8041.2, ups=1.16, wpb=6907.7, bsz=249.5, num_updates=64500, lr=8.62662e-06, gnorm=0.766, train_wall=52, wall=0
2021-01-09 07:08:27 | INFO | train_inner | epoch 081:    120 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.021, ppl=2.03, wps=13389.6, ups=1.9, wpb=7051.6, bsz=249.4, num_updates=64600, lr=8.61994e-06, gnorm=0.745, train_wall=52, wall=0
2021-01-09 07:09:20 | INFO | train_inner | epoch 081:    220 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.021, ppl=2.03, wps=13056.3, ups=1.89, wpb=6904.8, bsz=236.1, num_updates=64700, lr=8.61328e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 07:10:13 | INFO | train_inner | epoch 081:    320 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.03, ppl=2.04, wps=13223.8, ups=1.88, wpb=7024.8, bsz=252.6, num_updates=64800, lr=8.60663e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 07:11:05 | INFO | train_inner | epoch 081:    420 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.016, ppl=2.02, wps=13156.1, ups=1.91, wpb=6882.5, bsz=253.3, num_updates=64900, lr=8.6e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 07:11:58 | INFO | train_inner | epoch 081:    520 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.032, ppl=2.04, wps=13414.9, ups=1.89, wpb=7090.9, bsz=244, num_updates=65000, lr=8.59338e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 07:12:51 | INFO | train_inner | epoch 081:    620 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.026, ppl=2.04, wps=13318.5, ups=1.89, wpb=7043.1, bsz=248.8, num_updates=65100, lr=8.58678e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 07:13:44 | INFO | train_inner | epoch 081:    720 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.026, ppl=2.04, wps=13191.6, ups=1.89, wpb=6970.4, bsz=243.4, num_updates=65200, lr=8.58019e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 07:14:37 | INFO | train_inner | epoch 081:    820 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.013, ppl=2.02, wps=13201.1, ups=1.88, wpb=7006.5, bsz=248.5, num_updates=65300, lr=8.57362e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 07:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:14:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:14:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:14:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:14:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:14:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:15:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:15:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:15:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:15:16 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.42 | nll_loss 3.995 | ppl 15.95 | bleu 21.98 | wps 3348.5 | wpb 5162.1 | bsz 187.5 | num_updates 65321 | best_bleu 22.31
2021-01-09 07:15:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:15:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:15:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:15:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 81 @ 65321 updates, score 21.98) (writing took 2.799978420138359 seconds)
2021-01-09 07:15:19 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-09 07:15:19 | INFO | train | epoch 081 | symm_kl 0.488 | self_kl 0 | self_cv 0 | loss 3.411 | nll_loss 1.024 | ppl 2.03 | wps 12315.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 65321 | lr 8.57224e-06 | gnorm 0.753 | train_wall 442 | wall 0
2021-01-09 07:15:19 | INFO | fairseq.trainer | begin training epoch 82
2021-01-09 07:15:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:15:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:16:03 | INFO | train_inner | epoch 082:     79 / 841 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.033, ppl=2.05, wps=8128.1, ups=1.17, wpb=6965.6, bsz=233.1, num_updates=65400, lr=8.56706e-06, gnorm=0.768, train_wall=52, wall=0
2021-01-09 07:16:56 | INFO | train_inner | epoch 082:    179 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.025, ppl=2.03, wps=13233.5, ups=1.89, wpb=7011.1, bsz=246.2, num_updates=65500, lr=8.56052e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 07:17:48 | INFO | train_inner | epoch 082:    279 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.035, ppl=2.05, wps=13124.2, ups=1.9, wpb=6916.7, bsz=249.1, num_updates=65600, lr=8.55399e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 07:18:42 | INFO | train_inner | epoch 082:    379 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.021, ppl=2.03, wps=13284, ups=1.88, wpb=7084.6, bsz=238.9, num_updates=65700, lr=8.54748e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 07:19:35 | INFO | train_inner | epoch 082:    479 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.012, ppl=2.02, wps=13214.4, ups=1.87, wpb=7077.2, bsz=241.6, num_updates=65800, lr=8.54098e-06, gnorm=0.74, train_wall=53, wall=0
2021-01-09 07:20:28 | INFO | train_inner | epoch 082:    579 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.033, ppl=2.05, wps=13153.8, ups=1.9, wpb=6909.7, bsz=248.2, num_updates=65900, lr=8.5345e-06, gnorm=0.762, train_wall=52, wall=0
2021-01-09 07:21:20 | INFO | train_inner | epoch 082:    679 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.028, ppl=2.04, wps=13171.2, ups=1.91, wpb=6909.4, bsz=246.8, num_updates=66000, lr=8.52803e-06, gnorm=0.765, train_wall=52, wall=0
2021-01-09 07:22:13 | INFO | train_inner | epoch 082:    779 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.016, ppl=2.02, wps=13482.8, ups=1.9, wpb=7089.9, bsz=263.4, num_updates=66100, lr=8.52158e-06, gnorm=0.738, train_wall=52, wall=0
2021-01-09 07:22:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:22:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:22:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:22:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:22:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:22:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:22:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:22:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:22:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:23:14 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.418 | nll_loss 3.996 | ppl 15.95 | bleu 22.05 | wps 3321.1 | wpb 5162.1 | bsz 187.5 | num_updates 66162 | best_bleu 22.31
2021-01-09 07:23:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:23:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:23:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:23:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 82 @ 66162 updates, score 22.05) (writing took 2.762889489531517 seconds)
2021-01-09 07:23:16 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-09 07:23:16 | INFO | train | epoch 082 | symm_kl 0.488 | self_kl 0 | self_cv 0 | loss 3.411 | nll_loss 1.024 | ppl 2.03 | wps 12307.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 66162 | lr 8.51758e-06 | gnorm 0.754 | train_wall 442 | wall 0
2021-01-09 07:23:16 | INFO | fairseq.trainer | begin training epoch 83
2021-01-09 07:23:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:23:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:23:39 | INFO | train_inner | epoch 083:     38 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.025, ppl=2.04, wps=8033, ups=1.16, wpb=6900.3, bsz=240.6, num_updates=66200, lr=8.51514e-06, gnorm=0.754, train_wall=52, wall=0
2021-01-09 07:24:32 | INFO | train_inner | epoch 083:    138 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.016, ppl=2.02, wps=13277.9, ups=1.89, wpb=7024.7, bsz=246.6, num_updates=66300, lr=8.50871e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 07:25:24 | INFO | train_inner | epoch 083:    238 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.035, ppl=2.05, wps=13000.3, ups=1.91, wpb=6805.7, bsz=242.6, num_updates=66400, lr=8.5023e-06, gnorm=0.784, train_wall=52, wall=0
2021-01-09 07:26:18 | INFO | train_inner | epoch 083:    338 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.025, ppl=2.04, wps=13263, ups=1.87, wpb=7107.8, bsz=245, num_updates=66500, lr=8.49591e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 07:27:11 | INFO | train_inner | epoch 083:    438 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.022, ppl=2.03, wps=13161.9, ups=1.88, wpb=7001.1, bsz=245.3, num_updates=66600, lr=8.48953e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 07:28:03 | INFO | train_inner | epoch 083:    538 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.031, ppl=2.04, wps=13264.7, ups=1.91, wpb=6947, bsz=246.8, num_updates=66700, lr=8.48316e-06, gnorm=0.762, train_wall=52, wall=0
2021-01-09 07:28:56 | INFO | train_inner | epoch 083:    638 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.004, ppl=2.01, wps=13150.9, ups=1.89, wpb=6969.2, bsz=253.8, num_updates=66800, lr=8.47681e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 07:29:49 | INFO | train_inner | epoch 083:    738 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.035, ppl=2.05, wps=13331.8, ups=1.89, wpb=7066.4, bsz=258.2, num_updates=66900, lr=8.47047e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 07:30:42 | INFO | train_inner | epoch 083:    838 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.01, ppl=2.01, wps=13406.3, ups=1.89, wpb=7094.2, bsz=241.2, num_updates=67000, lr=8.46415e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 07:30:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:30:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:30:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:30:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:30:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:30:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:30:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:30:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:31:11 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.416 | nll_loss 3.994 | ppl 15.93 | bleu 21.98 | wps 3378.4 | wpb 5162.1 | bsz 187.5 | num_updates 67003 | best_bleu 22.31
2021-01-09 07:31:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:31:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:31:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 83 @ 67003 updates, score 21.98) (writing took 2.777657449245453 seconds)
2021-01-09 07:31:14 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-09 07:31:14 | INFO | train | epoch 083 | symm_kl 0.488 | self_kl 0 | self_cv 0 | loss 3.409 | nll_loss 1.023 | ppl 2.03 | wps 12317.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 67003 | lr 8.46396e-06 | gnorm 0.756 | train_wall 442 | wall 0
2021-01-09 07:31:14 | INFO | fairseq.trainer | begin training epoch 84
2021-01-09 07:31:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:31:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:31:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:32:08 | INFO | train_inner | epoch 084:     97 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.027, ppl=2.04, wps=8134.2, ups=1.17, wpb=6953.9, bsz=243.5, num_updates=67100, lr=8.45784e-06, gnorm=0.758, train_wall=52, wall=0
2021-01-09 07:33:01 | INFO | train_inner | epoch 084:    197 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.008, ppl=2.01, wps=13064.8, ups=1.87, wpb=6991, bsz=248.6, num_updates=67200, lr=8.45154e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 07:33:53 | INFO | train_inner | epoch 084:    297 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.046, ppl=2.06, wps=13198, ups=1.91, wpb=6915.9, bsz=234.5, num_updates=67300, lr=8.44526e-06, gnorm=0.77, train_wall=52, wall=0
2021-01-09 07:34:46 | INFO | train_inner | epoch 084:    397 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.012, ppl=2.02, wps=13266.7, ups=1.89, wpb=7011.5, bsz=259.5, num_updates=67400, lr=8.43899e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 07:35:39 | INFO | train_inner | epoch 084:    497 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.021, ppl=2.03, wps=13207.3, ups=1.89, wpb=6976, bsz=241.4, num_updates=67500, lr=8.43274e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 07:36:32 | INFO | train_inner | epoch 084:    597 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.02, ppl=2.03, wps=13047.8, ups=1.88, wpb=6952.9, bsz=255.3, num_updates=67600, lr=8.4265e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 07:37:25 | INFO | train_inner | epoch 084:    697 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.025, ppl=2.04, wps=13459.4, ups=1.89, wpb=7120.8, bsz=244.6, num_updates=67700, lr=8.42028e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 07:38:19 | INFO | train_inner | epoch 084:    797 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.008, ppl=2.01, wps=13181.2, ups=1.88, wpb=7010.4, bsz=248.6, num_updates=67800, lr=8.41406e-06, gnorm=0.746, train_wall=53, wall=0
2021-01-09 07:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:38:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:38:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:38:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:38:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:38:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:38:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:39:09 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.42 | nll_loss 3.996 | ppl 15.96 | bleu 21.9 | wps 3385.3 | wpb 5162.1 | bsz 187.5 | num_updates 67844 | best_bleu 22.31
2021-01-09 07:39:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:39:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:39:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:39:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 84 @ 67844 updates, score 21.9) (writing took 2.7438712809234858 seconds)
2021-01-09 07:39:12 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-09 07:39:12 | INFO | train | epoch 084 | symm_kl 0.487 | self_kl 0 | self_cv 0 | loss 3.408 | nll_loss 1.023 | ppl 2.03 | wps 12303.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 67844 | lr 8.41133e-06 | gnorm 0.756 | train_wall 443 | wall 0
2021-01-09 07:39:12 | INFO | fairseq.trainer | begin training epoch 85
2021-01-09 07:39:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:39:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:39:44 | INFO | train_inner | epoch 085:     56 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.014, ppl=2.02, wps=8228.1, ups=1.17, wpb=7024.9, bsz=257.4, num_updates=67900, lr=8.40787e-06, gnorm=0.743, train_wall=52, wall=0
2021-01-09 07:40:37 | INFO | train_inner | epoch 085:    156 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.02, ppl=2.03, wps=13326.3, ups=1.9, wpb=7030.4, bsz=247.8, num_updates=68000, lr=8.40168e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 07:41:30 | INFO | train_inner | epoch 085:    256 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.015, ppl=2.02, wps=13059.6, ups=1.89, wpb=6921.5, bsz=244.5, num_updates=68100, lr=8.39551e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 07:42:23 | INFO | train_inner | epoch 085:    356 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.03, ppl=2.04, wps=13235.3, ups=1.88, wpb=7024.1, bsz=252.2, num_updates=68200, lr=8.38935e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 07:43:16 | INFO | train_inner | epoch 085:    456 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.009, ppl=2.01, wps=13324.6, ups=1.88, wpb=7086.7, bsz=247.2, num_updates=68300, lr=8.38321e-06, gnorm=0.74, train_wall=53, wall=0
2021-01-09 07:44:09 | INFO | train_inner | epoch 085:    556 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.032, ppl=2.04, wps=13065.4, ups=1.88, wpb=6938.3, bsz=244.6, num_updates=68400, lr=8.37708e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 07:45:02 | INFO | train_inner | epoch 085:    656 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.035, ppl=2.05, wps=13253.4, ups=1.88, wpb=7046.8, bsz=237.6, num_updates=68500, lr=8.37096e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 07:45:56 | INFO | train_inner | epoch 085:    756 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.035, ppl=2.05, wps=12975.9, ups=1.87, wpb=6925.5, bsz=242.8, num_updates=68600, lr=8.36486e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 07:46:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:46:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:46:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:46:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:47:08 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.418 | nll_loss 3.996 | ppl 15.95 | bleu 22.15 | wps 3383.2 | wpb 5162.1 | bsz 187.5 | num_updates 68685 | best_bleu 22.31
2021-01-09 07:47:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:47:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:47:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 85 @ 68685 updates, score 22.15) (writing took 2.7316450104117393 seconds)
2021-01-09 07:47:10 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-09 07:47:10 | INFO | train | epoch 085 | symm_kl 0.487 | self_kl 0 | self_cv 0 | loss 3.407 | nll_loss 1.022 | ppl 2.03 | wps 12292.4 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 68685 | lr 8.35968e-06 | gnorm 0.755 | train_wall 443 | wall 0
2021-01-09 07:47:10 | INFO | fairseq.trainer | begin training epoch 86
2021-01-09 07:47:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:47:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:47:21 | INFO | train_inner | epoch 086:     15 / 841 symm_kl=0.49, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.031, ppl=2.04, wps=8059.8, ups=1.16, wpb=6921.7, bsz=238.9, num_updates=68700, lr=8.35877e-06, gnorm=0.77, train_wall=52, wall=0
2021-01-09 07:48:14 | INFO | train_inner | epoch 086:    115 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.015, ppl=2.02, wps=13463.7, ups=1.92, wpb=7024.6, bsz=247.3, num_updates=68800, lr=8.35269e-06, gnorm=0.754, train_wall=52, wall=0
2021-01-09 07:49:06 | INFO | train_inner | epoch 086:    215 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.007, ppl=2.01, wps=13274.5, ups=1.89, wpb=7015.8, bsz=247.4, num_updates=68900, lr=8.34663e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 07:49:59 | INFO | train_inner | epoch 086:    315 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.009, ppl=2.01, wps=13364.9, ups=1.9, wpb=7047.9, bsz=239, num_updates=69000, lr=8.34058e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 07:50:52 | INFO | train_inner | epoch 086:    415 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.041, ppl=2.06, wps=13000.1, ups=1.9, wpb=6853.3, bsz=252.4, num_updates=69100, lr=8.33454e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 07:51:45 | INFO | train_inner | epoch 086:    515 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.025, ppl=2.03, wps=13141.6, ups=1.89, wpb=6942.1, bsz=234.2, num_updates=69200, lr=8.32851e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 07:52:38 | INFO | train_inner | epoch 086:    615 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.038, ppl=2.05, wps=13289, ups=1.88, wpb=7063.9, bsz=244.2, num_updates=69300, lr=8.3225e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 07:53:31 | INFO | train_inner | epoch 086:    715 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.016, ppl=2.02, wps=13266.2, ups=1.87, wpb=7076.9, bsz=259.9, num_updates=69400, lr=8.31651e-06, gnorm=0.743, train_wall=53, wall=0
2021-01-09 07:54:24 | INFO | train_inner | epoch 086:    815 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.024, ppl=2.03, wps=13071.5, ups=1.88, wpb=6935, bsz=257, num_updates=69500, lr=8.31052e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 07:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 07:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:54:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:54:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 07:54:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 07:54:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 07:55:05 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.419 | nll_loss 3.997 | ppl 15.97 | bleu 22.04 | wps 3411.6 | wpb 5162.1 | bsz 187.5 | num_updates 69526 | best_bleu 22.31
2021-01-09 07:55:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 07:55:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:55:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:55:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 86 @ 69526 updates, score 22.04) (writing took 2.739508990198374 seconds)
2021-01-09 07:55:08 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-09 07:55:08 | INFO | train | epoch 086 | symm_kl 0.486 | self_kl 0 | self_cv 0 | loss 3.406 | nll_loss 1.022 | ppl 2.03 | wps 12313.3 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 69526 | lr 8.30897e-06 | gnorm 0.758 | train_wall 443 | wall 0
2021-01-09 07:55:08 | INFO | fairseq.trainer | begin training epoch 87
2021-01-09 07:55:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 07:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 07:55:50 | INFO | train_inner | epoch 087:     74 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.008, ppl=2.01, wps=8230.8, ups=1.17, wpb=7037.4, bsz=236.7, num_updates=69600, lr=8.30455e-06, gnorm=0.751, train_wall=52, wall=0
2021-01-09 07:56:42 | INFO | train_inner | epoch 087:    174 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.029, ppl=2.04, wps=13027.7, ups=1.91, wpb=6834.5, bsz=246.2, num_updates=69700, lr=8.29859e-06, gnorm=0.773, train_wall=52, wall=0
2021-01-09 07:57:35 | INFO | train_inner | epoch 087:    274 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.019, ppl=2.03, wps=13097.3, ups=1.88, wpb=6966.3, bsz=252, num_updates=69800, lr=8.29264e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 07:58:28 | INFO | train_inner | epoch 087:    374 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.011, ppl=2.01, wps=13251, ups=1.89, wpb=7021.4, bsz=235.1, num_updates=69900, lr=8.28671e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 07:59:22 | INFO | train_inner | epoch 087:    474 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.036, ppl=2.05, wps=13274.3, ups=1.87, wpb=7082, bsz=242.7, num_updates=70000, lr=8.28079e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 08:00:15 | INFO | train_inner | epoch 087:    574 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.014, ppl=2.02, wps=13051, ups=1.86, wpb=6998.7, bsz=246.2, num_updates=70100, lr=8.27488e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:01:09 | INFO | train_inner | epoch 087:    674 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.028, ppl=2.04, wps=13184.4, ups=1.87, wpb=7054.1, bsz=250.1, num_updates=70200, lr=8.26898e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 08:02:02 | INFO | train_inner | epoch 087:    774 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.034, ppl=2.05, wps=13079, ups=1.89, wpb=6910.5, bsz=256.7, num_updates=70300, lr=8.2631e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 08:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:02:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:02:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:02:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:02:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:03:05 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.421 | nll_loss 3.998 | ppl 15.98 | bleu 21.85 | wps 3330.1 | wpb 5162.1 | bsz 187.5 | num_updates 70367 | best_bleu 22.31
2021-01-09 08:03:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:03:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:03:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:03:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 87 @ 70367 updates, score 21.85) (writing took 2.7280723843723536 seconds)
2021-01-09 08:03:08 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-09 08:03:08 | INFO | train | epoch 087 | symm_kl 0.486 | self_kl 0 | self_cv 0 | loss 3.406 | nll_loss 1.022 | ppl 2.03 | wps 12261.2 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 70367 | lr 8.25916e-06 | gnorm 0.756 | train_wall 444 | wall 0
2021-01-09 08:03:08 | INFO | fairseq.trainer | begin training epoch 88
2021-01-09 08:03:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:03:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:03:28 | INFO | train_inner | epoch 088:     33 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.019, ppl=2.03, wps=8120.5, ups=1.16, wpb=6985, bsz=245.1, num_updates=70400, lr=8.25723e-06, gnorm=0.758, train_wall=52, wall=0
2021-01-09 08:04:20 | INFO | train_inner | epoch 088:    133 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.022, ppl=2.03, wps=13406.6, ups=1.91, wpb=7025.1, bsz=243.4, num_updates=70500, lr=8.25137e-06, gnorm=0.749, train_wall=52, wall=0
2021-01-09 08:05:13 | INFO | train_inner | epoch 088:    233 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.012, ppl=2.02, wps=13237, ups=1.88, wpb=7024, bsz=238.2, num_updates=70600, lr=8.24552e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:06:06 | INFO | train_inner | epoch 088:    333 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.007, ppl=2.01, wps=12915.8, ups=1.89, wpb=6834, bsz=247.6, num_updates=70700, lr=8.23969e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 08:06:59 | INFO | train_inner | epoch 088:    433 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.025, ppl=2.03, wps=13271.3, ups=1.88, wpb=7069.9, bsz=274.2, num_updates=70800, lr=8.23387e-06, gnorm=0.743, train_wall=53, wall=0
2021-01-09 08:07:52 | INFO | train_inner | epoch 088:    533 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.032, ppl=2.04, wps=13332.4, ups=1.89, wpb=7065.9, bsz=237.4, num_updates=70900, lr=8.22806e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 08:08:45 | INFO | train_inner | epoch 088:    633 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.029, ppl=2.04, wps=12932, ups=1.9, wpb=6818.3, bsz=226.5, num_updates=71000, lr=8.22226e-06, gnorm=0.775, train_wall=53, wall=0
2021-01-09 08:09:38 | INFO | train_inner | epoch 088:    733 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.029, ppl=2.04, wps=13287.9, ups=1.88, wpb=7072.8, bsz=253.8, num_updates=71100, lr=8.21648e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 08:10:31 | INFO | train_inner | epoch 088:    833 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.017, ppl=2.02, wps=13434.9, ups=1.89, wpb=7095.7, bsz=254.3, num_updates=71200, lr=8.21071e-06, gnorm=0.742, train_wall=53, wall=0
2021-01-09 08:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:10:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:10:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:10:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:10:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:10:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:11:03 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.418 | nll_loss 3.994 | ppl 15.94 | bleu 21.89 | wps 3345.3 | wpb 5162.1 | bsz 187.5 | num_updates 71208 | best_bleu 22.31
2021-01-09 08:11:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:11:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:11:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:11:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 88 @ 71208 updates, score 21.89) (writing took 2.727710260078311 seconds)
2021-01-09 08:11:06 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-09 08:11:06 | INFO | train | epoch 088 | symm_kl 0.485 | self_kl 0 | self_cv 0 | loss 3.404 | nll_loss 1.021 | ppl 2.03 | wps 12293.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 71208 | lr 8.21025e-06 | gnorm 0.756 | train_wall 443 | wall 0
2021-01-09 08:11:06 | INFO | fairseq.trainer | begin training epoch 89
2021-01-09 08:11:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:11:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:11:57 | INFO | train_inner | epoch 089:     92 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.019, ppl=2.03, wps=8104.3, ups=1.16, wpb=6980.3, bsz=252.2, num_updates=71300, lr=8.20495e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 08:12:50 | INFO | train_inner | epoch 089:    192 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.01, ppl=2.01, wps=13002.6, ups=1.88, wpb=6899.8, bsz=244.6, num_updates=71400, lr=8.1992e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 08:13:44 | INFO | train_inner | epoch 089:    292 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.004, ppl=2, wps=13221.1, ups=1.88, wpb=7028.6, bsz=248.6, num_updates=71500, lr=8.19346e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 08:14:36 | INFO | train_inner | epoch 089:    392 / 841 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.033, ppl=2.05, wps=13323.6, ups=1.9, wpb=7029.9, bsz=227.8, num_updates=71600, lr=8.18774e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 08:15:29 | INFO | train_inner | epoch 089:    492 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.023, ppl=2.03, wps=13277.2, ups=1.9, wpb=6994.6, bsz=266.6, num_updates=71700, lr=8.18203e-06, gnorm=0.753, train_wall=52, wall=0
2021-01-09 08:16:22 | INFO | train_inner | epoch 089:    592 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.025, ppl=2.04, wps=13239.4, ups=1.89, wpb=7007.2, bsz=230.8, num_updates=71800, lr=8.17633e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 08:17:15 | INFO | train_inner | epoch 089:    692 / 841 symm_kl=0.489, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.025, ppl=2.03, wps=13323.9, ups=1.88, wpb=7091.4, bsz=244.1, num_updates=71900, lr=8.17064e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 08:18:08 | INFO | train_inner | epoch 089:    792 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.035, ppl=2.05, wps=13106.9, ups=1.9, wpb=6905.2, bsz=245.5, num_updates=72000, lr=8.16497e-06, gnorm=0.772, train_wall=52, wall=0
2021-01-09 08:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:18:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:18:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:18:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:18:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:18:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:19:01 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.422 | nll_loss 4 | ppl 16 | bleu 21.92 | wps 3370.7 | wpb 5162.1 | bsz 187.5 | num_updates 72049 | best_bleu 22.31
2021-01-09 08:19:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:19:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:19:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 89 @ 72049 updates, score 21.92) (writing took 2.8425328079611063 seconds)
2021-01-09 08:19:04 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-09 08:19:04 | INFO | train | epoch 089 | symm_kl 0.485 | self_kl 0 | self_cv 0 | loss 3.403 | nll_loss 1.021 | ppl 2.03 | wps 12312.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 72049 | lr 8.16219e-06 | gnorm 0.758 | train_wall 442 | wall 0
2021-01-09 08:19:04 | INFO | fairseq.trainer | begin training epoch 90
2021-01-09 08:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:19:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:19:33 | INFO | train_inner | epoch 090:     51 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.008, ppl=2.01, wps=8194.7, ups=1.17, wpb=6988.5, bsz=256.6, num_updates=72100, lr=8.1593e-06, gnorm=0.754, train_wall=51, wall=0
2021-01-09 08:20:26 | INFO | train_inner | epoch 090:    151 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.017, ppl=2.02, wps=13194.4, ups=1.88, wpb=7027.8, bsz=231.4, num_updates=72200, lr=8.15365e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 08:21:20 | INFO | train_inner | epoch 090:    251 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.016, ppl=2.02, wps=13045.5, ups=1.88, wpb=6940.8, bsz=235.3, num_updates=72300, lr=8.14801e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 08:22:13 | INFO | train_inner | epoch 090:    351 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.013, ppl=2.02, wps=13213.1, ups=1.88, wpb=7024, bsz=247.5, num_updates=72400, lr=8.14238e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 08:23:06 | INFO | train_inner | epoch 090:    451 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.029, ppl=2.04, wps=13333.2, ups=1.88, wpb=7073.7, bsz=262.2, num_updates=72500, lr=8.13676e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 08:23:58 | INFO | train_inner | epoch 090:    551 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.032, ppl=2.05, wps=13109.5, ups=1.9, wpb=6885.8, bsz=244.8, num_updates=72600, lr=8.13116e-06, gnorm=0.773, train_wall=52, wall=0
2021-01-09 08:24:51 | INFO | train_inner | epoch 090:    651 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.021, ppl=2.03, wps=13029.7, ups=1.89, wpb=6883.2, bsz=243.9, num_updates=72700, lr=8.12556e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 08:25:44 | INFO | train_inner | epoch 090:    751 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.009, ppl=2.01, wps=13323.1, ups=1.88, wpb=7100.7, bsz=252, num_updates=72800, lr=8.11998e-06, gnorm=0.742, train_wall=53, wall=0
2021-01-09 08:26:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:26:59 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.421 | nll_loss 3.999 | ppl 15.99 | bleu 21.94 | wps 3373.5 | wpb 5162.1 | bsz 187.5 | num_updates 72890 | best_bleu 22.31
2021-01-09 08:26:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:27:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:27:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 90 @ 72890 updates, score 21.94) (writing took 2.7559731360524893 seconds)
2021-01-09 08:27:02 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-09 08:27:02 | INFO | train | epoch 090 | symm_kl 0.484 | self_kl 0 | self_cv 0 | loss 3.402 | nll_loss 1.02 | ppl 2.03 | wps 12295.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 72890 | lr 8.11496e-06 | gnorm 0.756 | train_wall 443 | wall 0
2021-01-09 08:27:02 | INFO | fairseq.trainer | begin training epoch 91
2021-01-09 08:27:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:27:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:27:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:27:10 | INFO | train_inner | epoch 091:     10 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.037, ppl=2.05, wps=8128.5, ups=1.16, wpb=6979.4, bsz=256.7, num_updates=72900, lr=8.11441e-06, gnorm=0.763, train_wall=52, wall=0
2021-01-09 08:28:03 | INFO | train_inner | epoch 091:    110 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.012, ppl=2.02, wps=13390.4, ups=1.9, wpb=7033.8, bsz=249.5, num_updates=73000, lr=8.10885e-06, gnorm=0.747, train_wall=52, wall=0
2021-01-09 08:28:56 | INFO | train_inner | epoch 091:    210 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.018, ppl=2.03, wps=13196.8, ups=1.9, wpb=6963.1, bsz=251.8, num_updates=73100, lr=8.1033e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:29:49 | INFO | train_inner | epoch 091:    310 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.026, ppl=2.04, wps=13072.5, ups=1.89, wpb=6916.6, bsz=247.1, num_updates=73200, lr=8.09776e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 08:30:41 | INFO | train_inner | epoch 091:    410 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.032, ppl=2.04, wps=13213.1, ups=1.9, wpb=6952.6, bsz=240.7, num_updates=73300, lr=8.09224e-06, gnorm=0.764, train_wall=52, wall=0
2021-01-09 08:31:34 | INFO | train_inner | epoch 091:    510 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.014, ppl=2.02, wps=13140.8, ups=1.88, wpb=7005.1, bsz=256.4, num_updates=73400, lr=8.08672e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 08:32:28 | INFO | train_inner | epoch 091:    610 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.007, ppl=2.01, wps=13351.4, ups=1.87, wpb=7154.2, bsz=239.5, num_updates=73500, lr=8.08122e-06, gnorm=0.74, train_wall=53, wall=0
2021-01-09 08:33:21 | INFO | train_inner | epoch 091:    710 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.018, ppl=2.03, wps=13167.2, ups=1.88, wpb=6986, bsz=241.9, num_updates=73600, lr=8.07573e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:34:14 | INFO | train_inner | epoch 091:    810 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.029, ppl=2.04, wps=13085.4, ups=1.88, wpb=6944.5, bsz=243.4, num_updates=73700, lr=8.07025e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 08:34:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:34:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:34:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:34:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:34:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:34:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:34:59 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.423 | nll_loss 4.001 | ppl 16.01 | bleu 21.97 | wps 3309.6 | wpb 5162.1 | bsz 187.5 | num_updates 73731 | best_bleu 22.31
2021-01-09 08:34:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:34:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:35:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:35:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 91 @ 73731 updates, score 21.97) (writing took 2.7966930512338877 seconds)
2021-01-09 08:35:01 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-09 08:35:01 | INFO | train | epoch 091 | symm_kl 0.484 | self_kl 0 | self_cv 0 | loss 3.402 | nll_loss 1.02 | ppl 2.03 | wps 12269.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 73731 | lr 8.06855e-06 | gnorm 0.756 | train_wall 444 | wall 0
2021-01-09 08:35:01 | INFO | fairseq.trainer | begin training epoch 92
2021-01-09 08:35:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:35:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:35:40 | INFO | train_inner | epoch 092:     69 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.02, ppl=2.03, wps=8083, ups=1.17, wpb=6935.2, bsz=247.5, num_updates=73800, lr=8.06478e-06, gnorm=0.762, train_wall=51, wall=0
2021-01-09 08:36:33 | INFO | train_inner | epoch 092:    169 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.012, ppl=2.02, wps=13186.3, ups=1.88, wpb=7009.2, bsz=244, num_updates=73900, lr=8.05932e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 08:37:26 | INFO | train_inner | epoch 092:    269 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.025, ppl=2.03, wps=13345.6, ups=1.9, wpb=7032.5, bsz=246.4, num_updates=74000, lr=8.05387e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 08:38:18 | INFO | train_inner | epoch 092:    369 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.026, ppl=2.04, wps=13207.8, ups=1.9, wpb=6940.6, bsz=236.1, num_updates=74100, lr=8.04844e-06, gnorm=0.764, train_wall=52, wall=0
2021-01-09 08:39:12 | INFO | train_inner | epoch 092:    469 / 841 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.018, ppl=2.02, wps=13151.9, ups=1.87, wpb=7047.7, bsz=248.5, num_updates=74200, lr=8.04301e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 08:40:05 | INFO | train_inner | epoch 092:    569 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.016, ppl=2.02, wps=13162.7, ups=1.87, wpb=7035.2, bsz=249.7, num_updates=74300, lr=8.0376e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 08:40:58 | INFO | train_inner | epoch 092:    669 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.022, ppl=2.03, wps=13114.3, ups=1.89, wpb=6951.3, bsz=249.8, num_updates=74400, lr=8.03219e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:41:51 | INFO | train_inner | epoch 092:    769 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.024, ppl=2.03, wps=13128.4, ups=1.89, wpb=6945, bsz=253, num_updates=74500, lr=8.0268e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 08:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:42:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:42:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:42:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:42:58 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.425 | nll_loss 4.005 | ppl 16.05 | bleu 21.87 | wps 3340.2 | wpb 5162.1 | bsz 187.5 | num_updates 74572 | best_bleu 22.31
2021-01-09 08:42:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:42:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:43:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:43:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 92 @ 74572 updates, score 21.87) (writing took 2.7936984915286303 seconds)
2021-01-09 08:43:01 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-09 08:43:01 | INFO | train | epoch 092 | symm_kl 0.484 | self_kl 0 | self_cv 0 | loss 3.401 | nll_loss 1.02 | ppl 2.03 | wps 12264.2 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 74572 | lr 8.02292e-06 | gnorm 0.757 | train_wall 444 | wall 0
2021-01-09 08:43:01 | INFO | fairseq.trainer | begin training epoch 93
2021-01-09 08:43:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:43:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:43:19 | INFO | train_inner | epoch 093:     28 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.017, ppl=2.02, wps=8013.4, ups=1.15, wpb=6984.7, bsz=248.3, num_updates=74600, lr=8.02142e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 08:44:11 | INFO | train_inner | epoch 093:    128 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.01, ppl=2.01, wps=13355.9, ups=1.91, wpb=7002.9, bsz=254.3, num_updates=74700, lr=8.01605e-06, gnorm=0.749, train_wall=52, wall=0
2021-01-09 08:45:04 | INFO | train_inner | epoch 093:    228 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.01, ppl=2.01, wps=13012.2, ups=1.88, wpb=6935.8, bsz=248.9, num_updates=74800, lr=8.01069e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 08:45:57 | INFO | train_inner | epoch 093:    328 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.019, ppl=2.03, wps=13191.5, ups=1.89, wpb=6972.5, bsz=236.1, num_updates=74900, lr=8.00534e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 08:46:50 | INFO | train_inner | epoch 093:    428 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.015, ppl=2.02, wps=12886.8, ups=1.88, wpb=6857, bsz=252.4, num_updates=75000, lr=8e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 08:47:43 | INFO | train_inner | epoch 093:    528 / 841 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.036, ppl=2.05, wps=13149.7, ups=1.88, wpb=6987.8, bsz=238.5, num_updates=75100, lr=7.99467e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 08:48:36 | INFO | train_inner | epoch 093:    628 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.03, ppl=2.04, wps=13464, ups=1.9, wpb=7097.8, bsz=237.7, num_updates=75200, lr=7.98935e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 08:49:29 | INFO | train_inner | epoch 093:    728 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.008, ppl=2.01, wps=13323.3, ups=1.88, wpb=7098, bsz=247.4, num_updates=75300, lr=7.98405e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 08:50:22 | INFO | train_inner | epoch 093:    828 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.032, ppl=2.04, wps=13345, ups=1.89, wpb=7055.2, bsz=253.9, num_updates=75400, lr=7.97875e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 08:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:50:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:50:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:50:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:50:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:50:57 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.425 | nll_loss 4.004 | ppl 16.05 | bleu 22 | wps 3360.8 | wpb 5162.1 | bsz 187.5 | num_updates 75413 | best_bleu 22.31
2021-01-09 08:50:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:50:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:50:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:51:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 93 @ 75413 updates, score 22.0) (writing took 2.7751416452229023 seconds)
2021-01-09 08:51:00 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-09 08:51:00 | INFO | train | epoch 093 | symm_kl 0.483 | self_kl 0 | self_cv 0 | loss 3.4 | nll_loss 1.019 | ppl 2.03 | wps 12289 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 75413 | lr 7.97806e-06 | gnorm 0.758 | train_wall 443 | wall 0
2021-01-09 08:51:00 | INFO | fairseq.trainer | begin training epoch 94
2021-01-09 08:51:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:51:48 | INFO | train_inner | epoch 094:     87 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.373, nll_loss=0.996, ppl=1.99, wps=8232, ups=1.17, wpb=7038.2, bsz=249.8, num_updates=75500, lr=7.97347e-06, gnorm=0.746, train_wall=52, wall=0
2021-01-09 08:52:41 | INFO | train_inner | epoch 094:    187 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.011, ppl=2.02, wps=13030.3, ups=1.88, wpb=6915.4, bsz=240.9, num_updates=75600, lr=7.96819e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 08:53:34 | INFO | train_inner | epoch 094:    287 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.021, ppl=2.03, wps=13320.9, ups=1.88, wpb=7090.6, bsz=242.2, num_updates=75700, lr=7.96293e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 08:54:27 | INFO | train_inner | epoch 094:    387 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.022, ppl=2.03, wps=13316.6, ups=1.88, wpb=7098.4, bsz=249.6, num_updates=75800, lr=7.95767e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 08:55:20 | INFO | train_inner | epoch 094:    487 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.021, ppl=2.03, wps=13094.7, ups=1.88, wpb=6949.4, bsz=247.1, num_updates=75900, lr=7.95243e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 08:56:14 | INFO | train_inner | epoch 094:    587 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.035, ppl=2.05, wps=13079.3, ups=1.88, wpb=6948.8, bsz=238.5, num_updates=76000, lr=7.94719e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 08:57:06 | INFO | train_inner | epoch 094:    687 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.034, ppl=2.05, wps=13018.4, ups=1.89, wpb=6876.7, bsz=241.6, num_updates=76100, lr=7.94197e-06, gnorm=0.775, train_wall=53, wall=0
2021-01-09 08:58:00 | INFO | train_inner | epoch 094:    787 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.015, ppl=2.02, wps=13133.7, ups=1.87, wpb=7017.8, bsz=255.5, num_updates=76200, lr=7.93676e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 08:58:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 08:58:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:58:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:58:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:58:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:58:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 08:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 08:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 08:58:56 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.425 | nll_loss 4.005 | ppl 16.05 | bleu 21.96 | wps 3372.1 | wpb 5162.1 | bsz 187.5 | num_updates 76254 | best_bleu 22.31
2021-01-09 08:58:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 08:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:58:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 94 @ 76254 updates, score 21.96) (writing took 2.7272447627037764 seconds)
2021-01-09 08:58:59 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-09 08:58:59 | INFO | train | epoch 094 | symm_kl 0.483 | self_kl 0 | self_cv 0 | loss 3.399 | nll_loss 1.02 | ppl 2.03 | wps 12275.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 76254 | lr 7.93395e-06 | gnorm 0.759 | train_wall 444 | wall 0
2021-01-09 08:58:59 | INFO | fairseq.trainer | begin training epoch 95
2021-01-09 08:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 08:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 08:59:26 | INFO | train_inner | epoch 095:     46 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.012, ppl=2.02, wps=8196.3, ups=1.17, wpb=7032.8, bsz=261.4, num_updates=76300, lr=7.93156e-06, gnorm=0.753, train_wall=52, wall=0
2021-01-09 09:00:18 | INFO | train_inner | epoch 095:    146 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.016, ppl=2.02, wps=13120.4, ups=1.9, wpb=6915.9, bsz=226.4, num_updates=76400, lr=7.92636e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 09:01:12 | INFO | train_inner | epoch 095:    246 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.027, ppl=2.04, wps=13105.4, ups=1.88, wpb=6983.5, bsz=252.1, num_updates=76500, lr=7.92118e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 09:02:04 | INFO | train_inner | epoch 095:    346 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.013, ppl=2.02, wps=12946.6, ups=1.9, wpb=6822.3, bsz=245.9, num_updates=76600, lr=7.91601e-06, gnorm=0.776, train_wall=53, wall=0
2021-01-09 09:02:58 | INFO | train_inner | epoch 095:    446 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.008, ppl=2.01, wps=13239.7, ups=1.88, wpb=7048.7, bsz=242.6, num_updates=76700, lr=7.91085e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 09:03:51 | INFO | train_inner | epoch 095:    546 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.022, ppl=2.03, wps=13327.1, ups=1.88, wpb=7073.1, bsz=247.1, num_updates=76800, lr=7.90569e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 09:04:44 | INFO | train_inner | epoch 095:    646 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.012, ppl=2.02, wps=13406.3, ups=1.89, wpb=7109.5, bsz=261.8, num_updates=76900, lr=7.90055e-06, gnorm=0.741, train_wall=53, wall=0
2021-01-09 09:05:37 | INFO | train_inner | epoch 095:    746 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.025, ppl=2.04, wps=13253.5, ups=1.89, wpb=7019, bsz=242.7, num_updates=77000, lr=7.89542e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 09:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:06:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:06:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:06:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:06:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:06:55 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.428 | nll_loss 4.006 | ppl 16.07 | bleu 21.99 | wps 3365.8 | wpb 5162.1 | bsz 187.5 | num_updates 77095 | best_bleu 22.31
2021-01-09 09:06:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:06:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:06:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:06:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 95 @ 77095 updates, score 21.99) (writing took 2.7792175617069006 seconds)
2021-01-09 09:06:57 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-09 09:06:57 | INFO | train | epoch 095 | symm_kl 0.482 | self_kl 0 | self_cv 0 | loss 3.398 | nll_loss 1.019 | ppl 2.03 | wps 12283.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 77095 | lr 7.89055e-06 | gnorm 0.76 | train_wall 444 | wall 0
2021-01-09 09:06:57 | INFO | fairseq.trainer | begin training epoch 96
2021-01-09 09:06:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:07:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:07:03 | INFO | train_inner | epoch 096:      5 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.028, ppl=2.04, wps=8017.2, ups=1.16, wpb=6940.9, bsz=246.3, num_updates=77100, lr=7.8903e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 09:07:55 | INFO | train_inner | epoch 096:    105 / 841 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.029, ppl=2.04, wps=13521.8, ups=1.92, wpb=7043.7, bsz=241, num_updates=77200, lr=7.88519e-06, gnorm=0.761, train_wall=52, wall=0
2021-01-09 09:08:49 | INFO | train_inner | epoch 096:    205 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.369, nll_loss=0.993, ppl=1.99, wps=13372.4, ups=1.88, wpb=7130.3, bsz=259.5, num_updates=77300, lr=7.88008e-06, gnorm=0.732, train_wall=53, wall=0
2021-01-09 09:09:41 | INFO | train_inner | epoch 096:    305 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.017, ppl=2.02, wps=13202.3, ups=1.9, wpb=6965.5, bsz=255, num_updates=77400, lr=7.87499e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 09:10:34 | INFO | train_inner | epoch 096:    405 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.026, ppl=2.04, wps=12987.8, ups=1.91, wpb=6806.5, bsz=246.5, num_updates=77500, lr=7.86991e-06, gnorm=0.781, train_wall=52, wall=0
2021-01-09 09:11:27 | INFO | train_inner | epoch 096:    505 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.009, ppl=2.01, wps=13291.8, ups=1.88, wpb=7066.3, bsz=247.8, num_updates=77600, lr=7.86484e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 09:12:20 | INFO | train_inner | epoch 096:    605 / 841 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.036, ppl=2.05, wps=13224, ups=1.89, wpb=6998.2, bsz=245.4, num_updates=77700, lr=7.85977e-06, gnorm=0.772, train_wall=53, wall=0
2021-01-09 09:13:13 | INFO | train_inner | epoch 096:    705 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.018, ppl=2.03, wps=13151.9, ups=1.88, wpb=6987.3, bsz=230.8, num_updates=77800, lr=7.85472e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 09:14:06 | INFO | train_inner | epoch 096:    805 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.021, ppl=2.03, wps=13101.5, ups=1.9, wpb=6909.5, bsz=243, num_updates=77900, lr=7.84968e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 09:14:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:14:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:14:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:14:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:14:53 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.425 | nll_loss 4.004 | ppl 16.05 | bleu 22.01 | wps 3327 | wpb 5162.1 | bsz 187.5 | num_updates 77936 | best_bleu 22.31
2021-01-09 09:14:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:14:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:14:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:14:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 96 @ 77936 updates, score 22.01) (writing took 2.7742380276322365 seconds)
2021-01-09 09:14:56 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-09 09:14:56 | INFO | train | epoch 096 | symm_kl 0.482 | self_kl 0 | self_cv 0 | loss 3.397 | nll_loss 1.018 | ppl 2.03 | wps 12295.8 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 77936 | lr 7.84787e-06 | gnorm 0.761 | train_wall 443 | wall 0
2021-01-09 09:14:56 | INFO | fairseq.trainer | begin training epoch 97
2021-01-09 09:14:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:14:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:15:32 | INFO | train_inner | epoch 097:     64 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.022, ppl=2.03, wps=8191.3, ups=1.16, wpb=7061.8, bsz=261.4, num_updates=78000, lr=7.84465e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 09:16:25 | INFO | train_inner | epoch 097:    164 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.021, ppl=2.03, wps=13334.2, ups=1.87, wpb=7112.5, bsz=254.6, num_updates=78100, lr=7.83962e-06, gnorm=0.745, train_wall=53, wall=0
2021-01-09 09:17:18 | INFO | train_inner | epoch 097:    264 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.024, ppl=2.03, wps=12850.5, ups=1.89, wpb=6812.2, bsz=247.8, num_updates=78200, lr=7.83461e-06, gnorm=0.777, train_wall=53, wall=0
2021-01-09 09:18:11 | INFO | train_inner | epoch 097:    364 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.02, ppl=2.03, wps=13404.5, ups=1.91, wpb=7031.5, bsz=228.2, num_updates=78300, lr=7.8296e-06, gnorm=0.762, train_wall=52, wall=0
2021-01-09 09:19:04 | INFO | train_inner | epoch 097:    464 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.012, ppl=2.02, wps=13324.1, ups=1.89, wpb=7061.4, bsz=258.1, num_updates=78400, lr=7.82461e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 09:19:56 | INFO | train_inner | epoch 097:    564 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.019, ppl=2.03, wps=13205.5, ups=1.9, wpb=6941.2, bsz=239.3, num_updates=78500, lr=7.81962e-06, gnorm=0.771, train_wall=52, wall=0
2021-01-09 09:20:49 | INFO | train_inner | epoch 097:    664 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.017, ppl=2.02, wps=13365.7, ups=1.9, wpb=7018.8, bsz=249.9, num_updates=78600, lr=7.81465e-06, gnorm=0.763, train_wall=52, wall=0
2021-01-09 09:21:42 | INFO | train_inner | epoch 097:    764 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.017, ppl=2.02, wps=13149.4, ups=1.88, wpb=6981.3, bsz=247.6, num_updates=78700, lr=7.80968e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 09:22:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:22:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:22:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:22:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:22:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:22:51 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.428 | nll_loss 4.007 | ppl 16.08 | bleu 21.92 | wps 3345.5 | wpb 5162.1 | bsz 187.5 | num_updates 78777 | best_bleu 22.31
2021-01-09 09:22:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:22:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:22:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 97 @ 78777 updates, score 21.92) (writing took 2.7959760762751102 seconds)
2021-01-09 09:22:53 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-09 09:22:53 | INFO | train | epoch 097 | symm_kl 0.482 | self_kl 0 | self_cv 0 | loss 3.397 | nll_loss 1.019 | ppl 2.03 | wps 12310.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 78777 | lr 7.80586e-06 | gnorm 0.762 | train_wall 442 | wall 0
2021-01-09 09:22:53 | INFO | fairseq.trainer | begin training epoch 98
2021-01-09 09:22:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:22:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:23:08 | INFO | train_inner | epoch 098:     23 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.016, ppl=2.02, wps=8038.4, ups=1.16, wpb=6941.8, bsz=238, num_updates=78800, lr=7.80472e-06, gnorm=0.767, train_wall=52, wall=0
2021-01-09 09:24:01 | INFO | train_inner | epoch 098:    123 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.994, ppl=1.99, wps=13144, ups=1.89, wpb=6967.8, bsz=248.2, num_updates=78900, lr=7.79978e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 09:24:54 | INFO | train_inner | epoch 098:    223 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.022, ppl=2.03, wps=13181.8, ups=1.89, wpb=6983.6, bsz=244.6, num_updates=79000, lr=7.79484e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 09:25:48 | INFO | train_inner | epoch 098:    323 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.007, ppl=2.01, wps=12999.6, ups=1.87, wpb=6940.7, bsz=257.2, num_updates=79100, lr=7.78991e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 09:26:40 | INFO | train_inner | epoch 098:    423 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.037, ppl=2.05, wps=13233.4, ups=1.9, wpb=6978.3, bsz=245.8, num_updates=79200, lr=7.78499e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 09:27:33 | INFO | train_inner | epoch 098:    523 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.023, ppl=2.03, wps=13195.9, ups=1.89, wpb=6972.8, bsz=240.7, num_updates=79300, lr=7.78008e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 09:28:27 | INFO | train_inner | epoch 098:    623 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.005, ppl=2.01, wps=13319.2, ups=1.88, wpb=7094.8, bsz=256, num_updates=79400, lr=7.77518e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 09:29:19 | INFO | train_inner | epoch 098:    723 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.021, ppl=2.03, wps=13285.4, ups=1.9, wpb=6984.9, bsz=236.1, num_updates=79500, lr=7.77029e-06, gnorm=0.762, train_wall=52, wall=0
2021-01-09 09:30:12 | INFO | train_inner | epoch 098:    823 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.029, ppl=2.04, wps=13159.8, ups=1.88, wpb=7009, bsz=244.1, num_updates=79600, lr=7.7654e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 09:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:30:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:30:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:30:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:30:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:30:49 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.429 | nll_loss 4.009 | ppl 16.1 | bleu 21.92 | wps 3370.8 | wpb 5162.1 | bsz 187.5 | num_updates 79618 | best_bleu 22.31
2021-01-09 09:30:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:30:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:30:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:30:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 98 @ 79618 updates, score 21.92) (writing took 2.7963709086179733 seconds)
2021-01-09 09:30:52 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-09 09:30:52 | INFO | train | epoch 098 | symm_kl 0.481 | self_kl 0 | self_cv 0 | loss 3.395 | nll_loss 1.018 | ppl 2.02 | wps 12281.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 79618 | lr 7.76453e-06 | gnorm 0.762 | train_wall 444 | wall 0
2021-01-09 09:30:52 | INFO | fairseq.trainer | begin training epoch 99
2021-01-09 09:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:30:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:31:38 | INFO | train_inner | epoch 099:     82 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.003, ppl=2, wps=8113.4, ups=1.17, wpb=6925.3, bsz=246, num_updates=79700, lr=7.76053e-06, gnorm=0.758, train_wall=52, wall=0
2021-01-09 09:32:31 | INFO | train_inner | epoch 099:    182 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.024, ppl=2.03, wps=13086.1, ups=1.86, wpb=7029.2, bsz=250.9, num_updates=79800, lr=7.75567e-06, gnorm=0.761, train_wall=54, wall=0
2021-01-09 09:33:24 | INFO | train_inner | epoch 099:    282 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.014, ppl=2.02, wps=13161.1, ups=1.89, wpb=6973, bsz=250.6, num_updates=79900, lr=7.75081e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 09:34:18 | INFO | train_inner | epoch 099:    382 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.012, ppl=2.02, wps=13131.4, ups=1.88, wpb=6974.2, bsz=245, num_updates=80000, lr=7.74597e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 09:35:11 | INFO | train_inner | epoch 099:    482 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.021, ppl=2.03, wps=13237.5, ups=1.87, wpb=7063.1, bsz=235.4, num_updates=80100, lr=7.74113e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 09:36:04 | INFO | train_inner | epoch 099:    582 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.024, ppl=2.03, wps=13266.4, ups=1.89, wpb=7029.6, bsz=234.6, num_updates=80200, lr=7.7363e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 09:36:57 | INFO | train_inner | epoch 099:    682 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.034, ppl=2.05, wps=13058.2, ups=1.9, wpb=6872.3, bsz=257.2, num_updates=80300, lr=7.73148e-06, gnorm=0.777, train_wall=52, wall=0
2021-01-09 09:37:50 | INFO | train_inner | epoch 099:    782 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.005, ppl=2.01, wps=13261.4, ups=1.87, wpb=7078.3, bsz=244.2, num_updates=80400, lr=7.72667e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 09:38:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:38:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:38:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:38:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:38:49 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.429 | nll_loss 4.008 | ppl 16.09 | bleu 21.8 | wps 3347.1 | wpb 5162.1 | bsz 187.5 | num_updates 80459 | best_bleu 22.31
2021-01-09 09:38:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:38:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:38:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:38:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 99 @ 80459 updates, score 21.8) (writing took 2.735980974510312 seconds)
2021-01-09 09:38:51 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-09 09:38:51 | INFO | train | epoch 099 | symm_kl 0.481 | self_kl 0 | self_cv 0 | loss 3.395 | nll_loss 1.017 | ppl 2.02 | wps 12277.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 80459 | lr 7.72384e-06 | gnorm 0.761 | train_wall 444 | wall 0
2021-01-09 09:38:51 | INFO | fairseq.trainer | begin training epoch 100
2021-01-09 09:38:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:38:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:39:15 | INFO | train_inner | epoch 100:     41 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.013, ppl=2.02, wps=8167.8, ups=1.17, wpb=6983, bsz=253.5, num_updates=80500, lr=7.72187e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 09:40:08 | INFO | train_inner | epoch 100:    141 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.373, nll_loss=1, ppl=2, wps=13372.7, ups=1.89, wpb=7063.3, bsz=265.9, num_updates=80600, lr=7.71708e-06, gnorm=0.742, train_wall=53, wall=0
2021-01-09 09:41:01 | INFO | train_inner | epoch 100:    241 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.001, ppl=2, wps=13316.6, ups=1.88, wpb=7067, bsz=245.4, num_updates=80700, lr=7.7123e-06, gnorm=0.748, train_wall=53, wall=0
2021-01-09 09:41:54 | INFO | train_inner | epoch 100:    341 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.016, ppl=2.02, wps=13137.7, ups=1.88, wpb=6972.9, bsz=248.7, num_updates=80800, lr=7.70752e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 09:42:48 | INFO | train_inner | epoch 100:    441 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.012, ppl=2.02, wps=13127.2, ups=1.86, wpb=7046.1, bsz=239.8, num_updates=80900, lr=7.70276e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 09:43:41 | INFO | train_inner | epoch 100:    541 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.016, ppl=2.02, wps=13131.4, ups=1.87, wpb=7016, bsz=249.3, num_updates=81000, lr=7.698e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 09:44:35 | INFO | train_inner | epoch 100:    641 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.038, ppl=2.05, wps=13147.7, ups=1.88, wpb=6996.5, bsz=235.4, num_updates=81100, lr=7.69326e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 09:45:27 | INFO | train_inner | epoch 100:    741 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.03, ppl=2.04, wps=12900.7, ups=1.9, wpb=6801.4, bsz=240.4, num_updates=81200, lr=7.68852e-06, gnorm=0.788, train_wall=53, wall=0
2021-01-09 09:46:21 | INFO | train_inner | epoch 100:    841 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.032, ppl=2.05, wps=13161.5, ups=1.88, wpb=7004.2, bsz=249.3, num_updates=81300, lr=7.68379e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 09:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:46:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:46:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:46:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:46:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:46:50 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.431 | nll_loss 4.01 | ppl 16.11 | bleu 21.96 | wps 3101.8 | wpb 5162.1 | bsz 187.5 | num_updates 81300 | best_bleu 22.31
2021-01-09 09:46:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:46:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:46:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:46:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 100 @ 81300 updates, score 21.96) (writing took 2.7305165566504 seconds)
2021-01-09 09:46:53 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-09 09:46:53 | INFO | train | epoch 100 | symm_kl 0.481 | self_kl 0 | self_cv 0 | loss 3.394 | nll_loss 1.017 | ppl 2.02 | wps 12208.8 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 81300 | lr 7.68379e-06 | gnorm 0.761 | train_wall 444 | wall 0
2021-01-09 09:46:53 | INFO | fairseq.trainer | begin training epoch 101
2021-01-09 09:46:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:46:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:47:49 | INFO | train_inner | epoch 101:    100 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.369, nll_loss=0.998, ppl=2, wps=7915.1, ups=1.14, wpb=6959.8, bsz=252.1, num_updates=81400, lr=7.67907e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 09:48:42 | INFO | train_inner | epoch 101:    200 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.007, ppl=2.01, wps=13031.7, ups=1.88, wpb=6923.5, bsz=247, num_updates=81500, lr=7.67435e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 09:49:35 | INFO | train_inner | epoch 101:    300 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.029, ppl=2.04, wps=13278.9, ups=1.89, wpb=7038.6, bsz=237.7, num_updates=81600, lr=7.66965e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 09:50:28 | INFO | train_inner | epoch 101:    400 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.024, ppl=2.03, wps=13327.5, ups=1.89, wpb=7041.1, bsz=253.3, num_updates=81700, lr=7.66495e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 09:51:21 | INFO | train_inner | epoch 101:    500 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.005, ppl=2.01, wps=13331.2, ups=1.88, wpb=7082.1, bsz=242.2, num_updates=81800, lr=7.66027e-06, gnorm=0.744, train_wall=53, wall=0
2021-01-09 09:52:14 | INFO | train_inner | epoch 101:    600 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.024, ppl=2.03, wps=13159.5, ups=1.88, wpb=6986.5, bsz=242.9, num_updates=81900, lr=7.65559e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 09:53:07 | INFO | train_inner | epoch 101:    700 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.019, ppl=2.03, wps=13169.7, ups=1.89, wpb=6975.6, bsz=253.4, num_updates=82000, lr=7.65092e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 09:54:00 | INFO | train_inner | epoch 101:    800 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.029, ppl=2.04, wps=13049.6, ups=1.89, wpb=6915.3, bsz=242.6, num_updates=82100, lr=7.64626e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 09:54:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 09:54:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:54:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:54:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:54:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 09:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 09:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 09:54:49 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.428 | nll_loss 4.008 | ppl 16.09 | bleu 22.01 | wps 3313.3 | wpb 5162.1 | bsz 187.5 | num_updates 82141 | best_bleu 22.31
2021-01-09 09:54:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 09:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:54:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 101 @ 82141 updates, score 22.01) (writing took 2.777543965727091 seconds)
2021-01-09 09:54:52 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-09 09:54:52 | INFO | train | epoch 101 | symm_kl 0.48 | self_kl 0 | self_cv 0 | loss 3.393 | nll_loss 1.017 | ppl 2.02 | wps 12281 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 82141 | lr 7.64435e-06 | gnorm 0.761 | train_wall 443 | wall 0
2021-01-09 09:54:52 | INFO | fairseq.trainer | begin training epoch 102
2021-01-09 09:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 09:54:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 09:55:26 | INFO | train_inner | epoch 102:     59 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.021, ppl=2.03, wps=8150.3, ups=1.16, wpb=7015.2, bsz=247.5, num_updates=82200, lr=7.64161e-06, gnorm=0.766, train_wall=52, wall=0
2021-01-09 09:56:18 | INFO | train_inner | epoch 102:    159 / 841 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.026, ppl=2.04, wps=13392.3, ups=1.91, wpb=7027.4, bsz=236, num_updates=82300, lr=7.63696e-06, gnorm=0.772, train_wall=52, wall=0
2021-01-09 09:57:11 | INFO | train_inner | epoch 102:    259 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.007, ppl=2.01, wps=13089.4, ups=1.89, wpb=6929, bsz=263.5, num_updates=82400, lr=7.63233e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 09:58:04 | INFO | train_inner | epoch 102:    359 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.01, ppl=2.01, wps=13192.8, ups=1.88, wpb=7001, bsz=243.3, num_updates=82500, lr=7.6277e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 09:58:57 | INFO | train_inner | epoch 102:    459 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.018, ppl=2.03, wps=13172.6, ups=1.9, wpb=6917.9, bsz=244.5, num_updates=82600, lr=7.62308e-06, gnorm=0.767, train_wall=52, wall=0
2021-01-09 09:59:50 | INFO | train_inner | epoch 102:    559 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.012, ppl=2.02, wps=13019.6, ups=1.87, wpb=6952.7, bsz=238.6, num_updates=82700, lr=7.61847e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:00:43 | INFO | train_inner | epoch 102:    659 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.006, ppl=2.01, wps=13237.7, ups=1.9, wpb=6984.5, bsz=257.6, num_updates=82800, lr=7.61387e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 10:01:36 | INFO | train_inner | epoch 102:    759 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.025, ppl=2.04, wps=13376.7, ups=1.89, wpb=7059.4, bsz=240.3, num_updates=82900, lr=7.60928e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:02:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:02:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:02:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:02:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:02:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:02:47 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.429 | nll_loss 4.009 | ppl 16.1 | bleu 22 | wps 3373.9 | wpb 5162.1 | bsz 187.5 | num_updates 82982 | best_bleu 22.31
2021-01-09 10:02:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:02:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:02:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 102 @ 82982 updates, score 22.0) (writing took 2.733513617888093 seconds)
2021-01-09 10:02:50 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-09 10:02:50 | INFO | train | epoch 102 | symm_kl 0.48 | self_kl 0 | self_cv 0 | loss 3.392 | nll_loss 1.017 | ppl 2.02 | wps 12303.2 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 82982 | lr 7.60552e-06 | gnorm 0.762 | train_wall 443 | wall 0
2021-01-09 10:02:50 | INFO | fairseq.trainer | begin training epoch 103
2021-01-09 10:02:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:02:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:03:02 | INFO | train_inner | epoch 103:     18 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.025, ppl=2.04, wps=8152, ups=1.15, wpb=7062, bsz=257.1, num_updates=83000, lr=7.60469e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 10:03:55 | INFO | train_inner | epoch 103:    118 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.017, ppl=2.02, wps=13335.4, ups=1.9, wpb=7024.9, bsz=248.6, num_updates=83100, lr=7.60011e-06, gnorm=0.767, train_wall=52, wall=0
2021-01-09 10:04:48 | INFO | train_inner | epoch 103:    218 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.009, ppl=2.01, wps=13251.1, ups=1.89, wpb=7007.4, bsz=247.3, num_updates=83200, lr=7.59555e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 10:05:41 | INFO | train_inner | epoch 103:    318 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.009, ppl=2.01, wps=12946.1, ups=1.88, wpb=6869.4, bsz=240.3, num_updates=83300, lr=7.59098e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 10:06:34 | INFO | train_inner | epoch 103:    418 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.013, ppl=2.02, wps=13253.6, ups=1.89, wpb=6998.5, bsz=235.2, num_updates=83400, lr=7.58643e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 10:07:27 | INFO | train_inner | epoch 103:    518 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.012, ppl=2.02, wps=13295.9, ups=1.88, wpb=7081.6, bsz=243.2, num_updates=83500, lr=7.58189e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 10:08:20 | INFO | train_inner | epoch 103:    618 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.025, ppl=2.03, wps=13144, ups=1.88, wpb=7003.5, bsz=258.9, num_updates=83600, lr=7.57735e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:09:13 | INFO | train_inner | epoch 103:    718 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.023, ppl=2.03, wps=13323.8, ups=1.89, wpb=7051.3, bsz=235.2, num_updates=83700, lr=7.57282e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 10:10:06 | INFO | train_inner | epoch 103:    818 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.02, ppl=2.03, wps=13205.9, ups=1.88, wpb=7029.4, bsz=260.6, num_updates=83800, lr=7.5683e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 10:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:10:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:10:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:10:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:10:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:10:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:10:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:10:46 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.434 | nll_loss 4.012 | ppl 16.14 | bleu 21.93 | wps 3391.4 | wpb 5162.1 | bsz 187.5 | num_updates 83823 | best_bleu 22.31
2021-01-09 10:10:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:10:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:10:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 103 @ 83823 updates, score 21.93) (writing took 2.6100196596235037 seconds)
2021-01-09 10:10:48 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-09 10:10:48 | INFO | train | epoch 103 | symm_kl 0.48 | self_kl 0 | self_cv 0 | loss 3.391 | nll_loss 1.016 | ppl 2.02 | wps 12292.1 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 83823 | lr 7.56727e-06 | gnorm 0.764 | train_wall 444 | wall 0
2021-01-09 10:10:48 | INFO | fairseq.trainer | begin training epoch 104
2021-01-09 10:10:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:10:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:10:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:11:31 | INFO | train_inner | epoch 104:     77 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.027, ppl=2.04, wps=8135.1, ups=1.18, wpb=6894.7, bsz=242.2, num_updates=83900, lr=7.56379e-06, gnorm=0.774, train_wall=51, wall=0
2021-01-09 10:12:25 | INFO | train_inner | epoch 104:    177 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.016, ppl=2.02, wps=13062.9, ups=1.87, wpb=6986.6, bsz=236, num_updates=84000, lr=7.55929e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:13:18 | INFO | train_inner | epoch 104:    277 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.002, ppl=2, wps=13246.5, ups=1.89, wpb=7025.1, bsz=250.2, num_updates=84100, lr=7.55479e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:14:10 | INFO | train_inner | epoch 104:    377 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.015, ppl=2.02, wps=13136.7, ups=1.91, wpb=6894.5, bsz=252.1, num_updates=84200, lr=7.55031e-06, gnorm=0.768, train_wall=52, wall=0
2021-01-09 10:15:04 | INFO | train_inner | epoch 104:    477 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.017, ppl=2.02, wps=13296.5, ups=1.87, wpb=7100.4, bsz=248.5, num_updates=84300, lr=7.54583e-06, gnorm=0.756, train_wall=53, wall=0
2021-01-09 10:15:56 | INFO | train_inner | epoch 104:    577 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.021, ppl=2.03, wps=13167.1, ups=1.89, wpb=6959.1, bsz=243.6, num_updates=84400, lr=7.54136e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:16:50 | INFO | train_inner | epoch 104:    677 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.024, ppl=2.03, wps=13202.5, ups=1.88, wpb=7006.4, bsz=252.2, num_updates=84500, lr=7.53689e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 10:17:43 | INFO | train_inner | epoch 104:    777 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.017, ppl=2.02, wps=13094.4, ups=1.87, wpb=6988.1, bsz=246.9, num_updates=84600, lr=7.53244e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 10:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:18:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:18:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:18:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:18:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:18:45 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.43 | nll_loss 4.01 | ppl 16.11 | bleu 21.94 | wps 3362 | wpb 5162.1 | bsz 187.5 | num_updates 84664 | best_bleu 22.31
2021-01-09 10:18:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:18:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:18:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:18:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 104 @ 84664 updates, score 21.94) (writing took 2.7703464943915606 seconds)
2021-01-09 10:18:48 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-09 10:18:48 | INFO | train | epoch 104 | symm_kl 0.479 | self_kl 0 | self_cv 0 | loss 3.391 | nll_loss 1.017 | ppl 2.02 | wps 12273.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 84664 | lr 7.52959e-06 | gnorm 0.763 | train_wall 444 | wall 0
2021-01-09 10:18:48 | INFO | fairseq.trainer | begin training epoch 105
2021-01-09 10:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:19:09 | INFO | train_inner | epoch 105:     36 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.018, ppl=2.02, wps=8048.6, ups=1.16, wpb=6950.5, bsz=239.7, num_updates=84700, lr=7.52799e-06, gnorm=0.765, train_wall=52, wall=0
2021-01-09 10:20:02 | INFO | train_inner | epoch 105:    136 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.006, ppl=2.01, wps=13230.5, ups=1.9, wpb=6968.8, bsz=243.7, num_updates=84800, lr=7.52355e-06, gnorm=0.765, train_wall=52, wall=0
2021-01-09 10:20:55 | INFO | train_inner | epoch 105:    236 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.019, ppl=2.03, wps=13229.7, ups=1.89, wpb=7009.1, bsz=265, num_updates=84900, lr=7.51912e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 10:21:48 | INFO | train_inner | epoch 105:    336 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.008, ppl=2.01, wps=13186.1, ups=1.88, wpb=7003.7, bsz=236.5, num_updates=85000, lr=7.51469e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 10:22:41 | INFO | train_inner | epoch 105:    436 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.015, ppl=2.02, wps=13200.8, ups=1.88, wpb=7011.6, bsz=249.3, num_updates=85100, lr=7.51027e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 10:23:34 | INFO | train_inner | epoch 105:    536 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.023, ppl=2.03, wps=13142.3, ups=1.89, wpb=6945.6, bsz=230.7, num_updates=85200, lr=7.50587e-06, gnorm=0.775, train_wall=53, wall=0
2021-01-09 10:24:27 | INFO | train_inner | epoch 105:    636 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.025, ppl=2.03, wps=13326.9, ups=1.88, wpb=7087.3, bsz=254.4, num_updates=85300, lr=7.50147e-06, gnorm=0.752, train_wall=53, wall=0
2021-01-09 10:25:20 | INFO | train_inner | epoch 105:    736 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.02, ppl=2.03, wps=13125.4, ups=1.88, wpb=6980.3, bsz=253.5, num_updates=85400, lr=7.49707e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 10:26:14 | INFO | train_inner | epoch 105:    836 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.008, ppl=2.01, wps=13107.9, ups=1.87, wpb=7008.2, bsz=243, num_updates=85500, lr=7.49269e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:26:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:26:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:26:44 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.429 | nll_loss 4.008 | ppl 16.09 | bleu 22.16 | wps 3385.8 | wpb 5162.1 | bsz 187.5 | num_updates 85505 | best_bleu 22.31
2021-01-09 10:26:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:26:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 105 @ 85505 updates, score 22.16) (writing took 2.7273297868669033 seconds)
2021-01-09 10:26:47 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-09 10:26:47 | INFO | train | epoch 105 | symm_kl 0.479 | self_kl 0 | self_cv 0 | loss 3.39 | nll_loss 1.016 | ppl 2.02 | wps 12276.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 85505 | lr 7.49247e-06 | gnorm 0.763 | train_wall 444 | wall 0
2021-01-09 10:26:47 | INFO | fairseq.trainer | begin training epoch 106
2021-01-09 10:26:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:26:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:27:39 | INFO | train_inner | epoch 106:     95 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.009, ppl=2.01, wps=8126.6, ups=1.17, wpb=6932.5, bsz=245, num_updates=85600, lr=7.48831e-06, gnorm=0.763, train_wall=52, wall=0
2021-01-09 10:28:32 | INFO | train_inner | epoch 106:    195 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.018, ppl=2.03, wps=13125, ups=1.89, wpb=6954.5, bsz=237.8, num_updates=85700, lr=7.48394e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 10:29:25 | INFO | train_inner | epoch 106:    295 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.016, ppl=2.02, wps=13224.7, ups=1.9, wpb=6966.8, bsz=256.4, num_updates=85800, lr=7.47958e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 10:30:18 | INFO | train_inner | epoch 106:    395 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.365, nll_loss=0.995, ppl=1.99, wps=13257.8, ups=1.89, wpb=7024.9, bsz=253.4, num_updates=85900, lr=7.47522e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 10:31:11 | INFO | train_inner | epoch 106:    495 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.032, ppl=2.04, wps=12976.9, ups=1.88, wpb=6911.6, bsz=244.1, num_updates=86000, lr=7.47087e-06, gnorm=0.774, train_wall=53, wall=0
2021-01-09 10:32:04 | INFO | train_inner | epoch 106:    595 / 841 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.025, ppl=2.04, wps=13277.6, ups=1.88, wpb=7065, bsz=239.8, num_updates=86100, lr=7.46653e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 10:32:58 | INFO | train_inner | epoch 106:    695 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.002, ppl=2, wps=13129.4, ups=1.87, wpb=7014.8, bsz=253.1, num_updates=86200, lr=7.4622e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:33:51 | INFO | train_inner | epoch 106:    795 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.022, ppl=2.03, wps=13190.4, ups=1.89, wpb=6997, bsz=244.6, num_updates=86300, lr=7.45788e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 10:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:34:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:34:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:34:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:34:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:34:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:34:43 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.434 | nll_loss 4.015 | ppl 16.16 | bleu 21.93 | wps 3338.8 | wpb 5162.1 | bsz 187.5 | num_updates 86346 | best_bleu 22.31
2021-01-09 10:34:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:34:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:34:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:34:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 106 @ 86346 updates, score 21.93) (writing took 2.752659035846591 seconds)
2021-01-09 10:34:46 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-09 10:34:46 | INFO | train | epoch 106 | symm_kl 0.479 | self_kl 0 | self_cv 0 | loss 3.389 | nll_loss 1.015 | ppl 2.02 | wps 12273.5 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 86346 | lr 7.45589e-06 | gnorm 0.762 | train_wall 444 | wall 0
2021-01-09 10:34:46 | INFO | fairseq.trainer | begin training epoch 107
2021-01-09 10:34:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:35:17 | INFO | train_inner | epoch 107:     54 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.004, ppl=2.01, wps=8227.5, ups=1.16, wpb=7114.7, bsz=247, num_updates=86400, lr=7.45356e-06, gnorm=0.749, train_wall=52, wall=0
2021-01-09 10:36:10 | INFO | train_inner | epoch 107:    154 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.026, ppl=2.04, wps=13091.1, ups=1.88, wpb=6962.5, bsz=235.9, num_updates=86500, lr=7.44925e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 10:37:03 | INFO | train_inner | epoch 107:    254 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.009, ppl=2.01, wps=13279.5, ups=1.9, wpb=6994.2, bsz=235.5, num_updates=86600, lr=7.44495e-06, gnorm=0.767, train_wall=52, wall=0
2021-01-09 10:37:56 | INFO | train_inner | epoch 107:    354 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.013, ppl=2.02, wps=13325.3, ups=1.89, wpb=7051, bsz=270.1, num_updates=86700, lr=7.44065e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 10:38:49 | INFO | train_inner | epoch 107:    454 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.011, ppl=2.01, wps=13023.8, ups=1.88, wpb=6912.8, bsz=254.9, num_updates=86800, lr=7.43637e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 10:39:42 | INFO | train_inner | epoch 107:    554 / 841 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.411, nll_loss=1.033, ppl=2.05, wps=13174.4, ups=1.88, wpb=7018.6, bsz=236.6, num_updates=86900, lr=7.43209e-06, gnorm=0.779, train_wall=53, wall=0
2021-01-09 10:40:36 | INFO | train_inner | epoch 107:    654 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.022, ppl=2.03, wps=13031.4, ups=1.88, wpb=6934.9, bsz=244.6, num_updates=87000, lr=7.42781e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:41:29 | INFO | train_inner | epoch 107:    754 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.018, ppl=2.02, wps=13007.2, ups=1.88, wpb=6918.8, bsz=251.8, num_updates=87100, lr=7.42355e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 10:42:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:42:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:42:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:42:43 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.429 | nll_loss 4.009 | ppl 16.1 | bleu 22.04 | wps 3324.3 | wpb 5162.1 | bsz 187.5 | num_updates 87187 | best_bleu 22.31
2021-01-09 10:42:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:42:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:42:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 107 @ 87187 updates, score 22.04) (writing took 2.764119915664196 seconds)
2021-01-09 10:42:46 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-09 10:42:46 | INFO | train | epoch 107 | symm_kl 0.478 | self_kl 0 | self_cv 0 | loss 3.388 | nll_loss 1.015 | ppl 2.02 | wps 12248.5 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 87187 | lr 7.41984e-06 | gnorm 0.764 | train_wall 445 | wall 0
2021-01-09 10:42:46 | INFO | fairseq.trainer | begin training epoch 108
2021-01-09 10:42:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:42:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:42:56 | INFO | train_inner | epoch 108:     13 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.372, nll_loss=0.999, ppl=2, wps=8138.5, ups=1.15, wpb=7078.4, bsz=238.4, num_updates=87200, lr=7.41929e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 10:43:48 | INFO | train_inner | epoch 108:    113 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.003, ppl=2, wps=13478.8, ups=1.91, wpb=7073.3, bsz=263.2, num_updates=87300, lr=7.41504e-06, gnorm=0.743, train_wall=52, wall=0
2021-01-09 10:44:41 | INFO | train_inner | epoch 108:    213 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.012, ppl=2.02, wps=13094.8, ups=1.9, wpb=6882.9, bsz=231.9, num_updates=87400, lr=7.4108e-06, gnorm=0.769, train_wall=52, wall=0
2021-01-09 10:45:34 | INFO | train_inner | epoch 108:    313 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.02, ppl=2.03, wps=13226.6, ups=1.88, wpb=7025.3, bsz=241.3, num_updates=87500, lr=7.40656e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 10:46:27 | INFO | train_inner | epoch 108:    413 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.013, ppl=2.02, wps=12942.9, ups=1.87, wpb=6922, bsz=234.8, num_updates=87600, lr=7.40233e-06, gnorm=0.79, train_wall=53, wall=0
2021-01-09 10:47:21 | INFO | train_inner | epoch 108:    513 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.01, ppl=2.01, wps=13101.9, ups=1.87, wpb=6992.3, bsz=252.8, num_updates=87700, lr=7.39811e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:48:14 | INFO | train_inner | epoch 108:    613 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.011, ppl=2.01, wps=13086.6, ups=1.86, wpb=7021.7, bsz=247.5, num_updates=87800, lr=7.3939e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:49:08 | INFO | train_inner | epoch 108:    713 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.024, ppl=2.03, wps=13081.6, ups=1.87, wpb=6993.9, bsz=238.9, num_updates=87900, lr=7.38969e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 10:50:01 | INFO | train_inner | epoch 108:    813 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.027, ppl=2.04, wps=13245.2, ups=1.87, wpb=7077.1, bsz=264.6, num_updates=88000, lr=7.38549e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 10:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:50:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:50:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:50:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:50:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:50:44 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.433 | nll_loss 4.013 | ppl 16.14 | bleu 22.03 | wps 3341.6 | wpb 5162.1 | bsz 187.5 | num_updates 88028 | best_bleu 22.31
2021-01-09 10:50:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:50:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:50:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 108 @ 88028 updates, score 22.03) (writing took 2.7319870479404926 seconds)
2021-01-09 10:50:47 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-09 10:50:47 | INFO | train | epoch 108 | symm_kl 0.478 | self_kl 0 | self_cv 0 | loss 3.387 | nll_loss 1.015 | ppl 2.02 | wps 12233.1 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 88028 | lr 7.38431e-06 | gnorm 0.766 | train_wall 445 | wall 0
2021-01-09 10:50:47 | INFO | fairseq.trainer | begin training epoch 109
2021-01-09 10:50:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:50:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:51:27 | INFO | train_inner | epoch 109:     72 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.013, ppl=2.02, wps=7977, ups=1.16, wpb=6870.5, bsz=248.4, num_updates=88100, lr=7.3813e-06, gnorm=0.774, train_wall=52, wall=0
2021-01-09 10:52:21 | INFO | train_inner | epoch 109:    172 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.023, ppl=2.03, wps=13166.9, ups=1.88, wpb=6993.8, bsz=254.3, num_updates=88200, lr=7.37711e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 10:53:13 | INFO | train_inner | epoch 109:    272 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.021, ppl=2.03, wps=13257.7, ups=1.89, wpb=7005.4, bsz=242.6, num_updates=88300, lr=7.37293e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 10:54:06 | INFO | train_inner | epoch 109:    372 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.013, ppl=2.02, wps=13223.2, ups=1.89, wpb=6989.2, bsz=240.2, num_updates=88400, lr=7.36876e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:55:00 | INFO | train_inner | epoch 109:    472 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.009, ppl=2.01, wps=13014.8, ups=1.88, wpb=6935.8, bsz=250.8, num_updates=88500, lr=7.3646e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 10:55:53 | INFO | train_inner | epoch 109:    572 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.007, ppl=2.01, wps=13136.6, ups=1.86, wpb=7051.5, bsz=245, num_updates=88600, lr=7.36044e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 10:56:46 | INFO | train_inner | epoch 109:    672 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.008, ppl=2.01, wps=13161.2, ups=1.89, wpb=6970.9, bsz=243, num_updates=88700, lr=7.35629e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 10:57:40 | INFO | train_inner | epoch 109:    772 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.018, ppl=2.02, wps=13187.2, ups=1.87, wpb=7036.4, bsz=253.7, num_updates=88800, lr=7.35215e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 10:58:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 10:58:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:58:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:58:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:58:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 10:58:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 10:58:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 10:58:44 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.431 | nll_loss 4.012 | ppl 16.14 | bleu 22.09 | wps 3383.9 | wpb 5162.1 | bsz 187.5 | num_updates 88869 | best_bleu 22.31
2021-01-09 10:58:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 10:58:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:58:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:58:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 109 @ 88869 updates, score 22.09) (writing took 2.7520235422998667 seconds)
2021-01-09 10:58:47 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-09 10:58:47 | INFO | train | epoch 109 | symm_kl 0.478 | self_kl 0 | self_cv 0 | loss 3.387 | nll_loss 1.014 | ppl 2.02 | wps 12253.8 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 88869 | lr 7.34929e-06 | gnorm 0.763 | train_wall 445 | wall 0
2021-01-09 10:58:47 | INFO | fairseq.trainer | begin training epoch 110
2021-01-09 10:58:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 10:58:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 10:59:06 | INFO | train_inner | epoch 110:     31 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.014, ppl=2.02, wps=8118.9, ups=1.16, wpb=6988.5, bsz=240.2, num_updates=88900, lr=7.34801e-06, gnorm=0.764, train_wall=52, wall=0
2021-01-09 10:59:58 | INFO | train_inner | epoch 110:    131 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.011, ppl=2.02, wps=13336.5, ups=1.91, wpb=6979.6, bsz=245.5, num_updates=89000, lr=7.34388e-06, gnorm=0.76, train_wall=52, wall=0
2021-01-09 11:00:51 | INFO | train_inner | epoch 110:    231 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.014, ppl=2.02, wps=13037.6, ups=1.87, wpb=6964.3, bsz=237.8, num_updates=89100, lr=7.33976e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 11:01:44 | INFO | train_inner | epoch 110:    331 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.003, ppl=2, wps=13334.3, ups=1.88, wpb=7078.5, bsz=230.1, num_updates=89200, lr=7.33564e-06, gnorm=0.759, train_wall=53, wall=0
2021-01-09 11:02:37 | INFO | train_inner | epoch 110:    431 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.016, ppl=2.02, wps=13049.6, ups=1.9, wpb=6885.6, bsz=249.4, num_updates=89300, lr=7.33153e-06, gnorm=0.785, train_wall=53, wall=0
2021-01-09 11:03:30 | INFO | train_inner | epoch 110:    531 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.02, ppl=2.03, wps=13179, ups=1.89, wpb=6981, bsz=254, num_updates=89400, lr=7.32743e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 11:04:23 | INFO | train_inner | epoch 110:    631 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.029, ppl=2.04, wps=13225.4, ups=1.9, wpb=6970.4, bsz=253, num_updates=89500, lr=7.32334e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 11:05:16 | INFO | train_inner | epoch 110:    731 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.006, ppl=2.01, wps=13369.7, ups=1.89, wpb=7087.5, bsz=260.5, num_updates=89600, lr=7.31925e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 11:06:09 | INFO | train_inner | epoch 110:    831 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.022, ppl=2.03, wps=13191.6, ups=1.87, wpb=7044.9, bsz=246.4, num_updates=89700, lr=7.31517e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 11:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:06:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:06:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:06:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:06:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:06:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:06:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:06:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:06:42 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.432 | nll_loss 4.013 | ppl 16.14 | bleu 21.95 | wps 3355.7 | wpb 5162.1 | bsz 187.5 | num_updates 89710 | best_bleu 22.31
2021-01-09 11:06:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:06:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 110 @ 89710 updates, score 21.95) (writing took 2.7248022705316544 seconds)
2021-01-09 11:06:45 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-09 11:06:45 | INFO | train | epoch 110 | symm_kl 0.477 | self_kl 0 | self_cv 0 | loss 3.386 | nll_loss 1.014 | ppl 2.02 | wps 12300 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 89710 | lr 7.31476e-06 | gnorm 0.767 | train_wall 443 | wall 0
2021-01-09 11:06:45 | INFO | fairseq.trainer | begin training epoch 111
2021-01-09 11:06:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:06:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:07:35 | INFO | train_inner | epoch 111:     90 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.019, ppl=2.03, wps=8227.9, ups=1.17, wpb=7050.2, bsz=247.9, num_updates=89800, lr=7.3111e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 11:08:29 | INFO | train_inner | epoch 111:    190 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.996, ppl=2, wps=13287.5, ups=1.87, wpb=7118.3, bsz=250.6, num_updates=89900, lr=7.30703e-06, gnorm=0.747, train_wall=53, wall=0
2021-01-09 11:09:22 | INFO | train_inner | epoch 111:    290 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.021, ppl=2.03, wps=13191.2, ups=1.88, wpb=7018.5, bsz=244.8, num_updates=90000, lr=7.30297e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 11:10:15 | INFO | train_inner | epoch 111:    390 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.019, ppl=2.03, wps=13119.8, ups=1.89, wpb=6934.6, bsz=237.8, num_updates=90100, lr=7.29891e-06, gnorm=0.776, train_wall=53, wall=0
2021-01-09 11:11:07 | INFO | train_inner | epoch 111:    490 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.022, ppl=2.03, wps=13198.2, ups=1.9, wpb=6947.1, bsz=238.2, num_updates=90200, lr=7.29487e-06, gnorm=0.77, train_wall=52, wall=0
2021-01-09 11:12:01 | INFO | train_inner | epoch 111:    590 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.011, ppl=2.01, wps=13096.3, ups=1.87, wpb=6987.4, bsz=252.5, num_updates=90300, lr=7.29083e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 11:12:53 | INFO | train_inner | epoch 111:    690 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.011, ppl=2.02, wps=13164.9, ups=1.89, wpb=6949, bsz=241.6, num_updates=90400, lr=7.28679e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 11:13:47 | INFO | train_inner | epoch 111:    790 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.007, ppl=2.01, wps=12946.9, ups=1.87, wpb=6928.2, bsz=258, num_updates=90500, lr=7.28277e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 11:14:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:14:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:14:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:14:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:14:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:14:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:14:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:14:42 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.435 | nll_loss 4.015 | ppl 16.17 | bleu 21.99 | wps 3351.3 | wpb 5162.1 | bsz 187.5 | num_updates 90551 | best_bleu 22.31
2021-01-09 11:14:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:14:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:14:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:14:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 111 @ 90551 updates, score 21.99) (writing took 2.777562163770199 seconds)
2021-01-09 11:14:45 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-09 11:14:45 | INFO | train | epoch 111 | symm_kl 0.477 | self_kl 0 | self_cv 0 | loss 3.385 | nll_loss 1.013 | ppl 2.02 | wps 12252.1 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 90551 | lr 7.28071e-06 | gnorm 0.765 | train_wall 445 | wall 0
2021-01-09 11:14:45 | INFO | fairseq.trainer | begin training epoch 112
2021-01-09 11:14:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:14:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:15:13 | INFO | train_inner | epoch 112:     49 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.015, ppl=2.02, wps=8148.1, ups=1.16, wpb=7048.3, bsz=244.3, num_updates=90600, lr=7.27875e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 11:16:07 | INFO | train_inner | epoch 112:    149 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.007, ppl=2.01, wps=13097.2, ups=1.87, wpb=6997.2, bsz=249.9, num_updates=90700, lr=7.27473e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 11:17:00 | INFO | train_inner | epoch 112:    249 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.023, ppl=2.03, wps=13255.9, ups=1.89, wpb=7015.2, bsz=235.9, num_updates=90800, lr=7.27072e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 11:17:53 | INFO | train_inner | epoch 112:    349 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.358, nll_loss=0.992, ppl=1.99, wps=13080, ups=1.88, wpb=6946.8, bsz=255, num_updates=90900, lr=7.26672e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 11:18:46 | INFO | train_inner | epoch 112:    449 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.02, ppl=2.03, wps=12885.1, ups=1.88, wpb=6862.7, bsz=235.8, num_updates=91000, lr=7.26273e-06, gnorm=0.779, train_wall=53, wall=0
2021-01-09 11:19:39 | INFO | train_inner | epoch 112:    549 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.028, ppl=2.04, wps=13076.8, ups=1.88, wpb=6971.5, bsz=230.8, num_updates=91100, lr=7.25874e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 11:20:32 | INFO | train_inner | epoch 112:    649 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.003, ppl=2, wps=13300.4, ups=1.89, wpb=7055, bsz=255.2, num_updates=91200, lr=7.25476e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 11:21:26 | INFO | train_inner | epoch 112:    749 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.013, ppl=2.02, wps=13153.8, ups=1.88, wpb=7014.4, bsz=251.5, num_updates=91300, lr=7.25079e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 11:22:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:22:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:22:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:22:45 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.437 | nll_loss 4.018 | ppl 16.2 | bleu 22 | wps 3194.5 | wpb 5162.1 | bsz 187.5 | num_updates 91392 | best_bleu 22.31
2021-01-09 11:22:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:22:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:22:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 112 @ 91392 updates, score 22.0) (writing took 2.848096700385213 seconds)
2021-01-09 11:22:47 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-09 11:22:47 | INFO | train | epoch 112 | symm_kl 0.477 | self_kl 0 | self_cv 0 | loss 3.384 | nll_loss 1.013 | ppl 2.02 | wps 12185.6 | ups 1.74 | wpb 6993.1 | bsz 246.6 | num_updates 91392 | lr 7.24714e-06 | gnorm 0.764 | train_wall 446 | wall 0
2021-01-09 11:22:47 | INFO | fairseq.trainer | begin training epoch 113
2021-01-09 11:22:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:22:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:22:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:22:55 | INFO | train_inner | epoch 113:      8 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.012, ppl=2.02, wps=7832.2, ups=1.12, wpb=6995.2, bsz=256.3, num_updates=91400, lr=7.24682e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 11:23:48 | INFO | train_inner | epoch 113:    108 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.028, ppl=2.04, wps=13304.2, ups=1.9, wpb=6997.9, bsz=242, num_updates=91500, lr=7.24286e-06, gnorm=0.772, train_wall=52, wall=0
2021-01-09 11:24:41 | INFO | train_inner | epoch 113:    208 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.998, ppl=2, wps=13140, ups=1.86, wpb=7052.7, bsz=242.2, num_updates=91600, lr=7.23891e-06, gnorm=0.75, train_wall=53, wall=0
2021-01-09 11:25:35 | INFO | train_inner | epoch 113:    308 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.015, ppl=2.02, wps=13169.3, ups=1.88, wpb=7015, bsz=255.9, num_updates=91700, lr=7.23496e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 11:26:28 | INFO | train_inner | epoch 113:    408 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.002, ppl=2, wps=13231.9, ups=1.87, wpb=7061.1, bsz=270.9, num_updates=91800, lr=7.23102e-06, gnorm=0.749, train_wall=53, wall=0
2021-01-09 11:27:22 | INFO | train_inner | epoch 113:    508 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.003, ppl=2, wps=12941.2, ups=1.84, wpb=7016.5, bsz=244.8, num_updates=91900, lr=7.22708e-06, gnorm=0.764, train_wall=54, wall=0
2021-01-09 11:28:17 | INFO | train_inner | epoch 113:    608 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.011, ppl=2.02, wps=12871.5, ups=1.84, wpb=6997, bsz=258.6, num_updates=92000, lr=7.22315e-06, gnorm=0.76, train_wall=54, wall=0
2021-01-09 11:29:10 | INFO | train_inner | epoch 113:    708 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.024, ppl=2.03, wps=12994.5, ups=1.88, wpb=6901.1, bsz=233.6, num_updates=92100, lr=7.21923e-06, gnorm=0.783, train_wall=53, wall=0
2021-01-09 11:30:03 | INFO | train_inner | epoch 113:    808 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.024, ppl=2.03, wps=13167.8, ups=1.89, wpb=6978.8, bsz=237.8, num_updates=92200, lr=7.21531e-06, gnorm=0.782, train_wall=53, wall=0
2021-01-09 11:30:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:30:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:30:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:30:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:30:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:30:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:30:49 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.436 | nll_loss 4.017 | ppl 16.18 | bleu 21.98 | wps 3330.4 | wpb 5162.1 | bsz 187.5 | num_updates 92233 | best_bleu 22.31
2021-01-09 11:30:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:30:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:30:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:30:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 113 @ 92233 updates, score 21.98) (writing took 2.812257207930088 seconds)
2021-01-09 11:30:51 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-09 11:30:51 | INFO | train | epoch 113 | symm_kl 0.476 | self_kl 0 | self_cv 0 | loss 3.384 | nll_loss 1.014 | ppl 2.02 | wps 12153.3 | ups 1.74 | wpb 6993.1 | bsz 246.6 | num_updates 92233 | lr 7.21402e-06 | gnorm 0.767 | train_wall 448 | wall 0
2021-01-09 11:30:51 | INFO | fairseq.trainer | begin training epoch 114
2021-01-09 11:30:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:30:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:31:29 | INFO | train_inner | epoch 114:     67 / 841 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.027, ppl=2.04, wps=8074, ups=1.15, wpb=6998.6, bsz=232.3, num_updates=92300, lr=7.2114e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 11:32:22 | INFO | train_inner | epoch 114:    167 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.022, ppl=2.03, wps=13232.7, ups=1.89, wpb=7007.7, bsz=234.2, num_updates=92400, lr=7.2075e-06, gnorm=0.774, train_wall=53, wall=0
2021-01-09 11:33:16 | INFO | train_inner | epoch 114:    267 / 841 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.023, ppl=2.03, wps=13011.6, ups=1.88, wpb=6918.1, bsz=244.9, num_updates=92500, lr=7.2036e-06, gnorm=0.774, train_wall=53, wall=0
2021-01-09 11:34:08 | INFO | train_inner | epoch 114:    367 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.02, ppl=2.03, wps=13190.5, ups=1.89, wpb=6983.5, bsz=237.1, num_updates=92600, lr=7.19971e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 11:35:02 | INFO | train_inner | epoch 114:    467 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.005, ppl=2.01, wps=13136.7, ups=1.87, wpb=7030.9, bsz=252.6, num_updates=92700, lr=7.19583e-06, gnorm=0.758, train_wall=53, wall=0
2021-01-09 11:35:55 | INFO | train_inner | epoch 114:    567 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.013, ppl=2.02, wps=12998.7, ups=1.88, wpb=6908.9, bsz=237.6, num_updates=92800, lr=7.19195e-06, gnorm=0.782, train_wall=53, wall=0
2021-01-09 11:36:48 | INFO | train_inner | epoch 114:    667 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.002, ppl=2, wps=13175.1, ups=1.88, wpb=7014.7, bsz=252.2, num_updates=92900, lr=7.18808e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 11:37:42 | INFO | train_inner | epoch 114:    767 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.009, ppl=2.01, wps=13184.8, ups=1.87, wpb=7037.2, bsz=260.3, num_updates=93000, lr=7.18421e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 11:38:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:38:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:38:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:38:49 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.437 | nll_loss 4.019 | ppl 16.21 | bleu 21.99 | wps 3339.2 | wpb 5162.1 | bsz 187.5 | num_updates 93074 | best_bleu 22.31
2021-01-09 11:38:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:38:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:38:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:38:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 114 @ 93074 updates, score 21.99) (writing took 2.740918878465891 seconds)
2021-01-09 11:38:52 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-09 11:38:52 | INFO | train | epoch 114 | symm_kl 0.476 | self_kl 0 | self_cv 0 | loss 3.383 | nll_loss 1.013 | ppl 2.02 | wps 12233.4 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 93074 | lr 7.18136e-06 | gnorm 0.766 | train_wall 445 | wall 0
2021-01-09 11:38:52 | INFO | fairseq.trainer | begin training epoch 115
2021-01-09 11:38:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:38:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:39:09 | INFO | train_inner | epoch 115:     26 / 841 symm_kl=0.47, self_kl=0, self_cv=0, loss=3.36, nll_loss=0.998, ppl=2, wps=8039.5, ups=1.15, wpb=6996.8, bsz=261.4, num_updates=93100, lr=7.18035e-06, gnorm=0.755, train_wall=53, wall=0
2021-01-09 11:40:01 | INFO | train_inner | epoch 115:    126 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.021, ppl=2.03, wps=13133.5, ups=1.9, wpb=6904.8, bsz=247.2, num_updates=93200, lr=7.1765e-06, gnorm=0.776, train_wall=52, wall=0
2021-01-09 11:40:55 | INFO | train_inner | epoch 115:    226 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.367, nll_loss=1, ppl=2, wps=13154.5, ups=1.86, wpb=7058.7, bsz=250.6, num_updates=93300, lr=7.17265e-06, gnorm=0.751, train_wall=53, wall=0
2021-01-09 11:41:49 | INFO | train_inner | epoch 115:    326 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.03, ppl=2.04, wps=13013.9, ups=1.87, wpb=6959.2, bsz=237.3, num_updates=93400, lr=7.16881e-06, gnorm=0.777, train_wall=53, wall=0
2021-01-09 11:42:42 | INFO | train_inner | epoch 115:    426 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.367, nll_loss=0.997, ppl=2, wps=13195.9, ups=1.88, wpb=7037.2, bsz=243.4, num_updates=93500, lr=7.16498e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 11:43:35 | INFO | train_inner | epoch 115:    526 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.008, ppl=2.01, wps=13128.3, ups=1.89, wpb=6958.7, bsz=239.8, num_updates=93600, lr=7.16115e-06, gnorm=0.78, train_wall=53, wall=0
2021-01-09 11:44:28 | INFO | train_inner | epoch 115:    626 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.005, ppl=2.01, wps=13017, ups=1.87, wpb=6949.6, bsz=257.4, num_updates=93700, lr=7.15733e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 11:45:21 | INFO | train_inner | epoch 115:    726 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.024, ppl=2.03, wps=13286.5, ups=1.89, wpb=7037.9, bsz=241, num_updates=93800, lr=7.15351e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 11:46:14 | INFO | train_inner | epoch 115:    826 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.009, ppl=2.01, wps=13248.2, ups=1.88, wpb=7042.5, bsz=252.9, num_updates=93900, lr=7.1497e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 11:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:46:50 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.437 | nll_loss 4.017 | ppl 16.19 | bleu 21.95 | wps 3336.6 | wpb 5162.1 | bsz 187.5 | num_updates 93915 | best_bleu 22.31
2021-01-09 11:46:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:46:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:46:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:46:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 115 @ 93915 updates, score 21.95) (writing took 2.8206578511744738 seconds)
2021-01-09 11:46:53 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-09 11:46:53 | INFO | train | epoch 115 | symm_kl 0.476 | self_kl 0 | self_cv 0 | loss 3.382 | nll_loss 1.012 | ppl 2.02 | wps 12231 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 93915 | lr 7.14913e-06 | gnorm 0.766 | train_wall 445 | wall 0
2021-01-09 11:46:53 | INFO | fairseq.trainer | begin training epoch 116
2021-01-09 11:46:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:46:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:47:40 | INFO | train_inner | epoch 116:     85 / 841 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.361, nll_loss=0.997, ppl=2, wps=8180.8, ups=1.16, wpb=7040.3, bsz=253.3, num_updates=94000, lr=7.1459e-06, gnorm=0.756, train_wall=52, wall=0
2021-01-09 11:48:34 | INFO | train_inner | epoch 116:    185 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.015, ppl=2.02, wps=12999.6, ups=1.85, wpb=7022.4, bsz=242.2, num_updates=94100, lr=7.1421e-06, gnorm=0.774, train_wall=54, wall=0
2021-01-09 11:49:28 | INFO | train_inner | epoch 116:    285 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.99, ppl=1.99, wps=13199.8, ups=1.86, wpb=7088.3, bsz=257.1, num_updates=94200, lr=7.13831e-06, gnorm=0.75, train_wall=54, wall=0
2021-01-09 11:50:22 | INFO | train_inner | epoch 116:    385 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.013, ppl=2.02, wps=13019.9, ups=1.87, wpb=6951.5, bsz=244.6, num_updates=94300, lr=7.13452e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 11:51:14 | INFO | train_inner | epoch 116:    485 / 841 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.022, ppl=2.03, wps=13212.2, ups=1.9, wpb=6958.7, bsz=237.1, num_updates=94400, lr=7.13074e-06, gnorm=0.779, train_wall=52, wall=0
2021-01-09 11:52:07 | INFO | train_inner | epoch 116:    585 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.013, ppl=2.02, wps=12982.1, ups=1.9, wpb=6844.9, bsz=240.7, num_updates=94500, lr=7.12697e-06, gnorm=0.795, train_wall=53, wall=0
2021-01-09 11:53:00 | INFO | train_inner | epoch 116:    685 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.02, ppl=2.03, wps=13366.8, ups=1.9, wpb=7037.3, bsz=248.4, num_updates=94600, lr=7.1232e-06, gnorm=0.769, train_wall=52, wall=0
2021-01-09 11:53:53 | INFO | train_inner | epoch 116:    785 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.017, ppl=2.02, wps=13223.3, ups=1.89, wpb=7006.6, bsz=253.3, num_updates=94700, lr=7.11944e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 11:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 11:54:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:54:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:54:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:54:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 11:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 11:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 11:54:50 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.44 | nll_loss 4.02 | ppl 16.23 | bleu 21.89 | wps 3302.1 | wpb 5162.1 | bsz 187.5 | num_updates 94756 | best_bleu 22.31
2021-01-09 11:54:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 11:54:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:54:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:54:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 116 @ 94756 updates, score 21.89) (writing took 2.770919205620885 seconds)
2021-01-09 11:54:53 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-09 11:54:53 | INFO | train | epoch 116 | symm_kl 0.476 | self_kl 0 | self_cv 0 | loss 3.381 | nll_loss 1.012 | ppl 2.02 | wps 12258.7 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 94756 | lr 7.11733e-06 | gnorm 0.77 | train_wall 444 | wall 0
2021-01-09 11:54:53 | INFO | fairseq.trainer | begin training epoch 117
2021-01-09 11:54:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 11:54:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 11:55:19 | INFO | train_inner | epoch 117:     44 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.022, ppl=2.03, wps=8157, ups=1.16, wpb=7014.1, bsz=237.5, num_updates=94800, lr=7.11568e-06, gnorm=0.775, train_wall=52, wall=0
2021-01-09 11:56:11 | INFO | train_inner | epoch 117:    144 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.027, ppl=2.04, wps=13069.7, ups=1.89, wpb=6910.5, bsz=238.7, num_updates=94900, lr=7.11193e-06, gnorm=0.776, train_wall=53, wall=0
2021-01-09 11:57:05 | INFO | train_inner | epoch 117:    244 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.365, nll_loss=0.999, ppl=2, wps=13266.2, ups=1.88, wpb=7062.7, bsz=250.4, num_updates=95000, lr=7.10819e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 11:57:58 | INFO | train_inner | epoch 117:    344 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.007, ppl=2.01, wps=13282.5, ups=1.88, wpb=7048.3, bsz=251, num_updates=95100, lr=7.10445e-06, gnorm=0.754, train_wall=53, wall=0
2021-01-09 11:58:50 | INFO | train_inner | epoch 117:    444 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.012, ppl=2.02, wps=13349.6, ups=1.9, wpb=7042.4, bsz=250.5, num_updates=95200, lr=7.10072e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 11:59:43 | INFO | train_inner | epoch 117:    544 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.004, ppl=2.01, wps=13085.8, ups=1.89, wpb=6929.6, bsz=248.6, num_updates=95300, lr=7.09699e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 12:00:36 | INFO | train_inner | epoch 117:    644 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.011, ppl=2.02, wps=13031.1, ups=1.89, wpb=6876.7, bsz=241.8, num_updates=95400, lr=7.09327e-06, gnorm=0.778, train_wall=53, wall=0
2021-01-09 12:01:30 | INFO | train_inner | epoch 117:    744 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.014, ppl=2.02, wps=13129.7, ups=1.87, wpb=7037.9, bsz=245.8, num_updates=95500, lr=7.08955e-06, gnorm=0.766, train_wall=53, wall=0
2021-01-09 12:02:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 12:02:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:02:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:02:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:02:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:02:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:02:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:02:49 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.44 | nll_loss 4.02 | ppl 16.22 | bleu 21.92 | wps 3364.1 | wpb 5162.1 | bsz 187.5 | num_updates 95597 | best_bleu 22.31
2021-01-09 12:02:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 12:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:02:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 117 @ 95597 updates, score 21.92) (writing took 2.769191324710846 seconds)
2021-01-09 12:02:52 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-09 12:02:52 | INFO | train | epoch 117 | symm_kl 0.475 | self_kl 0 | self_cv 0 | loss 3.381 | nll_loss 1.012 | ppl 2.02 | wps 12271.9 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 95597 | lr 7.08596e-06 | gnorm 0.769 | train_wall 444 | wall 0
2021-01-09 12:02:52 | INFO | fairseq.trainer | begin training epoch 118
2021-01-09 12:02:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:02:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:02:57 | INFO | train_inner | epoch 118:      3 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.018, ppl=2.03, wps=8051.1, ups=1.15, wpb=7007.1, bsz=250.7, num_updates=95600, lr=7.08585e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 12:03:49 | INFO | train_inner | epoch 118:    103 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.011, ppl=2.02, wps=13512.5, ups=1.92, wpb=7044.8, bsz=250.6, num_updates=95700, lr=7.08214e-06, gnorm=0.759, train_wall=52, wall=0
2021-01-09 12:04:42 | INFO | train_inner | epoch 118:    203 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.011, ppl=2.02, wps=13214.1, ups=1.88, wpb=7025.5, bsz=233.3, num_updates=95800, lr=7.07845e-06, gnorm=0.771, train_wall=53, wall=0
2021-01-09 12:05:35 | INFO | train_inner | epoch 118:    303 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.01, ppl=2.01, wps=13244.2, ups=1.91, wpb=6945.8, bsz=246.5, num_updates=95900, lr=7.07475e-06, gnorm=0.767, train_wall=52, wall=0
2021-01-09 12:06:28 | INFO | train_inner | epoch 118:    403 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.01, ppl=2.01, wps=13319.4, ups=1.89, wpb=7055.3, bsz=239, num_updates=96000, lr=7.07107e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 12:07:20 | INFO | train_inner | epoch 118:    503 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.009, ppl=2.01, wps=13092.1, ups=1.91, wpb=6854.6, bsz=240.6, num_updates=96100, lr=7.06739e-06, gnorm=0.777, train_wall=52, wall=0
2021-01-09 12:08:13 | INFO | train_inner | epoch 118:    603 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.022, ppl=2.03, wps=13193.1, ups=1.89, wpb=6971.8, bsz=259.3, num_updates=96200, lr=7.06371e-06, gnorm=0.768, train_wall=53, wall=0
2021-01-09 12:09:06 | INFO | train_inner | epoch 118:    703 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.013, ppl=2.02, wps=13140.1, ups=1.88, wpb=6985.2, bsz=247.2, num_updates=96300, lr=7.06005e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 12:09:59 | INFO | train_inner | epoch 118:    803 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.018, ppl=2.03, wps=13396.2, ups=1.9, wpb=7060.9, bsz=256.6, num_updates=96400, lr=7.05638e-06, gnorm=0.762, train_wall=53, wall=0
2021-01-09 12:10:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 12:10:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:10:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:10:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:10:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:10:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:10:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:10:47 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.438 | nll_loss 4.02 | ppl 16.22 | bleu 21.85 | wps 3333 | wpb 5162.1 | bsz 187.5 | num_updates 96438 | best_bleu 22.31
2021-01-09 12:10:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 12:10:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:10:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:10:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 118 @ 96438 updates, score 21.85) (writing took 2.7650871947407722 seconds)
2021-01-09 12:10:50 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-09 12:10:50 | INFO | train | epoch 118 | symm_kl 0.475 | self_kl 0 | self_cv 0 | loss 3.38 | nll_loss 1.012 | ppl 2.02 | wps 12313.8 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 96438 | lr 7.05499e-06 | gnorm 0.767 | train_wall 442 | wall 0
2021-01-09 12:10:50 | INFO | fairseq.trainer | begin training epoch 119
2021-01-09 12:10:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:10:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:11:25 | INFO | train_inner | epoch 119:     62 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.008, ppl=2.01, wps=7999.4, ups=1.16, wpb=6893.2, bsz=244.9, num_updates=96500, lr=7.05273e-06, gnorm=0.771, train_wall=52, wall=0
2021-01-09 12:12:17 | INFO | train_inner | epoch 119:    162 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.999, ppl=2, wps=13376.7, ups=1.9, wpb=7040.1, bsz=248.4, num_updates=96600, lr=7.04907e-06, gnorm=0.757, train_wall=52, wall=0
2021-01-09 12:13:10 | INFO | train_inner | epoch 119:    262 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.005, ppl=2.01, wps=13251.6, ups=1.89, wpb=7005.2, bsz=241, num_updates=96700, lr=7.04543e-06, gnorm=0.761, train_wall=53, wall=0
2021-01-09 12:14:03 | INFO | train_inner | epoch 119:    362 / 841 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.016, ppl=2.02, wps=13229, ups=1.9, wpb=6953.4, bsz=232, num_updates=96800, lr=7.04179e-06, gnorm=0.778, train_wall=52, wall=0
2021-01-09 12:14:56 | INFO | train_inner | epoch 119:    462 / 841 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.011, ppl=2.01, wps=13366.4, ups=1.9, wpb=7047.3, bsz=239.3, num_updates=96900, lr=7.03815e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 12:15:49 | INFO | train_inner | epoch 119:    562 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.029, ppl=2.04, wps=13278.7, ups=1.89, wpb=7030.6, bsz=248.6, num_updates=97000, lr=7.03452e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 12:16:42 | INFO | train_inner | epoch 119:    662 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.02, ppl=2.03, wps=13030.4, ups=1.89, wpb=6908.9, bsz=257.2, num_updates=97100, lr=7.0309e-06, gnorm=0.778, train_wall=53, wall=0
2021-01-09 12:17:35 | INFO | train_inner | epoch 119:    762 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.36, nll_loss=0.995, ppl=1.99, wps=13168.1, ups=1.89, wpb=6984.8, bsz=256.2, num_updates=97200, lr=7.02728e-06, gnorm=0.76, train_wall=53, wall=0
2021-01-09 12:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 12:18:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:18:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:18:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:18:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:18:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:18:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:18:44 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.439 | nll_loss 4.019 | ppl 16.21 | bleu 21.9 | wps 3371.7 | wpb 5162.1 | bsz 187.5 | num_updates 97279 | best_bleu 22.31
2021-01-09 12:18:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 12:18:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:18:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:18:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 119 @ 97279 updates, score 21.9) (writing took 2.7909535858780146 seconds)
2021-01-09 12:18:47 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-09 12:18:47 | INFO | train | epoch 119 | symm_kl 0.475 | self_kl 0 | self_cv 0 | loss 3.379 | nll_loss 1.011 | ppl 2.02 | wps 12314.7 | ups 1.76 | wpb 6993.1 | bsz 246.6 | num_updates 97279 | lr 7.02443e-06 | gnorm 0.767 | train_wall 442 | wall 0
2021-01-09 12:18:47 | INFO | fairseq.trainer | begin training epoch 120
2021-01-09 12:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:19:01 | INFO | train_inner | epoch 120:     21 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.005, ppl=2.01, wps=8161.9, ups=1.15, wpb=7066.9, bsz=254.7, num_updates=97300, lr=7.02367e-06, gnorm=0.757, train_wall=53, wall=0
2021-01-09 12:19:54 | INFO | train_inner | epoch 120:    121 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.008, ppl=2.01, wps=13250.6, ups=1.89, wpb=7000, bsz=248.4, num_updates=97400, lr=7.02007e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 12:20:47 | INFO | train_inner | epoch 120:    221 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.007, ppl=2.01, wps=13292.1, ups=1.88, wpb=7054.7, bsz=248.5, num_updates=97500, lr=7.01646e-06, gnorm=0.763, train_wall=53, wall=0
2021-01-09 12:21:40 | INFO | train_inner | epoch 120:    321 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.007, ppl=2.01, wps=13148, ups=1.88, wpb=6984.7, bsz=235.7, num_updates=97600, lr=7.01287e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 12:22:33 | INFO | train_inner | epoch 120:    421 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.01, ppl=2.01, wps=13062, ups=1.88, wpb=6937.6, bsz=242.5, num_updates=97700, lr=7.00928e-06, gnorm=0.774, train_wall=53, wall=0
2021-01-09 12:23:26 | INFO | train_inner | epoch 120:    521 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.005, ppl=2.01, wps=13081.7, ups=1.88, wpb=6944.6, bsz=235, num_updates=97800, lr=7.00569e-06, gnorm=0.773, train_wall=53, wall=0
2021-01-09 12:24:19 | INFO | train_inner | epoch 120:    621 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.009, ppl=2.01, wps=13324, ups=1.9, wpb=6997, bsz=243.6, num_updates=97900, lr=7.00212e-06, gnorm=0.769, train_wall=52, wall=0
2021-01-09 12:25:12 | INFO | train_inner | epoch 120:    721 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.019, ppl=2.03, wps=13150.4, ups=1.88, wpb=6983.9, bsz=254.6, num_updates=98000, lr=6.99854e-06, gnorm=0.769, train_wall=53, wall=0
2021-01-09 12:26:05 | INFO | train_inner | epoch 120:    821 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.031, ppl=2.04, wps=13243.4, ups=1.87, wpb=7066.9, bsz=254.5, num_updates=98100, lr=6.99497e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 12:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 12:26:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:26:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:26:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:26:44 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.439 | nll_loss 4.02 | ppl 16.22 | bleu 22.08 | wps 3365.1 | wpb 5162.1 | bsz 187.5 | num_updates 98120 | best_bleu 22.31
2021-01-09 12:26:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 12:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:26:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:26:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 120 @ 98120 updates, score 22.08) (writing took 2.836411565542221 seconds)
2021-01-09 12:26:47 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-09 12:26:47 | INFO | train | epoch 120 | symm_kl 0.474 | self_kl 0 | self_cv 0 | loss 3.379 | nll_loss 1.011 | ppl 2.02 | wps 12267.3 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 98120 | lr 6.99426e-06 | gnorm 0.769 | train_wall 444 | wall 0
2021-01-09 12:26:47 | INFO | fairseq.trainer | begin training epoch 121
2021-01-09 12:26:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:26:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:27:31 | INFO | train_inner | epoch 121:     80 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.008, ppl=2.01, wps=8049.3, ups=1.16, wpb=6917.8, bsz=255.4, num_updates=98200, lr=6.99141e-06, gnorm=0.776, train_wall=52, wall=0
2021-01-09 12:28:25 | INFO | train_inner | epoch 121:    180 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.364, nll_loss=0.998, ppl=2, wps=13127.5, ups=1.87, wpb=7002.3, bsz=240.1, num_updates=98300, lr=6.98785e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 12:29:18 | INFO | train_inner | epoch 121:    280 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.021, ppl=2.03, wps=13110.1, ups=1.89, wpb=6949.6, bsz=246.1, num_updates=98400, lr=6.9843e-06, gnorm=0.775, train_wall=53, wall=0
2021-01-09 12:30:11 | INFO | train_inner | epoch 121:    380 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.369, nll_loss=1, ppl=2, wps=13074.8, ups=1.88, wpb=6956.4, bsz=250.7, num_updates=98500, lr=6.98076e-06, gnorm=0.77, train_wall=53, wall=0
2021-01-09 12:31:04 | INFO | train_inner | epoch 121:    480 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.014, ppl=2.02, wps=13249.6, ups=1.89, wpb=7009.1, bsz=256.6, num_updates=98600, lr=6.97722e-06, gnorm=0.764, train_wall=53, wall=0
2021-01-09 12:31:58 | INFO | train_inner | epoch 121:    580 / 841 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.002, ppl=2, wps=13351.1, ups=1.86, wpb=7172.8, bsz=247.4, num_updates=98700, lr=6.97368e-06, gnorm=0.747, train_wall=54, wall=0
2021-01-09 12:32:51 | INFO | train_inner | epoch 121:    680 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.026, ppl=2.04, wps=13044.3, ups=1.88, wpb=6925.4, bsz=257.7, num_updates=98800, lr=6.97015e-06, gnorm=0.783, train_wall=53, wall=0
2021-01-09 12:33:44 | INFO | train_inner | epoch 121:    780 / 841 symm_kl=0.476, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.021, ppl=2.03, wps=13026.1, ups=1.87, wpb=6948.6, bsz=227, num_updates=98900, lr=6.96663e-06, gnorm=0.787, train_wall=53, wall=0
2021-01-09 12:34:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 12:34:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:34:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:34:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:34:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 12:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 12:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 12:34:45 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.443 | nll_loss 4.024 | ppl 16.27 | bleu 21.82 | wps 3278.6 | wpb 5162.1 | bsz 187.5 | num_updates 98961 | best_bleu 22.31
2021-01-09 12:34:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 12:34:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:34:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std/checkpoint_last.pt (epoch 121 @ 98961 updates, score 21.82) (writing took 2.7063627429306507 seconds)
2021-01-09 12:34:48 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-09 12:34:48 | INFO | train | epoch 121 | symm_kl 0.474 | self_kl 0 | self_cv 0 | loss 3.378 | nll_loss 1.011 | ppl 2.02 | wps 12222.8 | ups 1.75 | wpb 6993.1 | bsz 246.6 | num_updates 98961 | lr 6.96448e-06 | gnorm 0.77 | train_wall 445 | wall 0
2021-01-09 12:34:48 | INFO | fairseq.trainer | begin training epoch 122
2021-01-09 12:34:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 12:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 12:35:11 | INFO | train_inner | epoch 122:     39 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.367, nll_loss=1, ppl=2, wps=8111, ups=1.15, wpb=7072.6, bsz=240.3, num_updates=99000, lr=6.96311e-06, gnorm=0.753, train_wall=53, wall=0
2021-01-09 12:36:03 | INFO | train_inner | epoch 122:    139 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.019, ppl=2.03, wps=13266.1, ups=1.91, wpb=6931.8, bsz=245.7, num_updates=99100, lr=6.95959e-06, gnorm=0.772, train_wall=52, wall=0
2021-01-09 12:36:57 | INFO | train_inner | epoch 122:    239 / 841 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.008, ppl=2.01, wps=13211.3, ups=1.87, wpb=7058.9, bsz=245, num_updates=99200, lr=6.95608e-06, gnorm=0.765, train_wall=53, wall=0
2021-01-09 12:37:50 | INFO | train_inner | epoch 122:    339 / 841 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.017, ppl=2.02, wps=13288.2, ups=1.88, wpb=7077.6, bsz=249.8, num_updates=99300, lr=6.95258e-06, gnorm=0.767, train_wall=53, wall=0
2021-01-09 12:38:43 | INFO | train_inner | epoch 122:    439 / 841 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.02, ppl=2.03, wps=13148.8, ups=1.89, wpb=6951.1, bsz=248.6, num_updates=99400, lr=6.94908e-06, gnorm=0.768, train_wall=53, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 468 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
