nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/de_std1
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --noised-no-grad '
2021-01-09 15:58:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:58:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:58:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:58:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:58:52 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15537
2021-01-09 15:58:52 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15537
2021-01-09 15:58:52 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15537
2021-01-09 15:58:53 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-09 15:58:53 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-09 15:58:53 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-09 15:58:56 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=False, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15537', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/de_std1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-09 15:58:56 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-09 15:58:56 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-09 15:58:56 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-09 15:58:56 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-09 15:58:56 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-09 15:58:57 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-09 15:58:57 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-09 15:58:57 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-09 15:58:57 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-09 15:58:57 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-09 15:58:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-09 15:58:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-09 15:58:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 15:58:58 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 15:58:58 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 15:58:58 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 15:58:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 15:58:58 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-09 15:58:58 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None
2021-01-09 15:58:58 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-09 15:58:58 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-09 15:58:58 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-09 15:58:58 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-09 15:58:58 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-09 15:58:58 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-09 15:58:58 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-09 15:58:58 | INFO | fairseq.trainer | begin training epoch 1
2021-01-09 15:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 15:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 15:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:02:57 | INFO | train_inner | epoch 001:    100 / 448 symm_kl=0.283, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.85, ppl=1.8, wps=5590.4, ups=0.43, wpb=13082.5, bsz=459.4, num_updates=100, lr=1.43e-06, gnorm=0.81, train_wall=234, wall=239
2021-01-09 16:06:58 | INFO | train_inner | epoch 001:    200 / 448 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.093, nll_loss=0.878, ppl=1.84, wps=5453.1, ups=0.41, wpb=13158.6, bsz=457, num_updates=200, lr=2.76e-06, gnorm=0.727, train_wall=241, wall=480
2021-01-09 16:11:25 | INFO | train_inner | epoch 001:    300 / 448 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.896, ppl=1.86, wps=4888, ups=0.37, wpb=13074.7, bsz=475, num_updates=300, lr=4.09e-06, gnorm=0.684, train_wall=267, wall=748
2021-01-09 16:15:19 | INFO | train_inner | epoch 001:    400 / 448 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.9, ppl=1.87, wps=5670.5, ups=0.43, wpb=13273.6, bsz=478.9, num_updates=400, lr=5.42e-06, gnorm=0.66, train_wall=234, wall=982
2021-01-09 16:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 16:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:17:32 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.728 | nll_loss 4.192 | ppl 18.27 | bleu 21.79 | wps 4886.6 | wpb 9177.1 | bsz 333.3 | num_updates 448
2021-01-09 16:17:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 16:17:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_best.pt (epoch 1 @ 448 updates, score 21.79) (writing took 2.069988301023841 seconds)
2021-01-09 16:17:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-09 16:17:34 | INFO | train | epoch 001 | symm_kl 0.303 | self_kl 0 | self_cv 0 | loss 3.097 | nll_loss 0.884 | ppl 1.85 | wps 5290.2 | ups 0.4 | wpb 13127.6 | bsz 462.9 | num_updates 448 | lr 6.0584e-06 | gnorm 0.716 | train_wall 1089 | wall 1117
2021-01-09 16:17:34 | INFO | fairseq.trainer | begin training epoch 2
2021-01-09 16:17:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:17:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:17:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:17:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:19:39 | INFO | train_inner | epoch 002:     52 / 448 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.908, ppl=1.88, wps=5027.8, ups=0.38, wpb=13064, bsz=443, num_updates=500, lr=6.75e-06, gnorm=0.677, train_wall=234, wall=1242
2021-01-09 16:23:34 | INFO | train_inner | epoch 002:    152 / 448 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.111, nll_loss=0.895, ppl=1.86, wps=5588.6, ups=0.43, wpb=13120.8, bsz=473, num_updates=600, lr=8.08e-06, gnorm=0.66, train_wall=235, wall=1477
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 42 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/de_std1
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --noised-no-grad '
2021-01-09 16:27:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:27:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:27:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19199
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:19199
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19199
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-09 16:27:52 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-09 16:27:56 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=False, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19199', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/de_std1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-09 16:27:56 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-09 16:27:56 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-09 16:27:56 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-09 16:27:56 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-09 16:27:56 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-09 16:27:57 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-09 16:27:57 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-09 16:27:57 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-09 16:27:57 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-09 16:27:57 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-09 16:27:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-09 16:27:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-09 16:27:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 16:27:57 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:27:57 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:27:57 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:27:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 16:27:57 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-09 16:27:57 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-09 16:27:58 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-09 16:27:58 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 2 @ 448 updates)
2021-01-09 16:27:58 | INFO | fairseq.trainer | loading train data for epoch 2
2021-01-09 16:27:58 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-09 16:27:58 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-09 16:27:58 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-09 16:27:59 | INFO | fairseq.trainer | begin training epoch 2
2021-01-09 16:27:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:27:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:28:25 | INFO | train_inner | epoch 002:     52 / 561 symm_kl=0.629, self_kl=0, self_cv=0, loss=3.564, nll_loss=0.894, ppl=1.86, wps=7322.9, ups=0.63, wpb=11678.8, bsz=408, num_updates=500, lr=6.75e-06, gnorm=1.283, train_wall=136, wall=0
2021-01-09 16:29:09 | INFO | train_inner | epoch 002:    152 / 561 symm_kl=0.877, self_kl=0, self_cv=0, loss=3.932, nll_loss=0.869, ppl=1.83, wps=23818.3, ups=2.27, wpb=10497.2, bsz=365.9, num_updates=600, lr=8.08e-06, gnorm=1.66, train_wall=44, wall=0
2021-01-09 16:29:54 | INFO | train_inner | epoch 002:    252 / 561 symm_kl=0.822, self_kl=0, self_cv=0, loss=3.843, nll_loss=0.851, ppl=1.8, wps=23416.3, ups=2.23, wpb=10496.7, bsz=368.4, num_updates=700, lr=9.41e-06, gnorm=1.534, train_wall=45, wall=0
2021-01-09 16:30:39 | INFO | train_inner | epoch 002:    352 / 561 symm_kl=0.804, self_kl=0, self_cv=0, loss=3.819, nll_loss=0.854, ppl=1.81, wps=23379.4, ups=2.21, wpb=10556, bsz=368.1, num_updates=800, lr=1.074e-05, gnorm=1.511, train_wall=45, wall=0
2021-01-09 16:31:24 | INFO | train_inner | epoch 002:    452 / 561 symm_kl=0.795, self_kl=0, self_cv=0, loss=3.819, nll_loss=0.867, ppl=1.82, wps=23176.9, ups=2.22, wpb=10439.5, bsz=369.8, num_updates=900, lr=1.207e-05, gnorm=1.489, train_wall=45, wall=0
2021-01-09 16:32:10 | INFO | train_inner | epoch 002:    552 / 561 symm_kl=0.788, self_kl=0, self_cv=0, loss=3.811, nll_loss=0.874, ppl=1.83, wps=22954.9, ups=2.19, wpb=10499.6, bsz=371.1, num_updates=1000, lr=1.34e-05, gnorm=1.47, train_wall=46, wall=0
2021-01-09 16:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 16:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:32:35 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.656 | nll_loss 4.087 | ppl 16.99 | bleu 22.19 | wps 4576.8 | wpb 7508.5 | bsz 272.7 | num_updates 1009 | best_bleu 22.19
2021-01-09 16:32:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 16:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:32:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_best.pt (epoch 2 @ 1009 updates, score 22.19) (writing took 4.6786088570952415 seconds)
2021-01-09 16:32:40 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-09 16:32:40 | INFO | train | epoch 002 | symm_kl 0.568 | self_kl 0 | self_cv 0 | loss 3.482 | nll_loss 0.875 | ppl 1.83 | wps 8459.2 | ups 0.73 | wpb 11657.4 | bsz 411 | num_updates 1009 | lr 1.35197e-05 | gnorm 1.187 | train_wall 1340 | wall 0
2021-01-09 16:32:40 | INFO | fairseq.trainer | begin training epoch 3
2021-01-09 16:32:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:32:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:33:24 | INFO | train_inner | epoch 003:     91 / 561 symm_kl=0.766, self_kl=0, self_cv=0, loss=3.772, nll_loss=0.867, ppl=1.82, wps=14129.1, ups=1.36, wpb=10411.6, bsz=366.7, num_updates=1100, lr=1.473e-05, gnorm=1.444, train_wall=44, wall=0
2021-01-09 16:34:09 | INFO | train_inner | epoch 003:    191 / 561 symm_kl=0.759, self_kl=0, self_cv=0, loss=3.762, nll_loss=0.865, ppl=1.82, wps=23000.5, ups=2.2, wpb=10448.5, bsz=375.5, num_updates=1200, lr=1.606e-05, gnorm=1.423, train_wall=45, wall=0
2021-01-09 16:34:54 | INFO | train_inner | epoch 003:    291 / 561 symm_kl=0.755, self_kl=0, self_cv=0, loss=3.762, nll_loss=0.874, ppl=1.83, wps=22906.5, ups=2.2, wpb=10394.3, bsz=361.8, num_updates=1300, lr=1.739e-05, gnorm=1.431, train_wall=45, wall=0
2021-01-09 16:35:40 | INFO | train_inner | epoch 003:    391 / 561 symm_kl=0.744, self_kl=0, self_cv=0, loss=3.754, nll_loss=0.882, ppl=1.84, wps=23320.3, ups=2.22, wpb=10519.7, bsz=380.1, num_updates=1400, lr=1.872e-05, gnorm=1.375, train_wall=45, wall=0
2021-01-09 16:36:25 | INFO | train_inner | epoch 003:    491 / 561 symm_kl=0.728, self_kl=0, self_cv=0, loss=3.72, nll_loss=0.87, ppl=1.83, wps=23205.2, ups=2.18, wpb=10647, bsz=368.2, num_updates=1500, lr=2.005e-05, gnorm=1.365, train_wall=46, wall=0
2021-01-09 16:36:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 16:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:36:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 16:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 16:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 16:37:18 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.61 | nll_loss 4.041 | ppl 16.46 | bleu 22.43 | wps 4561.2 | wpb 7508.5 | bsz 272.7 | num_updates 1570 | best_bleu 22.43
2021-01-09 16:37:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 16:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:37:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_best.pt (epoch 3 @ 1570 updates, score 22.43) (writing took 4.827533723786473 seconds)
2021-01-09 16:37:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-09 16:37:23 | INFO | train | epoch 003 | symm_kl 0.747 | self_kl 0 | self_cv 0 | loss 3.748 | nll_loss 0.871 | ppl 1.83 | wps 20770.9 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 1570 | lr 2.0981e-05 | gnorm 1.402 | train_wall 253 | wall 0
2021-01-09 16:37:23 | INFO | fairseq.trainer | begin training epoch 4
2021-01-09 16:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:37:40 | INFO | train_inner | epoch 004:     30 / 561 symm_kl=0.728, self_kl=0, self_cv=0, loss=3.717, nll_loss=0.867, ppl=1.82, wps=14064.5, ups=1.35, wpb=10438.1, bsz=352.5, num_updates=1600, lr=2.138e-05, gnorm=1.39, train_wall=45, wall=0
2021-01-09 16:38:24 | INFO | train_inner | epoch 004:    130 / 561 symm_kl=0.722, self_kl=0, self_cv=0, loss=3.712, nll_loss=0.871, ppl=1.83, wps=23247.1, ups=2.23, wpb=10404.4, bsz=367.7, num_updates=1700, lr=2.271e-05, gnorm=1.376, train_wall=45, wall=0
2021-01-09 16:39:10 | INFO | train_inner | epoch 004:    230 / 561 symm_kl=0.725, self_kl=0, self_cv=0, loss=3.731, nll_loss=0.886, ppl=1.85, wps=22672.5, ups=2.19, wpb=10342.5, bsz=367.9, num_updates=1800, lr=2.404e-05, gnorm=1.365, train_wall=45, wall=0
2021-01-09 16:39:56 | INFO | train_inner | epoch 004:    330 / 561 symm_kl=0.71, self_kl=0, self_cv=0, loss=3.695, nll_loss=0.872, ppl=1.83, wps=23186.5, ups=2.2, wpb=10555.4, bsz=352.7, num_updates=1900, lr=2.537e-05, gnorm=1.341, train_wall=45, wall=0
2021-01-09 16:40:41 | INFO | train_inner | epoch 004:    430 / 561 symm_kl=0.7, self_kl=0, self_cv=0, loss=3.685, nll_loss=0.879, ppl=1.84, wps=23120.9, ups=2.19, wpb=10561.8, bsz=374.2, num_updates=2000, lr=2.67e-05, gnorm=1.311, train_wall=45, wall=0
2021-01-09 16:41:27 | INFO | train_inner | epoch 004:    530 / 561 symm_kl=0.692, self_kl=0, self_cv=0, loss=3.675, nll_loss=0.88, ppl=1.84, wps=23243.8, ups=2.19, wpb=10614.1, bsz=389.2, num_updates=2100, lr=2.803e-05, gnorm=1.3, train_wall=45, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 15 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/de_std1
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --noised-no-grad '
2021-01-09 16:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:11 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19358
2021-01-09 16:50:11 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19358
2021-01-09 16:50:11 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:19358
2021-01-09 16:50:11 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-09 16:50:11 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-09 16:50:12 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-09 16:50:15 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=False, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19358', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/de_std1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-09 16:50:15 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-09 16:50:15 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-09 16:50:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-09 16:50:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-09 16:50:15 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-09 16:50:16 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-09 16:50:16 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-09 16:50:16 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-09 16:50:16 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-09 16:50:16 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-09 16:50:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-09 16:50:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-09 16:50:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 16:50:16 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:50:16 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:50:16 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-09 16:50:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-09 16:50:16 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-09 16:50:16 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-09 16:50:17 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-09 16:50:18 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 4 @ 1570 updates)
2021-01-09 16:50:18 | INFO | fairseq.trainer | loading train data for epoch 4
2021-01-09 16:50:18 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-09 16:50:18 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-09 16:50:18 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-09 16:50:18 | INFO | fairseq.trainer | begin training epoch 4
2021-01-09 16:50:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 16:50:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:50:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 16:51:27 | INFO | train_inner | epoch 004:     30 / 561 symm_kl=0.59, self_kl=0, self_cv=0, loss=3.517, nll_loss=0.874, ppl=1.83, wps=8522.6, ups=0.82, wpb=10438.1, bsz=352.5, num_updates=1600, lr=2.138e-05, gnorm=1.21, train_wall=97, wall=0
2021-01-09 16:54:36 | INFO | train_inner | epoch 004:    130 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.914, ppl=1.88, wps=5518, ups=0.53, wpb=10404.4, bsz=367.7, num_updates=1700, lr=2.271e-05, gnorm=0.734, train_wall=188, wall=0
2021-01-09 16:57:52 | INFO | train_inner | epoch 004:    230 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.915, ppl=1.89, wps=5264.3, ups=0.51, wpb=10342.5, bsz=367.9, num_updates=1800, lr=2.404e-05, gnorm=0.753, train_wall=196, wall=0
2021-01-09 17:01:14 | INFO | train_inner | epoch 004:    330 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.089, nll_loss=0.891, ppl=1.85, wps=5238.5, ups=0.5, wpb=10555.4, bsz=352.7, num_updates=1900, lr=2.537e-05, gnorm=0.701, train_wall=201, wall=0
2021-01-09 17:04:29 | INFO | train_inner | epoch 004:    430 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.09, nll_loss=0.893, ppl=1.86, wps=5412.4, ups=0.51, wpb=10561.8, bsz=374.2, num_updates=2000, lr=2.67e-05, gnorm=0.702, train_wall=195, wall=0
2021-01-09 17:07:55 | INFO | train_inner | epoch 004:    530 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.089, nll_loss=0.893, ppl=1.86, wps=5151.2, ups=0.49, wpb=10614.1, bsz=389.2, num_updates=2100, lr=2.803e-05, gnorm=0.702, train_wall=206, wall=0
2021-01-09 17:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 17:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:09:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:09:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:09:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:09:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:09:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:09:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:09:13 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.667 | nll_loss 4.131 | ppl 17.52 | bleu 22.02 | wps 4683.3 | wpb 7508.5 | bsz 272.7 | num_updates 2131 | best_bleu 22.43
2021-01-09 17:09:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 17:09:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:09:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:09:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:09:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:09:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 4 @ 2131 updates, score 22.02) (writing took 2.860901355743408 seconds)
2021-01-09 17:09:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-09 17:09:16 | INFO | train | epoch 004 | symm_kl 0.528 | self_kl 0 | self_cv 0 | loss 3.422 | nll_loss 0.885 | ppl 1.85 | wps 8303.1 | ups 0.79 | wpb 10483.4 | bsz 369.6 | num_updates 2131 | lr 2.84423e-05 | gnorm 1.062 | train_wall 1362 | wall 0
2021-01-09 17:09:16 | INFO | fairseq.trainer | begin training epoch 5
2021-01-09 17:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:11:28 | INFO | train_inner | epoch 005:     69 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.082, nll_loss=0.882, ppl=1.84, wps=4915.5, ups=0.47, wpb=10488.4, bsz=372.2, num_updates=2200, lr=2.936e-05, gnorm=0.708, train_wall=186, wall=0
2021-01-09 17:15:30 | INFO | train_inner | epoch 005:    169 / 561 symm_kl=0.322, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.889, ppl=1.85, wps=4279.8, ups=0.41, wpb=10340.9, bsz=370.2, num_updates=2300, lr=3.069e-05, gnorm=0.72, train_wall=241, wall=0
2021-01-09 17:19:35 | INFO | train_inner | epoch 005:    269 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.089, nll_loss=0.889, ppl=1.85, wps=4320.5, ups=0.41, wpb=10577.5, bsz=360.5, num_updates=2400, lr=3.202e-05, gnorm=0.714, train_wall=245, wall=0
2021-01-09 17:22:39 | INFO | train_inner | epoch 005:    369 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.89, ppl=1.85, wps=5697.7, ups=0.54, wpb=10468.2, bsz=359.1, num_updates=2500, lr=3.335e-05, gnorm=0.724, train_wall=184, wall=0
2021-01-09 17:25:41 | INFO | train_inner | epoch 005:    469 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.092, nll_loss=0.882, ppl=1.84, wps=5729.8, ups=0.55, wpb=10483.5, bsz=387, num_updates=2600, lr=3.468e-05, gnorm=0.714, train_wall=183, wall=0
2021-01-09 17:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 17:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:28:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:28:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:28:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:28:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:28:49 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.681 | nll_loss 4.146 | ppl 17.7 | bleu 21.96 | wps 4723.7 | wpb 7508.5 | bsz 272.7 | num_updates 2692 | best_bleu 22.43
2021-01-09 17:28:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 17:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:28:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 5 @ 2692 updates, score 21.96) (writing took 2.8246863950043917 seconds)
2021-01-09 17:28:52 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-09 17:28:52 | INFO | train | epoch 005 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.092 | nll_loss 0.887 | ppl 1.85 | wps 5000.1 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 2692 | lr 3.59036e-05 | gnorm 0.717 | train_wall 1148 | wall 0
2021-01-09 17:28:52 | INFO | fairseq.trainer | begin training epoch 6
2021-01-09 17:28:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:28:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:29:10 | INFO | train_inner | epoch 006:      8 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.085, nll_loss=0.886, ppl=1.85, wps=5013, ups=0.48, wpb=10435.7, bsz=371, num_updates=2700, lr=3.601e-05, gnorm=0.725, train_wall=181, wall=0
2021-01-09 17:32:13 | INFO | train_inner | epoch 006:    108 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.878, ppl=1.84, wps=5693, ups=0.55, wpb=10411.7, bsz=385.2, num_updates=2800, lr=3.734e-05, gnorm=0.721, train_wall=183, wall=0
2021-01-09 17:35:16 | INFO | train_inner | epoch 006:    208 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.879, ppl=1.84, wps=5806.6, ups=0.54, wpb=10661.5, bsz=363.4, num_updates=2900, lr=3.867e-05, gnorm=0.712, train_wall=183, wall=0
2021-01-09 17:38:18 | INFO | train_inner | epoch 006:    308 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.091, nll_loss=0.884, ppl=1.85, wps=5755.2, ups=0.55, wpb=10481.9, bsz=376.5, num_updates=3000, lr=4e-05, gnorm=0.723, train_wall=182, wall=0
2021-01-09 17:41:19 | INFO | train_inner | epoch 006:    408 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.095, nll_loss=0.894, ppl=1.86, wps=5792.2, ups=0.55, wpb=10494, bsz=353, num_updates=3100, lr=3.93496e-05, gnorm=0.731, train_wall=181, wall=0
2021-01-09 17:44:22 | INFO | train_inner | epoch 006:    508 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.877, ppl=1.84, wps=5771.6, ups=0.55, wpb=10508.9, bsz=370.7, num_updates=3200, lr=3.87298e-05, gnorm=0.717, train_wall=182, wall=0
2021-01-09 17:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 17:45:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:45:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:45:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:46:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:46:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:46:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 17:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 17:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 17:46:19 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.682 | nll_loss 4.149 | ppl 17.74 | bleu 21.9 | wps 4552.3 | wpb 7508.5 | bsz 272.7 | num_updates 3253 | best_bleu 22.43
2021-01-09 17:46:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 17:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:46:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 6 @ 3253 updates, score 21.9) (writing took 2.900937292724848 seconds)
2021-01-09 17:46:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-09 17:46:22 | INFO | train | epoch 006 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.09 | nll_loss 0.884 | ppl 1.84 | wps 5600.8 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 3253 | lr 3.8413e-05 | gnorm 0.723 | train_wall 1021 | wall 0
2021-01-09 17:46:22 | INFO | fairseq.trainer | begin training epoch 7
2021-01-09 17:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 17:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 17:47:52 | INFO | train_inner | epoch 007:     47 / 561 symm_kl=0.327, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.888, ppl=1.85, wps=4883.9, ups=0.48, wpb=10257.8, bsz=370.6, num_updates=3300, lr=3.81385e-05, gnorm=0.732, train_wall=182, wall=0
2021-01-09 17:50:54 | INFO | train_inner | epoch 007:    147 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.085, nll_loss=0.879, ppl=1.84, wps=5764, ups=0.55, wpb=10541.1, bsz=365.2, num_updates=3400, lr=3.75735e-05, gnorm=0.725, train_wall=183, wall=0
2021-01-09 17:53:57 | INFO | train_inner | epoch 007:    247 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.88, ppl=1.84, wps=5700.4, ups=0.55, wpb=10378.2, bsz=371.8, num_updates=3500, lr=3.70328e-05, gnorm=0.727, train_wall=182, wall=0
2021-01-09 17:56:59 | INFO | train_inner | epoch 007:    347 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.878, ppl=1.84, wps=5759, ups=0.55, wpb=10488.5, bsz=374.2, num_updates=3600, lr=3.65148e-05, gnorm=0.721, train_wall=182, wall=0
2021-01-09 18:00:01 | INFO | train_inner | epoch 007:    447 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.884, ppl=1.85, wps=5746.5, ups=0.55, wpb=10477.7, bsz=369.5, num_updates=3700, lr=3.6018e-05, gnorm=0.735, train_wall=182, wall=0
2021-01-09 18:03:04 | INFO | train_inner | epoch 007:    547 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.075, nll_loss=0.88, ppl=1.84, wps=5842.7, ups=0.55, wpb=10679.1, bsz=365.9, num_updates=3800, lr=3.55409e-05, gnorm=0.716, train_wall=183, wall=0
2021-01-09 18:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 18:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:03:50 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.692 | nll_loss 4.158 | ppl 17.85 | bleu 21.91 | wps 4581.3 | wpb 7508.5 | bsz 272.7 | num_updates 3814 | best_bleu 22.43
2021-01-09 18:03:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 18:03:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:03:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:03:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 7 @ 3814 updates, score 21.91) (writing took 2.8648065235465765 seconds)
2021-01-09 18:03:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-09 18:03:53 | INFO | train | epoch 007 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.086 | nll_loss 0.879 | ppl 1.84 | wps 5596.6 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 3814 | lr 3.54756e-05 | gnorm 0.725 | train_wall 1022 | wall 0
2021-01-09 18:03:53 | INFO | fairseq.trainer | begin training epoch 8
2021-01-09 18:03:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:03:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:06:34 | INFO | train_inner | epoch 008:     86 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.867, ppl=1.82, wps=5008.6, ups=0.47, wpb=10544.9, bsz=372.1, num_updates=3900, lr=3.50823e-05, gnorm=0.715, train_wall=183, wall=0
2021-01-09 18:09:36 | INFO | train_inner | epoch 008:    186 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.074, nll_loss=0.87, ppl=1.83, wps=5756.8, ups=0.55, wpb=10489.3, bsz=360.4, num_updates=4000, lr=3.4641e-05, gnorm=0.724, train_wall=182, wall=0
2021-01-09 18:12:38 | INFO | train_inner | epoch 008:    286 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.082, nll_loss=0.871, ppl=1.83, wps=5731.5, ups=0.55, wpb=10424.3, bsz=379, num_updates=4100, lr=3.4216e-05, gnorm=0.72, train_wall=182, wall=0
2021-01-09 18:15:41 | INFO | train_inner | epoch 008:    386 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.076, nll_loss=0.868, ppl=1.83, wps=5739.4, ups=0.55, wpb=10453.9, bsz=386.4, num_updates=4200, lr=3.38062e-05, gnorm=0.72, train_wall=182, wall=0
2021-01-09 18:18:44 | INFO | train_inner | epoch 008:    486 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.081, nll_loss=0.881, ppl=1.84, wps=5752.4, ups=0.54, wpb=10583.5, bsz=372.6, num_updates=4300, lr=3.34108e-05, gnorm=0.722, train_wall=184, wall=0
2021-01-09 18:21:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 18:21:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:21:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:21:23 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.701 | nll_loss 4.166 | ppl 17.95 | bleu 21.93 | wps 4707.6 | wpb 7508.5 | bsz 272.7 | num_updates 4375 | best_bleu 22.43
2021-01-09 18:21:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 18:21:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:21:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:21:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 8 @ 4375 updates, score 21.93) (writing took 2.8712073788046837 seconds)
2021-01-09 18:21:26 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-09 18:21:26 | INFO | train | epoch 008 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.081 | nll_loss 0.874 | ppl 1.83 | wps 5585.7 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 4375 | lr 3.31231e-05 | gnorm 0.722 | train_wall 1025 | wall 0
2021-01-09 18:21:26 | INFO | fairseq.trainer | begin training epoch 9
2021-01-09 18:21:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:21:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:22:15 | INFO | train_inner | epoch 009:     25 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.881, ppl=1.84, wps=4903.8, ups=0.47, wpb=10329.8, bsz=355.2, num_updates=4400, lr=3.30289e-05, gnorm=0.734, train_wall=184, wall=0
2021-01-09 18:25:19 | INFO | train_inner | epoch 009:    125 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.059, nll_loss=0.856, ppl=1.81, wps=5734.8, ups=0.54, wpb=10531.4, bsz=366.8, num_updates=4500, lr=3.26599e-05, gnorm=0.715, train_wall=183, wall=0
2021-01-09 18:28:23 | INFO | train_inner | epoch 009:    225 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.084, nll_loss=0.877, ppl=1.84, wps=5708.2, ups=0.54, wpb=10516.5, bsz=349.4, num_updates=4600, lr=3.23029e-05, gnorm=0.725, train_wall=184, wall=0
2021-01-09 18:31:28 | INFO | train_inner | epoch 009:    325 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.864, ppl=1.82, wps=5650.8, ups=0.54, wpb=10425.9, bsz=377.7, num_updates=4700, lr=3.19574e-05, gnorm=0.721, train_wall=184, wall=0
2021-01-09 18:34:31 | INFO | train_inner | epoch 009:    425 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.06, nll_loss=0.864, ppl=1.82, wps=5745.9, ups=0.54, wpb=10556, bsz=378.7, num_updates=4800, lr=3.16228e-05, gnorm=0.714, train_wall=184, wall=0
2021-01-09 18:37:36 | INFO | train_inner | epoch 009:    525 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.071, nll_loss=0.867, ppl=1.82, wps=5701.9, ups=0.54, wpb=10522.4, bsz=376.2, num_updates=4900, lr=3.12984e-05, gnorm=0.718, train_wall=184, wall=0
2021-01-09 18:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 18:38:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:38:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:38:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:38:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:38:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:38:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:38:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:38:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:38:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:39:06 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.688 | nll_loss 4.155 | ppl 17.81 | bleu 21.92 | wps 4092.8 | wpb 7508.5 | bsz 272.7 | num_updates 4936 | best_bleu 22.43
2021-01-09 18:39:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 18:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 9 @ 4936 updates, score 21.92) (writing took 2.830133084207773 seconds)
2021-01-09 18:39:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-09 18:39:09 | INFO | train | epoch 009 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.074 | nll_loss 0.868 | ppl 1.82 | wps 5533.5 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 4936 | lr 3.11841e-05 | gnorm 0.721 | train_wall 1032 | wall 0
2021-01-09 18:39:09 | INFO | fairseq.trainer | begin training epoch 10
2021-01-09 18:39:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:39:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:41:10 | INFO | train_inner | epoch 010:     64 / 561 symm_kl=0.323, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.875, ppl=1.83, wps=4815.3, ups=0.47, wpb=10331.9, bsz=364.2, num_updates=5000, lr=3.09839e-05, gnorm=0.733, train_wall=185, wall=0
2021-01-09 18:44:16 | INFO | train_inner | epoch 010:    164 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.865, ppl=1.82, wps=5695.9, ups=0.54, wpb=10549.4, bsz=361, num_updates=5100, lr=3.06786e-05, gnorm=0.718, train_wall=185, wall=0
2021-01-09 18:47:19 | INFO | train_inner | epoch 010:    264 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.067, nll_loss=0.863, ppl=1.82, wps=5730.1, ups=0.54, wpb=10515, bsz=358.9, num_updates=5200, lr=3.03822e-05, gnorm=0.724, train_wall=183, wall=0
2021-01-09 18:50:23 | INFO | train_inner | epoch 010:    364 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.079, nll_loss=0.866, ppl=1.82, wps=5688.4, ups=0.54, wpb=10476.4, bsz=384.9, num_updates=5300, lr=3.00942e-05, gnorm=0.722, train_wall=184, wall=0
2021-01-09 18:53:27 | INFO | train_inner | epoch 010:    464 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.07, nll_loss=0.863, ppl=1.82, wps=5698.3, ups=0.54, wpb=10470.5, bsz=366.5, num_updates=5400, lr=2.98142e-05, gnorm=0.722, train_wall=184, wall=0
2021-01-09 18:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 18:56:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 18:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 18:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 18:56:47 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.694 | nll_loss 4.163 | ppl 17.91 | bleu 21.81 | wps 4493.7 | wpb 7508.5 | bsz 272.7 | num_updates 5497 | best_bleu 22.43
2021-01-09 18:56:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 18:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 10 @ 5497 updates, score 21.81) (writing took 2.94772582501173 seconds)
2021-01-09 18:56:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-09 18:56:50 | INFO | train | epoch 010 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.072 | nll_loss 0.864 | ppl 1.82 | wps 5544.7 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 5497 | lr 2.955e-05 | gnorm 0.722 | train_wall 1032 | wall 0
2021-01-09 18:56:50 | INFO | fairseq.trainer | begin training epoch 11
2021-01-09 18:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 18:56:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 18:56:58 | INFO | train_inner | epoch 011:      3 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.068, nll_loss=0.865, ppl=1.82, wps=4942.8, ups=0.47, wpb=10439.7, bsz=373.2, num_updates=5500, lr=2.9542e-05, gnorm=0.724, train_wall=183, wall=0
2021-01-09 19:00:01 | INFO | train_inner | epoch 011:    103 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.059, nll_loss=0.854, ppl=1.81, wps=5704.1, ups=0.55, wpb=10416.5, bsz=362.7, num_updates=5600, lr=2.9277e-05, gnorm=0.719, train_wall=182, wall=0
2021-01-09 19:03:05 | INFO | train_inner | epoch 011:    203 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.053, nll_loss=0.852, ppl=1.8, wps=5726.3, ups=0.54, wpb=10554.8, bsz=376, num_updates=5700, lr=2.90191e-05, gnorm=0.709, train_wall=184, wall=0
2021-01-09 19:06:10 | INFO | train_inner | epoch 011:    303 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.08, nll_loss=0.865, ppl=1.82, wps=5650, ups=0.54, wpb=10419.6, bsz=362.4, num_updates=5800, lr=2.87678e-05, gnorm=0.726, train_wall=184, wall=0
2021-01-09 19:09:14 | INFO | train_inner | epoch 011:    403 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.086, nll_loss=0.866, ppl=1.82, wps=5650, ups=0.54, wpb=10426.5, bsz=375.8, num_updates=5900, lr=2.8523e-05, gnorm=0.724, train_wall=184, wall=0
2021-01-09 19:12:19 | INFO | train_inner | epoch 011:    503 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.058, nll_loss=0.86, ppl=1.82, wps=5743.5, ups=0.54, wpb=10643.4, bsz=377.5, num_updates=6000, lr=2.82843e-05, gnorm=0.713, train_wall=185, wall=0
2021-01-09 19:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 19:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:14:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:14:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:14:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:14:27 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.695 | nll_loss 4.164 | ppl 17.92 | bleu 21.74 | wps 4508.3 | wpb 7508.5 | bsz 272.7 | num_updates 6058 | best_bleu 22.43
2021-01-09 19:14:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 19:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:14:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 11 @ 6058 updates, score 21.74) (writing took 2.8664301801472902 seconds)
2021-01-09 19:14:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-09 19:14:30 | INFO | train | epoch 011 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.067 | nll_loss 0.86 | ppl 1.81 | wps 5544.2 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 6058 | lr 2.81485e-05 | gnorm 0.719 | train_wall 1032 | wall 0
2021-01-09 19:14:30 | INFO | fairseq.trainer | begin training epoch 12
2021-01-09 19:14:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:14:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:15:51 | INFO | train_inner | epoch 012:     42 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.055, nll_loss=0.851, ppl=1.8, wps=4949.8, ups=0.47, wpb=10480.2, bsz=363.2, num_updates=6100, lr=2.80515e-05, gnorm=0.716, train_wall=184, wall=0
2021-01-09 19:18:54 | INFO | train_inner | epoch 012:    142 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.855, ppl=1.81, wps=5683.5, ups=0.55, wpb=10402.4, bsz=377.8, num_updates=6200, lr=2.78243e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-09 19:21:58 | INFO | train_inner | epoch 012:    242 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.07, nll_loss=0.859, ppl=1.81, wps=5693.6, ups=0.54, wpb=10448.1, bsz=368, num_updates=6300, lr=2.76026e-05, gnorm=0.721, train_wall=183, wall=0
2021-01-09 19:25:02 | INFO | train_inner | epoch 012:    342 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.045, nll_loss=0.848, ppl=1.8, wps=5753.8, ups=0.54, wpb=10621.2, bsz=375.4, num_updates=6400, lr=2.73861e-05, gnorm=0.71, train_wall=184, wall=0
2021-01-09 19:28:07 | INFO | train_inner | epoch 012:    442 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.865, ppl=1.82, wps=5684.7, ups=0.54, wpb=10494.2, bsz=373.1, num_updates=6500, lr=2.71746e-05, gnorm=0.723, train_wall=184, wall=0
2021-01-09 19:31:10 | INFO | train_inner | epoch 012:    542 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.053, nll_loss=0.854, ppl=1.81, wps=5746.4, ups=0.55, wpb=10537.1, bsz=357.4, num_updates=6600, lr=2.6968e-05, gnorm=0.717, train_wall=183, wall=0
2021-01-09 19:31:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 19:31:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:31:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:31:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:31:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:31:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:31:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:31:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:31:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:31:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:32:07 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.699 | nll_loss 4.169 | ppl 17.99 | bleu 21.93 | wps 4531.5 | wpb 7508.5 | bsz 272.7 | num_updates 6619 | best_bleu 22.43
2021-01-09 19:32:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 19:32:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:32:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:32:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:32:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:32:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 12 @ 6619 updates, score 21.93) (writing took 2.9190856274217367 seconds)
2021-01-09 19:32:10 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-09 19:32:10 | INFO | train | epoch 012 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.062 | nll_loss 0.855 | ppl 1.81 | wps 5552.5 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 6619 | lr 2.69293e-05 | gnorm 0.718 | train_wall 1030 | wall 0
2021-01-09 19:32:10 | INFO | fairseq.trainer | begin training epoch 13
2021-01-09 19:32:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:32:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:34:40 | INFO | train_inner | epoch 013:     81 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.059, nll_loss=0.85, ppl=1.8, wps=4934.6, ups=0.48, wpb=10357.7, bsz=362.7, num_updates=6700, lr=2.6766e-05, gnorm=0.725, train_wall=182, wall=0
2021-01-09 19:37:42 | INFO | train_inner | epoch 013:    181 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.056, nll_loss=0.854, ppl=1.81, wps=5804.1, ups=0.55, wpb=10560.9, bsz=352.5, num_updates=6800, lr=2.65684e-05, gnorm=0.716, train_wall=182, wall=0
2021-01-09 19:40:45 | INFO | train_inner | epoch 013:    281 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.065, nll_loss=0.853, ppl=1.81, wps=5762, ups=0.55, wpb=10533.3, bsz=365.4, num_updates=6900, lr=2.63752e-05, gnorm=0.718, train_wall=183, wall=0
2021-01-09 19:43:47 | INFO | train_inner | epoch 013:    381 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.059, nll_loss=0.851, ppl=1.8, wps=5760.4, ups=0.55, wpb=10485.2, bsz=391.6, num_updates=7000, lr=2.61861e-05, gnorm=0.711, train_wall=182, wall=0
2021-01-09 19:47:01 | INFO | train_inner | epoch 013:    481 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.063, nll_loss=0.856, ppl=1.81, wps=5394.1, ups=0.52, wpb=10466.2, bsz=368.6, num_updates=7100, lr=2.60011e-05, gnorm=0.722, train_wall=194, wall=0
2021-01-09 19:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 19:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:49:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:49:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:49:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:50:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 19:50:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 19:50:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 19:50:06 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.696 | nll_loss 4.164 | ppl 17.93 | bleu 21.86 | wps 4547.7 | wpb 7508.5 | bsz 272.7 | num_updates 7180 | best_bleu 22.43
2021-01-09 19:50:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 19:50:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:50:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:50:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 13 @ 7180 updates, score 21.86) (writing took 2.8981317412108183 seconds)
2021-01-09 19:50:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-09 19:50:09 | INFO | train | epoch 013 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.061 | nll_loss 0.853 | ppl 1.81 | wps 5449.5 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 7180 | lr 2.58558e-05 | gnorm 0.718 | train_wall 1050 | wall 0
2021-01-09 19:50:09 | INFO | fairseq.trainer | begin training epoch 14
2021-01-09 19:50:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 19:50:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 19:50:53 | INFO | train_inner | epoch 014:     20 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.068, nll_loss=0.856, ppl=1.81, wps=4475, ups=0.43, wpb=10371.5, bsz=374, num_updates=7200, lr=2.58199e-05, gnorm=0.723, train_wall=204, wall=0
2021-01-09 19:54:19 | INFO | train_inner | epoch 014:    120 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.045, nll_loss=0.846, ppl=1.8, wps=5168.2, ups=0.48, wpb=10670.5, bsz=369, num_updates=7300, lr=2.56424e-05, gnorm=0.708, train_wall=206, wall=0
2021-01-09 19:57:25 | INFO | train_inner | epoch 014:    220 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.048, nll_loss=0.845, ppl=1.8, wps=5653.1, ups=0.54, wpb=10515.1, bsz=372, num_updates=7400, lr=2.54686e-05, gnorm=0.712, train_wall=186, wall=0
2021-01-09 20:00:27 | INFO | train_inner | epoch 014:    320 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.063, nll_loss=0.852, ppl=1.81, wps=5682, ups=0.55, wpb=10344.7, bsz=361.3, num_updates=7500, lr=2.52982e-05, gnorm=0.728, train_wall=182, wall=0
2021-01-09 20:03:30 | INFO | train_inner | epoch 014:    420 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.859, ppl=1.81, wps=5674, ups=0.55, wpb=10364.3, bsz=358.1, num_updates=7600, lr=2.51312e-05, gnorm=0.728, train_wall=182, wall=0
2021-01-09 20:06:33 | INFO | train_inner | epoch 014:    520 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.05, nll_loss=0.847, ppl=1.8, wps=5766.3, ups=0.54, wpb=10585.6, bsz=379.9, num_updates=7700, lr=2.49675e-05, gnorm=0.709, train_wall=183, wall=0
2021-01-09 20:07:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 20:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:08:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:08:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:08:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:08:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:08:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:08:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:08:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:08:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:08:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:08:13 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.701 | nll_loss 4.171 | ppl 18.01 | bleu 21.93 | wps 4620.4 | wpb 7508.5 | bsz 272.7 | num_updates 7741 | best_bleu 22.43
2021-01-09 20:08:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 20:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:08:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 14 @ 7741 updates, score 21.93) (writing took 3.0566516872495413 seconds)
2021-01-09 20:08:16 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-09 20:08:16 | INFO | train | epoch 014 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.057 | nll_loss 0.85 | ppl 1.8 | wps 5409 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 7741 | lr 2.49013e-05 | gnorm 0.718 | train_wall 1059 | wall 0
2021-01-09 20:08:16 | INFO | fairseq.trainer | begin training epoch 15
2021-01-09 20:08:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:08:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:10:21 | INFO | train_inner | epoch 015:     59 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.065, nll_loss=0.853, ppl=1.81, wps=4546.4, ups=0.44, wpb=10357.1, bsz=378.1, num_updates=7800, lr=2.48069e-05, gnorm=0.728, train_wall=200, wall=0
2021-01-09 20:13:37 | INFO | train_inner | epoch 015:    159 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.055, nll_loss=0.847, ppl=1.8, wps=5372.9, ups=0.51, wpb=10493.8, bsz=369, num_updates=7900, lr=2.46494e-05, gnorm=0.713, train_wall=195, wall=0
2021-01-09 20:16:43 | INFO | train_inner | epoch 015:    259 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.049, nll_loss=0.839, ppl=1.79, wps=5632.3, ups=0.54, wpb=10500.6, bsz=381, num_updates=8000, lr=2.44949e-05, gnorm=0.714, train_wall=186, wall=0
2021-01-09 20:19:47 | INFO | train_inner | epoch 015:    359 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.05, nll_loss=0.847, ppl=1.8, wps=5726.3, ups=0.54, wpb=10526.3, bsz=358.2, num_updates=8100, lr=2.43432e-05, gnorm=0.716, train_wall=184, wall=0
2021-01-09 20:22:49 | INFO | train_inner | epoch 015:    459 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.045, nll_loss=0.847, ppl=1.8, wps=5822.6, ups=0.55, wpb=10577.7, bsz=378.6, num_updates=8200, lr=2.41943e-05, gnorm=0.711, train_wall=181, wall=0
2021-01-09 20:25:51 | INFO | train_inner | epoch 015:    559 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.067, nll_loss=0.856, ppl=1.81, wps=5730.5, ups=0.55, wpb=10450.8, bsz=363.6, num_updates=8300, lr=2.40481e-05, gnorm=0.724, train_wall=182, wall=0
2021-01-09 20:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:26:16 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.7 | nll_loss 4.172 | ppl 18.02 | bleu 21.84 | wps 4612.2 | wpb 7508.5 | bsz 272.7 | num_updates 8302 | best_bleu 22.43
2021-01-09 20:26:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 20:26:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:26:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:26:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 15 @ 8302 updates, score 21.84) (writing took 2.5044596195220947 seconds)
2021-01-09 20:26:18 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-09 20:26:18 | INFO | train | epoch 015 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.055 | nll_loss 0.847 | ppl 1.8 | wps 5435.1 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 8302 | lr 2.40452e-05 | gnorm 0.718 | train_wall 1054 | wall 0
2021-01-09 20:26:18 | INFO | fairseq.trainer | begin training epoch 16
2021-01-09 20:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:26:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:26:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:26:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:29:44 | INFO | train_inner | epoch 016:     98 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.055, nll_loss=0.84, ppl=1.79, wps=4435.6, ups=0.43, wpb=10350.7, bsz=371.4, num_updates=8400, lr=2.39046e-05, gnorm=0.718, train_wall=206, wall=0
2021-01-09 20:33:17 | INFO | train_inner | epoch 016:    198 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.053, nll_loss=0.845, ppl=1.8, wps=4943.1, ups=0.47, wpb=10507.7, bsz=381.6, num_updates=8500, lr=2.37635e-05, gnorm=0.718, train_wall=212, wall=0
2021-01-09 20:36:52 | INFO | train_inner | epoch 016:    298 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.05, nll_loss=0.843, ppl=1.79, wps=4899.7, ups=0.47, wpb=10518.3, bsz=380.5, num_updates=8600, lr=2.3625e-05, gnorm=0.714, train_wall=214, wall=0
2021-01-09 20:40:11 | INFO | train_inner | epoch 016:    398 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.046, nll_loss=0.842, ppl=1.79, wps=5257.1, ups=0.5, wpb=10509.9, bsz=366.6, num_updates=8700, lr=2.34888e-05, gnorm=0.712, train_wall=200, wall=0
2021-01-09 20:43:33 | INFO | train_inner | epoch 016:    498 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.054, nll_loss=0.849, ppl=1.8, wps=5175.3, ups=0.49, wpb=10456.6, bsz=359.6, num_updates=8800, lr=2.3355e-05, gnorm=0.721, train_wall=202, wall=0
2021-01-09 20:45:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 20:45:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:45:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:45:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:45:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:45:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:45:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:46:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 20:46:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 20:46:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 20:46:06 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.701 | nll_loss 4.171 | ppl 18.01 | bleu 21.89 | wps 4474.2 | wpb 7508.5 | bsz 272.7 | num_updates 8863 | best_bleu 22.43
2021-01-09 20:46:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 20:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:46:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 16 @ 8863 updates, score 21.89) (writing took 2.5615847557783127 seconds)
2021-01-09 20:46:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-09 20:46:08 | INFO | train | epoch 016 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.052 | nll_loss 0.845 | ppl 1.8 | wps 4940.9 | ups 0.47 | wpb 10483.4 | bsz 369.6 | num_updates 8863 | lr 2.32718e-05 | gnorm 0.717 | train_wall 1162 | wall 0
2021-01-09 20:46:08 | INFO | fairseq.trainer | begin training epoch 17
2021-01-09 20:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:46:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 20:46:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 20:47:29 | INFO | train_inner | epoch 017:     37 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.051, nll_loss=0.845, ppl=1.8, wps=4446.3, ups=0.43, wpb=10461.7, bsz=362.5, num_updates=8900, lr=2.32234e-05, gnorm=0.722, train_wall=208, wall=0
2021-01-09 20:51:03 | INFO | train_inner | epoch 017:    137 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.044, nll_loss=0.839, ppl=1.79, wps=4913, ups=0.47, wpb=10540, bsz=367.1, num_updates=9000, lr=2.3094e-05, gnorm=0.712, train_wall=214, wall=0
2021-01-09 20:54:17 | INFO | train_inner | epoch 017:    237 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.028, nll_loss=0.827, ppl=1.77, wps=5458, ups=0.52, wpb=10577.5, bsz=374.7, num_updates=9100, lr=2.29668e-05, gnorm=0.702, train_wall=194, wall=0
2021-01-09 20:57:41 | INFO | train_inner | epoch 017:    337 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.043, nll_loss=0.839, ppl=1.79, wps=5135.5, ups=0.49, wpb=10486.9, bsz=372, num_updates=9200, lr=2.28416e-05, gnorm=0.717, train_wall=204, wall=0
2021-01-09 21:01:11 | INFO | train_inner | epoch 017:    437 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.051, nll_loss=0.845, ppl=1.8, wps=4976, ups=0.48, wpb=10438.8, bsz=372.9, num_updates=9300, lr=2.27185e-05, gnorm=0.72, train_wall=210, wall=0
2021-01-09 21:04:51 | INFO | train_inner | epoch 017:    537 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.074, nll_loss=0.859, ppl=1.81, wps=4762.1, ups=0.45, wpb=10471.5, bsz=366.3, num_updates=9400, lr=2.25973e-05, gnorm=0.727, train_wall=220, wall=0
2021-01-09 21:05:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 21:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:05:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:05:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:06:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:06:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:06:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:06:05 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.704 | nll_loss 4.175 | ppl 18.06 | bleu 21.82 | wps 4505.7 | wpb 7508.5 | bsz 272.7 | num_updates 9424 | best_bleu 22.43
2021-01-09 21:06:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 21:06:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:06:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:06:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 17 @ 9424 updates, score 21.82) (writing took 2.660069912672043 seconds)
2021-01-09 21:06:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-09 21:06:08 | INFO | train | epoch 017 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.049 | nll_loss 0.842 | ppl 1.79 | wps 4903.5 | ups 0.47 | wpb 10483.4 | bsz 369.6 | num_updates 9424 | lr 2.25685e-05 | gnorm 0.716 | train_wall 1171 | wall 0
2021-01-09 21:06:08 | INFO | fairseq.trainer | begin training epoch 18
2021-01-09 21:06:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:06:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:06:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:06:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:08:37 | INFO | train_inner | epoch 018:     76 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.052, nll_loss=0.842, ppl=1.79, wps=4588, ups=0.44, wpb=10375.3, bsz=355, num_updates=9500, lr=2.24781e-05, gnorm=0.725, train_wall=198, wall=0
2021-01-09 21:11:55 | INFO | train_inner | epoch 018:    176 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.038, nll_loss=0.835, ppl=1.78, wps=5330.4, ups=0.51, wpb=10541.3, bsz=366, num_updates=9600, lr=2.23607e-05, gnorm=0.712, train_wall=198, wall=0
2021-01-09 21:15:23 | INFO | train_inner | epoch 018:    276 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.05, nll_loss=0.836, ppl=1.78, wps=5009.2, ups=0.48, wpb=10435.8, bsz=371.8, num_updates=9700, lr=2.22451e-05, gnorm=0.716, train_wall=208, wall=0
2021-01-09 21:19:01 | INFO | train_inner | epoch 018:    376 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.051, nll_loss=0.842, ppl=1.79, wps=4790.7, ups=0.46, wpb=10415, bsz=375, num_updates=9800, lr=2.21313e-05, gnorm=0.72, train_wall=217, wall=0
2021-01-09 21:22:21 | INFO | train_inner | epoch 018:    476 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.044, nll_loss=0.841, ppl=1.79, wps=5293.1, ups=0.5, wpb=10590.2, bsz=373.3, num_updates=9900, lr=2.20193e-05, gnorm=0.709, train_wall=200, wall=0
2021-01-09 21:25:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 21:25:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:25:22 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.7 | nll_loss 4.172 | ppl 18.03 | bleu 21.87 | wps 4537.2 | wpb 7508.5 | bsz 272.7 | num_updates 9985 | best_bleu 22.43
2021-01-09 21:25:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 21:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 18 @ 9985 updates, score 21.87) (writing took 2.698048831894994 seconds)
2021-01-09 21:25:25 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-09 21:25:25 | INFO | train | epoch 018 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.047 | nll_loss 0.84 | ppl 1.79 | wps 5081.3 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 9985 | lr 2.19254e-05 | gnorm 0.717 | train_wall 1129 | wall 0
2021-01-09 21:25:25 | INFO | fairseq.trainer | begin training epoch 19
2021-01-09 21:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:25:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:25:56 | INFO | train_inner | epoch 019:     15 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.059, nll_loss=0.851, ppl=1.8, wps=4840.3, ups=0.46, wpb=10424.8, bsz=362.8, num_updates=10000, lr=2.19089e-05, gnorm=0.727, train_wall=188, wall=0
2021-01-09 21:29:25 | INFO | train_inner | epoch 019:    115 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.04, nll_loss=0.832, ppl=1.78, wps=5016.4, ups=0.48, wpb=10469.6, bsz=380.5, num_updates=10100, lr=2.18002e-05, gnorm=0.709, train_wall=209, wall=0
2021-01-09 21:32:56 | INFO | train_inner | epoch 019:    215 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.025, nll_loss=0.828, ppl=1.77, wps=5028.8, ups=0.47, wpb=10646.8, bsz=378.2, num_updates=10200, lr=2.1693e-05, gnorm=0.7, train_wall=212, wall=0
2021-01-09 21:36:04 | INFO | train_inner | epoch 019:    315 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.054, nll_loss=0.842, ppl=1.79, wps=5561, ups=0.53, wpb=10434.3, bsz=361.3, num_updates=10300, lr=2.15875e-05, gnorm=0.72, train_wall=187, wall=0
2021-01-09 21:39:18 | INFO | train_inner | epoch 019:    415 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.058, nll_loss=0.845, ppl=1.8, wps=5384.7, ups=0.51, wpb=10460, bsz=366.1, num_updates=10400, lr=2.14834e-05, gnorm=0.721, train_wall=194, wall=0
2021-01-09 21:42:52 | INFO | train_inner | epoch 019:    515 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.037, nll_loss=0.833, ppl=1.78, wps=4934.4, ups=0.47, wpb=10524.5, bsz=370.6, num_updates=10500, lr=2.13809e-05, gnorm=0.716, train_wall=213, wall=0
2021-01-09 21:44:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 21:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 21:44:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 21:44:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 21:44:54 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.703 | nll_loss 4.176 | ppl 18.08 | bleu 21.87 | wps 3940.4 | wpb 7508.5 | bsz 272.7 | num_updates 10546 | best_bleu 22.43
2021-01-09 21:44:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 21:44:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:44:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:44:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 19 @ 10546 updates, score 21.87) (writing took 2.7620995566248894 seconds)
2021-01-09 21:44:56 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-09 21:44:56 | INFO | train | epoch 019 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.045 | nll_loss 0.837 | ppl 1.79 | wps 5020.9 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 10546 | lr 2.13342e-05 | gnorm 0.717 | train_wall 1140 | wall 0
2021-01-09 21:44:57 | INFO | fairseq.trainer | begin training epoch 20
2021-01-09 21:44:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 21:44:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 21:46:40 | INFO | train_inner | epoch 020:     54 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.04, nll_loss=0.836, ppl=1.79, wps=4579.6, ups=0.44, wpb=10462.5, bsz=364.1, num_updates=10600, lr=2.12798e-05, gnorm=0.728, train_wall=198, wall=0
2021-01-09 21:49:47 | INFO | train_inner | epoch 020:    154 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.045, nll_loss=0.839, ppl=1.79, wps=5618.8, ups=0.53, wpb=10518.9, bsz=373.8, num_updates=10700, lr=2.11801e-05, gnorm=0.717, train_wall=187, wall=0
2021-01-09 21:53:07 | INFO | train_inner | epoch 020:    254 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.045, nll_loss=0.835, ppl=1.78, wps=5230, ups=0.5, wpb=10440.2, bsz=369.3, num_updates=10800, lr=2.10819e-05, gnorm=0.718, train_wall=199, wall=0
2021-01-09 21:56:29 | INFO | train_inner | epoch 020:    354 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.034, nll_loss=0.834, ppl=1.78, wps=5234.6, ups=0.5, wpb=10573.8, bsz=366.8, num_updates=10900, lr=2.09849e-05, gnorm=0.714, train_wall=202, wall=0
2021-01-09 21:59:36 | INFO | train_inner | epoch 020:    454 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.05, nll_loss=0.836, ppl=1.79, wps=5574.5, ups=0.54, wpb=10401.8, bsz=372.2, num_updates=11000, lr=2.08893e-05, gnorm=0.721, train_wall=186, wall=0
2021-01-09 22:02:49 | INFO | train_inner | epoch 020:    554 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.049, nll_loss=0.842, ppl=1.79, wps=5418.7, ups=0.52, wpb=10502, bsz=368.1, num_updates=11100, lr=2.0795e-05, gnorm=0.716, train_wall=194, wall=0
2021-01-09 22:03:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 22:03:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:03:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:03:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:03:26 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.705 | nll_loss 4.178 | ppl 18.1 | bleu 21.89 | wps 4511 | wpb 7508.5 | bsz 272.7 | num_updates 11107 | best_bleu 22.43
2021-01-09 22:03:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 22:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:03:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 20 @ 11107 updates, score 21.89) (writing took 2.754488203674555 seconds)
2021-01-09 22:03:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-09 22:03:29 | INFO | train | epoch 020 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.043 | nll_loss 0.836 | ppl 1.79 | wps 5287.1 | ups 0.5 | wpb 10483.4 | bsz 369.6 | num_updates 11107 | lr 2.07885e-05 | gnorm 0.717 | train_wall 1084 | wall 0
2021-01-09 22:03:29 | INFO | fairseq.trainer | begin training epoch 21
2021-01-09 22:03:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:03:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:03:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:06:49 | INFO | train_inner | epoch 021:     93 / 561 symm_kl=0.324, self_kl=0, self_cv=0, loss=3.061, nll_loss=0.835, ppl=1.78, wps=4267, ups=0.42, wpb=10239, bsz=373.8, num_updates=11200, lr=2.0702e-05, gnorm=0.729, train_wall=212, wall=0
2021-01-09 22:09:56 | INFO | train_inner | epoch 021:    193 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.044, nll_loss=0.835, ppl=1.78, wps=5623.3, ups=0.54, wpb=10510.3, bsz=361, num_updates=11300, lr=2.06102e-05, gnorm=0.715, train_wall=187, wall=0
2021-01-09 22:13:04 | INFO | train_inner | epoch 021:    293 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.821, ppl=1.77, wps=5607.4, ups=0.53, wpb=10547.5, bsz=372.3, num_updates=11400, lr=2.05196e-05, gnorm=0.708, train_wall=188, wall=0
2021-01-09 22:16:34 | INFO | train_inner | epoch 021:    393 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.031, nll_loss=0.834, ppl=1.78, wps=5057.1, ups=0.48, wpb=10591, bsz=382.6, num_updates=11500, lr=2.04302e-05, gnorm=0.709, train_wall=209, wall=0
2021-01-09 22:19:53 | INFO | train_inner | epoch 021:    493 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.044, nll_loss=0.838, ppl=1.79, wps=5278.4, ups=0.5, wpb=10532.7, bsz=368.5, num_updates=11600, lr=2.03419e-05, gnorm=0.716, train_wall=199, wall=0
2021-01-09 22:21:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 22:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:22:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:22:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:22:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:22:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:22:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:22:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:22:21 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.708 | nll_loss 4.182 | ppl 18.16 | bleu 21.93 | wps 4011.2 | wpb 7508.5 | bsz 272.7 | num_updates 11668 | best_bleu 22.43
2021-01-09 22:22:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 22:22:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:22:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:22:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:22:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:22:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 21 @ 11668 updates, score 21.93) (writing took 2.8999256137758493 seconds)
2021-01-09 22:22:24 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-09 22:22:24 | INFO | train | epoch 021 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.041 | nll_loss 0.834 | ppl 1.78 | wps 5182 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 11668 | lr 2.02825e-05 | gnorm 0.716 | train_wall 1104 | wall 0
2021-01-09 22:22:24 | INFO | fairseq.trainer | begin training epoch 22
2021-01-09 22:22:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:22:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:23:26 | INFO | train_inner | epoch 022:     32 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.038, nll_loss=0.838, ppl=1.79, wps=4929, ups=0.47, wpb=10463.1, bsz=361.3, num_updates=11700, lr=2.02548e-05, gnorm=0.721, train_wall=182, wall=0
2021-01-09 22:26:40 | INFO | train_inner | epoch 022:    132 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.024, nll_loss=0.823, ppl=1.77, wps=5443.7, ups=0.51, wpb=10600.1, bsz=377.6, num_updates=11800, lr=2.01688e-05, gnorm=0.704, train_wall=195, wall=0
2021-01-09 22:29:50 | INFO | train_inner | epoch 022:    232 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.043, nll_loss=0.83, ppl=1.78, wps=5488.9, ups=0.53, wpb=10438.6, bsz=370.8, num_updates=11900, lr=2.00839e-05, gnorm=0.716, train_wall=190, wall=0
2021-01-09 22:32:55 | INFO | train_inner | epoch 022:    332 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.049, nll_loss=0.836, ppl=1.78, wps=5674.4, ups=0.54, wpb=10452.7, bsz=368.4, num_updates=12000, lr=2e-05, gnorm=0.719, train_wall=184, wall=0
2021-01-09 22:36:08 | INFO | train_inner | epoch 022:    432 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.036, nll_loss=0.829, ppl=1.78, wps=5418.8, ups=0.52, wpb=10468.1, bsz=359.8, num_updates=12100, lr=1.99172e-05, gnorm=0.718, train_wall=193, wall=0
2021-01-09 22:39:15 | INFO | train_inner | epoch 022:    532 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.04, nll_loss=0.838, ppl=1.79, wps=5610.7, ups=0.53, wpb=10490.6, bsz=375.2, num_updates=12200, lr=1.98354e-05, gnorm=0.718, train_wall=187, wall=0
2021-01-09 22:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 22:40:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:40:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:40:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:40:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:40:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:40:30 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.707 | nll_loss 4.179 | ppl 18.11 | bleu 21.88 | wps 4574.9 | wpb 7508.5 | bsz 272.7 | num_updates 12229 | best_bleu 22.43
2021-01-09 22:40:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 22:40:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:40:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:40:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 22 @ 12229 updates, score 21.88) (writing took 2.932994019240141 seconds)
2021-01-09 22:40:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-09 22:40:33 | INFO | train | epoch 022 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.039 | nll_loss 0.832 | ppl 1.78 | wps 5401.7 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 12229 | lr 1.98119e-05 | gnorm 0.716 | train_wall 1060 | wall 0
2021-01-09 22:40:33 | INFO | fairseq.trainer | begin training epoch 23
2021-01-09 22:40:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:40:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:42:44 | INFO | train_inner | epoch 023:     71 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.051, nll_loss=0.839, ppl=1.79, wps=4988.6, ups=0.48, wpb=10413.9, bsz=347.8, num_updates=12300, lr=1.97546e-05, gnorm=0.728, train_wall=181, wall=0
2021-01-09 22:45:45 | INFO | train_inner | epoch 023:    171 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.043, nll_loss=0.829, ppl=1.78, wps=5790.4, ups=0.55, wpb=10481.6, bsz=374.2, num_updates=12400, lr=1.96748e-05, gnorm=0.713, train_wall=181, wall=0
2021-01-09 22:48:45 | INFO | train_inner | epoch 023:    271 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.036, nll_loss=0.831, ppl=1.78, wps=5830.7, ups=0.55, wpb=10524.5, bsz=360.9, num_updates=12500, lr=1.95959e-05, gnorm=0.717, train_wall=180, wall=0
2021-01-09 22:51:46 | INFO | train_inner | epoch 023:    371 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.037, nll_loss=0.832, ppl=1.78, wps=5833.2, ups=0.55, wpb=10539.6, bsz=376.3, num_updates=12600, lr=1.9518e-05, gnorm=0.712, train_wall=180, wall=0
2021-01-09 22:54:46 | INFO | train_inner | epoch 023:    471 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.033, nll_loss=0.826, ppl=1.77, wps=5783.9, ups=0.55, wpb=10435.2, bsz=365.1, num_updates=12700, lr=1.9441e-05, gnorm=0.715, train_wall=180, wall=0
2021-01-09 22:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 22:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 22:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 22:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 22:57:50 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.714 | nll_loss 4.187 | ppl 18.21 | bleu 21.88 | wps 4501.5 | wpb 7508.5 | bsz 272.7 | num_updates 12790 | best_bleu 22.43
2021-01-09 22:57:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 22:57:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:57:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:57:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 23 @ 12790 updates, score 21.88) (writing took 2.9582229666411877 seconds)
2021-01-09 22:57:53 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-09 22:57:53 | INFO | train | epoch 023 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.038 | nll_loss 0.83 | ppl 1.78 | wps 5652.3 | ups 0.54 | wpb 10483.4 | bsz 369.6 | num_updates 12790 | lr 1.93725e-05 | gnorm 0.716 | train_wall 1012 | wall 0
2021-01-09 22:57:53 | INFO | fairseq.trainer | begin training epoch 24
2021-01-09 22:57:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 22:57:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 22:58:15 | INFO | train_inner | epoch 024:     10 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.032, nll_loss=0.831, ppl=1.78, wps=5018.1, ups=0.48, wpb=10462.2, bsz=387.8, num_updates=12800, lr=1.93649e-05, gnorm=0.718, train_wall=180, wall=0
2021-01-09 23:01:16 | INFO | train_inner | epoch 024:    110 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.027, nll_loss=0.826, ppl=1.77, wps=5844.3, ups=0.55, wpb=10569, bsz=365, num_updates=12900, lr=1.92897e-05, gnorm=0.711, train_wall=181, wall=0
2021-01-09 23:04:17 | INFO | train_inner | epoch 024:    210 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.041, nll_loss=0.828, ppl=1.77, wps=5761.7, ups=0.55, wpb=10465, bsz=374.5, num_updates=13000, lr=1.92154e-05, gnorm=0.714, train_wall=181, wall=0
2021-01-09 23:07:18 | INFO | train_inner | epoch 024:    310 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.034, nll_loss=0.827, ppl=1.77, wps=5789.2, ups=0.55, wpb=10462.9, bsz=361.7, num_updates=13100, lr=1.91419e-05, gnorm=0.717, train_wall=181, wall=0
2021-01-09 23:10:18 | INFO | train_inner | epoch 024:    410 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.057, nll_loss=0.836, ppl=1.79, wps=5712.6, ups=0.56, wpb=10285.5, bsz=365, num_updates=13200, lr=1.90693e-05, gnorm=0.727, train_wall=180, wall=0
2021-01-09 23:13:19 | INFO | train_inner | epoch 024:    510 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.038, nll_loss=0.836, ppl=1.78, wps=5822.3, ups=0.55, wpb=10565.4, bsz=380.9, num_updates=13300, lr=1.89974e-05, gnorm=0.715, train_wall=181, wall=0
2021-01-09 23:14:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 23:14:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:14:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:14:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:14:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:14:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:14:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:14:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:14:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:14:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:15:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:15:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:15:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:15:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:15:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:15:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:15:16 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.707 | nll_loss 4.181 | ppl 18.14 | bleu 21.79 | wps 3921.8 | wpb 7508.5 | bsz 272.7 | num_updates 13351 | best_bleu 22.43
2021-01-09 23:15:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 23:15:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:15:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:15:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:15:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:15:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 24 @ 13351 updates, score 21.79) (writing took 2.8607253674417734 seconds)
2021-01-09 23:15:19 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-09 23:15:19 | INFO | train | epoch 024 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.036 | nll_loss 0.829 | ppl 1.78 | wps 5623.9 | ups 0.54 | wpb 10483.4 | bsz 369.6 | num_updates 13351 | lr 1.89611e-05 | gnorm 0.716 | train_wall 1014 | wall 0
2021-01-09 23:15:19 | INFO | fairseq.trainer | begin training epoch 25
2021-01-09 23:15:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:15:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:16:51 | INFO | train_inner | epoch 025:     49 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.819, ppl=1.76, wps=4971.6, ups=0.47, wpb=10515.7, bsz=369.3, num_updates=13400, lr=1.89264e-05, gnorm=0.71, train_wall=181, wall=0
2021-01-09 23:19:53 | INFO | train_inner | epoch 025:    149 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.827, ppl=1.77, wps=5832, ups=0.55, wpb=10598.5, bsz=358.8, num_updates=13500, lr=1.88562e-05, gnorm=0.712, train_wall=182, wall=0
2021-01-09 23:22:53 | INFO | train_inner | epoch 025:    249 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.043, nll_loss=0.826, ppl=1.77, wps=5741.3, ups=0.55, wpb=10355.8, bsz=370.5, num_updates=13600, lr=1.87867e-05, gnorm=0.716, train_wall=180, wall=0
2021-01-09 23:25:55 | INFO | train_inner | epoch 025:    349 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.028, nll_loss=0.827, ppl=1.77, wps=5817.4, ups=0.55, wpb=10561.1, bsz=370, num_updates=13700, lr=1.8718e-05, gnorm=0.712, train_wall=181, wall=0
2021-01-09 23:28:57 | INFO | train_inner | epoch 025:    449 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.037, nll_loss=0.836, ppl=1.78, wps=5819, ups=0.55, wpb=10605.9, bsz=370.9, num_updates=13800, lr=1.86501e-05, gnorm=0.712, train_wall=182, wall=0
2021-01-09 23:31:59 | INFO | train_inner | epoch 025:    549 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.036, nll_loss=0.822, ppl=1.77, wps=5681, ups=0.55, wpb=10361.6, bsz=385.6, num_updates=13900, lr=1.85829e-05, gnorm=0.715, train_wall=182, wall=0
2021-01-09 23:32:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 23:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:32:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:32:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:32:43 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.717 | nll_loss 4.189 | ppl 18.23 | bleu 21.96 | wps 4435.5 | wpb 7508.5 | bsz 272.7 | num_updates 13912 | best_bleu 22.43
2021-01-09 23:32:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 23:32:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:32:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:32:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 25 @ 13912 updates, score 21.96) (writing took 2.909052859991789 seconds)
2021-01-09 23:32:46 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-09 23:32:46 | INFO | train | epoch 025 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.035 | nll_loss 0.828 | ppl 1.77 | wps 5616.6 | ups 0.54 | wpb 10483.4 | bsz 369.6 | num_updates 13912 | lr 1.85749e-05 | gnorm 0.715 | train_wall 1018 | wall 0
2021-01-09 23:32:46 | INFO | fairseq.trainer | begin training epoch 26
2021-01-09 23:32:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:32:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:35:30 | INFO | train_inner | epoch 026:     88 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.033, nll_loss=0.819, ppl=1.76, wps=4910.5, ups=0.47, wpb=10364.3, bsz=387, num_updates=14000, lr=1.85164e-05, gnorm=0.716, train_wall=183, wall=0
2021-01-09 23:38:33 | INFO | train_inner | epoch 026:    188 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.029, nll_loss=0.822, ppl=1.77, wps=5720, ups=0.55, wpb=10455.9, bsz=353.1, num_updates=14100, lr=1.84506e-05, gnorm=0.717, train_wall=183, wall=0
2021-01-09 23:41:37 | INFO | train_inner | epoch 026:    288 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.824, ppl=1.77, wps=5803.1, ups=0.54, wpb=10683.2, bsz=394.8, num_updates=14200, lr=1.83855e-05, gnorm=0.704, train_wall=184, wall=0
2021-01-09 23:44:40 | INFO | train_inner | epoch 026:    388 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.033, nll_loss=0.832, ppl=1.78, wps=5768.1, ups=0.55, wpb=10515.8, bsz=357.6, num_updates=14300, lr=1.83211e-05, gnorm=0.719, train_wall=182, wall=0
2021-01-09 23:47:42 | INFO | train_inner | epoch 026:    488 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.035, nll_loss=0.828, ppl=1.78, wps=5748.5, ups=0.55, wpb=10503.7, bsz=366.4, num_updates=14400, lr=1.82574e-05, gnorm=0.714, train_wall=183, wall=0
2021-01-09 23:49:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-09 23:49:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:49:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:49:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:49:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:49:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:49:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:50:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-09 23:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-09 23:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-09 23:50:17 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.712 | nll_loss 4.186 | ppl 18.2 | bleu 21.82 | wps 4608.2 | wpb 7508.5 | bsz 272.7 | num_updates 14473 | best_bleu 22.43
2021-01-09 23:50:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-09 23:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:50:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:50:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:50:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:50:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 26 @ 14473 updates, score 21.82) (writing took 2.9118684716522694 seconds)
2021-01-09 23:50:20 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-09 23:50:20 | INFO | train | epoch 026 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.032 | nll_loss 0.825 | ppl 1.77 | wps 5581.2 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 14473 | lr 1.82113e-05 | gnorm 0.715 | train_wall 1025 | wall 0
2021-01-09 23:50:20 | INFO | fairseq.trainer | begin training epoch 27
2021-01-09 23:50:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-09 23:50:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-09 23:51:12 | INFO | train_inner | epoch 027:     27 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.046, nll_loss=0.83, ppl=1.78, wps=4911.9, ups=0.48, wpb=10319.9, bsz=360.1, num_updates=14500, lr=1.81944e-05, gnorm=0.726, train_wall=182, wall=0
2021-01-09 23:54:15 | INFO | train_inner | epoch 027:    127 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.041, nll_loss=0.83, ppl=1.78, wps=5725.4, ups=0.55, wpb=10485, bsz=356.3, num_updates=14600, lr=1.81319e-05, gnorm=0.717, train_wall=183, wall=0
2021-01-09 23:57:18 | INFO | train_inner | epoch 027:    227 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.029, nll_loss=0.824, ppl=1.77, wps=5739.9, ups=0.55, wpb=10489.9, bsz=362.8, num_updates=14700, lr=1.80702e-05, gnorm=0.716, train_wall=183, wall=0
2021-01-10 00:00:22 | INFO | train_inner | epoch 027:    327 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.821, ppl=1.77, wps=5775.2, ups=0.54, wpb=10626.3, bsz=383.5, num_updates=14800, lr=1.8009e-05, gnorm=0.703, train_wall=184, wall=0
2021-01-10 00:03:25 | INFO | train_inner | epoch 027:    427 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.027, nll_loss=0.819, ppl=1.76, wps=5704.6, ups=0.55, wpb=10432.3, bsz=372.5, num_updates=14900, lr=1.79485e-05, gnorm=0.717, train_wall=183, wall=0
2021-01-10 00:06:29 | INFO | train_inner | epoch 027:    527 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.032, nll_loss=0.823, ppl=1.77, wps=5731.3, ups=0.54, wpb=10519.2, bsz=367.3, num_updates=15000, lr=1.78885e-05, gnorm=0.711, train_wall=183, wall=0
2021-01-10 00:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 00:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:07:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:07:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:07:52 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.714 | nll_loss 4.186 | ppl 18.2 | bleu 21.8 | wps 4530.4 | wpb 7508.5 | bsz 272.7 | num_updates 15034 | best_bleu 22.43
2021-01-10 00:07:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 00:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:07:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 27 @ 15034 updates, score 21.8) (writing took 2.817056993022561 seconds)
2021-01-10 00:07:55 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-10 00:07:55 | INFO | train | epoch 027 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.031 | nll_loss 0.824 | ppl 1.77 | wps 5573.6 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 15034 | lr 1.78683e-05 | gnorm 0.714 | train_wall 1026 | wall 0
2021-01-10 00:07:55 | INFO | fairseq.trainer | begin training epoch 28
2021-01-10 00:07:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:07:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:10:00 | INFO | train_inner | epoch 028:     66 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.03, nll_loss=0.814, ppl=1.76, wps=4886.7, ups=0.47, wpb=10304.3, bsz=380.3, num_updates=15100, lr=1.78292e-05, gnorm=0.716, train_wall=183, wall=0
2021-01-10 00:13:02 | INFO | train_inner | epoch 028:    166 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.04, nll_loss=0.826, ppl=1.77, wps=5706.5, ups=0.55, wpb=10411.3, bsz=345, num_updates=15200, lr=1.77705e-05, gnorm=0.722, train_wall=182, wall=0
2021-01-10 00:16:06 | INFO | train_inner | epoch 028:    266 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.817, ppl=1.76, wps=5779.4, ups=0.54, wpb=10625.6, bsz=372.2, num_updates=15300, lr=1.77123e-05, gnorm=0.703, train_wall=184, wall=0
2021-01-10 00:19:09 | INFO | train_inner | epoch 028:    366 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.822, ppl=1.77, wps=5745.3, ups=0.55, wpb=10539.2, bsz=360.4, num_updates=15400, lr=1.76547e-05, gnorm=0.715, train_wall=183, wall=0
2021-01-10 00:22:12 | INFO | train_inner | epoch 028:    466 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.042, nll_loss=0.829, ppl=1.78, wps=5701.8, ups=0.55, wpb=10434.9, bsz=391.3, num_updates=15500, lr=1.75977e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-10 00:25:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 00:25:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:25:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:25:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:25:27 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.718 | nll_loss 4.191 | ppl 18.27 | bleu 21.83 | wps 4537 | wpb 7508.5 | bsz 272.7 | num_updates 15595 | best_bleu 22.43
2021-01-10 00:25:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 00:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 28 @ 15595 updates, score 21.83) (writing took 2.992697559297085 seconds)
2021-01-10 00:25:30 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-10 00:25:30 | INFO | train | epoch 028 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.03 | nll_loss 0.823 | ppl 1.77 | wps 5573 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 15595 | lr 1.7544e-05 | gnorm 0.715 | train_wall 1026 | wall 0
2021-01-10 00:25:30 | INFO | fairseq.trainer | begin training epoch 29
2021-01-10 00:25:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:25:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:25:43 | INFO | train_inner | epoch 029:      5 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.037, nll_loss=0.831, ppl=1.78, wps=4969.2, ups=0.47, wpb=10466.6, bsz=364, num_updates=15600, lr=1.75412e-05, gnorm=0.723, train_wall=183, wall=0
2021-01-10 00:28:47 | INFO | train_inner | epoch 029:    105 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.82, ppl=1.76, wps=5741.4, ups=0.54, wpb=10571.4, bsz=377.4, num_updates=15700, lr=1.74852e-05, gnorm=0.709, train_wall=184, wall=0
2021-01-10 00:31:50 | INFO | train_inner | epoch 029:    205 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.818, ppl=1.76, wps=5757.7, ups=0.55, wpb=10552.2, bsz=363.9, num_updates=15800, lr=1.74298e-05, gnorm=0.712, train_wall=183, wall=0
2021-01-10 00:34:54 | INFO | train_inner | epoch 029:    305 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.024, nll_loss=0.822, ppl=1.77, wps=5749.3, ups=0.55, wpb=10535.6, bsz=371.3, num_updates=15900, lr=1.73749e-05, gnorm=0.712, train_wall=183, wall=0
2021-01-10 00:37:57 | INFO | train_inner | epoch 029:    405 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.039, nll_loss=0.822, ppl=1.77, wps=5660.5, ups=0.55, wpb=10376.3, bsz=381.1, num_updates=16000, lr=1.73205e-05, gnorm=0.719, train_wall=183, wall=0
2021-01-10 00:41:00 | INFO | train_inner | epoch 029:    505 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.047, nll_loss=0.825, ppl=1.77, wps=5638.7, ups=0.54, wpb=10347.7, bsz=364.6, num_updates=16100, lr=1.72666e-05, gnorm=0.724, train_wall=183, wall=0
2021-01-10 00:42:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 00:42:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:42:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:42:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:42:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:42:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:42:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:42:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:43:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 00:43:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 00:43:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 00:43:05 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.712 | nll_loss 4.187 | ppl 18.21 | bleu 21.86 | wps 4576.7 | wpb 7508.5 | bsz 272.7 | num_updates 16156 | best_bleu 22.43
2021-01-10 00:43:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 00:43:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:43:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:43:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 29 @ 16156 updates, score 21.86) (writing took 2.9098471645265818 seconds)
2021-01-10 00:43:07 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-10 00:43:07 | INFO | train | epoch 029 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.029 | nll_loss 0.822 | ppl 1.77 | wps 5562.3 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 16156 | lr 1.72367e-05 | gnorm 0.715 | train_wall 1029 | wall 0
2021-01-10 00:43:07 | INFO | fairseq.trainer | begin training epoch 30
2021-01-10 00:43:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 00:43:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 00:44:31 | INFO | train_inner | epoch 030:     44 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.819, ppl=1.76, wps=4978, ups=0.47, wpb=10503.4, bsz=372.7, num_updates=16200, lr=1.72133e-05, gnorm=0.714, train_wall=183, wall=0
2021-01-10 00:47:33 | INFO | train_inner | epoch 030:    144 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.821, ppl=1.77, wps=5814.9, ups=0.55, wpb=10537.9, bsz=361.8, num_updates=16300, lr=1.71604e-05, gnorm=0.711, train_wall=181, wall=0
2021-01-10 00:50:35 | INFO | train_inner | epoch 030:    244 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.025, nll_loss=0.814, ppl=1.76, wps=5732.2, ups=0.55, wpb=10471.7, bsz=365.8, num_updates=16400, lr=1.7108e-05, gnorm=0.711, train_wall=182, wall=0
2021-01-10 00:53:38 | INFO | train_inner | epoch 030:    344 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.042, nll_loss=0.823, ppl=1.77, wps=5698.8, ups=0.55, wpb=10395, bsz=378.7, num_updates=16500, lr=1.70561e-05, gnorm=0.719, train_wall=182, wall=0
2021-01-10 00:56:40 | INFO | train_inner | epoch 030:    444 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.025, nll_loss=0.825, ppl=1.77, wps=5806.3, ups=0.55, wpb=10612.9, bsz=368.2, num_updates=16600, lr=1.70046e-05, gnorm=0.71, train_wall=183, wall=0
2021-01-10 00:59:43 | INFO | train_inner | epoch 030:    544 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.03, nll_loss=0.826, ppl=1.77, wps=5730.2, ups=0.55, wpb=10435.4, bsz=362.1, num_updates=16700, lr=1.69536e-05, gnorm=0.72, train_wall=182, wall=0
2021-01-10 01:00:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 01:00:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:00:35 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.716 | nll_loss 4.191 | ppl 18.27 | bleu 21.8 | wps 4567.1 | wpb 7508.5 | bsz 272.7 | num_updates 16717 | best_bleu 22.43
2021-01-10 01:00:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 01:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:00:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:00:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 30 @ 16717 updates, score 21.8) (writing took 2.9184804130345583 seconds)
2021-01-10 01:00:38 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-10 01:00:38 | INFO | train | epoch 030 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.027 | nll_loss 0.821 | ppl 1.77 | wps 5598.5 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 16717 | lr 1.6945e-05 | gnorm 0.714 | train_wall 1022 | wall 0
2021-01-10 01:00:38 | INFO | fairseq.trainer | begin training epoch 31
2021-01-10 01:00:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:03:14 | INFO | train_inner | epoch 031:     83 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.029, nll_loss=0.82, ppl=1.77, wps=4942.9, ups=0.47, wpb=10441.8, bsz=365, num_updates=16800, lr=1.69031e-05, gnorm=0.722, train_wall=184, wall=0
2021-01-10 01:06:19 | INFO | train_inner | epoch 031:    183 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.813, ppl=1.76, wps=5718.8, ups=0.54, wpb=10568.9, bsz=384.7, num_updates=16900, lr=1.6853e-05, gnorm=0.705, train_wall=185, wall=0
2021-01-10 01:09:22 | INFO | train_inner | epoch 031:    283 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.028, nll_loss=0.819, ppl=1.76, wps=5683.8, ups=0.54, wpb=10433.9, bsz=359.3, num_updates=17000, lr=1.68034e-05, gnorm=0.722, train_wall=183, wall=0
2021-01-10 01:12:27 | INFO | train_inner | epoch 031:    383 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.035, nll_loss=0.823, ppl=1.77, wps=5660.9, ups=0.54, wpb=10455.6, bsz=354.2, num_updates=17100, lr=1.67542e-05, gnorm=0.719, train_wall=185, wall=0
2021-01-10 01:15:31 | INFO | train_inner | epoch 031:    483 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.019, nll_loss=0.817, ppl=1.76, wps=5706.5, ups=0.54, wpb=10533.1, bsz=377.4, num_updates=17200, lr=1.67054e-05, gnorm=0.714, train_wall=184, wall=0
2021-01-10 01:17:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 01:17:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:17:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:17:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:17:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:17:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:17:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:17:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:17:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:17:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:17:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:18:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:18:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:18:16 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.71 | nll_loss 4.185 | ppl 18.19 | bleu 21.85 | wps 4442.4 | wpb 7508.5 | bsz 272.7 | num_updates 17278 | best_bleu 22.43
2021-01-10 01:18:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 01:18:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:18:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:18:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:18:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:18:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 31 @ 17278 updates, score 21.85) (writing took 2.936345800757408 seconds)
2021-01-10 01:18:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-10 01:18:19 | INFO | train | epoch 031 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.026 | nll_loss 0.819 | ppl 1.76 | wps 5540.5 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 17278 | lr 1.66676e-05 | gnorm 0.716 | train_wall 1032 | wall 0
2021-01-10 01:18:19 | INFO | fairseq.trainer | begin training epoch 32
2021-01-10 01:18:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:18:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:19:03 | INFO | train_inner | epoch 032:     22 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.032, nll_loss=0.824, ppl=1.77, wps=4911.3, ups=0.47, wpb=10374.2, bsz=378.7, num_updates=17300, lr=1.6657e-05, gnorm=0.722, train_wall=183, wall=0
2021-01-10 01:22:08 | INFO | train_inner | epoch 032:    122 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.033, nll_loss=0.821, ppl=1.77, wps=5634, ups=0.54, wpb=10466.3, bsz=364.6, num_updates=17400, lr=1.66091e-05, gnorm=0.716, train_wall=186, wall=0
2021-01-10 01:25:16 | INFO | train_inner | epoch 032:    222 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.808, ppl=1.75, wps=5691.8, ups=0.53, wpb=10668.9, bsz=366.7, num_updates=17500, lr=1.65616e-05, gnorm=0.701, train_wall=187, wall=0
2021-01-10 01:28:22 | INFO | train_inner | epoch 032:    322 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.01, nll_loss=0.812, ppl=1.76, wps=5665.9, ups=0.54, wpb=10570.1, bsz=367.2, num_updates=17600, lr=1.65145e-05, gnorm=0.71, train_wall=186, wall=0
2021-01-10 01:31:29 | INFO | train_inner | epoch 032:    422 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.041, nll_loss=0.822, ppl=1.77, wps=5546.2, ups=0.54, wpb=10355.9, bsz=346.2, num_updates=17700, lr=1.64677e-05, gnorm=0.727, train_wall=187, wall=0
2021-01-10 01:34:38 | INFO | train_inner | epoch 032:    522 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.819, ppl=1.76, wps=5559.3, ups=0.53, wpb=10517.4, bsz=399.7, num_updates=17800, lr=1.64214e-05, gnorm=0.712, train_wall=189, wall=0
2021-01-10 01:35:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 01:35:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:35:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:35:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:35:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:35:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:35:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:36:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:36:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:36:15 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.714 | nll_loss 4.191 | ppl 18.26 | bleu 21.92 | wps 4191.1 | wpb 7508.5 | bsz 272.7 | num_updates 17839 | best_bleu 22.43
2021-01-10 01:36:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 01:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:36:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 32 @ 17839 updates, score 21.92) (writing took 2.889640999957919 seconds)
2021-01-10 01:36:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-10 01:36:18 | INFO | train | epoch 032 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.024 | nll_loss 0.817 | ppl 1.76 | wps 5452.3 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 17839 | lr 1.64035e-05 | gnorm 0.715 | train_wall 1049 | wall 0
2021-01-10 01:36:18 | INFO | fairseq.trainer | begin training epoch 33
2021-01-10 01:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:36:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:38:17 | INFO | train_inner | epoch 033:     61 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.037, nll_loss=0.823, ppl=1.77, wps=4734.2, ups=0.46, wpb=10342.3, bsz=377.5, num_updates=17900, lr=1.63755e-05, gnorm=0.723, train_wall=189, wall=0
2021-01-10 01:41:27 | INFO | train_inner | epoch 033:    161 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.808, ppl=1.75, wps=5533.9, ups=0.52, wpb=10541, bsz=377.5, num_updates=18000, lr=1.63299e-05, gnorm=0.705, train_wall=190, wall=0
2021-01-10 01:44:40 | INFO | train_inner | epoch 033:    261 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.815, ppl=1.76, wps=5473.6, ups=0.52, wpb=10557.3, bsz=381.8, num_updates=18100, lr=1.62848e-05, gnorm=0.711, train_wall=193, wall=0
2021-01-10 01:47:54 | INFO | train_inner | epoch 033:    361 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.817, ppl=1.76, wps=5395.5, ups=0.52, wpb=10470.7, bsz=367, num_updates=18200, lr=1.624e-05, gnorm=0.718, train_wall=194, wall=0
2021-01-10 01:51:10 | INFO | train_inner | epoch 033:    461 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.032, nll_loss=0.818, ppl=1.76, wps=5345.1, ups=0.51, wpb=10461, bsz=350.6, num_updates=18300, lr=1.61955e-05, gnorm=0.717, train_wall=196, wall=0
2021-01-10 01:54:28 | INFO | train_inner | epoch 033:    561 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.035, nll_loss=0.823, ppl=1.77, wps=5259.3, ups=0.51, wpb=10401.1, bsz=367, num_updates=18400, lr=1.61515e-05, gnorm=0.721, train_wall=198, wall=0
2021-01-10 01:54:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 01:54:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:54:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:54:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 01:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 01:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 01:54:49 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.714 | nll_loss 4.19 | ppl 18.26 | bleu 21.83 | wps 4540.3 | wpb 7508.5 | bsz 272.7 | num_updates 18400 | best_bleu 22.43
2021-01-10 01:54:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 01:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:54:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:54:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 33 @ 18400 updates, score 21.83) (writing took 2.888826673850417 seconds)
2021-01-10 01:54:52 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-10 01:54:52 | INFO | train | epoch 033 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.024 | nll_loss 0.817 | ppl 1.76 | wps 5280.7 | ups 0.5 | wpb 10483.4 | bsz 369.6 | num_updates 18400 | lr 1.61515e-05 | gnorm 0.715 | train_wall 1085 | wall 0
2021-01-10 01:54:52 | INFO | fairseq.trainer | begin training epoch 34
2021-01-10 01:54:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 01:54:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 01:58:15 | INFO | train_inner | epoch 034:    100 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.815, ppl=1.76, wps=4644.4, ups=0.44, wpb=10540.6, bsz=386.2, num_updates=18500, lr=1.61077e-05, gnorm=0.709, train_wall=199, wall=0
2021-01-10 02:01:35 | INFO | train_inner | epoch 034:    200 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.027, nll_loss=0.824, ppl=1.77, wps=5228.9, ups=0.5, wpb=10456, bsz=367.2, num_updates=18600, lr=1.60644e-05, gnorm=0.721, train_wall=200, wall=0
2021-01-10 02:04:57 | INFO | train_inner | epoch 034:    300 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.024, nll_loss=0.812, ppl=1.76, wps=5182.1, ups=0.49, wpb=10499, bsz=369, num_updates=18700, lr=1.60214e-05, gnorm=0.713, train_wall=202, wall=0
2021-01-10 02:08:19 | INFO | train_inner | epoch 034:    400 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.024, nll_loss=0.811, ppl=1.75, wps=5168.6, ups=0.5, wpb=10428.7, bsz=380.5, num_updates=18800, lr=1.59787e-05, gnorm=0.717, train_wall=202, wall=0
2021-01-10 02:11:45 | INFO | train_inner | epoch 034:    500 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.007, nll_loss=0.807, ppl=1.75, wps=5148, ups=0.49, wpb=10584.1, bsz=365, num_updates=18900, lr=1.59364e-05, gnorm=0.707, train_wall=205, wall=0
2021-01-10 02:13:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 02:13:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:13:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:13:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:13:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:13:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:13:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:13:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:14:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:14:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:14:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:14:14 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.718 | nll_loss 4.195 | ppl 18.32 | bleu 21.97 | wps 4432.1 | wpb 7508.5 | bsz 272.7 | num_updates 18961 | best_bleu 22.43
2021-01-10 02:14:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 02:14:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:14:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:14:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:14:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:14:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 34 @ 18961 updates, score 21.97) (writing took 2.8864963613450527 seconds)
2021-01-10 02:14:17 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-10 02:14:17 | INFO | train | epoch 034 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.022 | nll_loss 0.816 | ppl 1.76 | wps 5048.7 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 18961 | lr 1.59107e-05 | gnorm 0.716 | train_wall 1136 | wall 0
2021-01-10 02:14:17 | INFO | fairseq.trainer | begin training epoch 35
2021-01-10 02:14:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:14:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:15:42 | INFO | train_inner | epoch 035:     39 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.816, ppl=1.76, wps=4403.8, ups=0.42, wpb=10452.4, bsz=354.3, num_updates=19000, lr=1.58944e-05, gnorm=0.72, train_wall=209, wall=0
2021-01-10 02:19:17 | INFO | train_inner | epoch 035:    139 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.811, ppl=1.75, wps=4868.7, ups=0.47, wpb=10458.1, bsz=363.5, num_updates=19100, lr=1.58527e-05, gnorm=0.713, train_wall=215, wall=0
2021-01-10 02:22:54 | INFO | train_inner | epoch 035:    239 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.017, nll_loss=0.81, ppl=1.75, wps=4833.4, ups=0.46, wpb=10479.3, bsz=380.7, num_updates=19200, lr=1.58114e-05, gnorm=0.711, train_wall=217, wall=0
2021-01-10 02:26:28 | INFO | train_inner | epoch 035:    339 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.025, nll_loss=0.818, ppl=1.76, wps=4909.7, ups=0.47, wpb=10528.1, bsz=362.6, num_updates=19300, lr=1.57704e-05, gnorm=0.717, train_wall=214, wall=0
2021-01-10 02:29:58 | INFO | train_inner | epoch 035:    439 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.816, ppl=1.76, wps=5043.1, ups=0.48, wpb=10579.5, bsz=383.5, num_updates=19400, lr=1.57297e-05, gnorm=0.711, train_wall=210, wall=0
2021-01-10 02:33:21 | INFO | train_inner | epoch 035:    539 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.04, nll_loss=0.823, ppl=1.77, wps=5099.5, ups=0.49, wpb=10365.9, bsz=354.1, num_updates=19500, lr=1.56893e-05, gnorm=0.724, train_wall=203, wall=0
2021-01-10 02:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 02:34:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:34:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:34:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:34:28 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.72 | nll_loss 4.195 | ppl 18.31 | bleu 21.84 | wps 4292.4 | wpb 7508.5 | bsz 272.7 | num_updates 19522 | best_bleu 22.43
2021-01-10 02:34:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 02:34:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 35 @ 19522 updates, score 21.84) (writing took 2.8966528680175543 seconds)
2021-01-10 02:34:31 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-10 02:34:31 | INFO | train | epoch 035 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.021 | nll_loss 0.814 | ppl 1.76 | wps 4844.8 | ups 0.46 | wpb 10483.4 | bsz 369.6 | num_updates 19522 | lr 1.56804e-05 | gnorm 0.715 | train_wall 1184 | wall 0
2021-01-10 02:34:31 | INFO | fairseq.trainer | begin training epoch 36
2021-01-10 02:34:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:34:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:34:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:34:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:37:12 | INFO | train_inner | epoch 036:     78 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.019, nll_loss=0.808, ppl=1.75, wps=4475.5, ups=0.43, wpb=10351.9, bsz=364.4, num_updates=19600, lr=1.56492e-05, gnorm=0.718, train_wall=203, wall=0
2021-01-10 02:40:36 | INFO | train_inner | epoch 036:    178 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.025, nll_loss=0.814, ppl=1.76, wps=5112.9, ups=0.49, wpb=10402.5, bsz=353.6, num_updates=19700, lr=1.56094e-05, gnorm=0.721, train_wall=203, wall=0
2021-01-10 02:44:00 | INFO | train_inner | epoch 036:    278 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.016, nll_loss=0.815, ppl=1.76, wps=5171.8, ups=0.49, wpb=10574.6, bsz=387.2, num_updates=19800, lr=1.557e-05, gnorm=0.709, train_wall=204, wall=0
2021-01-10 02:47:25 | INFO | train_inner | epoch 036:    378 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.812, ppl=1.76, wps=5170.9, ups=0.49, wpb=10577.9, bsz=376.2, num_updates=19900, lr=1.55308e-05, gnorm=0.709, train_wall=204, wall=0
2021-01-10 02:50:52 | INFO | train_inner | epoch 036:    478 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.007, nll_loss=0.81, ppl=1.75, wps=5160.4, ups=0.48, wpb=10663.9, bsz=375.9, num_updates=20000, lr=1.54919e-05, gnorm=0.705, train_wall=206, wall=0
2021-01-10 02:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 02:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:53:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:53:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:53:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 02:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 02:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 02:54:03 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.718 | nll_loss 4.195 | ppl 18.32 | bleu 21.84 | wps 4562.5 | wpb 7508.5 | bsz 272.7 | num_updates 20083 | best_bleu 22.43
2021-01-10 02:54:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 02:54:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:54:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:54:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 36 @ 20083 updates, score 21.84) (writing took 2.9358742516487837 seconds)
2021-01-10 02:54:06 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-10 02:54:06 | INFO | train | epoch 036 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.02 | nll_loss 0.814 | ppl 1.76 | wps 5002.1 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 20083 | lr 1.54599e-05 | gnorm 0.716 | train_wall 1147 | wall 0
2021-01-10 02:54:06 | INFO | fairseq.trainer | begin training epoch 37
2021-01-10 02:54:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 02:54:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 02:54:45 | INFO | train_inner | epoch 037:     17 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.042, nll_loss=0.823, ppl=1.77, wps=4408.3, ups=0.43, wpb=10299.9, bsz=354.9, num_updates=20100, lr=1.54533e-05, gnorm=0.732, train_wall=206, wall=0
2021-01-10 02:58:13 | INFO | train_inner | epoch 037:    117 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.024, nll_loss=0.811, ppl=1.75, wps=5000.8, ups=0.48, wpb=10416.4, bsz=367.9, num_updates=20200, lr=1.5415e-05, gnorm=0.723, train_wall=208, wall=0
2021-01-10 03:01:36 | INFO | train_inner | epoch 037:    217 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.814, ppl=1.76, wps=5185.6, ups=0.49, wpb=10486.1, bsz=358.3, num_updates=20300, lr=1.5377e-05, gnorm=0.718, train_wall=202, wall=0
2021-01-10 03:05:00 | INFO | train_inner | epoch 037:    317 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.809, ppl=1.75, wps=5128.8, ups=0.49, wpb=10464.4, bsz=375, num_updates=20400, lr=1.53393e-05, gnorm=0.712, train_wall=204, wall=0
2021-01-10 03:08:24 | INFO | train_inner | epoch 037:    417 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.809, ppl=1.75, wps=5193.6, ups=0.49, wpb=10606.3, bsz=373.8, num_updates=20500, lr=1.53018e-05, gnorm=0.704, train_wall=204, wall=0
2021-01-10 03:11:50 | INFO | train_inner | epoch 037:    517 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.818, ppl=1.76, wps=5114.1, ups=0.49, wpb=10533.6, bsz=369.3, num_updates=20600, lr=1.52647e-05, gnorm=0.715, train_wall=206, wall=0
2021-01-10 03:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 03:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:13:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:13:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:13:41 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.716 | nll_loss 4.194 | ppl 18.3 | bleu 21.96 | wps 4599.1 | wpb 7508.5 | bsz 272.7 | num_updates 20644 | best_bleu 22.43
2021-01-10 03:13:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 03:13:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:13:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:13:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 37 @ 20644 updates, score 21.96) (writing took 2.812822686508298 seconds)
2021-01-10 03:13:44 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-10 03:13:44 | INFO | train | epoch 037 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.019 | nll_loss 0.812 | ppl 1.76 | wps 4994.1 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 20644 | lr 1.52484e-05 | gnorm 0.715 | train_wall 1149 | wall 0
2021-01-10 03:13:44 | INFO | fairseq.trainer | begin training epoch 38
2021-01-10 03:13:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:15:42 | INFO | train_inner | epoch 038:     56 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.03, nll_loss=0.812, ppl=1.76, wps=4401.3, ups=0.43, wpb=10216.8, bsz=376.2, num_updates=20700, lr=1.52277e-05, gnorm=0.724, train_wall=205, wall=0
2021-01-10 03:19:06 | INFO | train_inner | epoch 038:    156 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.026, nll_loss=0.811, ppl=1.75, wps=5112.7, ups=0.49, wpb=10430.5, bsz=375.5, num_updates=20800, lr=1.51911e-05, gnorm=0.716, train_wall=204, wall=0
2021-01-10 03:22:26 | INFO | train_inner | epoch 038:    256 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.014, nll_loss=0.813, ppl=1.76, wps=5271.8, ups=0.5, wpb=10563.2, bsz=369.2, num_updates=20900, lr=1.51547e-05, gnorm=0.716, train_wall=200, wall=0
2021-01-10 03:25:50 | INFO | train_inner | epoch 038:    356 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.809, ppl=1.75, wps=5215.4, ups=0.49, wpb=10631.5, bsz=376.8, num_updates=21000, lr=1.51186e-05, gnorm=0.709, train_wall=204, wall=0
2021-01-10 03:29:18 | INFO | train_inner | epoch 038:    456 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.03, nll_loss=0.816, ppl=1.76, wps=5035.6, ups=0.48, wpb=10459.4, bsz=365, num_updates=21100, lr=1.50827e-05, gnorm=0.72, train_wall=208, wall=0
2021-01-10 03:32:45 | INFO | train_inner | epoch 038:    556 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.807, ppl=1.75, wps=5089.9, ups=0.48, wpb=10536.9, bsz=366.9, num_updates=21200, lr=1.50471e-05, gnorm=0.712, train_wall=207, wall=0
2021-01-10 03:32:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 03:32:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:32:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:32:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:32:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:32:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:32:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:33:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:33:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:33:16 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.722 | nll_loss 4.198 | ppl 18.36 | bleu 21.93 | wps 4654.5 | wpb 7508.5 | bsz 272.7 | num_updates 21205 | best_bleu 22.43
2021-01-10 03:33:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 03:33:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:33:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:33:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:33:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 38 @ 21205 updates, score 21.93) (writing took 2.790864322334528 seconds)
2021-01-10 03:33:19 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-10 03:33:19 | INFO | train | epoch 038 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.018 | nll_loss 0.811 | ppl 1.75 | wps 5004.9 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 21205 | lr 1.50453e-05 | gnorm 0.716 | train_wall 1147 | wall 0
2021-01-10 03:33:19 | INFO | fairseq.trainer | begin training epoch 39
2021-01-10 03:33:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:33:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:33:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:36:42 | INFO | train_inner | epoch 039:     95 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.807, ppl=1.75, wps=4387.6, ups=0.42, wpb=10382.8, bsz=364.1, num_updates=21300, lr=1.50117e-05, gnorm=0.717, train_wall=209, wall=0
2021-01-10 03:40:15 | INFO | train_inner | epoch 039:    195 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.802, ppl=1.74, wps=4958.5, ups=0.47, wpb=10577.3, bsz=384.2, num_updates=21400, lr=1.49766e-05, gnorm=0.704, train_wall=213, wall=0
2021-01-10 03:43:49 | INFO | train_inner | epoch 039:    295 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.807, ppl=1.75, wps=4936.7, ups=0.47, wpb=10552.9, bsz=361.6, num_updates=21500, lr=1.49417e-05, gnorm=0.713, train_wall=214, wall=0
2021-01-10 03:47:22 | INFO | train_inner | epoch 039:    395 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.814, ppl=1.76, wps=4946.5, ups=0.47, wpb=10560.9, bsz=380.2, num_updates=21600, lr=1.49071e-05, gnorm=0.712, train_wall=213, wall=0
2021-01-10 03:50:59 | INFO | train_inner | epoch 039:    495 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.034, nll_loss=0.815, ppl=1.76, wps=4793.6, ups=0.46, wpb=10412.6, bsz=362.7, num_updates=21700, lr=1.48727e-05, gnorm=0.722, train_wall=217, wall=0
2021-01-10 03:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 03:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:53:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:53:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:53:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 03:53:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 03:53:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 03:53:43 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.724 | nll_loss 4.199 | ppl 18.37 | bleu 22 | wps 4626.6 | wpb 7508.5 | bsz 272.7 | num_updates 21766 | best_bleu 22.43
2021-01-10 03:53:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 03:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:53:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:53:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 39 @ 21766 updates, score 22.0) (writing took 2.954452235251665 seconds)
2021-01-10 03:53:46 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-10 03:53:46 | INFO | train | epoch 039 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.016 | nll_loss 0.81 | ppl 1.75 | wps 4794.9 | ups 0.46 | wpb 10483.4 | bsz 369.6 | num_updates 21766 | lr 1.48502e-05 | gnorm 0.715 | train_wall 1198 | wall 0
2021-01-10 03:53:46 | INFO | fairseq.trainer | begin training epoch 40
2021-01-10 03:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 03:53:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 03:55:02 | INFO | train_inner | epoch 040:     34 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.031, nll_loss=0.813, ppl=1.76, wps=4259.3, ups=0.41, wpb=10316.6, bsz=355.4, num_updates=21800, lr=1.48386e-05, gnorm=0.726, train_wall=215, wall=0
2021-01-10 03:58:37 | INFO | train_inner | epoch 040:    134 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.811, ppl=1.75, wps=4854.9, ups=0.47, wpb=10433.7, bsz=354.6, num_updates=21900, lr=1.48047e-05, gnorm=0.721, train_wall=215, wall=0
2021-01-10 04:02:13 | INFO | train_inner | epoch 040:    234 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.013, nll_loss=0.807, ppl=1.75, wps=4850.3, ups=0.46, wpb=10472, bsz=378.4, num_updates=22000, lr=1.4771e-05, gnorm=0.711, train_wall=216, wall=0
2021-01-10 04:05:50 | INFO | train_inner | epoch 040:    334 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.805, ppl=1.75, wps=4882.6, ups=0.46, wpb=10605.8, bsz=373.6, num_updates=22100, lr=1.47375e-05, gnorm=0.709, train_wall=217, wall=0
2021-01-10 04:09:26 | INFO | train_inner | epoch 040:    434 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.01, nll_loss=0.811, ppl=1.75, wps=4883.7, ups=0.46, wpb=10579.8, bsz=371.2, num_updates=22200, lr=1.47043e-05, gnorm=0.712, train_wall=216, wall=0
2021-01-10 04:13:01 | INFO | train_inner | epoch 040:    534 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.022, nll_loss=0.814, ppl=1.76, wps=4878, ups=0.47, wpb=10460.9, bsz=380.1, num_updates=22300, lr=1.46713e-05, gnorm=0.719, train_wall=214, wall=0
2021-01-10 04:13:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 04:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:13:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:14:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:14:22 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.719 | nll_loss 4.196 | ppl 18.33 | bleu 22.01 | wps 4117.1 | wpb 7508.5 | bsz 272.7 | num_updates 22327 | best_bleu 22.43
2021-01-10 04:14:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 04:14:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:14:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:14:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:14:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:14:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 40 @ 22327 updates, score 22.01) (writing took 2.8645625337958336 seconds)
2021-01-10 04:14:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-10 04:14:24 | INFO | train | epoch 040 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.016 | nll_loss 0.809 | ppl 1.75 | wps 4747.2 | ups 0.45 | wpb 10483.4 | bsz 369.6 | num_updates 22327 | lr 1.46624e-05 | gnorm 0.715 | train_wall 1208 | wall 0
2021-01-10 04:14:24 | INFO | fairseq.trainer | begin training epoch 41
2021-01-10 04:14:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:14:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:17:03 | INFO | train_inner | epoch 041:     73 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.807, ppl=1.75, wps=4285.3, ups=0.41, wpb=10396, bsz=373.9, num_updates=22400, lr=1.46385e-05, gnorm=0.715, train_wall=213, wall=0
2021-01-10 04:20:38 | INFO | train_inner | epoch 041:    173 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.809, ppl=1.75, wps=4866.8, ups=0.47, wpb=10462.1, bsz=344.3, num_updates=22500, lr=1.46059e-05, gnorm=0.721, train_wall=215, wall=0
2021-01-10 04:24:13 | INFO | train_inner | epoch 041:    273 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.797, ppl=1.74, wps=4921.6, ups=0.47, wpb=10541.5, bsz=387.8, num_updates=22600, lr=1.45736e-05, gnorm=0.705, train_wall=214, wall=0
2021-01-10 04:27:48 | INFO | train_inner | epoch 041:    373 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.016, nll_loss=0.81, ppl=1.75, wps=4878.2, ups=0.46, wpb=10528.3, bsz=390.2, num_updates=22700, lr=1.45414e-05, gnorm=0.714, train_wall=216, wall=0
2021-01-10 04:31:23 | INFO | train_inner | epoch 041:    473 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.033, nll_loss=0.816, ppl=1.76, wps=4840.7, ups=0.47, wpb=10391.4, bsz=365.2, num_updates=22800, lr=1.45095e-05, gnorm=0.726, train_wall=214, wall=0
2021-01-10 04:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 04:34:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:34:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:34:43 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.725 | nll_loss 4.202 | ppl 18.4 | bleu 21.96 | wps 4618.9 | wpb 7508.5 | bsz 272.7 | num_updates 22888 | best_bleu 22.43
2021-01-10 04:34:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 04:34:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:34:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:34:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 41 @ 22888 updates, score 21.96) (writing took 2.7661903221160173 seconds)
2021-01-10 04:34:46 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-10 04:34:46 | INFO | train | epoch 041 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.014 | nll_loss 0.808 | ppl 1.75 | wps 4815.6 | ups 0.46 | wpb 10483.4 | bsz 369.6 | num_updates 22888 | lr 1.44816e-05 | gnorm 0.716 | train_wall 1193 | wall 0
2021-01-10 04:34:46 | INFO | fairseq.trainer | begin training epoch 42
2021-01-10 04:34:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:35:11 | INFO | train_inner | epoch 042:     12 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.014, nll_loss=0.808, ppl=1.75, wps=4579, ups=0.44, wpb=10438.2, bsz=348.2, num_updates=22900, lr=1.44778e-05, gnorm=0.723, train_wall=200, wall=0
2021-01-10 04:38:20 | INFO | train_inner | epoch 042:    112 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.804, ppl=1.75, wps=5587.1, ups=0.53, wpb=10548.5, bsz=357.4, num_updates=23000, lr=1.44463e-05, gnorm=0.709, train_wall=189, wall=0
2021-01-10 04:41:28 | INFO | train_inner | epoch 042:    212 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.81, ppl=1.75, wps=5556.6, ups=0.53, wpb=10446, bsz=366.7, num_updates=23100, lr=1.4415e-05, gnorm=0.721, train_wall=188, wall=0
2021-01-10 04:44:37 | INFO | train_inner | epoch 042:    312 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.801, ppl=1.74, wps=5572.8, ups=0.53, wpb=10522.8, bsz=385, num_updates=23200, lr=1.43839e-05, gnorm=0.709, train_wall=189, wall=0
2021-01-10 04:47:44 | INFO | train_inner | epoch 042:    412 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.809, ppl=1.75, wps=5646.7, ups=0.53, wpb=10604.4, bsz=380.1, num_updates=23300, lr=1.4353e-05, gnorm=0.711, train_wall=188, wall=0
2021-01-10 04:50:53 | INFO | train_inner | epoch 042:    512 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.022, nll_loss=0.81, ppl=1.75, wps=5548.2, ups=0.53, wpb=10436.5, bsz=366.8, num_updates=23400, lr=1.43223e-05, gnorm=0.719, train_wall=188, wall=0
2021-01-10 04:52:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 04:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 04:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 04:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 04:52:46 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.723 | nll_loss 4.2 | ppl 18.38 | bleu 21.99 | wps 4624.5 | wpb 7508.5 | bsz 272.7 | num_updates 23449 | best_bleu 22.43
2021-01-10 04:52:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 04:52:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:52:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:52:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 42 @ 23449 updates, score 21.99) (writing took 2.8716629799455404 seconds)
2021-01-10 04:52:49 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-10 04:52:49 | INFO | train | epoch 042 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.015 | nll_loss 0.808 | ppl 1.75 | wps 5430.6 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 23449 | lr 1.43073e-05 | gnorm 0.716 | train_wall 1055 | wall 0
2021-01-10 04:52:49 | INFO | fairseq.trainer | begin training epoch 43
2021-01-10 04:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 04:52:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 04:54:27 | INFO | train_inner | epoch 043:     51 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.807, ppl=1.75, wps=4809.4, ups=0.47, wpb=10327.8, bsz=360.6, num_updates=23500, lr=1.42918e-05, gnorm=0.725, train_wall=187, wall=0
2021-01-10 04:57:37 | INFO | train_inner | epoch 043:    151 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.807, ppl=1.75, wps=5550.1, ups=0.53, wpb=10511.9, bsz=373.1, num_updates=23600, lr=1.42615e-05, gnorm=0.713, train_wall=189, wall=0
2021-01-10 05:00:45 | INFO | train_inner | epoch 043:    251 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.018, nll_loss=0.807, ppl=1.75, wps=5534.7, ups=0.53, wpb=10419.5, bsz=361.4, num_updates=23700, lr=1.42314e-05, gnorm=0.722, train_wall=188, wall=0
2021-01-10 05:03:54 | INFO | train_inner | epoch 043:    351 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.814, ppl=1.76, wps=5579, ups=0.53, wpb=10573.6, bsz=364.6, num_updates=23800, lr=1.42014e-05, gnorm=0.716, train_wall=189, wall=0
2021-01-10 05:07:03 | INFO | train_inner | epoch 043:    451 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.807, ppl=1.75, wps=5549.7, ups=0.53, wpb=10463.4, bsz=385.4, num_updates=23900, lr=1.41717e-05, gnorm=0.714, train_wall=188, wall=0
2021-01-10 05:10:12 | INFO | train_inner | epoch 043:    551 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.803, ppl=1.74, wps=5603.2, ups=0.53, wpb=10580, bsz=372.6, num_updates=24000, lr=1.41421e-05, gnorm=0.71, train_wall=189, wall=0
2021-01-10 05:10:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 05:10:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:10:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:10:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:10:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:10:53 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.721 | nll_loss 4.199 | ppl 18.37 | bleu 22.04 | wps 4287.4 | wpb 7508.5 | bsz 272.7 | num_updates 24010 | best_bleu 22.43
2021-01-10 05:10:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 05:10:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:10:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:10:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 43 @ 24010 updates, score 22.04) (writing took 2.9114475529640913 seconds)
2021-01-10 05:10:56 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-10 05:10:56 | INFO | train | epoch 043 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.014 | nll_loss 0.807 | ppl 1.75 | wps 5410.2 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 24010 | lr 1.41392e-05 | gnorm 0.717 | train_wall 1057 | wall 0
2021-01-10 05:10:56 | INFO | fairseq.trainer | begin training epoch 44
2021-01-10 05:10:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:10:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:13:49 | INFO | train_inner | epoch 044:     90 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.804, ppl=1.75, wps=4799.8, ups=0.46, wpb=10409.1, bsz=368.6, num_updates=24100, lr=1.41128e-05, gnorm=0.727, train_wall=188, wall=0
2021-01-10 05:16:57 | INFO | train_inner | epoch 044:    190 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.798, ppl=1.74, wps=5564.7, ups=0.53, wpb=10493.4, bsz=372.1, num_updates=24200, lr=1.40836e-05, gnorm=0.707, train_wall=188, wall=0
2021-01-10 05:20:06 | INFO | train_inner | epoch 044:    290 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.797, ppl=1.74, wps=5562.6, ups=0.53, wpb=10499.2, bsz=374.5, num_updates=24300, lr=1.40546e-05, gnorm=0.707, train_wall=189, wall=0
2021-01-10 05:23:15 | INFO | train_inner | epoch 044:    390 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.805, ppl=1.75, wps=5557.3, ups=0.53, wpb=10514, bsz=384.4, num_updates=24400, lr=1.40257e-05, gnorm=0.71, train_wall=189, wall=0
2021-01-10 05:26:22 | INFO | train_inner | epoch 044:    490 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.019, nll_loss=0.815, ppl=1.76, wps=5583.1, ups=0.53, wpb=10453.5, bsz=351.7, num_updates=24500, lr=1.39971e-05, gnorm=0.726, train_wall=187, wall=0
2021-01-10 05:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:28:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:28:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:28:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:28:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:28:57 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.721 | nll_loss 4.198 | ppl 18.35 | bleu 22.04 | wps 4663.3 | wpb 7508.5 | bsz 272.7 | num_updates 24571 | best_bleu 22.43
2021-01-10 05:28:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 05:28:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:28:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:28:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:28:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:29:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 44 @ 24571 updates, score 22.04) (writing took 2.7418088652193546 seconds)
2021-01-10 05:29:00 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-10 05:29:00 | INFO | train | epoch 044 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.011 | nll_loss 0.805 | ppl 1.75 | wps 5426 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 24571 | lr 1.39768e-05 | gnorm 0.714 | train_wall 1056 | wall 0
2021-01-10 05:29:00 | INFO | fairseq.trainer | begin training epoch 45
2021-01-10 05:29:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:29:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:29:58 | INFO | train_inner | epoch 045:     29 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.014, nll_loss=0.81, ppl=1.75, wps=4871.6, ups=0.46, wpb=10481, bsz=358.2, num_updates=24600, lr=1.39686e-05, gnorm=0.722, train_wall=188, wall=0
2021-01-10 05:33:06 | INFO | train_inner | epoch 045:    129 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.804, ppl=1.75, wps=5512.3, ups=0.53, wpb=10371, bsz=375.4, num_updates=24700, lr=1.39403e-05, gnorm=0.717, train_wall=188, wall=0
2021-01-10 05:36:14 | INFO | train_inner | epoch 045:    229 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.803, ppl=1.74, wps=5612, ups=0.53, wpb=10544.9, bsz=384.6, num_updates=24800, lr=1.39122e-05, gnorm=0.711, train_wall=188, wall=0
2021-01-10 05:39:21 | INFO | train_inner | epoch 045:    329 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.811, ppl=1.75, wps=5561.3, ups=0.53, wpb=10437.9, bsz=358.3, num_updates=24900, lr=1.38842e-05, gnorm=0.722, train_wall=187, wall=0
2021-01-10 05:42:30 | INFO | train_inner | epoch 045:    429 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.8, ppl=1.74, wps=5585.4, ups=0.53, wpb=10564, bsz=373.2, num_updates=25000, lr=1.38564e-05, gnorm=0.714, train_wall=189, wall=0
2021-01-10 05:45:39 | INFO | train_inner | epoch 045:    529 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.806, ppl=1.75, wps=5593.1, ups=0.53, wpb=10558.9, bsz=362.4, num_updates=25100, lr=1.38288e-05, gnorm=0.713, train_wall=189, wall=0
2021-01-10 05:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 05:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:46:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:46:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 05:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 05:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 05:46:59 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.725 | nll_loss 4.203 | ppl 18.42 | bleu 21.92 | wps 4630.4 | wpb 7508.5 | bsz 272.7 | num_updates 25132 | best_bleu 22.43
2021-01-10 05:46:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 05:47:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:47:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:47:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:47:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:47:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 45 @ 25132 updates, score 21.92) (writing took 2.9301395062357187 seconds)
2021-01-10 05:47:02 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-10 05:47:02 | INFO | train | epoch 045 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.011 | nll_loss 0.805 | ppl 1.75 | wps 5431.3 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 25132 | lr 1.382e-05 | gnorm 0.717 | train_wall 1054 | wall 0
2021-01-10 05:47:02 | INFO | fairseq.trainer | begin training epoch 46
2021-01-10 05:47:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 05:47:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 05:49:12 | INFO | train_inner | epoch 046:     68 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.797, ppl=1.74, wps=4925.9, ups=0.47, wpb=10470.4, bsz=364.8, num_updates=25200, lr=1.38013e-05, gnorm=0.717, train_wall=185, wall=0
2021-01-10 05:52:18 | INFO | train_inner | epoch 046:    168 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.032, nll_loss=0.811, ppl=1.76, wps=5550.1, ups=0.54, wpb=10340.1, bsz=379.1, num_updates=25300, lr=1.3774e-05, gnorm=0.724, train_wall=186, wall=0
2021-01-10 05:55:25 | INFO | train_inner | epoch 046:    268 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.797, ppl=1.74, wps=5652, ups=0.53, wpb=10588.7, bsz=379.8, num_updates=25400, lr=1.37469e-05, gnorm=0.704, train_wall=187, wall=0
2021-01-10 05:58:32 | INFO | train_inner | epoch 046:    368 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.809, ppl=1.75, wps=5659, ups=0.54, wpb=10569.7, bsz=362.7, num_updates=25500, lr=1.37199e-05, gnorm=0.716, train_wall=187, wall=0
2021-01-10 06:01:39 | INFO | train_inner | epoch 046:    468 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.801, ppl=1.74, wps=5632.2, ups=0.54, wpb=10496.4, bsz=362.8, num_updates=25600, lr=1.36931e-05, gnorm=0.716, train_wall=186, wall=0
2021-01-10 06:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 06:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:04:52 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.728 | nll_loss 4.207 | ppl 18.47 | bleu 21.95 | wps 4600.1 | wpb 7508.5 | bsz 272.7 | num_updates 25693 | best_bleu 22.43
2021-01-10 06:04:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 06:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:04:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 46 @ 25693 updates, score 21.95) (writing took 2.713302880525589 seconds)
2021-01-10 06:04:55 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-10 06:04:55 | INFO | train | epoch 046 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.01 | nll_loss 0.804 | ppl 1.75 | wps 5485.1 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 25693 | lr 1.36683e-05 | gnorm 0.716 | train_wall 1044 | wall 0
2021-01-10 06:04:55 | INFO | fairseq.trainer | begin training epoch 47
2021-01-10 06:04:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:04:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:05:11 | INFO | train_inner | epoch 047:      7 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.016, nll_loss=0.808, ppl=1.75, wps=4891, ups=0.47, wpb=10366.9, bsz=369.1, num_updates=25700, lr=1.36664e-05, gnorm=0.724, train_wall=185, wall=0
2021-01-10 06:08:17 | INFO | train_inner | epoch 047:    107 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.999, nll_loss=0.801, ppl=1.74, wps=5666.2, ups=0.53, wpb=10592.2, bsz=361.8, num_updates=25800, lr=1.36399e-05, gnorm=0.711, train_wall=187, wall=0
2021-01-10 06:11:25 | INFO | train_inner | epoch 047:    207 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.013, nll_loss=0.802, ppl=1.74, wps=5599.6, ups=0.53, wpb=10489.8, bsz=363.2, num_updates=25900, lr=1.36135e-05, gnorm=0.716, train_wall=187, wall=0
2021-01-10 06:14:32 | INFO | train_inner | epoch 047:    307 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.013, nll_loss=0.804, ppl=1.75, wps=5608.2, ups=0.53, wpb=10510.8, bsz=373.4, num_updates=26000, lr=1.35873e-05, gnorm=0.713, train_wall=187, wall=0
2021-01-10 06:17:39 | INFO | train_inner | epoch 047:    407 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.8, ppl=1.74, wps=5612.1, ups=0.54, wpb=10458.7, bsz=369.4, num_updates=26100, lr=1.35613e-05, gnorm=0.713, train_wall=186, wall=0
2021-01-10 06:20:45 | INFO | train_inner | epoch 047:    507 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.802, ppl=1.74, wps=5610.8, ups=0.54, wpb=10444.1, bsz=389.2, num_updates=26200, lr=1.35354e-05, gnorm=0.713, train_wall=186, wall=0
2021-01-10 06:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 06:22:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:22:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:22:46 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.73 | nll_loss 4.208 | ppl 18.49 | bleu 21.86 | wps 4560.4 | wpb 7508.5 | bsz 272.7 | num_updates 26254 | best_bleu 22.43
2021-01-10 06:22:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 06:22:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:22:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:22:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 47 @ 26254 updates, score 21.86) (writing took 2.9005997609347105 seconds)
2021-01-10 06:22:49 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-10 06:22:49 | INFO | train | epoch 047 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.009 | nll_loss 0.803 | ppl 1.74 | wps 5472.6 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 26254 | lr 1.35214e-05 | gnorm 0.716 | train_wall 1046 | wall 0
2021-01-10 06:22:49 | INFO | fairseq.trainer | begin training epoch 48
2021-01-10 06:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:24:18 | INFO | train_inner | epoch 048:     46 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.803, ppl=1.74, wps=4873.5, ups=0.47, wpb=10396.4, bsz=359.4, num_updates=26300, lr=1.35096e-05, gnorm=0.729, train_wall=186, wall=0
2021-01-10 06:27:24 | INFO | train_inner | epoch 048:    146 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.01, nll_loss=0.805, ppl=1.75, wps=5640.7, ups=0.54, wpb=10493.4, bsz=376.1, num_updates=26400, lr=1.3484e-05, gnorm=0.718, train_wall=186, wall=0
2021-01-10 06:30:31 | INFO | train_inner | epoch 048:    246 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.016, nll_loss=0.806, ppl=1.75, wps=5631.9, ups=0.54, wpb=10507.3, bsz=357.6, num_updates=26500, lr=1.34585e-05, gnorm=0.715, train_wall=186, wall=0
2021-01-10 06:33:36 | INFO | train_inner | epoch 048:    346 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.803, ppl=1.74, wps=5611.8, ups=0.54, wpb=10407.9, bsz=359.6, num_updates=26600, lr=1.34332e-05, gnorm=0.719, train_wall=185, wall=0
2021-01-10 06:36:43 | INFO | train_inner | epoch 048:    446 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.801, ppl=1.74, wps=5683.2, ups=0.54, wpb=10613.5, bsz=380.4, num_updates=26700, lr=1.3408e-05, gnorm=0.708, train_wall=187, wall=0
2021-01-10 06:39:50 | INFO | train_inner | epoch 048:    546 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.803, ppl=1.74, wps=5565.9, ups=0.53, wpb=10438.3, bsz=379.8, num_updates=26800, lr=1.3383e-05, gnorm=0.718, train_wall=187, wall=0
2021-01-10 06:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 06:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:40:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:40:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:40:39 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.731 | nll_loss 4.211 | ppl 18.52 | bleu 22.01 | wps 4660.1 | wpb 7508.5 | bsz 272.7 | num_updates 26815 | best_bleu 22.43
2021-01-10 06:40:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 06:40:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:40:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:40:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 48 @ 26815 updates, score 22.01) (writing took 2.778381757438183 seconds)
2021-01-10 06:40:42 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-10 06:40:42 | INFO | train | epoch 048 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.009 | nll_loss 0.802 | ppl 1.74 | wps 5484.1 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 26815 | lr 1.33792e-05 | gnorm 0.715 | train_wall 1044 | wall 0
2021-01-10 06:40:42 | INFO | fairseq.trainer | begin training epoch 49
2021-01-10 06:40:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:40:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:43:23 | INFO | train_inner | epoch 049:     85 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.794, ppl=1.73, wps=4904.5, ups=0.47, wpb=10444.1, bsz=373.3, num_updates=26900, lr=1.33581e-05, gnorm=0.716, train_wall=186, wall=0
2021-01-10 06:46:30 | INFO | train_inner | epoch 049:    185 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.802, ppl=1.74, wps=5617.1, ups=0.54, wpb=10459.2, bsz=377.4, num_updates=27000, lr=1.33333e-05, gnorm=0.716, train_wall=186, wall=0
2021-01-10 06:49:36 | INFO | train_inner | epoch 049:    285 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.808, ppl=1.75, wps=5602, ups=0.54, wpb=10450, bsz=354.2, num_updates=27100, lr=1.33087e-05, gnorm=0.724, train_wall=186, wall=0
2021-01-10 06:52:42 | INFO | train_inner | epoch 049:    385 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.798, ppl=1.74, wps=5612.5, ups=0.54, wpb=10443, bsz=370.4, num_updates=27200, lr=1.32842e-05, gnorm=0.715, train_wall=186, wall=0
2021-01-10 06:55:49 | INFO | train_inner | epoch 049:    485 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3, nll_loss=0.8, ppl=1.74, wps=5648.1, ups=0.54, wpb=10536.7, bsz=365.2, num_updates=27300, lr=1.32599e-05, gnorm=0.71, train_wall=186, wall=0
2021-01-10 06:58:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 06:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:58:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:58:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 06:58:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 06:58:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 06:58:35 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.731 | nll_loss 4.21 | ppl 18.5 | bleu 21.87 | wps 4027.7 | wpb 7508.5 | bsz 272.7 | num_updates 27376 | best_bleu 22.43
2021-01-10 06:58:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 06:58:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:58:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:58:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 49 @ 27376 updates, score 21.87) (writing took 2.7475375831127167 seconds)
2021-01-10 06:58:38 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-10 06:58:38 | INFO | train | epoch 049 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.008 | nll_loss 0.802 | ppl 1.74 | wps 5463.1 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 27376 | lr 1.32415e-05 | gnorm 0.716 | train_wall 1046 | wall 0
2021-01-10 06:58:38 | INFO | fairseq.trainer | begin training epoch 50
2021-01-10 06:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 06:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 06:59:27 | INFO | train_inner | epoch 050:     24 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.805, ppl=1.75, wps=4827, ups=0.46, wpb=10523.4, bsz=378, num_updates=27400, lr=1.32357e-05, gnorm=0.716, train_wall=188, wall=0
2021-01-10 07:02:35 | INFO | train_inner | epoch 050:    124 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.801, ppl=1.74, wps=5599.6, ups=0.53, wpb=10522.1, bsz=357.4, num_updates=27500, lr=1.32116e-05, gnorm=0.718, train_wall=188, wall=0
2021-01-10 07:05:43 | INFO | train_inner | epoch 050:    224 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.798, ppl=1.74, wps=5527.5, ups=0.53, wpb=10400.1, bsz=356.2, num_updates=27600, lr=1.31876e-05, gnorm=0.717, train_wall=188, wall=0
2021-01-10 07:08:53 | INFO | train_inner | epoch 050:    324 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3, nll_loss=0.799, ppl=1.74, wps=5591.8, ups=0.53, wpb=10609, bsz=387.3, num_updates=27700, lr=1.31638e-05, gnorm=0.706, train_wall=190, wall=0
2021-01-10 07:12:00 | INFO | train_inner | epoch 050:    424 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.799, ppl=1.74, wps=5577.7, ups=0.53, wpb=10453.3, bsz=375.4, num_updates=27800, lr=1.31401e-05, gnorm=0.713, train_wall=187, wall=0
2021-01-10 07:15:09 | INFO | train_inner | epoch 050:    524 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.806, ppl=1.75, wps=5551.7, ups=0.53, wpb=10522.6, bsz=377.2, num_updates=27900, lr=1.31165e-05, gnorm=0.715, train_wall=189, wall=0
2021-01-10 07:16:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 07:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:16:40 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.729 | nll_loss 4.209 | ppl 18.49 | bleu 21.94 | wps 4661.3 | wpb 7508.5 | bsz 272.7 | num_updates 27937 | best_bleu 22.43
2021-01-10 07:16:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 07:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:16:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 50 @ 27937 updates, score 21.94) (writing took 2.7190693765878677 seconds)
2021-01-10 07:16:43 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-10 07:16:43 | INFO | train | epoch 050 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.008 | nll_loss 0.802 | ppl 1.74 | wps 5421.7 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 27937 | lr 1.31078e-05 | gnorm 0.716 | train_wall 1057 | wall 0
2021-01-10 07:16:43 | INFO | fairseq.trainer | begin training epoch 51
2021-01-10 07:16:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:16:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:18:45 | INFO | train_inner | epoch 051:     63 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.03, nll_loss=0.811, ppl=1.75, wps=4789, ups=0.46, wpb=10341.3, bsz=345.5, num_updates=28000, lr=1.30931e-05, gnorm=0.736, train_wall=189, wall=0
2021-01-10 07:21:54 | INFO | train_inner | epoch 051:    163 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.797, ppl=1.74, wps=5574.9, ups=0.53, wpb=10529.4, bsz=381.1, num_updates=28100, lr=1.30698e-05, gnorm=0.71, train_wall=189, wall=0
2021-01-10 07:25:02 | INFO | train_inner | epoch 051:    263 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.805, ppl=1.75, wps=5591.8, ups=0.53, wpb=10489.5, bsz=380.4, num_updates=28200, lr=1.30466e-05, gnorm=0.717, train_wall=187, wall=0
2021-01-10 07:28:09 | INFO | train_inner | epoch 051:    363 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.803, ppl=1.75, wps=5572.3, ups=0.53, wpb=10440.1, bsz=365.6, num_updates=28300, lr=1.30235e-05, gnorm=0.72, train_wall=187, wall=0
2021-01-10 07:31:18 | INFO | train_inner | epoch 051:    463 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.794, ppl=1.73, wps=5593.4, ups=0.53, wpb=10534.9, bsz=360, num_updates=28400, lr=1.30005e-05, gnorm=0.711, train_wall=188, wall=0
2021-01-10 07:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 07:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:34:42 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.732 | nll_loss 4.211 | ppl 18.52 | bleu 22.11 | wps 4619 | wpb 7508.5 | bsz 272.7 | num_updates 28498 | best_bleu 22.43
2021-01-10 07:34:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 07:34:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 51 @ 28498 updates, score 22.11) (writing took 2.9549833107739687 seconds)
2021-01-10 07:34:45 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-10 07:34:45 | INFO | train | epoch 051 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.007 | nll_loss 0.8 | ppl 1.74 | wps 5436.8 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 28498 | lr 1.29782e-05 | gnorm 0.716 | train_wall 1053 | wall 0
2021-01-10 07:34:45 | INFO | fairseq.trainer | begin training epoch 52
2021-01-10 07:34:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:34:52 | INFO | train_inner | epoch 052:      2 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.797, ppl=1.74, wps=4898.8, ups=0.47, wpb=10490.3, bsz=374.2, num_updates=28500, lr=1.29777e-05, gnorm=0.712, train_wall=187, wall=0
2021-01-10 07:38:02 | INFO | train_inner | epoch 052:    102 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.798, ppl=1.74, wps=5554.5, ups=0.53, wpb=10571.1, bsz=366.6, num_updates=28600, lr=1.2955e-05, gnorm=0.712, train_wall=190, wall=0
2021-01-10 07:41:17 | INFO | train_inner | epoch 052:    202 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.801, ppl=1.74, wps=5386.9, ups=0.51, wpb=10522, bsz=372.1, num_updates=28700, lr=1.29324e-05, gnorm=0.713, train_wall=195, wall=0
2021-01-10 07:44:40 | INFO | train_inner | epoch 052:    302 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.02, nll_loss=0.81, ppl=1.75, wps=5128.6, ups=0.49, wpb=10407.2, bsz=344.3, num_updates=28800, lr=1.29099e-05, gnorm=0.726, train_wall=203, wall=0
2021-01-10 07:48:02 | INFO | train_inner | epoch 052:    402 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.788, ppl=1.73, wps=5178.1, ups=0.5, wpb=10460.8, bsz=376.8, num_updates=28900, lr=1.28876e-05, gnorm=0.707, train_wall=202, wall=0
2021-01-10 07:51:24 | INFO | train_inner | epoch 052:    502 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.806, ppl=1.75, wps=5183.5, ups=0.5, wpb=10430, bsz=369.4, num_updates=29000, lr=1.28654e-05, gnorm=0.725, train_wall=201, wall=0
2021-01-10 07:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 07:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:53:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:53:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:53:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 07:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 07:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 07:53:45 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.729 | nll_loss 4.21 | ppl 18.51 | bleu 21.97 | wps 4126.8 | wpb 7508.5 | bsz 272.7 | num_updates 29059 | best_bleu 22.43
2021-01-10 07:53:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 07:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:53:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:53:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 52 @ 29059 updates, score 21.97) (writing took 2.908403802663088 seconds)
2021-01-10 07:53:48 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-10 07:53:48 | INFO | train | epoch 052 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.006 | nll_loss 0.799 | ppl 1.74 | wps 5145.3 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 29059 | lr 1.28523e-05 | gnorm 0.716 | train_wall 1113 | wall 0
2021-01-10 07:53:48 | INFO | fairseq.trainer | begin training epoch 53
2021-01-10 07:53:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 07:53:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 07:55:13 | INFO | train_inner | epoch 053:     41 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.792, ppl=1.73, wps=4595.4, ups=0.44, wpb=10542.5, bsz=387.8, num_updates=29100, lr=1.28432e-05, gnorm=0.711, train_wall=200, wall=0
2021-01-10 07:58:33 | INFO | train_inner | epoch 053:    141 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.801, ppl=1.74, wps=5272.4, ups=0.5, wpb=10547.4, bsz=367.4, num_updates=29200, lr=1.28212e-05, gnorm=0.713, train_wall=200, wall=0
2021-01-10 08:01:51 | INFO | train_inner | epoch 053:    241 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.797, ppl=1.74, wps=5250.4, ups=0.51, wpb=10395, bsz=359.9, num_updates=29300, lr=1.27993e-05, gnorm=0.717, train_wall=198, wall=0
2021-01-10 08:05:12 | INFO | train_inner | epoch 053:    341 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.794, ppl=1.73, wps=5265.9, ups=0.5, wpb=10572.9, bsz=373.4, num_updates=29400, lr=1.27775e-05, gnorm=0.705, train_wall=201, wall=0
2021-01-10 08:08:31 | INFO | train_inner | epoch 053:    441 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.794, ppl=1.73, wps=5249, ups=0.5, wpb=10437.3, bsz=388.6, num_updates=29500, lr=1.27559e-05, gnorm=0.716, train_wall=199, wall=0
2021-01-10 08:11:51 | INFO | train_inner | epoch 053:    541 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.017, nll_loss=0.805, ppl=1.75, wps=5236.5, ups=0.5, wpb=10479.1, bsz=361.1, num_updates=29600, lr=1.27343e-05, gnorm=0.72, train_wall=200, wall=0
2021-01-10 08:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 08:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:12:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:12:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:12:54 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.732 | nll_loss 4.211 | ppl 18.52 | bleu 21.96 | wps 4078.4 | wpb 7508.5 | bsz 272.7 | num_updates 29620 | best_bleu 22.43
2021-01-10 08:12:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 08:12:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:12:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:12:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 53 @ 29620 updates, score 21.96) (writing took 2.922518352046609 seconds)
2021-01-10 08:12:57 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-10 08:12:57 | INFO | train | epoch 053 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.005 | nll_loss 0.799 | ppl 1.74 | wps 5118.8 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 29620 | lr 1.273e-05 | gnorm 0.715 | train_wall 1118 | wall 0
2021-01-10 08:12:57 | INFO | fairseq.trainer | begin training epoch 54
2021-01-10 08:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:12:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:15:40 | INFO | train_inner | epoch 054:     80 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3, nll_loss=0.797, ppl=1.74, wps=4565.6, ups=0.44, wpb=10469.5, bsz=370.6, num_updates=29700, lr=1.27128e-05, gnorm=0.716, train_wall=200, wall=0
2021-01-10 08:19:00 | INFO | train_inner | epoch 054:    180 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.798, ppl=1.74, wps=5269.1, ups=0.5, wpb=10517.5, bsz=363.1, num_updates=29800, lr=1.26915e-05, gnorm=0.717, train_wall=199, wall=0
2021-01-10 08:22:18 | INFO | train_inner | epoch 054:    280 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.014, nll_loss=0.803, ppl=1.75, wps=5290.6, ups=0.5, wpb=10506, bsz=375.4, num_updates=29900, lr=1.26702e-05, gnorm=0.716, train_wall=198, wall=0
2021-01-10 08:25:37 | INFO | train_inner | epoch 054:    380 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.793, ppl=1.73, wps=5323.8, ups=0.5, wpb=10585.1, bsz=376.6, num_updates=30000, lr=1.26491e-05, gnorm=0.705, train_wall=199, wall=0
2021-01-10 08:28:54 | INFO | train_inner | epoch 054:    480 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.8, ppl=1.74, wps=5290.5, ups=0.51, wpb=10432, bsz=366.7, num_updates=30100, lr=1.26281e-05, gnorm=0.721, train_wall=197, wall=0
2021-01-10 08:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 08:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:31:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:31:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:31:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:31:55 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.732 | nll_loss 4.212 | ppl 18.53 | bleu 21.99 | wps 4713.1 | wpb 7508.5 | bsz 272.7 | num_updates 30181 | best_bleu 22.43
2021-01-10 08:31:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 08:31:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:31:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:31:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 54 @ 30181 updates, score 21.99) (writing took 2.713105423375964 seconds)
2021-01-10 08:31:58 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-10 08:31:58 | INFO | train | epoch 054 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.004 | nll_loss 0.797 | ppl 1.74 | wps 5153.6 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 30181 | lr 1.26111e-05 | gnorm 0.715 | train_wall 1113 | wall 0
2021-01-10 08:31:58 | INFO | fairseq.trainer | begin training epoch 55
2021-01-10 08:31:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:32:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:32:38 | INFO | train_inner | epoch 055:     19 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.008, nll_loss=0.799, ppl=1.74, wps=4621.4, ups=0.45, wpb=10356.4, bsz=364.5, num_updates=30200, lr=1.26072e-05, gnorm=0.724, train_wall=197, wall=0
2021-01-10 08:35:57 | INFO | train_inner | epoch 055:    119 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.007, nll_loss=0.798, ppl=1.74, wps=5302.1, ups=0.5, wpb=10526.2, bsz=377.5, num_updates=30300, lr=1.25863e-05, gnorm=0.715, train_wall=198, wall=0
2021-01-10 08:39:16 | INFO | train_inner | epoch 055:    219 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.796, ppl=1.74, wps=5278.9, ups=0.5, wpb=10489, bsz=383, num_updates=30400, lr=1.25656e-05, gnorm=0.713, train_wall=199, wall=0
2021-01-10 08:42:34 | INFO | train_inner | epoch 055:    319 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.797, ppl=1.74, wps=5277.2, ups=0.5, wpb=10482.3, bsz=355.8, num_updates=30500, lr=1.2545e-05, gnorm=0.717, train_wall=198, wall=0
2021-01-10 08:45:53 | INFO | train_inner | epoch 055:    419 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.795, ppl=1.74, wps=5325.9, ups=0.5, wpb=10585.5, bsz=363.8, num_updates=30600, lr=1.25245e-05, gnorm=0.711, train_wall=199, wall=0
2021-01-10 08:49:10 | INFO | train_inner | epoch 055:    519 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.795, ppl=1.74, wps=5273.3, ups=0.51, wpb=10403.6, bsz=372.8, num_updates=30700, lr=1.25041e-05, gnorm=0.717, train_wall=197, wall=0
2021-01-10 08:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 08:50:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:50:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 08:50:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 08:50:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 08:50:54 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.734 | nll_loss 4.213 | ppl 18.54 | bleu 22 | wps 4640.9 | wpb 7508.5 | bsz 272.7 | num_updates 30742 | best_bleu 22.43
2021-01-10 08:50:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 08:50:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:50:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:50:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:50:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:50:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 55 @ 30742 updates, score 22.0) (writing took 2.735821168869734 seconds)
2021-01-10 08:50:57 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-10 08:50:57 | INFO | train | epoch 055 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.003 | nll_loss 0.797 | ppl 1.74 | wps 5162 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 30742 | lr 1.24955e-05 | gnorm 0.717 | train_wall 1111 | wall 0
2021-01-10 08:50:57 | INFO | fairseq.trainer | begin training epoch 56
2021-01-10 08:50:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 08:51:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 08:52:56 | INFO | train_inner | epoch 056:     58 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.023, nll_loss=0.802, ppl=1.74, wps=4564.5, ups=0.44, wpb=10297.2, bsz=361.8, num_updates=30800, lr=1.24838e-05, gnorm=0.739, train_wall=198, wall=0
2021-01-10 08:56:15 | INFO | train_inner | epoch 056:    158 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.792, ppl=1.73, wps=5337.5, ups=0.5, wpb=10642.8, bsz=371.4, num_updates=30900, lr=1.24635e-05, gnorm=0.704, train_wall=199, wall=0
2021-01-10 08:59:34 | INFO | train_inner | epoch 056:    258 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.795, ppl=1.74, wps=5315.1, ups=0.5, wpb=10566, bsz=406.7, num_updates=31000, lr=1.24434e-05, gnorm=0.708, train_wall=199, wall=0
2021-01-10 09:02:51 | INFO | train_inner | epoch 056:    358 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.794, ppl=1.73, wps=5259.7, ups=0.51, wpb=10373.1, bsz=344.2, num_updates=31100, lr=1.24234e-05, gnorm=0.722, train_wall=197, wall=0
2021-01-10 09:06:09 | INFO | train_inner | epoch 056:    458 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.795, ppl=1.74, wps=5258, ups=0.51, wpb=10398.5, bsz=371.9, num_updates=31200, lr=1.24035e-05, gnorm=0.722, train_wall=198, wall=0
2021-01-10 09:09:27 | INFO | train_inner | epoch 056:    558 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.799, ppl=1.74, wps=5351, ups=0.51, wpb=10590.4, bsz=361.4, num_updates=31300, lr=1.23836e-05, gnorm=0.712, train_wall=198, wall=0
2021-01-10 09:09:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 09:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:09:53 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.733 | nll_loss 4.212 | ppl 18.54 | bleu 21.95 | wps 4656.5 | wpb 7508.5 | bsz 272.7 | num_updates 31303 | best_bleu 22.43
2021-01-10 09:09:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 09:09:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 56 @ 31303 updates, score 21.95) (writing took 2.737720465287566 seconds)
2021-01-10 09:09:56 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-10 09:09:56 | INFO | train | epoch 056 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.002 | nll_loss 0.796 | ppl 1.74 | wps 5163.6 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 31303 | lr 1.23831e-05 | gnorm 0.716 | train_wall 1111 | wall 0
2021-01-10 09:09:56 | INFO | fairseq.trainer | begin training epoch 57
2021-01-10 09:09:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:09:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:09:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:13:11 | INFO | train_inner | epoch 057:     97 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.012, nll_loss=0.798, ppl=1.74, wps=4601.9, ups=0.45, wpb=10333.1, bsz=382.5, num_updates=31400, lr=1.23639e-05, gnorm=0.718, train_wall=198, wall=0
2021-01-10 09:16:30 | INFO | train_inner | epoch 057:    197 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.797, ppl=1.74, wps=5302.5, ups=0.5, wpb=10533, bsz=364.6, num_updates=31500, lr=1.23443e-05, gnorm=0.713, train_wall=198, wall=0
2021-01-10 09:19:47 | INFO | train_inner | epoch 057:    297 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.791, ppl=1.73, wps=5287, ups=0.51, wpb=10420.6, bsz=354.4, num_updates=31600, lr=1.23247e-05, gnorm=0.72, train_wall=197, wall=0
2021-01-10 09:23:06 | INFO | train_inner | epoch 057:    397 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.793, ppl=1.73, wps=5304.3, ups=0.5, wpb=10524, bsz=375.5, num_updates=31700, lr=1.23053e-05, gnorm=0.709, train_wall=198, wall=0
2021-01-10 09:26:25 | INFO | train_inner | epoch 057:    497 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.015, nll_loss=0.799, ppl=1.74, wps=5261.3, ups=0.5, wpb=10509.5, bsz=367, num_updates=31800, lr=1.22859e-05, gnorm=0.716, train_wall=200, wall=0
2021-01-10 09:28:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 09:28:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:28:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:28:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:28:53 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.734 | nll_loss 4.214 | ppl 18.56 | bleu 21.98 | wps 4634.9 | wpb 7508.5 | bsz 272.7 | num_updates 31864 | best_bleu 22.43
2021-01-10 09:28:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 09:28:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:28:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:28:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 57 @ 31864 updates, score 21.98) (writing took 2.810012860223651 seconds)
2021-01-10 09:28:56 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-10 09:28:56 | INFO | train | epoch 057 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.001 | nll_loss 0.795 | ppl 1.74 | wps 5159.8 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 31864 | lr 1.22736e-05 | gnorm 0.714 | train_wall 1112 | wall 0
2021-01-10 09:28:56 | INFO | fairseq.trainer | begin training epoch 58
2021-01-10 09:28:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:28:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:30:11 | INFO | train_inner | epoch 058:     36 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=2.985, nll_loss=0.793, ppl=1.73, wps=4689.9, ups=0.44, wpb=10581.7, bsz=365, num_updates=31900, lr=1.22666e-05, gnorm=0.709, train_wall=198, wall=0
2021-01-10 09:33:32 | INFO | train_inner | epoch 058:    136 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.787, ppl=1.73, wps=5267, ups=0.5, wpb=10580.1, bsz=394.2, num_updates=32000, lr=1.22474e-05, gnorm=0.701, train_wall=201, wall=0
2021-01-10 09:36:51 | INFO | train_inner | epoch 058:    236 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.803, ppl=1.74, wps=5184.6, ups=0.5, wpb=10343.3, bsz=367.8, num_updates=32100, lr=1.22284e-05, gnorm=0.728, train_wall=199, wall=0
2021-01-10 09:40:12 | INFO | train_inner | epoch 058:    336 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.792, ppl=1.73, wps=5250.1, ups=0.5, wpb=10555.5, bsz=363.4, num_updates=32200, lr=1.22094e-05, gnorm=0.714, train_wall=201, wall=0
2021-01-10 09:43:31 | INFO | train_inner | epoch 058:    436 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.799, ppl=1.74, wps=5209.4, ups=0.5, wpb=10359.6, bsz=353.8, num_updates=32300, lr=1.21904e-05, gnorm=0.726, train_wall=199, wall=0
2021-01-10 09:46:52 | INFO | train_inner | epoch 058:    536 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.797, ppl=1.74, wps=5248.1, ups=0.5, wpb=10516.1, bsz=363.7, num_updates=32400, lr=1.21716e-05, gnorm=0.711, train_wall=200, wall=0
2021-01-10 09:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 09:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:47:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:47:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:47:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:47:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:47:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:48:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 09:48:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 09:48:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 09:48:03 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.736 | nll_loss 4.217 | ppl 18.6 | bleu 21.92 | wps 4495.4 | wpb 7508.5 | bsz 272.7 | num_updates 32425 | best_bleu 22.43
2021-01-10 09:48:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 09:48:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:48:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:48:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:48:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:48:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 58 @ 32425 updates, score 21.92) (writing took 2.910164887085557 seconds)
2021-01-10 09:48:06 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-10 09:48:06 | INFO | train | epoch 058 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.001 | nll_loss 0.795 | ppl 1.74 | wps 5112.6 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 32425 | lr 1.21669e-05 | gnorm 0.715 | train_wall 1121 | wall 0
2021-01-10 09:48:06 | INFO | fairseq.trainer | begin training epoch 59
2021-01-10 09:48:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 09:48:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 09:50:40 | INFO | train_inner | epoch 059:     75 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.799, ppl=1.74, wps=4594.6, ups=0.44, wpb=10481.2, bsz=369.7, num_updates=32500, lr=1.21529e-05, gnorm=0.718, train_wall=200, wall=0
2021-01-10 09:54:01 | INFO | train_inner | epoch 059:    175 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.786, ppl=1.72, wps=5227.1, ups=0.5, wpb=10504.3, bsz=377.3, num_updates=32600, lr=1.21342e-05, gnorm=0.712, train_wall=201, wall=0
2021-01-10 09:57:22 | INFO | train_inner | epoch 059:    275 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.794, ppl=1.73, wps=5192.4, ups=0.5, wpb=10437.3, bsz=361, num_updates=32700, lr=1.21157e-05, gnorm=0.714, train_wall=201, wall=0
2021-01-10 10:00:40 | INFO | train_inner | epoch 059:    375 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.01, nll_loss=0.8, ppl=1.74, wps=5247.1, ups=0.5, wpb=10418.1, bsz=353.6, num_updates=32800, lr=1.20972e-05, gnorm=0.722, train_wall=198, wall=0
2021-01-10 10:03:59 | INFO | train_inner | epoch 059:    475 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.792, ppl=1.73, wps=5334.5, ups=0.5, wpb=10620.8, bsz=395.9, num_updates=32900, lr=1.20788e-05, gnorm=0.705, train_wall=199, wall=0
2021-01-10 10:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 10:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:07:11 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.735 | nll_loss 4.216 | ppl 18.58 | bleu 21.87 | wps 4675.6 | wpb 7508.5 | bsz 272.7 | num_updates 32986 | best_bleu 22.43
2021-01-10 10:07:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 10:07:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:07:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:07:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 59 @ 32986 updates, score 21.87) (writing took 2.626413134858012 seconds)
2021-01-10 10:07:14 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-10 10:07:14 | INFO | train | epoch 059 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.002 | nll_loss 0.795 | ppl 1.74 | wps 5124.5 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 32986 | lr 1.2063e-05 | gnorm 0.715 | train_wall 1120 | wall 0
2021-01-10 10:07:14 | INFO | fairseq.trainer | begin training epoch 60
2021-01-10 10:07:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:07:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:07:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:07:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:07:45 | INFO | train_inner | epoch 060:     14 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.007, nll_loss=0.8, ppl=1.74, wps=4622.7, ups=0.44, wpb=10412.2, bsz=365.9, num_updates=33000, lr=1.20605e-05, gnorm=0.723, train_wall=198, wall=0
2021-01-10 10:11:03 | INFO | train_inner | epoch 060:    114 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.985, nll_loss=0.789, ppl=1.73, wps=5329.2, ups=0.5, wpb=10597.3, bsz=375.4, num_updates=33100, lr=1.20422e-05, gnorm=0.708, train_wall=199, wall=0
2021-01-10 10:14:23 | INFO | train_inner | epoch 060:    214 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.017, nll_loss=0.798, ppl=1.74, wps=5214.6, ups=0.5, wpb=10411.2, bsz=371.4, num_updates=33200, lr=1.20241e-05, gnorm=0.724, train_wall=199, wall=0
2021-01-10 10:17:41 | INFO | train_inner | epoch 060:    314 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.792, ppl=1.73, wps=5277.4, ups=0.5, wpb=10462, bsz=365.5, num_updates=33300, lr=1.2006e-05, gnorm=0.72, train_wall=198, wall=0
2021-01-10 10:20:58 | INFO | train_inner | epoch 060:    414 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3, nll_loss=0.795, ppl=1.73, wps=5339.2, ups=0.51, wpb=10522.6, bsz=369.7, num_updates=33400, lr=1.1988e-05, gnorm=0.714, train_wall=197, wall=0
2021-01-10 10:24:10 | INFO | train_inner | epoch 060:    514 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.01, nll_loss=0.801, ppl=1.74, wps=5457.3, ups=0.52, wpb=10456.9, bsz=368.9, num_updates=33500, lr=1.19701e-05, gnorm=0.719, train_wall=191, wall=0
2021-01-10 10:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 10:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:26:02 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.734 | nll_loss 4.215 | ppl 18.57 | bleu 21.93 | wps 4541.7 | wpb 7508.5 | bsz 272.7 | num_updates 33547 | best_bleu 22.43
2021-01-10 10:26:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 10:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:26:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 60 @ 33547 updates, score 21.93) (writing took 2.876703279092908 seconds)
2021-01-10 10:26:05 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-10 10:26:05 | INFO | train | epoch 060 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3 | nll_loss 0.794 | ppl 1.73 | wps 5199.8 | ups 0.5 | wpb 10483.4 | bsz 369.6 | num_updates 33547 | lr 1.19617e-05 | gnorm 0.719 | train_wall 1102 | wall 0
2021-01-10 10:26:05 | INFO | fairseq.trainer | begin training epoch 61
2021-01-10 10:26:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:26:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:27:50 | INFO | train_inner | epoch 061:     53 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.788, ppl=1.73, wps=4794.2, ups=0.45, wpb=10546.2, bsz=381.8, num_updates=33600, lr=1.19523e-05, gnorm=0.719, train_wall=192, wall=0
2021-01-10 10:31:02 | INFO | train_inner | epoch 061:    153 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.791, ppl=1.73, wps=5412.4, ups=0.52, wpb=10395.8, bsz=369.8, num_updates=33700, lr=1.19345e-05, gnorm=0.717, train_wall=192, wall=0
2021-01-10 10:34:13 | INFO | train_inner | epoch 061:    253 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.795, ppl=1.74, wps=5453.8, ups=0.52, wpb=10396.8, bsz=369.9, num_updates=33800, lr=1.19169e-05, gnorm=0.723, train_wall=190, wall=0
2021-01-10 10:37:25 | INFO | train_inner | epoch 061:    353 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.795, ppl=1.74, wps=5471.3, ups=0.52, wpb=10530.5, bsz=364.3, num_updates=33900, lr=1.18993e-05, gnorm=0.717, train_wall=192, wall=0
2021-01-10 10:40:37 | INFO | train_inner | epoch 061:    453 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.789, ppl=1.73, wps=5536.3, ups=0.52, wpb=10639.9, bsz=367.5, num_updates=34000, lr=1.18818e-05, gnorm=0.707, train_wall=192, wall=0
2021-01-10 10:43:50 | INFO | train_inner | epoch 061:    553 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.793, ppl=1.73, wps=5443.3, ups=0.52, wpb=10458.1, bsz=369.3, num_updates=34100, lr=1.18643e-05, gnorm=0.714, train_wall=192, wall=0
2021-01-10 10:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 10:44:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:44:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:44:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 10:44:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 10:44:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 10:44:26 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.734 | nll_loss 4.215 | ppl 18.58 | bleu 21.98 | wps 4665.8 | wpb 7508.5 | bsz 272.7 | num_updates 34108 | best_bleu 22.43
2021-01-10 10:44:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 10:44:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:44:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:44:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 61 @ 34108 updates, score 21.98) (writing took 2.7998592853546143 seconds)
2021-01-10 10:44:29 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-10 10:44:29 | INFO | train | epoch 061 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.999 | nll_loss 0.793 | ppl 1.73 | wps 5329.3 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 34108 | lr 1.18629e-05 | gnorm 0.716 | train_wall 1075 | wall 0
2021-01-10 10:44:29 | INFO | fairseq.trainer | begin training epoch 62
2021-01-10 10:44:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 10:44:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 10:47:28 | INFO | train_inner | epoch 062:     92 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3, nll_loss=0.792, ppl=1.73, wps=4771.3, ups=0.46, wpb=10412.2, bsz=372, num_updates=34200, lr=1.1847e-05, gnorm=0.722, train_wall=191, wall=0
2021-01-10 10:50:38 | INFO | train_inner | epoch 062:    192 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.789, ppl=1.73, wps=5518.3, ups=0.53, wpb=10485.9, bsz=368.6, num_updates=34300, lr=1.18297e-05, gnorm=0.714, train_wall=190, wall=0
2021-01-10 10:53:48 | INFO | train_inner | epoch 062:    292 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3, nll_loss=0.796, ppl=1.74, wps=5558.2, ups=0.53, wpb=10551.2, bsz=369.8, num_updates=34400, lr=1.18125e-05, gnorm=0.717, train_wall=190, wall=0
2021-01-10 10:56:58 | INFO | train_inner | epoch 062:    392 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.789, ppl=1.73, wps=5528.2, ups=0.53, wpb=10508.4, bsz=375, num_updates=34500, lr=1.17954e-05, gnorm=0.713, train_wall=190, wall=0
2021-01-10 11:00:07 | INFO | train_inner | epoch 062:    492 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.793, ppl=1.73, wps=5497.8, ups=0.53, wpb=10390.8, bsz=357, num_updates=34600, lr=1.17783e-05, gnorm=0.724, train_wall=189, wall=0
2021-01-10 11:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:02:38 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.736 | nll_loss 4.219 | ppl 18.62 | bleu 21.86 | wps 4687.5 | wpb 7508.5 | bsz 272.7 | num_updates 34669 | best_bleu 22.43
2021-01-10 11:02:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 11:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:02:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 62 @ 34669 updates, score 21.86) (writing took 2.881488734856248 seconds)
2021-01-10 11:02:41 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-10 11:02:41 | INFO | train | epoch 062 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.999 | nll_loss 0.792 | ppl 1.73 | wps 5383.2 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 34669 | lr 1.17666e-05 | gnorm 0.717 | train_wall 1064 | wall 0
2021-01-10 11:02:41 | INFO | fairseq.trainer | begin training epoch 63
2021-01-10 11:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:03:43 | INFO | train_inner | epoch 063:     31 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.999, nll_loss=0.792, ppl=1.73, wps=4835.9, ups=0.46, wpb=10448.2, bsz=366.5, num_updates=34700, lr=1.17613e-05, gnorm=0.721, train_wall=189, wall=0
2021-01-10 11:06:53 | INFO | train_inner | epoch 063:    131 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=2.98, nll_loss=0.79, ppl=1.73, wps=5629, ups=0.53, wpb=10718.4, bsz=359.9, num_updates=34800, lr=1.17444e-05, gnorm=0.708, train_wall=190, wall=0
2021-01-10 11:10:03 | INFO | train_inner | epoch 063:    231 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.793, ppl=1.73, wps=5520.1, ups=0.53, wpb=10464.9, bsz=379.1, num_updates=34900, lr=1.17276e-05, gnorm=0.72, train_wall=189, wall=0
2021-01-10 11:13:12 | INFO | train_inner | epoch 063:    331 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.79, ppl=1.73, wps=5478.1, ups=0.53, wpb=10383.8, bsz=385.7, num_updates=35000, lr=1.17108e-05, gnorm=0.721, train_wall=189, wall=0
2021-01-10 11:16:22 | INFO | train_inner | epoch 063:    431 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.796, ppl=1.74, wps=5521.7, ups=0.53, wpb=10472.6, bsz=367.6, num_updates=35100, lr=1.16941e-05, gnorm=0.718, train_wall=189, wall=0
2021-01-10 11:19:31 | INFO | train_inner | epoch 063:    531 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.013, nll_loss=0.801, ppl=1.74, wps=5526.1, ups=0.53, wpb=10462.8, bsz=373.4, num_updates=35200, lr=1.16775e-05, gnorm=0.724, train_wall=189, wall=0
2021-01-10 11:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 11:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:20:48 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.737 | nll_loss 4.216 | ppl 18.59 | bleu 21.92 | wps 4622.2 | wpb 7508.5 | bsz 272.7 | num_updates 35230 | best_bleu 22.43
2021-01-10 11:20:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 11:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:20:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:20:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 63 @ 35230 updates, score 21.92) (writing took 2.7278037909418344 seconds)
2021-01-10 11:20:51 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-10 11:20:51 | INFO | train | epoch 063 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.999 | nll_loss 0.792 | ppl 1.73 | wps 5395.6 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 35230 | lr 1.16725e-05 | gnorm 0.719 | train_wall 1062 | wall 0
2021-01-10 11:20:51 | INFO | fairseq.trainer | begin training epoch 64
2021-01-10 11:20:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:20:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:23:07 | INFO | train_inner | epoch 064:     70 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.782, ppl=1.72, wps=4872, ups=0.46, wpb=10507.1, bsz=367.5, num_updates=35300, lr=1.16609e-05, gnorm=0.71, train_wall=188, wall=0
2021-01-10 11:26:18 | INFO | train_inner | epoch 064:    170 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.794, ppl=1.73, wps=5485.8, ups=0.52, wpb=10471.4, bsz=365, num_updates=35400, lr=1.16445e-05, gnorm=0.715, train_wall=191, wall=0
2021-01-10 11:29:26 | INFO | train_inner | epoch 064:    270 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.788, ppl=1.73, wps=5561.7, ups=0.53, wpb=10480.5, bsz=372.5, num_updates=35500, lr=1.1628e-05, gnorm=0.712, train_wall=188, wall=0
2021-01-10 11:32:36 | INFO | train_inner | epoch 064:    370 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.794, ppl=1.73, wps=5588, ups=0.53, wpb=10586.4, bsz=363.7, num_updates=35600, lr=1.16117e-05, gnorm=0.715, train_wall=189, wall=0
2021-01-10 11:35:45 | INFO | train_inner | epoch 064:    470 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.791, ppl=1.73, wps=5502.3, ups=0.53, wpb=10422.7, bsz=363.8, num_updates=35700, lr=1.15954e-05, gnorm=0.721, train_wall=189, wall=0
2021-01-10 11:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 11:38:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:38:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:38:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:38:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:38:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:38:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:38:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:38:58 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.736 | nll_loss 4.216 | ppl 18.58 | bleu 21.96 | wps 4677.9 | wpb 7508.5 | bsz 272.7 | num_updates 35791 | best_bleu 22.43
2021-01-10 11:38:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 11:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 64 @ 35791 updates, score 21.96) (writing took 2.8751458935439587 seconds)
2021-01-10 11:39:01 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-10 11:39:01 | INFO | train | epoch 064 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.996 | nll_loss 0.791 | ppl 1.73 | wps 5396.3 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 35791 | lr 1.15807e-05 | gnorm 0.715 | train_wall 1062 | wall 0
2021-01-10 11:39:01 | INFO | fairseq.trainer | begin training epoch 65
2021-01-10 11:39:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:39:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:39:21 | INFO | train_inner | epoch 065:      9 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3, nll_loss=0.791, ppl=1.73, wps=4806, ups=0.46, wpb=10354.3, bsz=373.5, num_updates=35800, lr=1.15792e-05, gnorm=0.721, train_wall=188, wall=0
2021-01-10 11:42:34 | INFO | train_inner | epoch 065:    109 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.788, ppl=1.73, wps=5413.6, ups=0.52, wpb=10490.7, bsz=363.6, num_updates=35900, lr=1.15631e-05, gnorm=0.715, train_wall=194, wall=0
2021-01-10 11:45:56 | INFO | train_inner | epoch 065:    209 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.791, ppl=1.73, wps=5210.4, ups=0.5, wpb=10482.6, bsz=358.7, num_updates=36000, lr=1.1547e-05, gnorm=0.718, train_wall=201, wall=0
2021-01-10 11:49:16 | INFO | train_inner | epoch 065:    309 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.006, nll_loss=0.792, ppl=1.73, wps=5197.5, ups=0.5, wpb=10393.6, bsz=382.6, num_updates=36100, lr=1.1531e-05, gnorm=0.723, train_wall=200, wall=0
2021-01-10 11:52:38 | INFO | train_inner | epoch 065:    409 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.789, ppl=1.73, wps=5231.2, ups=0.49, wpb=10585.9, bsz=386.1, num_updates=36200, lr=1.15151e-05, gnorm=0.71, train_wall=202, wall=0
2021-01-10 11:55:52 | INFO | train_inner | epoch 065:    509 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.79, ppl=1.73, wps=5423.4, ups=0.52, wpb=10503.1, bsz=360.2, num_updates=36300, lr=1.14992e-05, gnorm=0.715, train_wall=193, wall=0
2021-01-10 11:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:57:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 11:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 11:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 11:57:50 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.735 | nll_loss 4.217 | ppl 18.59 | bleu 21.97 | wps 4744.9 | wpb 7508.5 | bsz 272.7 | num_updates 36352 | best_bleu 22.43
2021-01-10 11:57:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 11:57:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:57:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:57:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 65 @ 36352 updates, score 21.97) (writing took 2.7618204466998577 seconds)
2021-01-10 11:57:53 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-10 11:57:53 | INFO | train | epoch 065 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.997 | nll_loss 0.79 | ppl 1.73 | wps 5196.3 | ups 0.5 | wpb 10483.4 | bsz 369.6 | num_updates 36352 | lr 1.1491e-05 | gnorm 0.717 | train_wall 1104 | wall 0
2021-01-10 11:57:53 | INFO | fairseq.trainer | begin training epoch 66
2021-01-10 11:57:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 11:57:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 11:59:26 | INFO | train_inner | epoch 066:     48 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.795, ppl=1.74, wps=4847.3, ups=0.47, wpb=10413, bsz=376.7, num_updates=36400, lr=1.14834e-05, gnorm=0.723, train_wall=188, wall=0
2021-01-10 12:02:38 | INFO | train_inner | epoch 066:    148 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.783, ppl=1.72, wps=5512.4, ups=0.52, wpb=10543.7, bsz=371.4, num_updates=36500, lr=1.14676e-05, gnorm=0.713, train_wall=191, wall=0
2021-01-10 12:05:48 | INFO | train_inner | epoch 066:    248 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.999, nll_loss=0.792, ppl=1.73, wps=5483.6, ups=0.52, wpb=10454.1, bsz=377, num_updates=36600, lr=1.1452e-05, gnorm=0.717, train_wall=190, wall=0
2021-01-10 12:09:00 | INFO | train_inner | epoch 066:    348 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.974, nll_loss=0.778, ppl=1.72, wps=5565.3, ups=0.52, wpb=10644.1, bsz=361.9, num_updates=36700, lr=1.14364e-05, gnorm=0.708, train_wall=191, wall=0
2021-01-10 12:12:09 | INFO | train_inner | epoch 066:    448 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.002, nll_loss=0.796, ppl=1.74, wps=5551.9, ups=0.53, wpb=10494.2, bsz=369.5, num_updates=36800, lr=1.14208e-05, gnorm=0.719, train_wall=189, wall=0
2021-01-10 12:15:16 | INFO | train_inner | epoch 066:    548 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.796, ppl=1.74, wps=5582.3, ups=0.53, wpb=10436.9, bsz=366.5, num_updates=36900, lr=1.14053e-05, gnorm=0.721, train_wall=187, wall=0
2021-01-10 12:15:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 12:15:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:15:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:15:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:15:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:15:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:15:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:15:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:15:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:15:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:15:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:16:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:16:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:16:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:16:04 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.737 | nll_loss 4.218 | ppl 18.61 | bleu 21.97 | wps 3991.4 | wpb 7508.5 | bsz 272.7 | num_updates 36913 | best_bleu 22.43
2021-01-10 12:16:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 12:16:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:16:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:16:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:16:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:16:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 66 @ 36913 updates, score 21.97) (writing took 2.835245007649064 seconds)
2021-01-10 12:16:06 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-10 12:16:06 | INFO | train | epoch 066 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.997 | nll_loss 0.791 | ppl 1.73 | wps 5376.7 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 36913 | lr 1.14033e-05 | gnorm 0.718 | train_wall 1063 | wall 0
2021-01-10 12:16:06 | INFO | fairseq.trainer | begin training epoch 67
2021-01-10 12:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:18:53 | INFO | train_inner | epoch 067:     87 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.793, ppl=1.73, wps=4793, ups=0.46, wpb=10433.3, bsz=359, num_updates=37000, lr=1.13899e-05, gnorm=0.723, train_wall=187, wall=0
2021-01-10 12:22:13 | INFO | train_inner | epoch 067:    187 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.794, ppl=1.73, wps=5277.6, ups=0.5, wpb=10515.4, bsz=373.2, num_updates=37100, lr=1.13745e-05, gnorm=0.72, train_wall=199, wall=0
2021-01-10 12:25:30 | INFO | train_inner | epoch 067:    287 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.79, ppl=1.73, wps=5318.2, ups=0.51, wpb=10517.3, bsz=367.8, num_updates=37200, lr=1.13592e-05, gnorm=0.712, train_wall=198, wall=0
2021-01-10 12:28:52 | INFO | train_inner | epoch 067:    387 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.781, ppl=1.72, wps=5197.3, ups=0.5, wpb=10478.9, bsz=370.5, num_updates=37300, lr=1.1344e-05, gnorm=0.712, train_wall=201, wall=0
2021-01-10 12:32:11 | INFO | train_inner | epoch 067:    487 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.791, ppl=1.73, wps=5287.5, ups=0.5, wpb=10535.4, bsz=367.8, num_updates=37400, lr=1.13288e-05, gnorm=0.718, train_wall=199, wall=0
2021-01-10 12:34:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 12:34:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:34:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:34:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:34:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:34:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:34:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:34:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:34:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:34:57 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.735 | nll_loss 4.216 | ppl 18.58 | bleu 21.94 | wps 4711.9 | wpb 7508.5 | bsz 272.7 | num_updates 37474 | best_bleu 22.43
2021-01-10 12:34:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 12:34:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:34:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:35:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:35:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:35:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 67 @ 37474 updates, score 21.94) (writing took 2.754281848669052 seconds)
2021-01-10 12:35:00 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-10 12:35:00 | INFO | train | epoch 067 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.996 | nll_loss 0.79 | ppl 1.73 | wps 5189.1 | ups 0.49 | wpb 10483.4 | bsz 369.6 | num_updates 37474 | lr 1.13176e-05 | gnorm 0.717 | train_wall 1105 | wall 0
2021-01-10 12:35:00 | INFO | fairseq.trainer | begin training epoch 68
2021-01-10 12:35:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:35:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:35:58 | INFO | train_inner | epoch 068:     26 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.788, ppl=1.73, wps=4559.1, ups=0.44, wpb=10340.9, bsz=375.8, num_updates=37500, lr=1.13137e-05, gnorm=0.72, train_wall=200, wall=0
2021-01-10 12:39:15 | INFO | train_inner | epoch 068:    126 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.785, ppl=1.72, wps=5281.2, ups=0.51, wpb=10416.5, bsz=384.1, num_updates=37600, lr=1.12987e-05, gnorm=0.718, train_wall=197, wall=0
2021-01-10 12:42:35 | INFO | train_inner | epoch 068:    226 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.782, ppl=1.72, wps=5262.8, ups=0.5, wpb=10521.4, bsz=367.9, num_updates=37700, lr=1.12837e-05, gnorm=0.71, train_wall=200, wall=0
2021-01-10 12:45:59 | INFO | train_inner | epoch 068:    326 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.019, nll_loss=0.798, ppl=1.74, wps=5110.7, ups=0.49, wpb=10420, bsz=373.6, num_updates=37800, lr=1.12687e-05, gnorm=0.726, train_wall=204, wall=0
2021-01-10 12:49:30 | INFO | train_inner | epoch 068:    426 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.985, nll_loss=0.79, ppl=1.73, wps=5034.5, ups=0.47, wpb=10601.2, bsz=372.4, num_updates=37900, lr=1.12538e-05, gnorm=0.712, train_wall=210, wall=0
2021-01-10 12:53:07 | INFO | train_inner | epoch 068:    526 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.791, ppl=1.73, wps=4829.2, ups=0.46, wpb=10499.8, bsz=349, num_updates=38000, lr=1.1239e-05, gnorm=0.72, train_wall=217, wall=0
2021-01-10 12:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 12:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 12:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 12:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 12:54:38 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.743 | nll_loss 4.224 | ppl 18.68 | bleu 21.95 | wps 4636 | wpb 7508.5 | bsz 272.7 | num_updates 38035 | best_bleu 22.43
2021-01-10 12:54:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 12:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:54:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 68 @ 38035 updates, score 21.95) (writing took 2.8831176348030567 seconds)
2021-01-10 12:54:41 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-10 12:54:41 | INFO | train | epoch 068 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.995 | nll_loss 0.789 | ppl 1.73 | wps 4979.9 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 38035 | lr 1.12339e-05 | gnorm 0.717 | train_wall 1153 | wall 0
2021-01-10 12:54:41 | INFO | fairseq.trainer | begin training epoch 69
2021-01-10 12:54:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 12:54:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 12:56:47 | INFO | train_inner | epoch 069:     65 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3, nll_loss=0.79, ppl=1.73, wps=4724.4, ups=0.46, wpb=10382.6, bsz=366.3, num_updates=38100, lr=1.12243e-05, gnorm=0.726, train_wall=192, wall=0
2021-01-10 13:00:08 | INFO | train_inner | epoch 069:    165 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.787, ppl=1.73, wps=5216.3, ups=0.5, wpb=10491.1, bsz=372.8, num_updates=38200, lr=1.12096e-05, gnorm=0.713, train_wall=201, wall=0
2021-01-10 13:03:48 | INFO | train_inner | epoch 069:    265 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.777, ppl=1.71, wps=4827.9, ups=0.45, wpb=10635.9, bsz=382.6, num_updates=38300, lr=1.11949e-05, gnorm=0.706, train_wall=220, wall=0
2021-01-10 13:07:32 | INFO | train_inner | epoch 069:    365 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.792, ppl=1.73, wps=4714.7, ups=0.45, wpb=10550.5, bsz=369.6, num_updates=38400, lr=1.11803e-05, gnorm=0.718, train_wall=224, wall=0
2021-01-10 13:10:57 | INFO | train_inner | epoch 069:    465 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.788, ppl=1.73, wps=5082.5, ups=0.49, wpb=10422.1, bsz=358.6, num_updates=38500, lr=1.11658e-05, gnorm=0.722, train_wall=205, wall=0
2021-01-10 13:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:13:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:13:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:13:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:13:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:13:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:14:15 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.743 | nll_loss 4.224 | ppl 18.69 | bleu 21.84 | wps 4619 | wpb 7508.5 | bsz 272.7 | num_updates 38596 | best_bleu 22.43
2021-01-10 13:14:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 13:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:14:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:14:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:14:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 69 @ 38596 updates, score 21.84) (writing took 2.6792361680418253 seconds)
2021-01-10 13:14:18 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-10 13:14:18 | INFO | train | epoch 069 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.994 | nll_loss 0.787 | ppl 1.73 | wps 4996.7 | ups 0.48 | wpb 10483.4 | bsz 369.6 | num_updates 38596 | lr 1.11519e-05 | gnorm 0.718 | train_wall 1149 | wall 0
2021-01-10 13:14:18 | INFO | fairseq.trainer | begin training epoch 70
2021-01-10 13:14:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:14:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:14:28 | INFO | train_inner | epoch 070:      4 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.793, ppl=1.73, wps=4895.4, ups=0.47, wpb=10340.4, bsz=365.8, num_updates=38600, lr=1.11513e-05, gnorm=0.731, train_wall=184, wall=0
2021-01-10 13:17:35 | INFO | train_inner | epoch 070:    104 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.78, ppl=1.72, wps=5658.6, ups=0.53, wpb=10592.7, bsz=378.3, num_updates=38700, lr=1.11369e-05, gnorm=0.707, train_wall=187, wall=0
2021-01-10 13:20:46 | INFO | train_inner | epoch 070:    204 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.787, ppl=1.73, wps=5507.7, ups=0.53, wpb=10481.5, bsz=370.6, num_updates=38800, lr=1.11226e-05, gnorm=0.714, train_wall=190, wall=0
2021-01-10 13:24:20 | INFO | train_inner | epoch 070:    304 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.79, ppl=1.73, wps=4892.3, ups=0.47, wpb=10493.3, bsz=365.7, num_updates=38900, lr=1.11083e-05, gnorm=0.717, train_wall=214, wall=0
2021-01-10 13:27:26 | INFO | train_inner | epoch 070:    404 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.784, ppl=1.72, wps=5682.3, ups=0.54, wpb=10551, bsz=373.1, num_updates=39000, lr=1.1094e-05, gnorm=0.712, train_wall=185, wall=0
2021-01-10 13:30:28 | INFO | train_inner | epoch 070:    504 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.791, ppl=1.73, wps=5735.9, ups=0.55, wpb=10437.6, bsz=370, num_updates=39100, lr=1.10798e-05, gnorm=0.718, train_wall=182, wall=0
2021-01-10 13:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 13:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:32:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:32:35 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.74 | nll_loss 4.221 | ppl 18.65 | bleu 21.9 | wps 4611.7 | wpb 7508.5 | bsz 272.7 | num_updates 39157 | best_bleu 22.43
2021-01-10 13:32:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:32:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 70 @ 39157 updates, score 21.9) (writing took 2.6749066803604364 seconds)
2021-01-10 13:32:38 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-10 13:32:38 | INFO | train | epoch 070 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.994 | nll_loss 0.788 | ppl 1.73 | wps 5346.5 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 39157 | lr 1.10717e-05 | gnorm 0.716 | train_wall 1072 | wall 0
2021-01-10 13:32:38 | INFO | fairseq.trainer | begin training epoch 71
2021-01-10 13:32:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:32:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:34:01 | INFO | train_inner | epoch 071:     43 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.021, nll_loss=0.797, ppl=1.74, wps=4862.8, ups=0.47, wpb=10351.5, bsz=353.9, num_updates=39200, lr=1.10657e-05, gnorm=0.732, train_wall=186, wall=0
2021-01-10 13:37:12 | INFO | train_inner | epoch 071:    143 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.788, ppl=1.73, wps=5507.9, ups=0.52, wpb=10552, bsz=365.3, num_updates=39300, lr=1.10516e-05, gnorm=0.717, train_wall=191, wall=0
2021-01-10 13:40:20 | INFO | train_inner | epoch 071:    243 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.785, ppl=1.72, wps=5573.8, ups=0.53, wpb=10476.7, bsz=379, num_updates=39400, lr=1.10375e-05, gnorm=0.711, train_wall=188, wall=0
2021-01-10 13:43:23 | INFO | train_inner | epoch 071:    343 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.789, ppl=1.73, wps=5771.6, ups=0.55, wpb=10523.4, bsz=366.7, num_updates=39500, lr=1.10236e-05, gnorm=0.718, train_wall=182, wall=0
2021-01-10 13:46:27 | INFO | train_inner | epoch 071:    443 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.79, ppl=1.73, wps=5698.6, ups=0.54, wpb=10501.3, bsz=372.3, num_updates=39600, lr=1.10096e-05, gnorm=0.715, train_wall=184, wall=0
2021-01-10 13:49:33 | INFO | train_inner | epoch 071:    543 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.777, ppl=1.71, wps=5618.5, ups=0.54, wpb=10454.4, bsz=385.7, num_updates=39700, lr=1.09958e-05, gnorm=0.713, train_wall=186, wall=0
2021-01-10 13:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 13:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:50:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:50:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 13:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 13:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 13:50:31 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.741 | nll_loss 4.223 | ppl 18.68 | bleu 21.94 | wps 4598.3 | wpb 7508.5 | bsz 272.7 | num_updates 39718 | best_bleu 22.43
2021-01-10 13:50:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 13:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:50:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:50:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 71 @ 39718 updates, score 21.94) (writing took 2.7356191780418158 seconds)
2021-01-10 13:50:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-10 13:50:34 | INFO | train | epoch 071 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.992 | nll_loss 0.786 | ppl 1.72 | wps 5464.2 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 39718 | lr 1.09933e-05 | gnorm 0.716 | train_wall 1048 | wall 0
2021-01-10 13:50:34 | INFO | fairseq.trainer | begin training epoch 72
2021-01-10 13:50:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 13:50:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 13:53:09 | INFO | train_inner | epoch 072:     82 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.784, ppl=1.72, wps=4775.7, ups=0.46, wpb=10302.2, bsz=366.4, num_updates=39800, lr=1.09819e-05, gnorm=0.724, train_wall=188, wall=0
2021-01-10 13:56:12 | INFO | train_inner | epoch 072:    182 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.788, ppl=1.73, wps=5738.9, ups=0.54, wpb=10542.6, bsz=367.8, num_updates=39900, lr=1.09682e-05, gnorm=0.714, train_wall=184, wall=0
2021-01-10 13:59:18 | INFO | train_inner | epoch 072:    282 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.793, ppl=1.73, wps=5663.8, ups=0.54, wpb=10536.1, bsz=357.6, num_updates=40000, lr=1.09545e-05, gnorm=0.719, train_wall=186, wall=0
2021-01-10 14:02:26 | INFO | train_inner | epoch 072:    382 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.983, nll_loss=0.78, ppl=1.72, wps=5631.5, ups=0.53, wpb=10577.4, bsz=376.6, num_updates=40100, lr=1.09408e-05, gnorm=0.709, train_wall=188, wall=0
2021-01-10 14:05:35 | INFO | train_inner | epoch 072:    482 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.787, ppl=1.72, wps=5502.5, ups=0.53, wpb=10383.6, bsz=378.4, num_updates=40200, lr=1.09272e-05, gnorm=0.72, train_wall=189, wall=0
2021-01-10 14:08:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 14:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:08:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:08:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:08:20 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.743 | nll_loss 4.224 | ppl 18.69 | bleu 21.88 | wps 4643.7 | wpb 7508.5 | bsz 272.7 | num_updates 40279 | best_bleu 22.43
2021-01-10 14:08:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 14:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 72 @ 40279 updates, score 21.88) (writing took 2.533633714541793 seconds)
2021-01-10 14:08:23 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-10 14:08:23 | INFO | train | epoch 072 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.992 | nll_loss 0.786 | ppl 1.72 | wps 5503 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 40279 | lr 1.09164e-05 | gnorm 0.717 | train_wall 1041 | wall 0
2021-01-10 14:08:23 | INFO | fairseq.trainer | begin training epoch 73
2021-01-10 14:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:09:05 | INFO | train_inner | epoch 073:     21 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.983, nll_loss=0.783, ppl=1.72, wps=4997, ups=0.48, wpb=10491.4, bsz=354.1, num_updates=40300, lr=1.09136e-05, gnorm=0.718, train_wall=183, wall=0
2021-01-10 14:12:10 | INFO | train_inner | epoch 073:    121 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.999, nll_loss=0.787, ppl=1.73, wps=5645.9, ups=0.54, wpb=10443.1, bsz=374.6, num_updates=40400, lr=1.09001e-05, gnorm=0.718, train_wall=185, wall=0
2021-01-10 14:15:16 | INFO | train_inner | epoch 073:    221 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.782, ppl=1.72, wps=5576.4, ups=0.54, wpb=10375.4, bsz=351.8, num_updates=40500, lr=1.08866e-05, gnorm=0.72, train_wall=186, wall=0
2021-01-10 14:18:26 | INFO | train_inner | epoch 073:    321 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.786, ppl=1.72, wps=5497.9, ups=0.53, wpb=10463.5, bsz=381.4, num_updates=40600, lr=1.08732e-05, gnorm=0.714, train_wall=190, wall=0
2021-01-10 14:21:28 | INFO | train_inner | epoch 073:    421 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.972, nll_loss=0.776, ppl=1.71, wps=5839.7, ups=0.55, wpb=10598.9, bsz=387.5, num_updates=40700, lr=1.08598e-05, gnorm=0.705, train_wall=181, wall=0
2021-01-10 14:24:30 | INFO | train_inner | epoch 073:    521 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.793, ppl=1.73, wps=5799.3, ups=0.55, wpb=10562.2, bsz=361.4, num_updates=40800, lr=1.08465e-05, gnorm=0.719, train_wall=182, wall=0
2021-01-10 14:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 14:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:25:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:25:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:25:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:26:04 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.739 | nll_loss 4.221 | ppl 18.65 | bleu 21.84 | wps 4614.9 | wpb 7508.5 | bsz 272.7 | num_updates 40840 | best_bleu 22.43
2021-01-10 14:26:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 14:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:26:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:26:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 73 @ 40840 updates, score 21.84) (writing took 2.8888799902051687 seconds)
2021-01-10 14:26:07 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-10 14:26:07 | INFO | train | epoch 073 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.992 | nll_loss 0.786 | ppl 1.72 | wps 5527.3 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 40840 | lr 1.08412e-05 | gnorm 0.717 | train_wall 1036 | wall 0
2021-01-10 14:26:07 | INFO | fairseq.trainer | begin training epoch 74
2021-01-10 14:26:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:26:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:28:02 | INFO | train_inner | epoch 074:     60 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.792, ppl=1.73, wps=4911.5, ups=0.47, wpb=10436.4, bsz=365.4, num_updates=40900, lr=1.08333e-05, gnorm=0.725, train_wall=185, wall=0
2021-01-10 14:31:21 | INFO | train_inner | epoch 074:    160 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.78, ppl=1.72, wps=5339.3, ups=0.5, wpb=10619.9, bsz=378.2, num_updates=41000, lr=1.082e-05, gnorm=0.705, train_wall=199, wall=0
2021-01-10 14:34:25 | INFO | train_inner | epoch 074:    260 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.785, ppl=1.72, wps=5742, ups=0.54, wpb=10536.6, bsz=366.3, num_updates=41100, lr=1.08069e-05, gnorm=0.716, train_wall=183, wall=0
2021-01-10 14:37:28 | INFO | train_inner | epoch 074:    360 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.787, ppl=1.73, wps=5728.9, ups=0.55, wpb=10487.5, bsz=364, num_updates=41200, lr=1.07937e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-10 14:40:33 | INFO | train_inner | epoch 074:    460 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.003, nll_loss=0.788, ppl=1.73, wps=5584.8, ups=0.54, wpb=10341.7, bsz=370.3, num_updates=41300, lr=1.07807e-05, gnorm=0.726, train_wall=185, wall=0
2021-01-10 14:43:50 | INFO | train_inner | epoch 074:    560 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.783, ppl=1.72, wps=5315.9, ups=0.51, wpb=10490.1, bsz=371.1, num_updates=41400, lr=1.07676e-05, gnorm=0.716, train_wall=197, wall=0
2021-01-10 14:43:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 14:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 14:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 14:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 14:44:13 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.74 | nll_loss 4.223 | ppl 18.67 | bleu 21.93 | wps 4608 | wpb 7508.5 | bsz 272.7 | num_updates 41401 | best_bleu 22.43
2021-01-10 14:44:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 14:44:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:44:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:44:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:44:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:44:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 74 @ 41401 updates, score 21.93) (writing took 2.9131860323250294 seconds)
2021-01-10 14:44:16 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-10 14:44:16 | INFO | train | epoch 074 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.991 | nll_loss 0.785 | ppl 1.72 | wps 5398.6 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 41401 | lr 1.07675e-05 | gnorm 0.718 | train_wall 1061 | wall 0
2021-01-10 14:44:16 | INFO | fairseq.trainer | begin training epoch 75
2021-01-10 14:44:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 14:44:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 14:47:22 | INFO | train_inner | epoch 075:     99 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.775, ppl=1.71, wps=4905.8, ups=0.47, wpb=10394.9, bsz=376.9, num_updates=41500, lr=1.07547e-05, gnorm=0.714, train_wall=184, wall=0
2021-01-10 14:50:28 | INFO | train_inner | epoch 075:    199 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.783, ppl=1.72, wps=5752.3, ups=0.54, wpb=10692.6, bsz=378.4, num_updates=41600, lr=1.07417e-05, gnorm=0.704, train_wall=186, wall=0
2021-01-10 14:53:37 | INFO | train_inner | epoch 075:    299 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.791, ppl=1.73, wps=5583.6, ups=0.53, wpb=10526.8, bsz=354.2, num_updates=41700, lr=1.07288e-05, gnorm=0.722, train_wall=188, wall=0
2021-01-10 14:57:06 | INFO | train_inner | epoch 075:    399 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.783, ppl=1.72, wps=4937.6, ups=0.48, wpb=10325.8, bsz=366.4, num_updates=41800, lr=1.0716e-05, gnorm=0.723, train_wall=209, wall=0
2021-01-10 15:00:17 | INFO | train_inner | epoch 075:    499 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.792, ppl=1.73, wps=5446.9, ups=0.52, wpb=10402.6, bsz=366.6, num_updates=41900, lr=1.07032e-05, gnorm=0.725, train_wall=191, wall=0
2021-01-10 15:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 15:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:02:40 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.743 | nll_loss 4.226 | ppl 18.71 | bleu 21.85 | wps 4610.9 | wpb 7508.5 | bsz 272.7 | num_updates 41962 | best_bleu 22.43
2021-01-10 15:02:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 15:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:02:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 75 @ 41962 updates, score 21.85) (writing took 2.633530566468835 seconds)
2021-01-10 15:02:42 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-10 15:02:42 | INFO | train | epoch 075 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.991 | nll_loss 0.785 | ppl 1.72 | wps 5317.4 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 41962 | lr 1.06953e-05 | gnorm 0.717 | train_wall 1078 | wall 0
2021-01-10 15:02:42 | INFO | fairseq.trainer | begin training epoch 76
2021-01-10 15:02:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:02:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:03:58 | INFO | train_inner | epoch 076:     38 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.974, nll_loss=0.78, ppl=1.72, wps=4737, ups=0.45, wpb=10495.1, bsz=385.4, num_updates=42000, lr=1.06904e-05, gnorm=0.716, train_wall=194, wall=0
2021-01-10 15:07:15 | INFO | train_inner | epoch 076:    138 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.784, ppl=1.72, wps=5380.8, ups=0.51, wpb=10563.9, bsz=378.2, num_updates=42100, lr=1.06777e-05, gnorm=0.711, train_wall=196, wall=0
2021-01-10 15:10:35 | INFO | train_inner | epoch 076:    238 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.785, ppl=1.72, wps=5245, ups=0.5, wpb=10532.4, bsz=392.1, num_updates=42200, lr=1.06651e-05, gnorm=0.715, train_wall=201, wall=0
2021-01-10 15:13:44 | INFO | train_inner | epoch 076:    338 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.783, ppl=1.72, wps=5538.5, ups=0.53, wpb=10445.6, bsz=346.4, num_updates=42300, lr=1.06525e-05, gnorm=0.721, train_wall=188, wall=0
2021-01-10 15:16:54 | INFO | train_inner | epoch 076:    438 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.783, ppl=1.72, wps=5536.6, ups=0.53, wpb=10500.7, bsz=353.9, num_updates=42400, lr=1.06399e-05, gnorm=0.719, train_wall=189, wall=0
2021-01-10 15:20:05 | INFO | train_inner | epoch 076:    538 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.79, ppl=1.73, wps=5473.8, ups=0.52, wpb=10452.6, bsz=371.8, num_updates=42500, lr=1.06274e-05, gnorm=0.722, train_wall=191, wall=0
2021-01-10 15:20:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 15:20:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:20:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:20:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:20:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:20:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:20:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:20:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:21:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:21:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:21:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:21:08 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.739 | nll_loss 4.224 | ppl 18.68 | bleu 21.88 | wps 4642.2 | wpb 7508.5 | bsz 272.7 | num_updates 42523 | best_bleu 22.43
2021-01-10 15:21:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 15:21:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:21:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:21:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 76 @ 42523 updates, score 21.88) (writing took 2.4321301598101854 seconds)
2021-01-10 15:21:10 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-10 15:21:10 | INFO | train | epoch 076 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.991 | nll_loss 0.785 | ppl 1.72 | wps 5308.7 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 42523 | lr 1.06245e-05 | gnorm 0.719 | train_wall 1080 | wall 0
2021-01-10 15:21:10 | INFO | fairseq.trainer | begin training epoch 77
2021-01-10 15:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:21:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:21:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:23:36 | INFO | train_inner | epoch 077:     77 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.783, ppl=1.72, wps=4917.5, ups=0.47, wpb=10366.7, bsz=362.6, num_updates=42600, lr=1.06149e-05, gnorm=0.725, train_wall=184, wall=0
2021-01-10 15:26:39 | INFO | train_inner | epoch 077:    177 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.975, nll_loss=0.777, ppl=1.71, wps=5753.2, ups=0.54, wpb=10559.9, bsz=377.3, num_updates=42700, lr=1.06025e-05, gnorm=0.707, train_wall=183, wall=0
2021-01-10 15:29:47 | INFO | train_inner | epoch 077:    277 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.786, ppl=1.72, wps=5578.2, ups=0.53, wpb=10502.1, bsz=360.1, num_updates=42800, lr=1.05901e-05, gnorm=0.717, train_wall=188, wall=0
2021-01-10 15:32:50 | INFO | train_inner | epoch 077:    377 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.785, ppl=1.72, wps=5722.3, ups=0.55, wpb=10459.7, bsz=365.9, num_updates=42900, lr=1.05777e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-10 15:35:54 | INFO | train_inner | epoch 077:    477 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.791, ppl=1.73, wps=5710.7, ups=0.54, wpb=10514.8, bsz=375.3, num_updates=43000, lr=1.05654e-05, gnorm=0.721, train_wall=184, wall=0
2021-01-10 15:38:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 15:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:38:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:38:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:38:50 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.745 | nll_loss 4.227 | ppl 18.73 | bleu 21.92 | wps 4096.6 | wpb 7508.5 | bsz 272.7 | num_updates 43084 | best_bleu 22.43
2021-01-10 15:38:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 15:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:38:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:38:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 77 @ 43084 updates, score 21.92) (writing took 2.8413993567228317 seconds)
2021-01-10 15:38:53 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-10 15:38:53 | INFO | train | epoch 077 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.99 | nll_loss 0.784 | ppl 1.72 | wps 5533.5 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 43084 | lr 1.05551e-05 | gnorm 0.717 | train_wall 1032 | wall 0
2021-01-10 15:38:53 | INFO | fairseq.trainer | begin training epoch 78
2021-01-10 15:38:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:38:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:39:25 | INFO | train_inner | epoch 078:     16 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.999, nll_loss=0.789, ppl=1.73, wps=4920.3, ups=0.47, wpb=10378.6, bsz=365.1, num_updates=43100, lr=1.05531e-05, gnorm=0.725, train_wall=181, wall=0
2021-01-10 15:42:26 | INFO | train_inner | epoch 078:    116 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.783, ppl=1.72, wps=5788.7, ups=0.55, wpb=10463.5, bsz=377.4, num_updates=43200, lr=1.05409e-05, gnorm=0.718, train_wall=181, wall=0
2021-01-10 15:45:29 | INFO | train_inner | epoch 078:    216 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.784, ppl=1.72, wps=5749.3, ups=0.55, wpb=10516.5, bsz=359, num_updates=43300, lr=1.05287e-05, gnorm=0.722, train_wall=183, wall=0
2021-01-10 15:48:31 | INFO | train_inner | epoch 078:    316 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.986, nll_loss=0.781, ppl=1.72, wps=5750.6, ups=0.55, wpb=10495.9, bsz=370.8, num_updates=43400, lr=1.05166e-05, gnorm=0.717, train_wall=182, wall=0
2021-01-10 15:51:35 | INFO | train_inner | epoch 078:    416 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.786, ppl=1.72, wps=5740.1, ups=0.55, wpb=10521.9, bsz=373.8, num_updates=43500, lr=1.05045e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-10 15:54:41 | INFO | train_inner | epoch 078:    516 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.782, ppl=1.72, wps=5673.5, ups=0.54, wpb=10550.4, bsz=380.2, num_updates=43600, lr=1.04925e-05, gnorm=0.714, train_wall=186, wall=0
2021-01-10 15:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 15:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:56:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 15:56:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 15:56:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 15:56:23 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.745 | nll_loss 4.227 | ppl 18.73 | bleu 21.86 | wps 4596.7 | wpb 7508.5 | bsz 272.7 | num_updates 43645 | best_bleu 22.43
2021-01-10 15:56:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 15:56:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:56:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:56:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 78 @ 43645 updates, score 21.86) (writing took 2.8338752891868353 seconds)
2021-01-10 15:56:26 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-10 15:56:26 | INFO | train | epoch 078 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.99 | nll_loss 0.784 | ppl 1.72 | wps 5585.1 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 43645 | lr 1.04871e-05 | gnorm 0.72 | train_wall 1025 | wall 0
2021-01-10 15:56:26 | INFO | fairseq.trainer | begin training epoch 79
2021-01-10 15:56:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 15:56:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 15:58:08 | INFO | train_inner | epoch 079:     55 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.989, nll_loss=0.784, ppl=1.72, wps=4981.8, ups=0.48, wpb=10349.2, bsz=354.9, num_updates=43700, lr=1.04804e-05, gnorm=0.725, train_wall=180, wall=0
2021-01-10 16:01:10 | INFO | train_inner | epoch 079:    155 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.782, ppl=1.72, wps=5756.2, ups=0.55, wpb=10481.5, bsz=365, num_updates=43800, lr=1.04685e-05, gnorm=0.715, train_wall=182, wall=0
2021-01-10 16:04:13 | INFO | train_inner | epoch 079:    255 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.781, ppl=1.72, wps=5750.5, ups=0.55, wpb=10507.9, bsz=377.4, num_updates=43900, lr=1.04565e-05, gnorm=0.714, train_wall=183, wall=0
2021-01-10 16:07:25 | INFO | train_inner | epoch 079:    355 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.782, ppl=1.72, wps=5516, ups=0.52, wpb=10562.9, bsz=372.6, num_updates=44000, lr=1.04447e-05, gnorm=0.713, train_wall=191, wall=0
2021-01-10 16:10:32 | INFO | train_inner | epoch 079:    455 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.995, nll_loss=0.785, ppl=1.72, wps=5551.1, ups=0.54, wpb=10374.8, bsz=359, num_updates=44100, lr=1.04328e-05, gnorm=0.726, train_wall=187, wall=0
2021-01-10 16:13:34 | INFO | train_inner | epoch 079:    555 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.788, ppl=1.73, wps=5817.7, ups=0.55, wpb=10605.1, bsz=381.2, num_updates=44200, lr=1.0421e-05, gnorm=0.714, train_wall=182, wall=0
2021-01-10 16:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 16:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:13:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:13:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:13:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:14:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:14:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:14:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:14:05 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.744 | nll_loss 4.227 | ppl 18.72 | bleu 21.87 | wps 4682.2 | wpb 7508.5 | bsz 272.7 | num_updates 44206 | best_bleu 22.43
2021-01-10 16:14:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 16:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:14:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 79 @ 44206 updates, score 21.87) (writing took 2.803945120424032 seconds)
2021-01-10 16:14:08 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-10 16:14:08 | INFO | train | epoch 079 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.989 | nll_loss 0.783 | ppl 1.72 | wps 5536.2 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 44206 | lr 1.04203e-05 | gnorm 0.718 | train_wall 1034 | wall 0
2021-01-10 16:14:08 | INFO | fairseq.trainer | begin training epoch 80
2021-01-10 16:14:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:14:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:17:06 | INFO | train_inner | epoch 080:     94 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.778, ppl=1.72, wps=4903.1, ups=0.47, wpb=10414.1, bsz=368.2, num_updates=44300, lr=1.04092e-05, gnorm=0.722, train_wall=185, wall=0
2021-01-10 16:20:11 | INFO | train_inner | epoch 080:    194 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.989, nll_loss=0.779, ppl=1.72, wps=5673.1, ups=0.54, wpb=10466.9, bsz=376.7, num_updates=44400, lr=1.03975e-05, gnorm=0.716, train_wall=184, wall=0
2021-01-10 16:23:31 | INFO | train_inner | epoch 080:    294 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.783, ppl=1.72, wps=5273.6, ups=0.5, wpb=10568.1, bsz=371.6, num_updates=44500, lr=1.03858e-05, gnorm=0.713, train_wall=200, wall=0
2021-01-10 16:26:34 | INFO | train_inner | epoch 080:    394 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.789, ppl=1.73, wps=5780.6, ups=0.55, wpb=10574.3, bsz=362.7, num_updates=44600, lr=1.03742e-05, gnorm=0.72, train_wall=183, wall=0
2021-01-10 16:29:38 | INFO | train_inner | epoch 080:    494 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.777, ppl=1.71, wps=5664, ups=0.54, wpb=10398.8, bsz=364.5, num_updates=44700, lr=1.03626e-05, gnorm=0.715, train_wall=183, wall=0
2021-01-10 16:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 16:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:31:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:31:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:31:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:32:03 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.742 | nll_loss 4.226 | ppl 18.71 | bleu 21.94 | wps 4617.5 | wpb 7508.5 | bsz 272.7 | num_updates 44767 | best_bleu 22.43
2021-01-10 16:32:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 16:32:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:32:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:32:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 80 @ 44767 updates, score 21.94) (writing took 2.6624256037175655 seconds)
2021-01-10 16:32:05 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-10 16:32:05 | INFO | train | epoch 080 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.988 | nll_loss 0.782 | ppl 1.72 | wps 5460.6 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 44767 | lr 1.03548e-05 | gnorm 0.718 | train_wall 1049 | wall 0
2021-01-10 16:32:05 | INFO | fairseq.trainer | begin training epoch 81
2021-01-10 16:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:32:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:32:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:33:10 | INFO | train_inner | epoch 081:     33 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.785, ppl=1.72, wps=4925.5, ups=0.47, wpb=10435.3, bsz=371.4, num_updates=44800, lr=1.0351e-05, gnorm=0.725, train_wall=185, wall=0
2021-01-10 16:36:44 | INFO | train_inner | epoch 081:    133 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.781, ppl=1.72, wps=4884.5, ups=0.47, wpb=10486.2, bsz=366.3, num_updates=44900, lr=1.03395e-05, gnorm=0.719, train_wall=214, wall=0
2021-01-10 16:39:48 | INFO | train_inner | epoch 081:    233 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.783, ppl=1.72, wps=5671.5, ups=0.54, wpb=10412.8, bsz=377, num_updates=45000, lr=1.0328e-05, gnorm=0.718, train_wall=183, wall=0
2021-01-10 16:42:52 | INFO | train_inner | epoch 081:    333 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.785, ppl=1.72, wps=5707, ups=0.54, wpb=10482.4, bsz=376.6, num_updates=45100, lr=1.03165e-05, gnorm=0.722, train_wall=183, wall=0
2021-01-10 16:45:59 | INFO | train_inner | epoch 081:    433 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=2.969, nll_loss=0.777, ppl=1.71, wps=5677.4, ups=0.53, wpb=10628.8, bsz=379.4, num_updates=45200, lr=1.03051e-05, gnorm=0.706, train_wall=187, wall=0
2021-01-10 16:49:08 | INFO | train_inner | epoch 081:    533 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.782, ppl=1.72, wps=5540.5, ups=0.53, wpb=10469.5, bsz=357.4, num_updates=45300, lr=1.02937e-05, gnorm=0.716, train_wall=189, wall=0
2021-01-10 16:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 16:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:50:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:50:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 16:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 16:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 16:50:23 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.743 | nll_loss 4.225 | ppl 18.71 | bleu 21.93 | wps 4557 | wpb 7508.5 | bsz 272.7 | num_updates 45328 | best_bleu 22.43
2021-01-10 16:50:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 16:50:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:50:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:50:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 81 @ 45328 updates, score 21.93) (writing took 2.8288429267704487 seconds)
2021-01-10 16:50:26 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-10 16:50:26 | INFO | train | epoch 081 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.988 | nll_loss 0.782 | ppl 1.72 | wps 5341.8 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 45328 | lr 1.02905e-05 | gnorm 0.718 | train_wall 1072 | wall 0
2021-01-10 16:50:26 | INFO | fairseq.trainer | begin training epoch 82
2021-01-10 16:50:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 16:50:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 16:52:40 | INFO | train_inner | epoch 082:     72 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.787, ppl=1.73, wps=4912.9, ups=0.47, wpb=10430.6, bsz=349.9, num_updates=45400, lr=1.02824e-05, gnorm=0.73, train_wall=185, wall=0
2021-01-10 16:55:41 | INFO | train_inner | epoch 082:    172 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.985, nll_loss=0.782, ppl=1.72, wps=5780.5, ups=0.55, wpb=10449, bsz=380, num_updates=45500, lr=1.02711e-05, gnorm=0.718, train_wall=181, wall=0
2021-01-10 16:58:49 | INFO | train_inner | epoch 082:    272 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.783, ppl=1.72, wps=5600.4, ups=0.53, wpb=10526.2, bsz=360.7, num_updates=45600, lr=1.02598e-05, gnorm=0.721, train_wall=188, wall=0
2021-01-10 17:01:55 | INFO | train_inner | epoch 082:    372 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.779, ppl=1.72, wps=5685.6, ups=0.54, wpb=10586.9, bsz=366.2, num_updates=45700, lr=1.02486e-05, gnorm=0.711, train_wall=186, wall=0
2021-01-10 17:05:07 | INFO | train_inner | epoch 082:    472 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.001, nll_loss=0.786, ppl=1.72, wps=5389.9, ups=0.52, wpb=10368.4, bsz=379.5, num_updates=45800, lr=1.02374e-05, gnorm=0.722, train_wall=192, wall=0
2021-01-10 17:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:07:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:08:11 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.745 | nll_loss 4.228 | ppl 18.74 | bleu 21.91 | wps 4395.5 | wpb 7508.5 | bsz 272.7 | num_updates 45889 | best_bleu 22.43
2021-01-10 17:08:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 17:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:08:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:08:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:08:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 82 @ 45889 updates, score 21.91) (writing took 2.7487473525106907 seconds)
2021-01-10 17:08:14 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-10 17:08:14 | INFO | train | epoch 082 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.987 | nll_loss 0.782 | ppl 1.72 | wps 5507.7 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 45889 | lr 1.02274e-05 | gnorm 0.718 | train_wall 1039 | wall 0
2021-01-10 17:08:14 | INFO | fairseq.trainer | begin training epoch 83
2021-01-10 17:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:08:37 | INFO | train_inner | epoch 083:     11 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.978, nll_loss=0.775, ppl=1.71, wps=4976, ups=0.48, wpb=10456, bsz=372.2, num_updates=45900, lr=1.02262e-05, gnorm=0.715, train_wall=182, wall=0
2021-01-10 17:11:42 | INFO | train_inner | epoch 083:    111 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3, nll_loss=0.788, ppl=1.73, wps=5653.9, ups=0.54, wpb=10423.5, bsz=368.6, num_updates=46000, lr=1.02151e-05, gnorm=0.723, train_wall=184, wall=0
2021-01-10 17:14:49 | INFO | train_inner | epoch 083:    211 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.78, ppl=1.72, wps=5572.3, ups=0.53, wpb=10422.1, bsz=363.6, num_updates=46100, lr=1.0204e-05, gnorm=0.719, train_wall=187, wall=0
2021-01-10 17:18:15 | INFO | train_inner | epoch 083:    311 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.779, ppl=1.72, wps=5114.9, ups=0.48, wpb=10566.6, bsz=370.2, num_updates=46200, lr=1.01929e-05, gnorm=0.711, train_wall=206, wall=0
2021-01-10 17:21:21 | INFO | train_inner | epoch 083:    411 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.996, nll_loss=0.781, ppl=1.72, wps=5588.5, ups=0.54, wpb=10381.9, bsz=373, num_updates=46300, lr=1.01819e-05, gnorm=0.721, train_wall=186, wall=0
2021-01-10 17:24:25 | INFO | train_inner | epoch 083:    511 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.782, ppl=1.72, wps=5792.7, ups=0.54, wpb=10647.1, bsz=382.5, num_updates=46400, lr=1.0171e-05, gnorm=0.712, train_wall=184, wall=0
2021-01-10 17:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 17:25:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:25:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:25:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:26:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:26:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:26:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:26:18 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.745 | nll_loss 4.229 | ppl 18.75 | bleu 22.04 | wps 4622.7 | wpb 7508.5 | bsz 272.7 | num_updates 46450 | best_bleu 22.43
2021-01-10 17:26:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 17:26:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:26:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:26:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:26:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:26:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 83 @ 46450 updates, score 22.04) (writing took 2.9076025914400816 seconds)
2021-01-10 17:26:21 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-10 17:26:21 | INFO | train | epoch 083 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.987 | nll_loss 0.781 | ppl 1.72 | wps 5410 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 46450 | lr 1.01655e-05 | gnorm 0.717 | train_wall 1059 | wall 0
2021-01-10 17:26:21 | INFO | fairseq.trainer | begin training epoch 84
2021-01-10 17:26:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:26:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:27:58 | INFO | train_inner | epoch 084:     50 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.978, nll_loss=0.776, ppl=1.71, wps=4920.2, ups=0.47, wpb=10494.1, bsz=366.1, num_updates=46500, lr=1.016e-05, gnorm=0.72, train_wall=186, wall=0
2021-01-10 17:31:07 | INFO | train_inner | epoch 084:    150 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.779, ppl=1.72, wps=5503.2, ups=0.53, wpb=10383.4, bsz=365.9, num_updates=46600, lr=1.01491e-05, gnorm=0.721, train_wall=188, wall=0
2021-01-10 17:34:19 | INFO | train_inner | epoch 084:    250 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.782, ppl=1.72, wps=5487.1, ups=0.52, wpb=10527, bsz=369, num_updates=46700, lr=1.01382e-05, gnorm=0.715, train_wall=192, wall=0
2021-01-10 17:37:21 | INFO | train_inner | epoch 084:    350 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.989, nll_loss=0.777, ppl=1.71, wps=5691, ups=0.55, wpb=10387.8, bsz=365.9, num_updates=46800, lr=1.01274e-05, gnorm=0.722, train_wall=182, wall=0
2021-01-10 17:40:27 | INFO | train_inner | epoch 084:    450 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.781, ppl=1.72, wps=5721.6, ups=0.54, wpb=10620, bsz=382.1, num_updates=46900, lr=1.01166e-05, gnorm=0.715, train_wall=185, wall=0
2021-01-10 17:43:32 | INFO | train_inner | epoch 084:    550 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.975, nll_loss=0.776, ppl=1.71, wps=5709, ups=0.54, wpb=10537.8, bsz=371.6, num_updates=47000, lr=1.01058e-05, gnorm=0.711, train_wall=184, wall=0
2021-01-10 17:43:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 17:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:43:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:43:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:43:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:44:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:44:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:44:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 17:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 17:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 17:44:13 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.749 | nll_loss 4.234 | ppl 18.82 | bleu 21.88 | wps 4614.6 | wpb 7508.5 | bsz 272.7 | num_updates 47011 | best_bleu 22.43
2021-01-10 17:44:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 17:44:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:44:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:44:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:44:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:44:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 84 @ 47011 updates, score 21.88) (writing took 2.8208568394184113 seconds)
2021-01-10 17:44:16 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-10 17:44:16 | INFO | train | epoch 084 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.986 | nll_loss 0.78 | ppl 1.72 | wps 5473.4 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 47011 | lr 1.01046e-05 | gnorm 0.719 | train_wall 1046 | wall 0
2021-01-10 17:44:16 | INFO | fairseq.trainer | begin training epoch 85
2021-01-10 17:44:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 17:44:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 17:47:20 | INFO | train_inner | epoch 085:     89 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.97, nll_loss=0.771, ppl=1.71, wps=4608.8, ups=0.44, wpb=10515.7, bsz=381.4, num_updates=47100, lr=1.00951e-05, gnorm=0.711, train_wall=201, wall=0
2021-01-10 17:50:22 | INFO | train_inner | epoch 085:    189 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.776, ppl=1.71, wps=5731.7, ups=0.55, wpb=10433.1, bsz=372.4, num_updates=47200, lr=1.00844e-05, gnorm=0.718, train_wall=182, wall=0
2021-01-10 17:53:27 | INFO | train_inner | epoch 085:    289 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.778, ppl=1.72, wps=5754, ups=0.54, wpb=10635.4, bsz=367.4, num_updates=47300, lr=1.00737e-05, gnorm=0.715, train_wall=185, wall=0
2021-01-10 17:56:32 | INFO | train_inner | epoch 085:    389 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.005, nll_loss=0.79, ppl=1.73, wps=5601.1, ups=0.54, wpb=10395.9, bsz=366.6, num_updates=47400, lr=1.00631e-05, gnorm=0.727, train_wall=185, wall=0
2021-01-10 17:59:44 | INFO | train_inner | epoch 085:    489 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.785, ppl=1.72, wps=5468.6, ups=0.52, wpb=10468.8, bsz=359.4, num_updates=47500, lr=1.00525e-05, gnorm=0.724, train_wall=191, wall=0
2021-01-10 18:02:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 18:02:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:02:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:02:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:02:25 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.748 | nll_loss 4.23 | ppl 18.77 | bleu 21.89 | wps 4624.6 | wpb 7508.5 | bsz 272.7 | num_updates 47572 | best_bleu 22.43
2021-01-10 18:02:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 18:02:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 85 @ 47572 updates, score 21.89) (writing took 2.86949510127306 seconds)
2021-01-10 18:02:28 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-10 18:02:28 | INFO | train | epoch 085 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.986 | nll_loss 0.78 | ppl 1.72 | wps 5383.6 | ups 0.51 | wpb 10483.4 | bsz 369.6 | num_updates 47572 | lr 1.00449e-05 | gnorm 0.72 | train_wall 1064 | wall 0
2021-01-10 18:02:28 | INFO | fairseq.trainer | begin training epoch 86
2021-01-10 18:02:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:02:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:02:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:02:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:03:22 | INFO | train_inner | epoch 086:     28 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.785, ppl=1.72, wps=4745.8, ups=0.46, wpb=10378.5, bsz=361.2, num_updates=47600, lr=1.00419e-05, gnorm=0.732, train_wall=191, wall=0
2021-01-10 18:06:26 | INFO | train_inner | epoch 086:    128 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.974, nll_loss=0.769, ppl=1.7, wps=5745, ups=0.54, wpb=10552.1, bsz=366.2, num_updates=47700, lr=1.00314e-05, gnorm=0.708, train_wall=183, wall=0
2021-01-10 18:09:32 | INFO | train_inner | epoch 086:    228 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.772, ppl=1.71, wps=5661.4, ups=0.54, wpb=10514.6, bsz=372.4, num_updates=47800, lr=1.00209e-05, gnorm=0.713, train_wall=186, wall=0
2021-01-10 18:12:34 | INFO | train_inner | epoch 086:    328 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.009, nll_loss=0.79, ppl=1.73, wps=5670.4, ups=0.55, wpb=10318.1, bsz=356.1, num_updates=47900, lr=1.00104e-05, gnorm=0.73, train_wall=182, wall=0
2021-01-10 18:15:36 | INFO | train_inner | epoch 086:    428 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.784, ppl=1.72, wps=5807.8, ups=0.55, wpb=10577.7, bsz=372.1, num_updates=48000, lr=1e-05, gnorm=0.715, train_wall=182, wall=0
2021-01-10 18:18:37 | INFO | train_inner | epoch 086:    528 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.779, ppl=1.72, wps=5790, ups=0.55, wpb=10498.1, bsz=388.2, num_updates=48100, lr=9.9896e-06, gnorm=0.717, train_wall=181, wall=0
2021-01-10 18:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 18:19:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:19:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:19:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:19:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:19:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:19:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:19:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:19:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:19:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:20:00 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.749 | nll_loss 4.233 | ppl 18.81 | bleu 21.77 | wps 3987.4 | wpb 7508.5 | bsz 272.7 | num_updates 48133 | best_bleu 22.43
2021-01-10 18:20:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 18:20:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:20:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:20:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:20:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:20:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 86 @ 48133 updates, score 21.77) (writing took 2.875957068055868 seconds)
2021-01-10 18:20:03 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-10 18:20:03 | INFO | train | epoch 086 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.985 | nll_loss 0.779 | ppl 1.72 | wps 5573.3 | ups 0.53 | wpb 10483.4 | bsz 369.6 | num_updates 48133 | lr 9.98617e-06 | gnorm 0.719 | train_wall 1024 | wall 0
2021-01-10 18:20:03 | INFO | fairseq.trainer | begin training epoch 87
2021-01-10 18:20:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:20:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:22:08 | INFO | train_inner | epoch 087:     67 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.974, nll_loss=0.772, ppl=1.71, wps=4967.3, ups=0.47, wpb=10473.3, bsz=358.6, num_updates=48200, lr=9.97923e-06, gnorm=0.721, train_wall=181, wall=0
2021-01-10 18:25:09 | INFO | train_inner | epoch 087:    167 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.008, nll_loss=0.786, ppl=1.72, wps=5669.7, ups=0.55, wpb=10289.9, bsz=374.2, num_updates=48300, lr=9.9689e-06, gnorm=0.728, train_wall=181, wall=0
2021-01-10 18:28:11 | INFO | train_inner | epoch 087:    267 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.97, nll_loss=0.776, ppl=1.71, wps=5838.9, ups=0.55, wpb=10592.1, bsz=358.6, num_updates=48400, lr=9.95859e-06, gnorm=0.714, train_wall=181, wall=0
2021-01-10 18:31:13 | INFO | train_inner | epoch 087:    367 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.778, ppl=1.71, wps=5761, ups=0.55, wpb=10507.1, bsz=366.8, num_updates=48500, lr=9.94832e-06, gnorm=0.721, train_wall=182, wall=0
2021-01-10 18:34:14 | INFO | train_inner | epoch 087:    467 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.783, ppl=1.72, wps=5805.5, ups=0.55, wpb=10506.3, bsz=380.9, num_updates=48600, lr=9.93808e-06, gnorm=0.718, train_wall=181, wall=0
2021-01-10 18:37:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 18:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:37:23 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.748 | nll_loss 4.232 | ppl 18.79 | bleu 21.91 | wps 4675.5 | wpb 7508.5 | bsz 272.7 | num_updates 48694 | best_bleu 22.43
2021-01-10 18:37:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 18:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 87 @ 48694 updates, score 21.91) (writing took 2.898225963115692 seconds)
2021-01-10 18:37:26 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-10 18:37:26 | INFO | train | epoch 087 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 2.986 | nll_loss 0.78 | ppl 1.72 | wps 5640.3 | ups 0.54 | wpb 10483.4 | bsz 369.6 | num_updates 48694 | lr 9.92848e-06 | gnorm 0.719 | train_wall 1014 | wall 0
2021-01-10 18:37:26 | INFO | fairseq.trainer | begin training epoch 88
2021-01-10 18:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:37:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:37:40 | INFO | train_inner | epoch 088:      6 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.98, nll_loss=0.784, ppl=1.72, wps=5103.2, ups=0.49, wpb=10504.8, bsz=373.8, num_updates=48700, lr=9.92787e-06, gnorm=0.721, train_wall=179, wall=0
2021-01-10 18:40:39 | INFO | train_inner | epoch 088:    106 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.985, nll_loss=0.779, ppl=1.72, wps=5854.8, ups=0.56, wpb=10474.6, bsz=364.6, num_updates=48800, lr=9.91769e-06, gnorm=0.72, train_wall=179, wall=0
2021-01-10 18:43:38 | INFO | train_inner | epoch 088:    206 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=2.989, nll_loss=0.772, ppl=1.71, wps=5768.9, ups=0.56, wpb=10332.8, bsz=363.8, num_updates=48900, lr=9.90755e-06, gnorm=0.72, train_wall=179, wall=0
2021-01-10 18:46:38 | INFO | train_inner | epoch 088:    306 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.777, ppl=1.71, wps=5883.9, ups=0.56, wpb=10596.2, bsz=393, num_updates=49000, lr=9.89743e-06, gnorm=0.708, train_wall=180, wall=0
2021-01-10 18:49:38 | INFO | train_inner | epoch 088:    406 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.993, nll_loss=0.784, ppl=1.72, wps=5801.6, ups=0.56, wpb=10416.9, bsz=349.4, num_updates=49100, lr=9.88735e-06, gnorm=0.73, train_wall=179, wall=0
2021-01-10 18:52:39 | INFO | train_inner | epoch 088:    506 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.989, nll_loss=0.78, ppl=1.72, wps=5839.1, ups=0.55, wpb=10564.4, bsz=374, num_updates=49200, lr=9.8773e-06, gnorm=0.717, train_wall=181, wall=0
2021-01-10 18:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 18:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 18:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 18:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 18:54:39 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.75 | nll_loss 4.233 | ppl 18.81 | bleu 21.79 | wps 4372.6 | wpb 7508.5 | bsz 272.7 | num_updates 49255 | best_bleu 22.43
2021-01-10 18:54:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 18:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:54:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 88 @ 49255 updates, score 21.79) (writing took 2.963258309289813 seconds)
2021-01-10 18:54:42 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-10 18:54:42 | INFO | train | epoch 088 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.985 | nll_loss 0.779 | ppl 1.72 | wps 5675.7 | ups 0.54 | wpb 10483.4 | bsz 369.6 | num_updates 49255 | lr 9.87178e-06 | gnorm 0.718 | train_wall 1007 | wall 0
2021-01-10 18:54:42 | INFO | fairseq.trainer | begin training epoch 89
2021-01-10 18:54:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 18:54:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 18:56:07 | INFO | train_inner | epoch 089:     45 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.979, nll_loss=0.781, ppl=1.72, wps=5056.6, ups=0.48, wpb=10529.8, bsz=376.6, num_updates=49300, lr=9.86727e-06, gnorm=0.717, train_wall=180, wall=0
2021-01-10 18:59:08 | INFO | train_inner | epoch 089:    145 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.774, ppl=1.71, wps=5753.3, ups=0.55, wpb=10395.5, bsz=368.9, num_updates=49400, lr=9.85728e-06, gnorm=0.716, train_wall=180, wall=0
2021-01-10 19:02:26 | INFO | train_inner | epoch 089:    245 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.979, nll_loss=0.775, ppl=1.71, wps=5312.3, ups=0.5, wpb=10522.5, bsz=364.6, num_updates=49500, lr=9.84732e-06, gnorm=0.72, train_wall=198, wall=0
2021-01-10 19:05:41 | INFO | train_inner | epoch 089:    345 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.782, ppl=1.72, wps=5355, ups=0.51, wpb=10474.1, bsz=373.8, num_updates=49600, lr=9.83739e-06, gnorm=0.721, train_wall=195, wall=0
2021-01-10 19:08:45 | INFO | train_inner | epoch 089:    445 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.776, ppl=1.71, wps=5775.5, ups=0.55, wpb=10586.2, bsz=365, num_updates=49700, lr=9.82749e-06, gnorm=0.714, train_wall=183, wall=0
2021-01-10 19:11:49 | INFO | train_inner | epoch 089:    545 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=2.992, nll_loss=0.782, ppl=1.72, wps=5669.3, ups=0.54, wpb=10457.8, bsz=366.6, num_updates=49800, lr=9.81761e-06, gnorm=0.723, train_wall=184, wall=0
2021-01-10 19:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 19:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:12:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:12:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:12:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:12:39 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.749 | nll_loss 4.233 | ppl 18.8 | bleu 21.94 | wps 4613.8 | wpb 7508.5 | bsz 272.7 | num_updates 49816 | best_bleu 22.43
2021-01-10 19:12:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 19:12:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:12:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:12:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 89 @ 49816 updates, score 21.94) (writing took 2.7543062921613455 seconds)
2021-01-10 19:12:41 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-10 19:12:41 | INFO | train | epoch 089 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.984 | nll_loss 0.778 | ppl 1.72 | wps 5450.2 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 49816 | lr 9.81604e-06 | gnorm 0.719 | train_wall 1051 | wall 0
2021-01-10 19:12:41 | INFO | fairseq.trainer | begin training epoch 90
2021-01-10 19:12:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:12:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:15:21 | INFO | train_inner | epoch 090:     84 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.775, ppl=1.71, wps=4928.2, ups=0.47, wpb=10432.7, bsz=362.7, num_updates=49900, lr=9.80777e-06, gnorm=0.722, train_wall=184, wall=0
2021-01-10 19:18:34 | INFO | train_inner | epoch 090:    184 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.981, nll_loss=0.776, ppl=1.71, wps=5398.7, ups=0.52, wpb=10454.9, bsz=351.4, num_updates=50000, lr=9.79796e-06, gnorm=0.721, train_wall=193, wall=0
2021-01-10 19:21:36 | INFO | train_inner | epoch 090:    284 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.778, ppl=1.71, wps=5867.1, ups=0.55, wpb=10660.3, bsz=387.4, num_updates=50100, lr=9.78818e-06, gnorm=0.706, train_wall=182, wall=0
2021-01-10 19:24:37 | INFO | train_inner | epoch 090:    384 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.004, nll_loss=0.782, ppl=1.72, wps=5672.6, ups=0.55, wpb=10287.5, bsz=367.4, num_updates=50200, lr=9.77842e-06, gnorm=0.732, train_wall=181, wall=0
2021-01-10 19:27:41 | INFO | train_inner | epoch 090:    484 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.778, ppl=1.71, wps=5738.1, ups=0.55, wpb=10521.8, bsz=371.8, num_updates=50300, lr=9.7687e-06, gnorm=0.713, train_wall=183, wall=0
2021-01-10 19:30:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 19:30:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:30:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:30:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:30:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:30:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:30:34 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.754 | nll_loss 4.238 | ppl 18.86 | bleu 21.81 | wps 4578 | wpb 7508.5 | bsz 272.7 | num_updates 50377 | best_bleu 22.43
2021-01-10 19:30:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 19:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:30:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 90 @ 50377 updates, score 21.81) (writing took 2.6092472597956657 seconds)
2021-01-10 19:30:36 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-10 19:30:36 | INFO | train | epoch 090 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.984 | nll_loss 0.778 | ppl 1.71 | wps 5470.2 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 50377 | lr 9.76123e-06 | gnorm 0.717 | train_wall 1047 | wall 0
2021-01-10 19:30:36 | INFO | fairseq.trainer | begin training epoch 91
2021-01-10 19:30:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:30:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:30:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:31:27 | INFO | train_inner | epoch 091:     23 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.984, nll_loss=0.782, ppl=1.72, wps=4638.2, ups=0.44, wpb=10489.4, bsz=374.8, num_updates=50400, lr=9.759e-06, gnorm=0.718, train_wall=199, wall=0
2021-01-10 19:34:27 | INFO | train_inner | epoch 091:    123 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.774, ppl=1.71, wps=5795.1, ups=0.55, wpb=10465, bsz=388, num_updates=50500, lr=9.74933e-06, gnorm=0.713, train_wall=180, wall=0
2021-01-10 19:37:28 | INFO | train_inner | epoch 091:    223 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.776, ppl=1.71, wps=5768.2, ups=0.55, wpb=10426.8, bsz=369.1, num_updates=50600, lr=9.7397e-06, gnorm=0.718, train_wall=181, wall=0
2021-01-10 19:40:37 | INFO | train_inner | epoch 091:    323 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.778, ppl=1.71, wps=5566.5, ups=0.53, wpb=10483.3, bsz=372.8, num_updates=50700, lr=9.73009e-06, gnorm=0.722, train_wall=188, wall=0
2021-01-10 19:43:54 | INFO | train_inner | epoch 091:    423 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.969, nll_loss=0.772, ppl=1.71, wps=5397.2, ups=0.51, wpb=10666, bsz=359.5, num_updates=50800, lr=9.7205e-06, gnorm=0.71, train_wall=197, wall=0
2021-01-10 19:47:11 | INFO | train_inner | epoch 091:    523 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.781, ppl=1.72, wps=5305.1, ups=0.51, wpb=10462.6, bsz=367, num_updates=50900, lr=9.71095e-06, gnorm=0.722, train_wall=197, wall=0
2021-01-10 19:48:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 19:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:48:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:48:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 19:48:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 19:48:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 19:48:40 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.751 | nll_loss 4.235 | ppl 18.83 | bleu 21.82 | wps 4597.2 | wpb 7508.5 | bsz 272.7 | num_updates 50938 | best_bleu 22.43
2021-01-10 19:48:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 19:48:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:48:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:48:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 91 @ 50938 updates, score 21.82) (writing took 2.749644758179784 seconds)
2021-01-10 19:48:43 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-10 19:48:43 | INFO | train | epoch 091 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.983 | nll_loss 0.777 | ppl 1.71 | wps 5411.4 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 50938 | lr 9.70733e-06 | gnorm 0.719 | train_wall 1058 | wall 0
2021-01-10 19:48:43 | INFO | fairseq.trainer | begin training epoch 92
2021-01-10 19:48:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 19:48:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 19:50:40 | INFO | train_inner | epoch 092:     62 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.991, nll_loss=0.782, ppl=1.72, wps=4936.1, ups=0.48, wpb=10295.3, bsz=358.4, num_updates=51000, lr=9.70143e-06, gnorm=0.733, train_wall=181, wall=0
2021-01-10 19:53:47 | INFO | train_inner | epoch 092:    162 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.971, nll_loss=0.771, ppl=1.71, wps=5649.8, ups=0.53, wpb=10590.5, bsz=379.5, num_updates=51100, lr=9.69193e-06, gnorm=0.706, train_wall=187, wall=0
2021-01-10 19:56:53 | INFO | train_inner | epoch 092:    262 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=2.998, nll_loss=0.784, ppl=1.72, wps=5618.3, ups=0.54, wpb=10403, bsz=356, num_updates=51200, lr=9.68246e-06, gnorm=0.727, train_wall=185, wall=0
2021-01-10 20:00:10 | INFO | train_inner | epoch 092:    362 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.978, nll_loss=0.776, ppl=1.71, wps=5339.5, ups=0.51, wpb=10554.8, bsz=375.4, num_updates=51300, lr=9.67302e-06, gnorm=0.715, train_wall=197, wall=0
2021-01-10 20:03:21 | INFO | train_inner | epoch 092:    462 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.974, nll_loss=0.772, ppl=1.71, wps=5530.2, ups=0.53, wpb=10528.2, bsz=381.3, num_updates=51400, lr=9.6636e-06, gnorm=0.711, train_wall=190, wall=0
2021-01-10 20:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 20:06:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:06:49 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.747 | nll_loss 4.231 | ppl 18.78 | bleu 21.82 | wps 3978 | wpb 7508.5 | bsz 272.7 | num_updates 51499 | best_bleu 22.43
2021-01-10 20:06:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 20:06:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 92 @ 51499 updates, score 21.82) (writing took 2.432808507233858 seconds)
2021-01-10 20:06:51 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-10 20:06:51 | INFO | train | epoch 092 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.983 | nll_loss 0.778 | ppl 1.71 | wps 5405.3 | ups 0.52 | wpb 10483.4 | bsz 369.6 | num_updates 51499 | lr 9.65431e-06 | gnorm 0.719 | train_wall 1057 | wall 0
2021-01-10 20:06:51 | INFO | fairseq.trainer | begin training epoch 93
2021-01-10 20:06:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:06:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:06:56 | INFO | train_inner | epoch 093:      1 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.99, nll_loss=0.783, ppl=1.72, wps=4819.8, ups=0.46, wpb=10399.2, bsz=366.8, num_updates=51500, lr=9.65422e-06, gnorm=0.727, train_wall=186, wall=0
2021-01-10 20:10:05 | INFO | train_inner | epoch 093:    101 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.978, nll_loss=0.774, ppl=1.71, wps=5558.9, ups=0.53, wpb=10476, bsz=376, num_updates=51600, lr=9.64486e-06, gnorm=0.714, train_wall=188, wall=0
2021-01-10 20:13:35 | INFO | train_inner | epoch 093:    201 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.771, ppl=1.71, wps=4941.6, ups=0.48, wpb=10395, bsz=365.2, num_updates=51700, lr=9.63552e-06, gnorm=0.718, train_wall=210, wall=0
2021-01-10 20:17:01 | INFO | train_inner | epoch 093:    301 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=2.997, nll_loss=0.777, ppl=1.71, wps=5024.5, ups=0.48, wpb=10362.5, bsz=371.6, num_updates=51800, lr=9.62622e-06, gnorm=0.726, train_wall=206, wall=0
2021-01-10 20:20:29 | INFO | train_inner | epoch 093:    401 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.786, ppl=1.72, wps=5097.3, ups=0.48, wpb=10584, bsz=359.8, num_updates=51900, lr=9.61694e-06, gnorm=0.722, train_wall=207, wall=0
2021-01-10 20:24:03 | INFO | train_inner | epoch 093:    501 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=2.969, nll_loss=0.776, ppl=1.71, wps=4998.1, ups=0.47, wpb=10685.2, bsz=375.7, num_updates=52000, lr=9.60769e-06, gnorm=0.705, train_wall=214, wall=0
2021-01-10 20:26:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 20:26:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:26:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:26:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:26:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:26:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:26:30 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.752 | nll_loss 4.235 | ppl 18.83 | bleu 21.74 | wps 4580.5 | wpb 7508.5 | bsz 272.7 | num_updates 52060 | best_bleu 22.43
2021-01-10 20:26:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 20:26:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 93 @ 52060 updates, score 21.74) (writing took 2.679919259622693 seconds)
2021-01-10 20:26:32 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-10 20:26:33 | INFO | train | epoch 093 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.983 | nll_loss 0.777 | ppl 1.71 | wps 4978.2 | ups 0.47 | wpb 10483.4 | bsz 369.6 | num_updates 52060 | lr 9.60215e-06 | gnorm 0.719 | train_wall 1152 | wall 0
2021-01-10 20:26:33 | INFO | fairseq.trainer | begin training epoch 94
2021-01-10 20:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:26:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:26:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:28:03 | INFO | train_inner | epoch 094:     40 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.973, nll_loss=0.773, ppl=1.71, wps=4368.5, ups=0.42, wpb=10476.3, bsz=375.8, num_updates=52100, lr=9.59846e-06, gnorm=0.719, train_wall=212, wall=0
2021-01-10 20:31:41 | INFO | train_inner | epoch 094:    140 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.771, ppl=1.71, wps=4789.4, ups=0.46, wpb=10438.6, bsz=355.4, num_updates=52200, lr=9.58927e-06, gnorm=0.719, train_wall=218, wall=0
2021-01-10 20:35:14 | INFO | train_inner | epoch 094:    240 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.778, ppl=1.72, wps=5022.4, ups=0.47, wpb=10708.8, bsz=376.9, num_updates=52300, lr=9.58009e-06, gnorm=0.706, train_wall=213, wall=0
2021-01-10 20:38:45 | INFO | train_inner | epoch 094:    340 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.779, ppl=1.72, wps=4940.9, ups=0.47, wpb=10420.7, bsz=360.3, num_updates=52400, lr=9.57095e-06, gnorm=0.729, train_wall=211, wall=0
2021-01-10 20:42:22 | INFO | train_inner | epoch 094:    440 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.011, nll_loss=0.786, ppl=1.72, wps=4739.1, ups=0.46, wpb=10312, bsz=352.5, num_updates=52500, lr=9.56183e-06, gnorm=0.733, train_wall=217, wall=0
2021-01-10 20:46:03 | INFO | train_inner | epoch 094:    540 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=2.977, nll_loss=0.775, ppl=1.71, wps=4765.3, ups=0.45, wpb=10517.7, bsz=399.8, num_updates=52600, lr=9.55274e-06, gnorm=0.713, train_wall=221, wall=0
2021-01-10 20:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 20:46:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:46:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:46:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:46:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:46:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:46:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:46:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:46:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:46:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:46:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:47:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:47:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:47:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:47:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 20:47:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 20:47:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 20:47:12 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.756 | nll_loss 4.239 | ppl 18.88 | bleu 21.82 | wps 4054.6 | wpb 7508.5 | bsz 272.7 | num_updates 52621 | best_bleu 22.43
2021-01-10 20:47:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 20:47:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:47:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:47:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 94 @ 52621 updates, score 21.82) (writing took 2.6232668552547693 seconds)
2021-01-10 20:47:15 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-10 20:47:15 | INFO | train | epoch 094 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.982 | nll_loss 0.776 | ppl 1.71 | wps 4733.4 | ups 0.45 | wpb 10483.4 | bsz 369.6 | num_updates 52621 | lr 9.55083e-06 | gnorm 0.72 | train_wall 1212 | wall 0
2021-01-10 20:47:15 | INFO | fairseq.trainer | begin training epoch 95
2021-01-10 20:47:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 20:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:47:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 20:50:12 | INFO | train_inner | epoch 095:     79 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.982, nll_loss=0.774, ppl=1.71, wps=4189.4, ups=0.4, wpb=10406.3, bsz=355, num_updates=52700, lr=9.54367e-06, gnorm=0.728, train_wall=218, wall=0
2021-01-10 20:53:54 | INFO | train_inner | epoch 095:    179 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=2.988, nll_loss=0.772, ppl=1.71, wps=4686.4, ups=0.45, wpb=10432.2, bsz=377, num_updates=52800, lr=9.53463e-06, gnorm=0.721, train_wall=222, wall=0
2021-01-10 20:57:35 | INFO | train_inner | epoch 095:    279 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=2.98, nll_loss=0.771, ppl=1.71, wps=4727.1, ups=0.45, wpb=10460.9, bsz=363.4, num_updates=52900, lr=9.52561e-06, gnorm=0.72, train_wall=221, wall=0
2021-01-10 21:01:15 | INFO | train_inner | epoch 095:    379 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=2.968, nll_loss=0.772, ppl=1.71, wps=4801.4, ups=0.45, wpb=10564.5, bsz=370.6, num_updates=53000, lr=9.51662e-06, gnorm=0.709, train_wall=220, wall=0
2021-01-10 21:04:56 | INFO | train_inner | epoch 095:    479 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=2.983, nll_loss=0.783, ppl=1.72, wps=4812.3, ups=0.45, wpb=10590.6, bsz=379, num_updates=53100, lr=9.50765e-06, gnorm=0.715, train_wall=220, wall=0
2021-01-10 21:07:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-10 21:07:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:07:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:07:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:08:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:08:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:08:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:08:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-10 21:08:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-10 21:08:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-10 21:08:21 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.752 | nll_loss 4.237 | ppl 18.86 | bleu 21.91 | wps 3910.2 | wpb 7508.5 | bsz 272.7 | num_updates 53182 | best_bleu 22.43
2021-01-10 21:08:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-10 21:08:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:08:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:08:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/de_std1/checkpoint_last.pt (epoch 95 @ 53182 updates, score 21.91) (writing took 2.6346797104924917 seconds)
2021-01-10 21:08:24 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-10 21:08:24 | INFO | train | epoch 095 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 2.981 | nll_loss 0.776 | ppl 1.71 | wps 4636 | ups 0.44 | wpb 10483.4 | bsz 369.6 | num_updates 53182 | lr 9.50032e-06 | gnorm 0.719 | train_wall 1237 | wall 0
2021-01-10 21:08:24 | INFO | fairseq.trainer | begin training epoch 96
2021-01-10 21:08:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:08:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:08:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-10 21:08:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-10 21:09:05 | INFO | train_inner | epoch 096:     18 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=2.98, nll_loss=0.779, ppl=1.72, wps=4171.4, ups=0.4, wpb=10410.9, bsz=364.1, num_updates=53200, lr=9.49871e-06, gnorm=0.724, train_wall=219, wall=0
2021-01-10 21:12:47 | INFO | train_inner | epoch 096:    118 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=2.966, nll_loss=0.772, ppl=1.71, wps=4808.5, ups=0.45, wpb=10682.8, bsz=378, num_updates=53300, lr=9.4898e-06, gnorm=0.705, train_wall=222, wall=0
2021-01-10 21:16:26 | INFO | train_inner | epoch 096:    218 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=2.987, nll_loss=0.774, ppl=1.71, wps=4756.2, ups=0.46, wpb=10417.9, bsz=376.6, num_updates=53400, lr=9.48091e-06, gnorm=0.718, train_wall=219, wall=0
2021-01-10 21:20:01 | INFO | train_inner | epoch 096:    318 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=2.976, nll_loss=0.771, ppl=1.71, wps=4878.7, ups=0.47, wpb=10461.1, bsz=374.6, num_updates=53500, lr=9.47204e-06, gnorm=0.719, train_wall=214, wall=0
2021-01-10 21:23:38 | INFO | train_inner | epoch 096:    418 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=2.994, nll_loss=0.786, ppl=1.72, wps=4823.5, ups=0.46, wpb=10460.9, bsz=368.1, num_updates=53600, lr=9.4632e-06, gnorm=0.73, train_wall=217, wall=0
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 588 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
