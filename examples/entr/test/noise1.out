nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/closer_gap
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00001
lrscheduler=fixed
warmup_updates=0
max_epoch=100
r3f_lambda=1
extr=''
2020-12-18 12:32:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:32:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:32:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:32:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:32:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:32:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:32:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:32:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:32:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:32:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:10064
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10064
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10064
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:10064
2020-12-18 12:32:57 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 3
2020-12-18 12:32:58 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-18 12:32:58 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-18 12:33:02 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10064', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[1e-05], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer_gap', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
2020-12-18 12:33:02 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-18 12:33:02 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-18 12:33:02 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-18 12:33:02 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-18 12:33:02 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-18 12:33:03 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-18 12:33:03 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-18 12:33:03 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-18 12:33:03 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-18 12:33:03 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-18 12:33:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-18 12:33:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-18 12:33:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-18 12:33:03 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-18 12:33:03 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-18 12:33:03 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-18 12:33:03 | INFO | fairseq.utils | rank   3: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-18 12:33:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2020-12-18 12:33:03 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2020-12-18 12:33:03 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-18 12:33:03 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-18 12:33:04 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-18 12:33:04 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-18 12:33:04 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-18 12:33:04 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-18 12:33:04 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-18 12:33:04 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-18 12:33:04 | INFO | fairseq.trainer | begin training epoch 1
2020-12-18 12:33:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:33:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:33:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:33:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:33:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:33:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:33:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:33:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:34:06 | INFO | train_inner | epoch 001:    100 / 421 symm_kl=0.741, self_kl=0, loss=3.534, nll_loss=0.377, ppl=1.3, wps=24434.9, ups=1.73, wpb=14145.4, bsz=485.9, num_updates=100, lr=1e-05, gnorm=1.871, train_wall=58, wall=63
2020-12-18 12:35:05 | INFO | train_inner | epoch 001:    200 / 421 symm_kl=0.649, self_kl=0, loss=3.406, nll_loss=0.376, ppl=1.3, wps=23839.6, ups=1.7, wpb=13995.2, bsz=501.2, num_updates=200, lr=1e-05, gnorm=1.484, train_wall=59, wall=122
2020-12-18 12:36:04 | INFO | train_inner | epoch 001:    300 / 421 symm_kl=0.631, self_kl=0, loss=3.359, nll_loss=0.385, ppl=1.31, wps=23591.8, ups=1.69, wpb=13972.8, bsz=508.7, num_updates=300, lr=1e-05, gnorm=1.412, train_wall=59, wall=181
2020-12-18 12:37:04 | INFO | train_inner | epoch 001:    400 / 421 symm_kl=0.622, self_kl=0, loss=3.333, nll_loss=0.396, ppl=1.32, wps=23290.8, ups=1.68, wpb=13886.9, bsz=487.4, num_updates=400, lr=1e-05, gnorm=1.408, train_wall=59, wall=241
2020-12-18 12:37:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 12:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:37:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:37:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:37:34 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 1.658 | self_kl 0 | loss 8.421 | nll_loss 4.366 | ppl 20.62 | bleu 19.8 | wps 5548.8 | wpb 10324.2 | bsz 375 | num_updates 421
2020-12-18 12:37:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 12:37:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 1 @ 421 updates, score 19.8) (writing took 2.3556629214435816 seconds)
2020-12-18 12:37:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-18 12:37:36 | INFO | train | epoch 001 | symm_kl 0.659 | self_kl 0 | loss 3.405 | nll_loss 0.384 | ppl 1.31 | wps 21941.4 | ups 1.57 | wpb 13969.5 | bsz 492.6 | num_updates 421 | lr 1e-05 | gnorm 1.54 | train_wall 248 | wall 273
2020-12-18 12:37:36 | INFO | fairseq.trainer | begin training epoch 2
2020-12-18 12:37:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:37:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:37:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:38:26 | INFO | train_inner | epoch 002:     79 / 421 symm_kl=0.607, self_kl=0, loss=3.287, nll_loss=0.401, ppl=1.32, wps=16854.6, ups=1.22, wpb=13861.1, bsz=487.7, num_updates=500, lr=1e-05, gnorm=1.402, train_wall=59, wall=323
2020-12-18 12:39:26 | INFO | train_inner | epoch 002:    179 / 421 symm_kl=0.582, self_kl=0, loss=3.228, nll_loss=0.413, ppl=1.33, wps=23431.7, ups=1.67, wpb=14007.7, bsz=489.9, num_updates=600, lr=1e-05, gnorm=1.316, train_wall=60, wall=383
2020-12-18 12:40:26 | INFO | train_inner | epoch 002:    279 / 421 symm_kl=0.572, self_kl=0, loss=3.199, nll_loss=0.43, ppl=1.35, wps=23306.2, ups=1.66, wpb=14033.1, bsz=489, num_updates=700, lr=1e-05, gnorm=1.298, train_wall=60, wall=443
2020-12-18 12:41:26 | INFO | train_inner | epoch 002:    379 / 421 symm_kl=0.571, self_kl=0, loss=3.195, nll_loss=0.45, ppl=1.37, wps=23362.9, ups=1.68, wpb=13928.4, bsz=496.1, num_updates=800, lr=1e-05, gnorm=1.275, train_wall=59, wall=502
2020-12-18 12:41:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 12:41:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:41:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:41:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:41:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:41:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:41:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:41:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:41:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:42:11 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 1.517 | self_kl 0 | loss 8.023 | nll_loss 4.275 | ppl 19.36 | bleu 19.81 | wps 4638.8 | wpb 10324.2 | bsz 375 | num_updates 842 | best_bleu 19.81
2020-12-18 12:42:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 12:42:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:42:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:42:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:42:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 2 @ 842 updates, score 19.81) (writing took 4.8638037499040365 seconds)
2020-12-18 12:42:16 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-18 12:42:16 | INFO | train | epoch 002 | symm_kl 0.579 | self_kl 0 | loss 3.217 | nll_loss 0.427 | ppl 1.34 | wps 21041.1 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 842 | lr 1e-05 | gnorm 1.312 | train_wall 250 | wall 553
2020-12-18 12:42:16 | INFO | fairseq.trainer | begin training epoch 3
2020-12-18 12:42:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:42:52 | INFO | train_inner | epoch 003:     58 / 421 symm_kl=0.556, self_kl=0, loss=3.157, nll_loss=0.448, ppl=1.36, wps=15989.3, ups=1.15, wpb=13895.5, bsz=482.9, num_updates=900, lr=1e-05, gnorm=1.257, train_wall=58, wall=589
2020-12-18 12:43:52 | INFO | train_inner | epoch 003:    158 / 421 symm_kl=0.553, self_kl=0, loss=3.151, nll_loss=0.455, ppl=1.37, wps=23278.1, ups=1.67, wpb=13938, bsz=507, num_updates=1000, lr=1e-05, gnorm=1.236, train_wall=60, wall=649
2020-12-18 12:44:52 | INFO | train_inner | epoch 003:    258 / 421 symm_kl=0.555, self_kl=0, loss=3.151, nll_loss=0.46, ppl=1.38, wps=23321.2, ups=1.67, wpb=13957.9, bsz=482.5, num_updates=1100, lr=1e-05, gnorm=1.237, train_wall=60, wall=709
2020-12-18 12:45:52 | INFO | train_inner | epoch 003:    358 / 421 symm_kl=0.542, self_kl=0, loss=3.127, nll_loss=0.463, ppl=1.38, wps=23562.5, ups=1.67, wpb=14135, bsz=507.8, num_updates=1200, lr=1e-05, gnorm=1.194, train_wall=60, wall=769
2020-12-18 12:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 12:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:46:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:46:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:46:47 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 1.453 | self_kl 0 | loss 7.843 | nll_loss 4.234 | ppl 18.82 | bleu 19.88 | wps 5590 | wpb 10324.2 | bsz 375 | num_updates 1263 | best_bleu 19.88
2020-12-18 12:46:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 12:46:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:46:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 3 @ 1263 updates, score 19.88) (writing took 5.273843782022595 seconds)
2020-12-18 12:46:53 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-18 12:46:53 | INFO | train | epoch 003 | symm_kl 0.55 | self_kl 0 | loss 3.141 | nll_loss 0.457 | ppl 1.37 | wps 21248.8 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 1263 | lr 1e-05 | gnorm 1.227 | train_wall 250 | wall 829
2020-12-18 12:46:53 | INFO | fairseq.trainer | begin training epoch 4
2020-12-18 12:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:46:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:47:17 | INFO | train_inner | epoch 004:     37 / 421 symm_kl=0.548, self_kl=0, loss=3.129, nll_loss=0.459, ppl=1.37, wps=16406.1, ups=1.18, wpb=13896.1, bsz=467.7, num_updates=1300, lr=1e-05, gnorm=1.232, train_wall=58, wall=854
2020-12-18 12:48:17 | INFO | train_inner | epoch 004:    137 / 421 symm_kl=0.542, self_kl=0, loss=3.119, nll_loss=0.459, ppl=1.37, wps=23032.9, ups=1.68, wpb=13735.4, bsz=496.9, num_updates=1400, lr=1e-05, gnorm=1.217, train_wall=59, wall=913
2020-12-18 12:49:17 | INFO | train_inner | epoch 004:    237 / 421 symm_kl=0.54, self_kl=0, loss=3.114, nll_loss=0.46, ppl=1.38, wps=23379.1, ups=1.66, wpb=14090.6, bsz=475.5, num_updates=1500, lr=1e-05, gnorm=1.189, train_wall=60, wall=974
2020-12-18 12:50:18 | INFO | train_inner | epoch 004:    337 / 421 symm_kl=0.534, self_kl=0, loss=3.104, nll_loss=0.462, ppl=1.38, wps=23096.4, ups=1.64, wpb=14057, bsz=499.7, num_updates=1600, lr=1e-05, gnorm=1.174, train_wall=61, wall=1035
2020-12-18 12:51:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 12:51:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:51:27 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 1.415 | self_kl 0 | loss 7.757 | nll_loss 4.224 | ppl 18.69 | bleu 19.63 | wps 5445.6 | wpb 10324.2 | bsz 375 | num_updates 1684 | best_bleu 19.88
2020-12-18 12:51:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 12:51:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 4 @ 1684 updates, score 19.63) (writing took 3.104959750548005 seconds)
2020-12-18 12:51:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-18 12:51:30 | INFO | train | epoch 004 | symm_kl 0.538 | self_kl 0 | loss 3.111 | nll_loss 0.461 | ppl 1.38 | wps 21214.6 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 1684 | lr 1e-05 | gnorm 1.191 | train_wall 252 | wall 1107
2020-12-18 12:51:30 | INFO | fairseq.trainer | begin training epoch 5
2020-12-18 12:51:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:51:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:51:43 | INFO | train_inner | epoch 005:     16 / 421 symm_kl=0.533, self_kl=0, loss=3.101, nll_loss=0.463, ppl=1.38, wps=16412.7, ups=1.17, wpb=13973, bsz=503, num_updates=1700, lr=1e-05, gnorm=1.174, train_wall=60, wall=1120
2020-12-18 12:52:43 | INFO | train_inner | epoch 005:    116 / 421 symm_kl=0.53, self_kl=0, loss=3.092, nll_loss=0.46, ppl=1.38, wps=23205.8, ups=1.66, wpb=13947.8, bsz=499.7, num_updates=1800, lr=1e-05, gnorm=1.192, train_wall=60, wall=1180
2020-12-18 12:53:44 | INFO | train_inner | epoch 005:    216 / 421 symm_kl=0.535, self_kl=0, loss=3.1, nll_loss=0.462, ppl=1.38, wps=23128.6, ups=1.65, wpb=14054.4, bsz=484.6, num_updates=1900, lr=1e-05, gnorm=1.176, train_wall=61, wall=1241
2020-12-18 12:54:44 | INFO | train_inner | epoch 005:    316 / 421 symm_kl=0.534, self_kl=0, loss=3.099, nll_loss=0.462, ppl=1.38, wps=23213.9, ups=1.67, wpb=13937.1, bsz=501.2, num_updates=2000, lr=1e-05, gnorm=1.178, train_wall=60, wall=1301
2020-12-18 12:55:43 | INFO | train_inner | epoch 005:    416 / 421 symm_kl=0.525, self_kl=0, loss=3.078, nll_loss=0.459, ppl=1.37, wps=23481, ups=1.68, wpb=14006.7, bsz=492.6, num_updates=2100, lr=1e-05, gnorm=1.155, train_wall=59, wall=1360
2020-12-18 12:55:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 12:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:55:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 12:55:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 12:55:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 12:56:04 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 1.387 | self_kl 0 | loss 7.699 | nll_loss 4.221 | ppl 18.65 | bleu 19.47 | wps 5606.2 | wpb 10324.2 | bsz 375 | num_updates 2105 | best_bleu 19.88
2020-12-18 12:56:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 12:56:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:56:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:56:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:56:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 5 @ 2105 updates, score 19.47) (writing took 2.963638225570321 seconds)
2020-12-18 12:56:07 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-18 12:56:07 | INFO | train | epoch 005 | symm_kl 0.532 | self_kl 0 | loss 3.094 | nll_loss 0.461 | ppl 1.38 | wps 21220.1 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 2105 | lr 1e-05 | gnorm 1.179 | train_wall 252 | wall 1384
2020-12-18 12:56:07 | INFO | fairseq.trainer | begin training epoch 6
2020-12-18 12:56:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 12:56:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 12:57:06 | INFO | train_inner | epoch 006:     95 / 421 symm_kl=0.528, self_kl=0, loss=3.082, nll_loss=0.458, ppl=1.37, wps=16785, ups=1.21, wpb=13827.8, bsz=506.6, num_updates=2200, lr=1e-05, gnorm=1.175, train_wall=58, wall=1443
2020-12-18 12:58:06 | INFO | train_inner | epoch 006:    195 / 421 symm_kl=0.529, self_kl=0, loss=3.085, nll_loss=0.46, ppl=1.38, wps=23373.9, ups=1.66, wpb=14087.8, bsz=489.6, num_updates=2300, lr=1e-05, gnorm=1.159, train_wall=60, wall=1503
2020-12-18 12:59:05 | INFO | train_inner | epoch 006:    295 / 421 symm_kl=0.532, self_kl=0, loss=3.09, nll_loss=0.463, ppl=1.38, wps=23694.6, ups=1.69, wpb=14036.1, bsz=477.8, num_updates=2400, lr=1e-05, gnorm=1.168, train_wall=59, wall=1562
2020-12-18 13:00:05 | INFO | train_inner | epoch 006:    395 / 421 symm_kl=0.519, self_kl=0, loss=3.065, nll_loss=0.457, ppl=1.37, wps=23391.7, ups=1.67, wpb=13994.5, bsz=506.2, num_updates=2500, lr=1e-05, gnorm=1.141, train_wall=60, wall=1622
2020-12-18 13:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:00:39 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 1.377 | self_kl 0 | loss 7.666 | nll_loss 4.213 | ppl 18.55 | bleu 19.9 | wps 5344.9 | wpb 10324.2 | bsz 375 | num_updates 2526 | best_bleu 19.9
2020-12-18 13:00:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:00:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 6 @ 2526 updates, score 19.9) (writing took 4.783203447237611 seconds)
2020-12-18 13:00:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-18 13:00:43 | INFO | train | epoch 006 | symm_kl 0.528 | self_kl 0 | loss 3.082 | nll_loss 0.46 | ppl 1.38 | wps 21263.3 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 2526 | lr 1e-05 | gnorm 1.162 | train_wall 250 | wall 1660
2020-12-18 13:00:43 | INFO | fairseq.trainer | begin training epoch 7
2020-12-18 13:00:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:00:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:01:30 | INFO | train_inner | epoch 007:     74 / 421 symm_kl=0.527, self_kl=0, loss=3.08, nll_loss=0.461, ppl=1.38, wps=16213.7, ups=1.18, wpb=13772.9, bsz=486.4, num_updates=2600, lr=1e-05, gnorm=1.172, train_wall=59, wall=1707
2020-12-18 13:02:29 | INFO | train_inner | epoch 007:    174 / 421 symm_kl=0.526, self_kl=0, loss=3.078, nll_loss=0.461, ppl=1.38, wps=23442.7, ups=1.69, wpb=13910.1, bsz=482.8, num_updates=2700, lr=1e-05, gnorm=1.154, train_wall=59, wall=1766
2020-12-18 13:03:29 | INFO | train_inner | epoch 007:    274 / 421 symm_kl=0.524, self_kl=0, loss=3.072, nll_loss=0.458, ppl=1.37, wps=23270.8, ups=1.68, wpb=13879.3, bsz=492.2, num_updates=2800, lr=1e-05, gnorm=1.161, train_wall=59, wall=1826
2020-12-18 13:04:29 | INFO | train_inner | epoch 007:    374 / 421 symm_kl=0.523, self_kl=0, loss=3.068, nll_loss=0.458, ppl=1.37, wps=23521.3, ups=1.66, wpb=14151.4, bsz=491.7, num_updates=2900, lr=1e-05, gnorm=1.144, train_wall=60, wall=1886
2020-12-18 13:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:04:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:04:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:04:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:04:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:05:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:05:15 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 1.357 | self_kl 0 | loss 7.64 | nll_loss 4.22 | ppl 18.64 | bleu 19.55 | wps 5362.4 | wpb 10324.2 | bsz 375 | num_updates 2947 | best_bleu 19.9
2020-12-18 13:05:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:05:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:05:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:05:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:05:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 7 @ 2947 updates, score 19.55) (writing took 2.979540066793561 seconds)
2020-12-18 13:05:18 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-18 13:05:18 | INFO | train | epoch 007 | symm_kl 0.524 | self_kl 0 | loss 3.072 | nll_loss 0.459 | ppl 1.37 | wps 21400.3 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 2947 | lr 1e-05 | gnorm 1.153 | train_wall 250 | wall 1935
2020-12-18 13:05:18 | INFO | fairseq.trainer | begin training epoch 8
2020-12-18 13:05:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:05:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:05:53 | INFO | train_inner | epoch 008:     53 / 421 symm_kl=0.52, self_kl=0, loss=3.063, nll_loss=0.458, ppl=1.37, wps=16914.6, ups=1.2, wpb=14109.9, bsz=506.2, num_updates=3000, lr=1e-05, gnorm=1.139, train_wall=59, wall=1970
2020-12-18 13:06:52 | INFO | train_inner | epoch 008:    153 / 421 symm_kl=0.526, self_kl=0, loss=3.071, nll_loss=0.455, ppl=1.37, wps=23391.7, ups=1.68, wpb=13958.9, bsz=475.6, num_updates=3100, lr=1e-05, gnorm=1.159, train_wall=59, wall=2029
2020-12-18 13:07:52 | INFO | train_inner | epoch 008:    253 / 421 symm_kl=0.52, self_kl=0, loss=3.062, nll_loss=0.458, ppl=1.37, wps=23296.5, ups=1.68, wpb=13843, bsz=502.1, num_updates=3200, lr=1e-05, gnorm=1.152, train_wall=59, wall=2089
2020-12-18 13:08:52 | INFO | train_inner | epoch 008:    353 / 421 symm_kl=0.512, self_kl=0, loss=3.048, nll_loss=0.457, ppl=1.37, wps=23590.2, ups=1.67, wpb=14141.4, bsz=514.8, num_updates=3300, lr=1e-05, gnorm=1.13, train_wall=60, wall=2149
2020-12-18 13:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:09:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:09:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:09:50 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 1.353 | self_kl 0 | loss 7.619 | nll_loss 4.215 | ppl 18.57 | bleu 20.07 | wps 5567.2 | wpb 10324.2 | bsz 375 | num_updates 3368 | best_bleu 20.07
2020-12-18 13:09:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:09:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 8 @ 3368 updates, score 20.07) (writing took 4.917325165122747 seconds)
2020-12-18 13:09:54 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-18 13:09:54 | INFO | train | epoch 008 | symm_kl 0.521 | self_kl 0 | loss 3.063 | nll_loss 0.458 | ppl 1.37 | wps 21294.6 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 3368 | lr 1e-05 | gnorm 1.15 | train_wall 249 | wall 2211
2020-12-18 13:09:54 | INFO | fairseq.trainer | begin training epoch 9
2020-12-18 13:09:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:09:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:10:16 | INFO | train_inner | epoch 009:     32 / 421 symm_kl=0.527, self_kl=0, loss=3.074, nll_loss=0.46, ppl=1.38, wps=16332.9, ups=1.18, wpb=13812.4, bsz=478.6, num_updates=3400, lr=1e-05, gnorm=1.172, train_wall=59, wall=2233
2020-12-18 13:11:16 | INFO | train_inner | epoch 009:    132 / 421 symm_kl=0.524, self_kl=0, loss=3.064, nll_loss=0.455, ppl=1.37, wps=23475, ups=1.68, wpb=13981.2, bsz=478.7, num_updates=3500, lr=1e-05, gnorm=1.154, train_wall=59, wall=2293
2020-12-18 13:12:16 | INFO | train_inner | epoch 009:    232 / 421 symm_kl=0.515, self_kl=0, loss=3.049, nll_loss=0.454, ppl=1.37, wps=23396.9, ups=1.67, wpb=14040.5, bsz=491.4, num_updates=3600, lr=1e-05, gnorm=1.134, train_wall=60, wall=2353
2020-12-18 13:13:16 | INFO | train_inner | epoch 009:    332 / 421 symm_kl=0.516, self_kl=0, loss=3.053, nll_loss=0.457, ppl=1.37, wps=23402.2, ups=1.67, wpb=14010.8, bsz=506.9, num_updates=3700, lr=1e-05, gnorm=1.14, train_wall=60, wall=2413
2020-12-18 13:14:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:14:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:14:28 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 1.35 | self_kl 0 | loss 7.616 | nll_loss 4.216 | ppl 18.58 | bleu 19.66 | wps 5200.8 | wpb 10324.2 | bsz 375 | num_updates 3789 | best_bleu 20.07
2020-12-18 13:14:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 9 @ 3789 updates, score 19.66) (writing took 4.366965897381306 seconds)
2020-12-18 13:14:32 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-18 13:14:32 | INFO | train | epoch 009 | symm_kl 0.519 | self_kl 0 | loss 3.057 | nll_loss 0.456 | ppl 1.37 | wps 21196.4 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 3789 | lr 1e-05 | gnorm 1.146 | train_wall 250 | wall 2489
2020-12-18 13:14:32 | INFO | fairseq.trainer | begin training epoch 10
2020-12-18 13:14:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:14:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:14:42 | INFO | train_inner | epoch 010:     11 / 421 symm_kl=0.52, self_kl=0, loss=3.061, nll_loss=0.459, ppl=1.37, wps=16025.4, ups=1.16, wpb=13765.2, bsz=485.4, num_updates=3800, lr=1e-05, gnorm=1.154, train_wall=59, wall=2498
2020-12-18 13:15:41 | INFO | train_inner | epoch 010:    111 / 421 symm_kl=0.519, self_kl=0, loss=3.056, nll_loss=0.457, ppl=1.37, wps=23868.7, ups=1.69, wpb=14110.2, bsz=485.5, num_updates=3900, lr=1e-05, gnorm=1.133, train_wall=59, wall=2558
2020-12-18 13:16:41 | INFO | train_inner | epoch 010:    211 / 421 symm_kl=0.525, self_kl=0, loss=3.064, nll_loss=0.455, ppl=1.37, wps=23259.6, ups=1.67, wpb=13947.9, bsz=478.8, num_updates=4000, lr=1e-05, gnorm=1.15, train_wall=60, wall=2618
2020-12-18 13:17:40 | INFO | train_inner | epoch 010:    311 / 421 symm_kl=0.509, self_kl=0, loss=3.035, nll_loss=0.45, ppl=1.37, wps=23480.9, ups=1.67, wpb=14032.4, bsz=515.1, num_updates=4100, lr=1e-05, gnorm=1.125, train_wall=60, wall=2677
2020-12-18 13:18:40 | INFO | train_inner | epoch 010:    411 / 421 symm_kl=0.518, self_kl=0, loss=3.053, nll_loss=0.455, ppl=1.37, wps=23355.3, ups=1.68, wpb=13937.4, bsz=501.3, num_updates=4200, lr=1e-05, gnorm=1.142, train_wall=59, wall=2737
2020-12-18 13:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:18:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:18:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:18:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:18:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:18:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:19:04 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 1.339 | self_kl 0 | loss 7.593 | nll_loss 4.217 | ppl 18.59 | bleu 19.85 | wps 5402.4 | wpb 10324.2 | bsz 375 | num_updates 4210 | best_bleu 20.07
2020-12-18 13:19:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:19:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:19:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:19:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 10 @ 4210 updates, score 19.85) (writing took 2.907196259126067 seconds)
2020-12-18 13:19:07 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-12-18 13:19:07 | INFO | train | epoch 010 | symm_kl 0.518 | self_kl 0 | loss 3.053 | nll_loss 0.454 | ppl 1.37 | wps 21380.5 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 4210 | lr 1e-05 | gnorm 1.141 | train_wall 250 | wall 2764
2020-12-18 13:19:07 | INFO | fairseq.trainer | begin training epoch 11
2020-12-18 13:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:20:03 | INFO | train_inner | epoch 011:     90 / 421 symm_kl=0.519, self_kl=0, loss=3.051, nll_loss=0.452, ppl=1.37, wps=16668.5, ups=1.21, wpb=13795, bsz=468.2, num_updates=4300, lr=1e-05, gnorm=1.157, train_wall=58, wall=2820
2020-12-18 13:21:03 | INFO | train_inner | epoch 011:    190 / 421 symm_kl=0.51, self_kl=0, loss=3.036, nll_loss=0.452, ppl=1.37, wps=23299.6, ups=1.66, wpb=14072.6, bsz=503.6, num_updates=4400, lr=1e-05, gnorm=1.125, train_wall=60, wall=2880
2020-12-18 13:22:04 | INFO | train_inner | epoch 011:    290 / 421 symm_kl=0.527, self_kl=0, loss=3.069, nll_loss=0.459, ppl=1.37, wps=22795.8, ups=1.65, wpb=13821.1, bsz=488.1, num_updates=4500, lr=1e-05, gnorm=1.151, train_wall=60, wall=2941
2020-12-18 13:23:05 | INFO | train_inner | epoch 011:    390 / 421 symm_kl=0.512, self_kl=0, loss=3.038, nll_loss=0.452, ppl=1.37, wps=23331.3, ups=1.65, wpb=14171.3, bsz=501.5, num_updates=4600, lr=1e-05, gnorm=1.131, train_wall=60, wall=3002
2020-12-18 13:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:23:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:23:43 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 1.332 | self_kl 0 | loss 7.588 | nll_loss 4.22 | ppl 18.63 | bleu 19.75 | wps 5153.4 | wpb 10324.2 | bsz 375 | num_updates 4631 | best_bleu 20.07
2020-12-18 13:23:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:23:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 11 @ 4631 updates, score 19.75) (writing took 2.877268470823765 seconds)
2020-12-18 13:23:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-12-18 13:23:46 | INFO | train | epoch 011 | symm_kl 0.516 | self_kl 0 | loss 3.047 | nll_loss 0.453 | ppl 1.37 | wps 21096.4 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 4631 | lr 1e-05 | gnorm 1.139 | train_wall 252 | wall 3043
2020-12-18 13:23:46 | INFO | fairseq.trainer | begin training epoch 12
2020-12-18 13:23:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:23:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:23:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:24:29 | INFO | train_inner | epoch 012:     69 / 421 symm_kl=0.512, self_kl=0, loss=3.038, nll_loss=0.451, ppl=1.37, wps=16366.9, ups=1.18, wpb=13876.4, bsz=488.4, num_updates=4700, lr=1e-05, gnorm=1.14, train_wall=59, wall=3086
2020-12-18 13:25:29 | INFO | train_inner | epoch 012:    169 / 421 symm_kl=0.517, self_kl=0, loss=3.048, nll_loss=0.454, ppl=1.37, wps=23281.8, ups=1.68, wpb=13884.5, bsz=497.5, num_updates=4800, lr=1e-05, gnorm=1.137, train_wall=59, wall=3146
2020-12-18 13:26:29 | INFO | train_inner | epoch 012:    269 / 421 symm_kl=0.508, self_kl=0, loss=3.03, nll_loss=0.449, ppl=1.37, wps=23530.6, ups=1.65, wpb=14222.4, bsz=501.7, num_updates=4900, lr=1e-05, gnorm=1.12, train_wall=60, wall=3206
2020-12-18 13:27:29 | INFO | train_inner | epoch 012:    369 / 421 symm_kl=0.519, self_kl=0, loss=3.051, nll_loss=0.455, ppl=1.37, wps=23180.9, ups=1.67, wpb=13899.9, bsz=487.4, num_updates=5000, lr=1e-05, gnorm=1.14, train_wall=60, wall=3266
2020-12-18 13:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:28:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:28:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:28:18 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 1.338 | self_kl 0 | loss 7.6 | nll_loss 4.223 | ppl 18.68 | bleu 19.73 | wps 5629 | wpb 10324.2 | bsz 375 | num_updates 5052 | best_bleu 20.07
2020-12-18 13:28:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:28:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 12 @ 5052 updates, score 19.73) (writing took 2.408785590901971 seconds)
2020-12-18 13:28:21 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-12-18 13:28:21 | INFO | train | epoch 012 | symm_kl 0.514 | self_kl 0 | loss 3.042 | nll_loss 0.452 | ppl 1.37 | wps 21400.8 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 5052 | lr 1e-05 | gnorm 1.135 | train_wall 251 | wall 3318
2020-12-18 13:28:21 | INFO | fairseq.trainer | begin training epoch 13
2020-12-18 13:28:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:28:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:28:52 | INFO | train_inner | epoch 013:     48 / 421 symm_kl=0.516, self_kl=0, loss=3.041, nll_loss=0.45, ppl=1.37, wps=16908.7, ups=1.22, wpb=13889.5, bsz=482.8, num_updates=5100, lr=1e-05, gnorm=1.149, train_wall=59, wall=3348
2020-12-18 13:29:52 | INFO | train_inner | epoch 013:    148 / 421 symm_kl=0.515, self_kl=0, loss=3.042, nll_loss=0.451, ppl=1.37, wps=23486.7, ups=1.67, wpb=14105.2, bsz=473, num_updates=5200, lr=1e-05, gnorm=1.129, train_wall=60, wall=3409
2020-12-18 13:30:52 | INFO | train_inner | epoch 013:    248 / 421 symm_kl=0.512, self_kl=0, loss=3.035, nll_loss=0.448, ppl=1.36, wps=23276.2, ups=1.67, wpb=13944.9, bsz=504.8, num_updates=5300, lr=1e-05, gnorm=1.146, train_wall=60, wall=3468
2020-12-18 13:31:51 | INFO | train_inner | epoch 013:    348 / 421 symm_kl=0.509, self_kl=0, loss=3.032, nll_loss=0.451, ppl=1.37, wps=23601.1, ups=1.68, wpb=14073.3, bsz=500.6, num_updates=5400, lr=1e-05, gnorm=1.114, train_wall=59, wall=3528
2020-12-18 13:32:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:32:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:32:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:32:53 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 1.33 | self_kl 0 | loss 7.576 | nll_loss 4.222 | ppl 18.66 | bleu 19.95 | wps 5534.5 | wpb 10324.2 | bsz 375 | num_updates 5473 | best_bleu 20.07
2020-12-18 13:32:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:32:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:32:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 13 @ 5473 updates, score 19.95) (writing took 2.8929374869912863 seconds)
2020-12-18 13:32:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-12-18 13:32:56 | INFO | train | epoch 013 | symm_kl 0.514 | self_kl 0 | loss 3.039 | nll_loss 0.45 | ppl 1.37 | wps 21385.4 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 5473 | lr 1e-05 | gnorm 1.136 | train_wall 250 | wall 3593
2020-12-18 13:32:56 | INFO | fairseq.trainer | begin training epoch 14
2020-12-18 13:32:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:32:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:33:14 | INFO | train_inner | epoch 014:     27 / 421 symm_kl=0.515, self_kl=0, loss=3.042, nll_loss=0.452, ppl=1.37, wps=16608.3, ups=1.2, wpb=13804.5, bsz=491.3, num_updates=5500, lr=1e-05, gnorm=1.148, train_wall=59, wall=3611
2020-12-18 13:34:14 | INFO | train_inner | epoch 014:    127 / 421 symm_kl=0.512, self_kl=0, loss=3.034, nll_loss=0.449, ppl=1.37, wps=23710.3, ups=1.68, wpb=14140.2, bsz=491.6, num_updates=5600, lr=1e-05, gnorm=1.127, train_wall=59, wall=3671
2020-12-18 13:35:13 | INFO | train_inner | epoch 014:    227 / 421 symm_kl=0.511, self_kl=0, loss=3.032, nll_loss=0.449, ppl=1.37, wps=23332.2, ups=1.68, wpb=13893.4, bsz=488.5, num_updates=5700, lr=1e-05, gnorm=1.135, train_wall=59, wall=3730
2020-12-18 13:36:13 | INFO | train_inner | epoch 014:    327 / 421 symm_kl=0.518, self_kl=0, loss=3.046, nll_loss=0.451, ppl=1.37, wps=23170.8, ups=1.67, wpb=13861.2, bsz=485.7, num_updates=5800, lr=1e-05, gnorm=1.146, train_wall=60, wall=3790
2020-12-18 13:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:37:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:37:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:37:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:37:26 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 1.327 | self_kl 0 | loss 7.582 | nll_loss 4.226 | ppl 18.72 | bleu 19.86 | wps 6177.6 | wpb 10324.2 | bsz 375 | num_updates 5894 | best_bleu 20.07
2020-12-18 13:37:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 14 @ 5894 updates, score 19.86) (writing took 3.1354823000729084 seconds)
2020-12-18 13:37:29 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-12-18 13:37:29 | INFO | train | epoch 014 | symm_kl 0.512 | self_kl 0 | loss 3.034 | nll_loss 0.449 | ppl 1.37 | wps 21483.4 | ups 1.54 | wpb 13969.5 | bsz 492.6 | num_updates 5894 | lr 1e-05 | gnorm 1.135 | train_wall 250 | wall 3866
2020-12-18 13:37:29 | INFO | fairseq.trainer | begin training epoch 15
2020-12-18 13:37:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:37:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:37:36 | INFO | train_inner | epoch 015:      6 / 421 symm_kl=0.508, self_kl=0, loss=3.028, nll_loss=0.449, ppl=1.37, wps=16871.5, ups=1.21, wpb=13964.8, bsz=507.2, num_updates=5900, lr=1e-05, gnorm=1.134, train_wall=60, wall=3873
2020-12-18 13:38:35 | INFO | train_inner | epoch 015:    106 / 421 symm_kl=0.516, self_kl=0, loss=3.041, nll_loss=0.451, ppl=1.37, wps=23498.9, ups=1.69, wpb=13909.8, bsz=489.7, num_updates=6000, lr=1e-05, gnorm=1.132, train_wall=59, wall=3932
2020-12-18 13:39:35 | INFO | train_inner | epoch 015:    206 / 421 symm_kl=0.505, self_kl=0, loss=3.017, nll_loss=0.443, ppl=1.36, wps=23295, ups=1.66, wpb=14025.9, bsz=508.1, num_updates=6100, lr=1e-05, gnorm=1.128, train_wall=60, wall=3992
2020-12-18 13:40:35 | INFO | train_inner | epoch 015:    306 / 421 symm_kl=0.513, self_kl=0, loss=3.035, nll_loss=0.448, ppl=1.36, wps=23329.5, ups=1.67, wpb=13965.9, bsz=486.5, num_updates=6200, lr=1e-05, gnorm=1.13, train_wall=60, wall=4052
2020-12-18 13:41:35 | INFO | train_inner | epoch 015:    406 / 421 symm_kl=0.516, self_kl=0, loss=3.041, nll_loss=0.453, ppl=1.37, wps=23452.4, ups=1.68, wpb=14000.5, bsz=488.7, num_updates=6300, lr=1e-05, gnorm=1.135, train_wall=60, wall=4112
2020-12-18 13:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:41:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:41:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:41:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:41:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:42:03 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 1.317 | self_kl 0 | loss 7.562 | nll_loss 4.228 | ppl 18.74 | bleu 19.94 | wps 5226.6 | wpb 10324.2 | bsz 375 | num_updates 6315 | best_bleu 20.07
2020-12-18 13:42:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:42:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:42:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:42:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:42:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:42:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:42:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:42:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 15 @ 6315 updates, score 19.94) (writing took 3.149906635284424 seconds)
2020-12-18 13:42:06 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-12-18 13:42:06 | INFO | train | epoch 015 | symm_kl 0.512 | self_kl 0 | loss 3.032 | nll_loss 0.448 | ppl 1.36 | wps 21273.7 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 6315 | lr 1e-05 | gnorm 1.131 | train_wall 251 | wall 4143
2020-12-18 13:42:06 | INFO | fairseq.trainer | begin training epoch 16
2020-12-18 13:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:42:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:43:00 | INFO | train_inner | epoch 016:     85 / 421 symm_kl=0.507, self_kl=0, loss=3.022, nll_loss=0.445, ppl=1.36, wps=16474.6, ups=1.18, wpb=13920.5, bsz=499.7, num_updates=6400, lr=1e-05, gnorm=1.126, train_wall=59, wall=4196
2020-12-18 13:44:00 | INFO | train_inner | epoch 016:    185 / 421 symm_kl=0.515, self_kl=0, loss=3.035, nll_loss=0.446, ppl=1.36, wps=22997, ups=1.65, wpb=13953.5, bsz=496.1, num_updates=6500, lr=1e-05, gnorm=1.14, train_wall=60, wall=4257
2020-12-18 13:45:00 | INFO | train_inner | epoch 016:    285 / 421 symm_kl=0.503, self_kl=0, loss=3.015, nll_loss=0.445, ppl=1.36, wps=23341.9, ups=1.66, wpb=14059.6, bsz=504.2, num_updates=6600, lr=1e-05, gnorm=1.118, train_wall=60, wall=4317
2020-12-18 13:46:00 | INFO | train_inner | epoch 016:    385 / 421 symm_kl=0.514, self_kl=0, loss=3.034, nll_loss=0.448, ppl=1.36, wps=23261.7, ups=1.67, wpb=13933.8, bsz=478.3, num_updates=6700, lr=1e-05, gnorm=1.141, train_wall=60, wall=4377
2020-12-18 13:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:46:40 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 1.329 | self_kl 0 | loss 7.583 | nll_loss 4.231 | ppl 18.77 | bleu 19.93 | wps 5607.2 | wpb 10324.2 | bsz 375 | num_updates 6736 | best_bleu 20.07
2020-12-18 13:46:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:46:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 16 @ 6736 updates, score 19.93) (writing took 3.173955237492919 seconds)
2020-12-18 13:46:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-12-18 13:46:43 | INFO | train | epoch 016 | symm_kl 0.511 | self_kl 0 | loss 3.029 | nll_loss 0.447 | ppl 1.36 | wps 21214 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 6736 | lr 1e-05 | gnorm 1.134 | train_wall 252 | wall 4420
2020-12-18 13:46:43 | INFO | fairseq.trainer | begin training epoch 17
2020-12-18 13:46:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:46:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:47:24 | INFO | train_inner | epoch 017:     64 / 421 symm_kl=0.514, self_kl=0, loss=3.036, nll_loss=0.45, ppl=1.37, wps=16630.4, ups=1.2, wpb=13884.8, bsz=477.7, num_updates=6800, lr=1e-05, gnorm=1.153, train_wall=59, wall=4461
2020-12-18 13:48:25 | INFO | train_inner | epoch 017:    164 / 421 symm_kl=0.499, self_kl=0, loss=3.002, nll_loss=0.438, ppl=1.35, wps=23254.3, ups=1.64, wpb=14155.6, bsz=509.1, num_updates=6900, lr=1e-05, gnorm=1.109, train_wall=61, wall=4522
2020-12-18 13:49:25 | INFO | train_inner | epoch 017:    264 / 421 symm_kl=0.507, self_kl=0, loss=3.019, nll_loss=0.443, ppl=1.36, wps=23130.4, ups=1.65, wpb=14019.6, bsz=491.8, num_updates=7000, lr=1e-05, gnorm=1.12, train_wall=60, wall=4582
2020-12-18 13:50:26 | INFO | train_inner | epoch 017:    364 / 421 symm_kl=0.515, self_kl=0, loss=3.038, nll_loss=0.45, ppl=1.37, wps=23079.8, ups=1.66, wpb=13911.3, bsz=494.4, num_updates=7100, lr=1e-05, gnorm=1.136, train_wall=60, wall=4643
2020-12-18 13:51:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:51:17 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 1.328 | self_kl 0 | loss 7.58 | nll_loss 4.236 | ppl 18.84 | bleu 19.7 | wps 5919.7 | wpb 10324.2 | bsz 375 | num_updates 7157 | best_bleu 20.07
2020-12-18 13:51:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:51:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 17 @ 7157 updates, score 19.7) (writing took 3.1308066360652447 seconds)
2020-12-18 13:51:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-12-18 13:51:20 | INFO | train | epoch 017 | symm_kl 0.51 | self_kl 0 | loss 3.026 | nll_loss 0.446 | ppl 1.36 | wps 21236.8 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 7157 | lr 1e-05 | gnorm 1.129 | train_wall 253 | wall 4697
2020-12-18 13:51:20 | INFO | fairseq.trainer | begin training epoch 18
2020-12-18 13:51:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:51:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:51:49 | INFO | train_inner | epoch 018:     43 / 421 symm_kl=0.516, self_kl=0, loss=3.036, nll_loss=0.448, ppl=1.36, wps=16734.5, ups=1.21, wpb=13876, bsz=478.1, num_updates=7200, lr=1e-05, gnorm=1.142, train_wall=59, wall=4725
2020-12-18 13:52:48 | INFO | train_inner | epoch 018:    143 / 421 symm_kl=0.509, self_kl=0, loss=3.021, nll_loss=0.443, ppl=1.36, wps=23405.3, ups=1.67, wpb=14024.4, bsz=489.7, num_updates=7300, lr=1e-05, gnorm=1.134, train_wall=60, wall=4785
2020-12-18 13:53:49 | INFO | train_inner | epoch 018:    243 / 421 symm_kl=0.51, self_kl=0, loss=3.023, nll_loss=0.444, ppl=1.36, wps=22976.7, ups=1.66, wpb=13842.7, bsz=494.3, num_updates=7400, lr=1e-05, gnorm=1.132, train_wall=60, wall=4846
2020-12-18 13:54:49 | INFO | train_inner | epoch 018:    343 / 421 symm_kl=0.507, self_kl=0, loss=3.019, nll_loss=0.444, ppl=1.36, wps=23368.6, ups=1.66, wpb=14074.6, bsz=501.3, num_updates=7500, lr=1e-05, gnorm=1.118, train_wall=60, wall=4906
2020-12-18 13:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 13:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 13:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 13:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 13:55:54 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 1.323 | self_kl 0 | loss 7.566 | nll_loss 4.231 | ppl 18.78 | bleu 19.78 | wps 5354.1 | wpb 10324.2 | bsz 375 | num_updates 7578 | best_bleu 20.07
2020-12-18 13:55:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 13:55:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:55:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 18 @ 7578 updates, score 19.78) (writing took 3.1655364241451025 seconds)
2020-12-18 13:55:57 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-12-18 13:55:57 | INFO | train | epoch 018 | symm_kl 0.509 | self_kl 0 | loss 3.023 | nll_loss 0.444 | ppl 1.36 | wps 21230.6 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 7578 | lr 1e-05 | gnorm 1.133 | train_wall 252 | wall 4974
2020-12-18 13:55:57 | INFO | fairseq.trainer | begin training epoch 19
2020-12-18 13:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:55:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 13:56:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 13:56:13 | INFO | train_inner | epoch 019:     22 / 421 symm_kl=0.512, self_kl=0, loss=3.029, nll_loss=0.447, ppl=1.36, wps=16434.7, ups=1.19, wpb=13829.2, bsz=486.7, num_updates=7600, lr=1e-05, gnorm=1.149, train_wall=59, wall=4990
2020-12-18 13:57:13 | INFO | train_inner | epoch 019:    122 / 421 symm_kl=0.506, self_kl=0, loss=3.017, nll_loss=0.444, ppl=1.36, wps=23541.5, ups=1.67, wpb=14075, bsz=504.1, num_updates=7700, lr=1e-05, gnorm=1.125, train_wall=60, wall=5050
2020-12-18 13:58:13 | INFO | train_inner | epoch 019:    222 / 421 symm_kl=0.507, self_kl=0, loss=3.017, nll_loss=0.443, ppl=1.36, wps=23237.2, ups=1.66, wpb=14017.6, bsz=488.3, num_updates=7800, lr=1e-05, gnorm=1.123, train_wall=60, wall=5110
2020-12-18 13:59:14 | INFO | train_inner | epoch 019:    322 / 421 symm_kl=0.514, self_kl=0, loss=3.03, nll_loss=0.445, ppl=1.36, wps=23061.8, ups=1.66, wpb=13925.5, bsz=485.6, num_updates=7900, lr=1e-05, gnorm=1.132, train_wall=60, wall=5170
2020-12-18 14:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:00:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:00:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:00:30 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 1.321 | self_kl 0 | loss 7.574 | nll_loss 4.243 | ppl 18.94 | bleu 19.78 | wps 6022.5 | wpb 10324.2 | bsz 375 | num_updates 7999 | best_bleu 20.07
2020-12-18 14:00:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:00:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 19 @ 7999 updates, score 19.78) (writing took 2.586698018014431 seconds)
2020-12-18 14:00:32 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-12-18 14:00:32 | INFO | train | epoch 019 | symm_kl 0.508 | self_kl 0 | loss 3.02 | nll_loss 0.443 | ppl 1.36 | wps 21365.1 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 7999 | lr 1e-05 | gnorm 1.135 | train_wall 252 | wall 5249
2020-12-18 14:00:32 | INFO | fairseq.trainer | begin training epoch 20
2020-12-18 14:00:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:00:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:00:36 | INFO | train_inner | epoch 020:      1 / 421 symm_kl=0.507, self_kl=0, loss=3.017, nll_loss=0.442, ppl=1.36, wps=16888.9, ups=1.21, wpb=13936.5, bsz=492.6, num_updates=8000, lr=1e-05, gnorm=1.158, train_wall=60, wall=5253
2020-12-18 14:01:35 | INFO | train_inner | epoch 020:    101 / 421 symm_kl=0.509, self_kl=0, loss=3.021, nll_loss=0.442, ppl=1.36, wps=23814, ups=1.7, wpb=14022.1, bsz=493.9, num_updates=8100, lr=1e-05, gnorm=1.12, train_wall=59, wall=5312
2020-12-18 14:02:35 | INFO | train_inner | epoch 020:    201 / 421 symm_kl=0.507, self_kl=0, loss=3.015, nll_loss=0.441, ppl=1.36, wps=23273.2, ups=1.66, wpb=13989.7, bsz=488, num_updates=8200, lr=1e-05, gnorm=1.132, train_wall=60, wall=5372
2020-12-18 14:03:35 | INFO | train_inner | epoch 020:    301 / 421 symm_kl=0.51, self_kl=0, loss=3.022, nll_loss=0.443, ppl=1.36, wps=23405.2, ups=1.66, wpb=14118.4, bsz=497.3, num_updates=8300, lr=1e-05, gnorm=1.122, train_wall=60, wall=5432
2020-12-18 14:04:36 | INFO | train_inner | epoch 020:    401 / 421 symm_kl=0.51, self_kl=0, loss=3.021, nll_loss=0.443, ppl=1.36, wps=22856.6, ups=1.66, wpb=13791.4, bsz=493.2, num_updates=8400, lr=1e-05, gnorm=1.144, train_wall=60, wall=5493
2020-12-18 14:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:04:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:04:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:05:07 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 1.316 | self_kl 0 | loss 7.565 | nll_loss 4.242 | ppl 18.92 | bleu 19.82 | wps 5355.9 | wpb 10324.2 | bsz 375 | num_updates 8420 | best_bleu 20.07
2020-12-18 14:05:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:05:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:05:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:05:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:05:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:05:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 20 @ 8420 updates, score 19.82) (writing took 3.125033564865589 seconds)
2020-12-18 14:05:10 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-12-18 14:05:10 | INFO | train | epoch 020 | symm_kl 0.508 | self_kl 0 | loss 3.019 | nll_loss 0.442 | ppl 1.36 | wps 21152.8 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 8420 | lr 1e-05 | gnorm 1.13 | train_wall 252 | wall 5527
2020-12-18 14:05:10 | INFO | fairseq.trainer | begin training epoch 21
2020-12-18 14:05:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:05:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:06:02 | INFO | train_inner | epoch 021:     80 / 421 symm_kl=0.51, self_kl=0, loss=3.022, nll_loss=0.443, ppl=1.36, wps=15890.6, ups=1.16, wpb=13696.4, bsz=490.3, num_updates=8500, lr=1e-05, gnorm=1.149, train_wall=60, wall=5579
2020-12-18 14:07:03 | INFO | train_inner | epoch 021:    180 / 421 symm_kl=0.508, self_kl=0, loss=3.015, nll_loss=0.44, ppl=1.36, wps=23071.3, ups=1.64, wpb=14078.8, bsz=487.9, num_updates=8600, lr=1e-05, gnorm=1.125, train_wall=61, wall=5640
2020-12-18 14:08:04 | INFO | train_inner | epoch 021:    280 / 421 symm_kl=0.497, self_kl=0, loss=2.995, nll_loss=0.437, ppl=1.35, wps=23340.1, ups=1.65, wpb=14138.2, bsz=508.5, num_updates=8700, lr=1e-05, gnorm=1.104, train_wall=60, wall=5700
2020-12-18 14:09:04 | INFO | train_inner | epoch 021:    380 / 421 symm_kl=0.51, self_kl=0, loss=3.022, nll_loss=0.442, ppl=1.36, wps=23185.7, ups=1.65, wpb=14011.6, bsz=493.4, num_updates=8800, lr=1e-05, gnorm=1.126, train_wall=60, wall=5761
2020-12-18 14:09:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:09:46 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 1.316 | self_kl 0 | loss 7.565 | nll_loss 4.242 | ppl 18.92 | bleu 19.79 | wps 5596.2 | wpb 10324.2 | bsz 375 | num_updates 8841 | best_bleu 20.07
2020-12-18 14:09:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:09:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:09:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 21 @ 8841 updates, score 19.79) (writing took 3.045696085318923 seconds)
2020-12-18 14:09:49 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-12-18 14:09:49 | INFO | train | epoch 021 | symm_kl 0.507 | self_kl 0 | loss 3.016 | nll_loss 0.441 | ppl 1.36 | wps 21098.3 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 8841 | lr 1e-05 | gnorm 1.127 | train_wall 254 | wall 5806
2020-12-18 14:09:49 | INFO | fairseq.trainer | begin training epoch 22
2020-12-18 14:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:10:27 | INFO | train_inner | epoch 022:     59 / 421 symm_kl=0.51, self_kl=0, loss=3.021, nll_loss=0.443, ppl=1.36, wps=16771.9, ups=1.2, wpb=13937.8, bsz=479, num_updates=8900, lr=1e-05, gnorm=1.132, train_wall=59, wall=5844
2020-12-18 14:11:27 | INFO | train_inner | epoch 022:    159 / 421 symm_kl=0.499, self_kl=0, loss=3, nll_loss=0.438, ppl=1.35, wps=23253.5, ups=1.66, wpb=14049.6, bsz=507.4, num_updates=9000, lr=1e-05, gnorm=1.118, train_wall=60, wall=5904
2020-12-18 14:12:28 | INFO | train_inner | epoch 022:    259 / 421 symm_kl=0.511, self_kl=0, loss=3.021, nll_loss=0.441, ppl=1.36, wps=23092.6, ups=1.66, wpb=13902.5, bsz=489.7, num_updates=9100, lr=1e-05, gnorm=1.133, train_wall=60, wall=5965
2020-12-18 14:13:28 | INFO | train_inner | epoch 022:    359 / 421 symm_kl=0.505, self_kl=0, loss=3.009, nll_loss=0.439, ppl=1.36, wps=23448.4, ups=1.67, wpb=14032.6, bsz=487.8, num_updates=9200, lr=1e-05, gnorm=1.121, train_wall=60, wall=6024
2020-12-18 14:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:14:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:14:23 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 1.313 | self_kl 0 | loss 7.563 | nll_loss 4.244 | ppl 18.95 | bleu 19.85 | wps 5454.5 | wpb 10324.2 | bsz 375 | num_updates 9262 | best_bleu 20.07
2020-12-18 14:14:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:14:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 22 @ 9262 updates, score 19.85) (writing took 2.8533291798084974 seconds)
2020-12-18 14:14:25 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-12-18 14:14:25 | INFO | train | epoch 022 | symm_kl 0.507 | self_kl 0 | loss 3.013 | nll_loss 0.44 | ppl 1.36 | wps 21282.3 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 9262 | lr 1e-05 | gnorm 1.128 | train_wall 252 | wall 6082
2020-12-18 14:14:25 | INFO | fairseq.trainer | begin training epoch 23
2020-12-18 14:14:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:14:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:14:51 | INFO | train_inner | epoch 023:     38 / 421 symm_kl=0.518, self_kl=0, loss=3.033, nll_loss=0.442, ppl=1.36, wps=16641.1, ups=1.2, wpb=13834.9, bsz=470.7, num_updates=9300, lr=1e-05, gnorm=1.157, train_wall=59, wall=6108
2020-12-18 14:15:51 | INFO | train_inner | epoch 023:    138 / 421 symm_kl=0.506, self_kl=0, loss=3.011, nll_loss=0.439, ppl=1.36, wps=23306.5, ups=1.67, wpb=13964.3, bsz=496.2, num_updates=9400, lr=1e-05, gnorm=1.128, train_wall=60, wall=6167
2020-12-18 14:16:51 | INFO | train_inner | epoch 023:    238 / 421 symm_kl=0.513, self_kl=0, loss=3.023, nll_loss=0.442, ppl=1.36, wps=23208.3, ups=1.66, wpb=13991.9, bsz=478, num_updates=9500, lr=1e-05, gnorm=1.144, train_wall=60, wall=6228
2020-12-18 14:17:51 | INFO | train_inner | epoch 023:    338 / 421 symm_kl=0.502, self_kl=0, loss=3.004, nll_loss=0.438, ppl=1.35, wps=23274.5, ups=1.67, wpb=13972.2, bsz=506.8, num_updates=9600, lr=1e-05, gnorm=1.115, train_wall=60, wall=6288
2020-12-18 14:18:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:18:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:18:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:18:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:18:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:18:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:18:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:18:58 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 1.312 | self_kl 0 | loss 7.56 | nll_loss 4.25 | ppl 19.02 | bleu 19.97 | wps 5814.8 | wpb 10324.2 | bsz 375 | num_updates 9683 | best_bleu 20.07
2020-12-18 14:18:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:18:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:18:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:19:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:19:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:19:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:19:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 23 @ 9683 updates, score 19.97) (writing took 2.735268572345376 seconds)
2020-12-18 14:19:00 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-12-18 14:19:00 | INFO | train | epoch 023 | symm_kl 0.507 | self_kl 0 | loss 3.012 | nll_loss 0.439 | ppl 1.36 | wps 21382.5 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 9683 | lr 1e-05 | gnorm 1.13 | train_wall 251 | wall 6357
2020-12-18 14:19:00 | INFO | fairseq.trainer | begin training epoch 24
2020-12-18 14:19:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:19:14 | INFO | train_inner | epoch 024:     17 / 421 symm_kl=0.498, self_kl=0, loss=2.997, nll_loss=0.437, ppl=1.35, wps=16911.6, ups=1.21, wpb=13990.7, bsz=508.2, num_updates=9700, lr=1e-05, gnorm=1.119, train_wall=59, wall=6371
2020-12-18 14:20:13 | INFO | train_inner | epoch 024:    117 / 421 symm_kl=0.509, self_kl=0, loss=3.014, nll_loss=0.437, ppl=1.35, wps=23675.8, ups=1.69, wpb=14043.6, bsz=489.9, num_updates=9800, lr=1e-05, gnorm=1.131, train_wall=59, wall=6430
2020-12-18 14:21:13 | INFO | train_inner | epoch 024:    217 / 421 symm_kl=0.504, self_kl=0, loss=3.004, nll_loss=0.436, ppl=1.35, wps=23281.6, ups=1.66, wpb=13992.3, bsz=488.8, num_updates=9900, lr=1e-05, gnorm=1.116, train_wall=60, wall=6490
2020-12-18 14:22:13 | INFO | train_inner | epoch 024:    317 / 421 symm_kl=0.512, self_kl=0, loss=3.021, nll_loss=0.441, ppl=1.36, wps=22863.3, ups=1.67, wpb=13726.7, bsz=487.9, num_updates=10000, lr=1e-05, gnorm=1.138, train_wall=60, wall=6550
2020-12-18 14:23:13 | INFO | train_inner | epoch 024:    417 / 421 symm_kl=0.502, self_kl=0, loss=3.003, nll_loss=0.438, ppl=1.35, wps=23609, ups=1.67, wpb=14165.6, bsz=504.5, num_updates=10100, lr=1e-05, gnorm=1.112, train_wall=60, wall=6610
2020-12-18 14:23:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:23:33 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 1.314 | self_kl 0 | loss 7.562 | nll_loss 4.25 | ppl 19.03 | bleu 19.67 | wps 5609.7 | wpb 10324.2 | bsz 375 | num_updates 10104 | best_bleu 20.07
2020-12-18 14:23:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:23:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:23:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 24 @ 10104 updates, score 19.67) (writing took 3.005062149837613 seconds)
2020-12-18 14:23:36 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-12-18 14:23:36 | INFO | train | epoch 024 | symm_kl 0.506 | self_kl 0 | loss 3.01 | nll_loss 0.438 | ppl 1.35 | wps 21324.1 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 10104 | lr 1e-05 | gnorm 1.126 | train_wall 251 | wall 6633
2020-12-18 14:23:36 | INFO | fairseq.trainer | begin training epoch 25
2020-12-18 14:23:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:23:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:24:36 | INFO | train_inner | epoch 025:     96 / 421 symm_kl=0.506, self_kl=0, loss=3.004, nll_loss=0.432, ppl=1.35, wps=16754.9, ups=1.2, wpb=13965.8, bsz=477.5, num_updates=10200, lr=1e-05, gnorm=1.14, train_wall=59, wall=6693
2020-12-18 14:25:37 | INFO | train_inner | epoch 025:    196 / 421 symm_kl=0.507, self_kl=0, loss=3.012, nll_loss=0.439, ppl=1.36, wps=23120.5, ups=1.66, wpb=13916.1, bsz=500.1, num_updates=10300, lr=1e-05, gnorm=1.134, train_wall=60, wall=6754
2020-12-18 14:26:37 | INFO | train_inner | epoch 025:    296 / 421 symm_kl=0.509, self_kl=0, loss=3.014, nll_loss=0.439, ppl=1.36, wps=23455.7, ups=1.67, wpb=14064.9, bsz=487, num_updates=10400, lr=1e-05, gnorm=1.129, train_wall=60, wall=6814
2020-12-18 14:27:37 | INFO | train_inner | epoch 025:    396 / 421 symm_kl=0.503, self_kl=0, loss=3.004, nll_loss=0.437, ppl=1.35, wps=23261.9, ups=1.66, wpb=14008.2, bsz=508.2, num_updates=10500, lr=1e-05, gnorm=1.119, train_wall=60, wall=6874
2020-12-18 14:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:28:09 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 1.323 | self_kl 0 | loss 7.583 | nll_loss 4.253 | ppl 19.07 | bleu 19.77 | wps 6006.2 | wpb 10324.2 | bsz 375 | num_updates 10525 | best_bleu 20.07
2020-12-18 14:28:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:28:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:28:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:28:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:28:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:28:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:28:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:28:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 25 @ 10525 updates, score 19.77) (writing took 2.624083198606968 seconds)
2020-12-18 14:28:11 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-12-18 14:28:11 | INFO | train | epoch 025 | symm_kl 0.507 | self_kl 0 | loss 3.009 | nll_loss 0.437 | ppl 1.35 | wps 21389.8 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 10525 | lr 1e-05 | gnorm 1.132 | train_wall 251 | wall 6908
2020-12-18 14:28:11 | INFO | fairseq.trainer | begin training epoch 26
2020-12-18 14:28:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:28:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:28:59 | INFO | train_inner | epoch 026:     75 / 421 symm_kl=0.5, self_kl=0, loss=2.994, nll_loss=0.432, ppl=1.35, wps=16792.4, ups=1.22, wpb=13766.8, bsz=507.2, num_updates=10600, lr=1e-05, gnorm=1.137, train_wall=59, wall=6956
2020-12-18 14:29:59 | INFO | train_inner | epoch 026:    175 / 421 symm_kl=0.504, self_kl=0, loss=3.002, nll_loss=0.434, ppl=1.35, wps=23360.4, ups=1.66, wpb=14063.1, bsz=499.9, num_updates=10700, lr=1e-05, gnorm=1.125, train_wall=60, wall=7016
2020-12-18 14:30:59 | INFO | train_inner | epoch 026:    275 / 421 symm_kl=0.507, self_kl=0, loss=3.011, nll_loss=0.439, ppl=1.36, wps=23401.4, ups=1.66, wpb=14095.9, bsz=494.4, num_updates=10800, lr=1e-05, gnorm=1.115, train_wall=60, wall=7076
2020-12-18 14:32:00 | INFO | train_inner | epoch 026:    375 / 421 symm_kl=0.511, self_kl=0, loss=3.016, nll_loss=0.438, ppl=1.35, wps=23024.7, ups=1.65, wpb=13928.6, bsz=473.6, num_updates=10900, lr=1e-05, gnorm=1.139, train_wall=60, wall=7137
2020-12-18 14:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:32:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:32:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:32:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:32:47 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 1.315 | self_kl 0 | loss 7.57 | nll_loss 4.257 | ppl 19.12 | bleu 19.72 | wps 4772.7 | wpb 10324.2 | bsz 375 | num_updates 10946 | best_bleu 20.07
2020-12-18 14:32:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:32:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:32:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 26 @ 10946 updates, score 19.72) (writing took 2.786898784339428 seconds)
2020-12-18 14:32:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-12-18 14:32:50 | INFO | train | epoch 026 | symm_kl 0.506 | self_kl 0 | loss 3.007 | nll_loss 0.436 | ppl 1.35 | wps 21115.5 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 10946 | lr 1e-05 | gnorm 1.128 | train_wall 252 | wall 7187
2020-12-18 14:32:50 | INFO | fairseq.trainer | begin training epoch 27
2020-12-18 14:32:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:32:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:33:25 | INFO | train_inner | epoch 027:     54 / 421 symm_kl=0.51, self_kl=0, loss=3.015, nll_loss=0.438, ppl=1.35, wps=16319.3, ups=1.18, wpb=13886.4, bsz=483.5, num_updates=11000, lr=1e-05, gnorm=1.14, train_wall=59, wall=7222
2020-12-18 14:34:25 | INFO | train_inner | epoch 027:    154 / 421 symm_kl=0.51, self_kl=0, loss=3.011, nll_loss=0.433, ppl=1.35, wps=23097.9, ups=1.65, wpb=14008.5, bsz=483.7, num_updates=11100, lr=1e-05, gnorm=1.141, train_wall=60, wall=7282
2020-12-18 14:35:26 | INFO | train_inner | epoch 027:    254 / 421 symm_kl=0.502, self_kl=0, loss=2.998, nll_loss=0.435, ppl=1.35, wps=23130.1, ups=1.64, wpb=14065.1, bsz=498.8, num_updates=11200, lr=1e-05, gnorm=1.122, train_wall=61, wall=7343
2020-12-18 14:36:27 | INFO | train_inner | epoch 027:    354 / 421 symm_kl=0.504, self_kl=0, loss=3.002, nll_loss=0.434, ppl=1.35, wps=23061.5, ups=1.65, wpb=13936.2, bsz=501.4, num_updates=11300, lr=1e-05, gnorm=1.13, train_wall=60, wall=7404
2020-12-18 14:37:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:37:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:37:24 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 1.304 | self_kl 0 | loss 7.554 | nll_loss 4.258 | ppl 19.13 | bleu 19.92 | wps 5627.3 | wpb 10324.2 | bsz 375 | num_updates 11367 | best_bleu 20.07
2020-12-18 14:37:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:37:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 27 @ 11367 updates, score 19.92) (writing took 2.889800153672695 seconds)
2020-12-18 14:37:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-12-18 14:37:27 | INFO | train | epoch 027 | symm_kl 0.506 | self_kl 0 | loss 3.005 | nll_loss 0.435 | ppl 1.35 | wps 21198.1 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 11367 | lr 1e-05 | gnorm 1.133 | train_wall 253 | wall 7464
2020-12-18 14:37:27 | INFO | fairseq.trainer | begin training epoch 28
2020-12-18 14:37:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:37:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:37:49 | INFO | train_inner | epoch 028:     33 / 421 symm_kl=0.501, self_kl=0, loss=2.994, nll_loss=0.431, ppl=1.35, wps=16628.5, ups=1.21, wpb=13736.5, bsz=489.8, num_updates=11400, lr=1e-05, gnorm=1.135, train_wall=59, wall=7486
2020-12-18 14:38:49 | INFO | train_inner | epoch 028:    133 / 421 symm_kl=0.508, self_kl=0, loss=3.007, nll_loss=0.434, ppl=1.35, wps=23220.7, ups=1.66, wpb=13970.3, bsz=473.9, num_updates=11500, lr=1e-05, gnorm=1.13, train_wall=60, wall=7546
2020-12-18 14:39:50 | INFO | train_inner | epoch 028:    233 / 421 symm_kl=0.503, self_kl=0, loss=2.999, nll_loss=0.432, ppl=1.35, wps=23393.5, ups=1.66, wpb=14051.6, bsz=492.8, num_updates=11600, lr=1e-05, gnorm=1.123, train_wall=60, wall=7606
2020-12-18 14:40:50 | INFO | train_inner | epoch 028:    333 / 421 symm_kl=0.508, self_kl=0, loss=3.011, nll_loss=0.437, ppl=1.35, wps=23220, ups=1.65, wpb=14062.4, bsz=500.3, num_updates=11700, lr=1e-05, gnorm=1.125, train_wall=60, wall=7667
2020-12-18 14:41:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:41:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:41:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:41:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:41:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:41:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:41:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:41:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:41:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:42:04 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 1.311 | self_kl 0 | loss 7.563 | nll_loss 4.256 | ppl 19.11 | bleu 19.96 | wps 4693.1 | wpb 10324.2 | bsz 375 | num_updates 11788 | best_bleu 20.07
2020-12-18 14:42:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:42:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:42:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:42:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:42:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:42:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:42:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:42:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 28 @ 11788 updates, score 19.96) (writing took 2.9348200522363186 seconds)
2020-12-18 14:42:07 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-12-18 14:42:07 | INFO | train | epoch 028 | symm_kl 0.505 | self_kl 0 | loss 3.003 | nll_loss 0.434 | ppl 1.35 | wps 21018.5 | ups 1.5 | wpb 13969.5 | bsz 492.6 | num_updates 11788 | lr 1e-05 | gnorm 1.13 | train_wall 252 | wall 7744
2020-12-18 14:42:07 | INFO | fairseq.trainer | begin training epoch 29
2020-12-18 14:42:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:42:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:42:17 | INFO | train_inner | epoch 029:     12 / 421 symm_kl=0.506, self_kl=0, loss=3.006, nll_loss=0.436, ppl=1.35, wps=15949.3, ups=1.15, wpb=13890.5, bsz=495.7, num_updates=11800, lr=1e-05, gnorm=1.144, train_wall=60, wall=7754
2020-12-18 14:43:16 | INFO | train_inner | epoch 029:    112 / 421 symm_kl=0.502, self_kl=0, loss=2.999, nll_loss=0.434, ppl=1.35, wps=23830.8, ups=1.69, wpb=14129.1, bsz=498.7, num_updates=11900, lr=1e-05, gnorm=1.125, train_wall=59, wall=7813
2020-12-18 14:44:16 | INFO | train_inner | epoch 029:    212 / 421 symm_kl=0.507, self_kl=0, loss=3.004, nll_loss=0.432, ppl=1.35, wps=23438.9, ups=1.67, wpb=14014.8, bsz=486.4, num_updates=12000, lr=1e-05, gnorm=1.139, train_wall=60, wall=7873
2020-12-18 14:45:17 | INFO | train_inner | epoch 029:    312 / 421 symm_kl=0.502, self_kl=0, loss=2.996, nll_loss=0.433, ppl=1.35, wps=23105.5, ups=1.66, wpb=13916.7, bsz=507.4, num_updates=12100, lr=1e-05, gnorm=1.128, train_wall=60, wall=7933
2020-12-18 14:46:17 | INFO | train_inner | epoch 029:    412 / 421 symm_kl=0.51, self_kl=0, loss=3.01, nll_loss=0.434, ppl=1.35, wps=23078.2, ups=1.67, wpb=13860.4, bsz=482.7, num_updates=12200, lr=1e-05, gnorm=1.146, train_wall=60, wall=7993
2020-12-18 14:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:46:39 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 1.317 | self_kl 0 | loss 7.578 | nll_loss 4.262 | ppl 19.19 | bleu 19.78 | wps 5636.8 | wpb 10324.2 | bsz 375 | num_updates 12209 | best_bleu 20.07
2020-12-18 14:46:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:46:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:46:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 29 @ 12209 updates, score 19.78) (writing took 3.1108689196407795 seconds)
2020-12-18 14:46:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-12-18 14:46:43 | INFO | train | epoch 029 | symm_kl 0.505 | self_kl 0 | loss 3.002 | nll_loss 0.433 | ppl 1.35 | wps 21337.3 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 12209 | lr 1e-05 | gnorm 1.136 | train_wall 251 | wall 8020
2020-12-18 14:46:43 | INFO | fairseq.trainer | begin training epoch 30
2020-12-18 14:46:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:46:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:47:40 | INFO | train_inner | epoch 030:     91 / 421 symm_kl=0.499, self_kl=0, loss=2.99, nll_loss=0.43, ppl=1.35, wps=16824.1, ups=1.2, wpb=14010.2, bsz=498.8, num_updates=12300, lr=1e-05, gnorm=1.12, train_wall=59, wall=8077
2020-12-18 14:48:40 | INFO | train_inner | epoch 030:    191 / 421 symm_kl=0.503, self_kl=0, loss=2.997, nll_loss=0.431, ppl=1.35, wps=23024.7, ups=1.65, wpb=13933.2, bsz=483.3, num_updates=12400, lr=1e-05, gnorm=1.131, train_wall=60, wall=8137
2020-12-18 14:49:41 | INFO | train_inner | epoch 030:    291 / 421 symm_kl=0.51, self_kl=0, loss=3.009, nll_loss=0.432, ppl=1.35, wps=23114.9, ups=1.66, wpb=13950.6, bsz=497.5, num_updates=12500, lr=1e-05, gnorm=1.139, train_wall=60, wall=8198
2020-12-18 14:50:41 | INFO | train_inner | epoch 030:    391 / 421 symm_kl=0.504, self_kl=0, loss=3, nll_loss=0.433, ppl=1.35, wps=23246.9, ups=1.66, wpb=14044.2, bsz=493.8, num_updates=12600, lr=1e-05, gnorm=1.126, train_wall=60, wall=8258
2020-12-18 14:50:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:51:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:51:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:51:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:51:18 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 1.31 | self_kl 0 | loss 7.568 | nll_loss 4.263 | ppl 19.2 | bleu 19.73 | wps 5053.2 | wpb 10324.2 | bsz 375 | num_updates 12630 | best_bleu 20.07
2020-12-18 14:51:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:51:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:51:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 30 @ 12630 updates, score 19.73) (writing took 2.9699756763875484 seconds)
2020-12-18 14:51:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-12-18 14:51:21 | INFO | train | epoch 030 | symm_kl 0.505 | self_kl 0 | loss 3.001 | nll_loss 0.432 | ppl 1.35 | wps 21115.2 | ups 1.51 | wpb 13969.5 | bsz 492.6 | num_updates 12630 | lr 1e-05 | gnorm 1.13 | train_wall 253 | wall 8298
2020-12-18 14:51:21 | INFO | fairseq.trainer | begin training epoch 31
2020-12-18 14:51:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:51:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:52:06 | INFO | train_inner | epoch 031:     70 / 421 symm_kl=0.512, self_kl=0, loss=3.012, nll_loss=0.433, ppl=1.35, wps=16388.5, ups=1.18, wpb=13873.1, bsz=481.6, num_updates=12700, lr=1e-05, gnorm=1.146, train_wall=59, wall=8343
2020-12-18 14:53:06 | INFO | train_inner | epoch 031:    170 / 421 symm_kl=0.5, self_kl=0, loss=2.99, nll_loss=0.43, ppl=1.35, wps=23275.4, ups=1.66, wpb=14050.2, bsz=507.7, num_updates=12800, lr=1e-05, gnorm=1.122, train_wall=60, wall=8403
2020-12-18 14:54:07 | INFO | train_inner | epoch 031:    270 / 421 symm_kl=0.51, self_kl=0, loss=3.008, nll_loss=0.432, ppl=1.35, wps=22978.2, ups=1.65, wpb=13924.8, bsz=465.5, num_updates=12900, lr=1e-05, gnorm=1.143, train_wall=60, wall=8464
2020-12-18 14:55:07 | INFO | train_inner | epoch 031:    370 / 421 symm_kl=0.502, self_kl=0, loss=2.993, nll_loss=0.43, ppl=1.35, wps=23317.7, ups=1.67, wpb=14000.3, bsz=501.4, num_updates=13000, lr=1e-05, gnorm=1.125, train_wall=60, wall=8524
2020-12-18 14:55:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 14:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 14:55:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 14:55:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 14:55:55 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 1.31 | self_kl 0 | loss 7.572 | nll_loss 4.269 | ppl 19.28 | bleu 19.82 | wps 5620.6 | wpb 10324.2 | bsz 375 | num_updates 13051 | best_bleu 20.07
2020-12-18 14:55:55 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 14:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:55:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 31 @ 13051 updates, score 19.82) (writing took 3.0307629238814116 seconds)
2020-12-18 14:55:58 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-12-18 14:55:58 | INFO | train | epoch 031 | symm_kl 0.505 | self_kl 0 | loss 3 | nll_loss 0.431 | ppl 1.35 | wps 21234.6 | ups 1.52 | wpb 13969.5 | bsz 492.6 | num_updates 13051 | lr 1e-05 | gnorm 1.133 | train_wall 252 | wall 8575
2020-12-18 14:55:58 | INFO | fairseq.trainer | begin training epoch 32
2020-12-18 14:55:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 14:56:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 14:56:30 | INFO | train_inner | epoch 032:     49 / 421 symm_kl=0.507, self_kl=0, loss=3.004, nll_loss=0.433, ppl=1.35, wps=16547.9, ups=1.2, wpb=13736.1, bsz=497.8, num_updates=13100, lr=1e-05, gnorm=1.145, train_wall=59, wall=8607
2020-12-18 14:57:30 | INFO | train_inner | epoch 032:    149 / 421 symm_kl=0.501, self_kl=0, loss=2.992, nll_loss=0.43, ppl=1.35, wps=23685.6, ups=1.67, wpb=14205.9, bsz=491.4, num_updates=13200, lr=1e-05, gnorm=1.11, train_wall=60, wall=8667
2020-12-18 14:58:30 | INFO | train_inner | epoch 032:    249 / 421 symm_kl=0.502, self_kl=0, loss=2.992, nll_loss=0.429, ppl=1.35, wps=23434.8, ups=1.66, wpb=14086.9, bsz=488.9, num_updates=13300, lr=1e-05, gnorm=1.126, train_wall=60, wall=8727
2020-12-18 14:59:30 | INFO | train_inner | epoch 032:    349 / 421 symm_kl=0.512, self_kl=0, loss=3.008, nll_loss=0.43, ppl=1.35, wps=22981.5, ups=1.66, wpb=13848.6, bsz=472.5, num_updates=13400, lr=1e-05, gnorm=1.157, train_wall=60, wall=8787
2020-12-18 15:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-18 15:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-18 15:00:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-18 15:00:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-18 15:00:31 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 1.308 | self_kl 0 | loss 7.569 | nll_loss 4.271 | ppl 19.3 | bleu 19.64 | wps 5578.4 | wpb 10324.2 | bsz 375 | num_updates 13472 | best_bleu 20.07
2020-12-18 15:00:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-18 15:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 32 @ 13472 updates, score 19.64) (writing took 2.8499622475355864 seconds)
2020-12-18 15:00:34 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-12-18 15:00:34 | INFO | train | epoch 032 | symm_kl 0.505 | self_kl 0 | loss 2.999 | nll_loss 0.43 | ppl 1.35 | wps 21338.1 | ups 1.53 | wpb 13969.5 | bsz 492.6 | num_updates 13472 | lr 1e-05 | gnorm 1.132 | train_wall 251 | wall 8851
2020-12-18 15:00:34 | INFO | fairseq.trainer | begin training epoch 33
2020-12-18 15:00:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-18 15:00:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-18 15:00:53 | INFO | train_inner | epoch 033:     28 / 421 symm_kl=0.504, self_kl=0, loss=2.999, nll_loss=0.432, ppl=1.35, wps=16720.1, ups=1.2, wpb=13913.9, bsz=525.4, num_updates=13500, lr=1e-05, gnorm=1.133, train_wall=59, wall=8870
2020-12-18 15:01:53 | INFO | train_inner | epoch 033:    128 / 421 symm_kl=0.498, self_kl=0, loss=2.986, nll_loss=0.429, ppl=1.35, wps=23367.5, ups=1.67, wpb=14000, bsz=500, num_updates=13600, lr=1e-05, gnorm=1.119, train_wall=60, wall=8930
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 304 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
