nohup: ignoring input
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.0000125
lrscheduler=fixed
warmup_updates=0
max_epoch=100
r3f_lambda=1
save_dir=./examples/entr/bash/../checkpoints/closer_gap
extr='--noised-no-grad --eps 1e-3 --noise-type uniform'
2020-12-14 10:48:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10904
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10904
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:10904
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-14 10:48:28 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-14 10:48:32 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10904', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=0.001, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[1.25e-05], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='uniform', noised_eval_model=False, noised_no_grad=True, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer_gap', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
2020-12-14 10:48:32 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-14 10:48:32 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-14 10:48:32 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-14 10:48:32 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-14 10:48:32 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-14 10:48:33 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-14 10:48:33 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-14 10:48:33 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-14 10:48:33 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-14 10:48:33 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-14 10:48:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-14 10:48:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-14 10:48:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-14 10:48:33 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-14 10:48:33 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-14 10:48:33 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-14 10:48:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-14 10:48:33 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-14 10:48:33 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-14 10:48:33 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-14 10:48:34 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-14 10:48:34 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-14 10:48:34 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-14 10:48:34 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-14 10:48:34 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-14 10:48:34 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-14 10:48:34 | INFO | fairseq.trainer | begin training epoch 1
2020-12-14 10:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:49:22 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=0.931, loss=4.008, nll_loss=0.861, ppl=1.82, wps=24165.5, ups=2.28, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.25e-05, gnorm=1.731, train_wall=44, wall=49
2020-12-14 10:50:06 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=0.831, loss=3.87, nll_loss=0.865, ppl=1.82, wps=24105, ups=2.28, wpb=10583.4, bsz=369.8, num_updates=200, lr=1.25e-05, gnorm=1.559, train_wall=44, wall=93
2020-12-14 10:50:51 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=0.803, loss=3.834, nll_loss=0.872, ppl=1.83, wps=23283.1, ups=2.25, wpb=10335, bsz=373, num_updates=300, lr=1.25e-05, gnorm=1.527, train_wall=44, wall=138
2020-12-14 10:51:35 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=0.778, loss=3.79, nll_loss=0.868, ppl=1.82, wps=23742.9, ups=2.25, wpb=10571.8, bsz=388.4, num_updates=400, lr=1.25e-05, gnorm=1.446, train_wall=44, wall=182
2020-12-14 10:52:20 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=0.768, loss=3.782, nll_loss=0.875, ppl=1.83, wps=23187.5, ups=2.23, wpb=10411.2, bsz=371.8, num_updates=500, lr=1.25e-05, gnorm=1.437, train_wall=45, wall=227
2020-12-14 10:52:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 10:52:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:52:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:52:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:52:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:52:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:52:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:52:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:52:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:52:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:52:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:53:09 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | loss 5.649 | nll_loss 4.08 | ppl 16.91 | bleu 22.25 | wps 4318.8 | wpb 7508.5 | bsz 272.7 | num_updates 561
2020-12-14 10:53:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 10:53:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:53:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:53:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.25) (writing took 2.57019873149693 seconds)
2020-12-14 10:53:12 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-14 10:53:12 | INFO | train | epoch 001 | symm_kl 0.819 | loss 3.854 | nll_loss 0.87 | ppl 1.83 | wps 21504.8 | ups 2.05 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 1.25e-05 | gnorm 1.533 | train_wall 248 | wall 279
2020-12-14 10:53:12 | INFO | fairseq.trainer | begin training epoch 2
2020-12-14 10:53:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:53:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:53:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:53:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:53:32 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=0.772, loss=3.787, nll_loss=0.872, ppl=1.83, wps=14298.5, ups=1.38, wpb=10345.6, bsz=358.4, num_updates=600, lr=1.25e-05, gnorm=1.452, train_wall=44, wall=299
2020-12-14 10:54:17 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=0.761, loss=3.772, nll_loss=0.873, ppl=1.83, wps=23507.2, ups=2.23, wpb=10532.9, bsz=366.4, num_updates=700, lr=1.25e-05, gnorm=1.422, train_wall=45, wall=344
2020-12-14 10:55:02 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=0.741, loss=3.735, nll_loss=0.864, ppl=1.82, wps=23272.6, ups=2.22, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.25e-05, gnorm=1.388, train_wall=45, wall=389
2020-12-14 10:55:48 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=0.737, loss=3.732, nll_loss=0.868, ppl=1.83, wps=23315.9, ups=2.21, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.25e-05, gnorm=1.38, train_wall=45, wall=435
2020-12-14 10:56:33 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=0.741, loss=3.746, nll_loss=0.875, ppl=1.83, wps=22905.7, ups=2.19, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.25e-05, gnorm=1.385, train_wall=45, wall=480
2020-12-14 10:57:18 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=0.738, loss=3.744, nll_loss=0.88, ppl=1.84, wps=23163, ups=2.22, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.25e-05, gnorm=1.384, train_wall=45, wall=525
2020-12-14 10:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 10:57:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:57:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 10:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 10:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 10:57:49 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | loss 5.608 | nll_loss 4.041 | ppl 16.47 | bleu 22.36 | wps 4585 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.36
2020-12-14 10:57:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 10:57:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:57:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:57:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 2 @ 1122 updates, score 22.36) (writing took 5.15631976723671 seconds)
2020-12-14 10:57:55 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-14 10:57:55 | INFO | train | epoch 002 | symm_kl 0.744 | loss 3.745 | nll_loss 0.871 | ppl 1.83 | wps 20803.6 | ups 1.98 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.25e-05 | gnorm 1.396 | train_wall 252 | wall 562
2020-12-14 10:57:55 | INFO | fairseq.trainer | begin training epoch 3
2020-12-14 10:57:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 10:57:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 10:58:33 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=0.727, loss=3.716, nll_loss=0.868, ppl=1.83, wps=13941.8, ups=1.34, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.25e-05, gnorm=1.378, train_wall=45, wall=600
2020-12-14 10:59:19 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=0.726, loss=3.719, nll_loss=0.873, ppl=1.83, wps=22683.2, ups=2.18, wpb=10420.6, bsz=376, num_updates=1300, lr=1.25e-05, gnorm=1.36, train_wall=46, wall=646
2020-12-14 11:00:07 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=0.717, loss=3.701, nll_loss=0.868, ppl=1.83, wps=21870.8, ups=2.09, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.25e-05, gnorm=1.348, train_wall=48, wall=694
2020-12-14 11:00:53 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=0.724, loss=3.726, nll_loss=0.885, ppl=1.85, wps=22605.1, ups=2.16, wpb=10472.3, bsz=374.7, num_updates=1500, lr=1.25e-05, gnorm=1.354, train_wall=46, wall=740
2020-12-14 11:01:39 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=0.705, loss=3.688, nll_loss=0.872, ppl=1.83, wps=23187.7, ups=2.18, wpb=10650.7, bsz=373.4, num_updates=1600, lr=1.25e-05, gnorm=1.313, train_wall=46, wall=786
2020-12-14 11:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:02:39 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | loss 5.585 | nll_loss 4.018 | ppl 16.2 | bleu 22.46 | wps 4544.2 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.46
2020-12-14 11:02:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.46) (writing took 5.0498307310044765 seconds)
2020-12-14 11:02:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-14 11:02:44 | INFO | train | epoch 003 | symm_kl 0.717 | loss 3.705 | nll_loss 0.872 | ppl 1.83 | wps 20341.5 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 1.25e-05 | gnorm 1.348 | train_wall 258 | wall 851
2020-12-14 11:02:44 | INFO | fairseq.trainer | begin training epoch 4
2020-12-14 11:02:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:02:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:02:55 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=0.707, loss=3.684, nll_loss=0.865, ppl=1.82, wps=13857.3, ups=1.33, wpb=10447.8, bsz=352, num_updates=1700, lr=1.25e-05, gnorm=1.351, train_wall=45, wall=862
2020-12-14 11:03:40 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=0.713, loss=3.7, nll_loss=0.873, ppl=1.83, wps=23084.3, ups=2.2, wpb=10469.1, bsz=365.6, num_updates=1800, lr=1.25e-05, gnorm=1.335, train_wall=45, wall=907
2020-12-14 11:04:26 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=0.712, loss=3.704, nll_loss=0.878, ppl=1.84, wps=22364.7, ups=2.18, wpb=10271.1, bsz=367.4, num_updates=1900, lr=1.25e-05, gnorm=1.353, train_wall=46, wall=953
2020-12-14 11:05:12 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=0.699, loss=3.674, nll_loss=0.866, ppl=1.82, wps=22867, ups=2.16, wpb=10571.4, bsz=356.9, num_updates=2000, lr=1.25e-05, gnorm=1.314, train_wall=46, wall=999
2020-12-14 11:05:58 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=0.696, loss=3.673, nll_loss=0.87, ppl=1.83, wps=22821.8, ups=2.17, wpb=10532.7, bsz=370.6, num_updates=2100, lr=1.25e-05, gnorm=1.31, train_wall=46, wall=1045
2020-12-14 11:06:45 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=0.689, loss=3.663, nll_loss=0.872, ppl=1.83, wps=23005.3, ups=2.17, wpb=10614.4, bsz=387.6, num_updates=2200, lr=1.25e-05, gnorm=1.298, train_wall=46, wall=1092
2020-12-14 11:07:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:07:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:07:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:07:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:07:33 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | loss 5.569 | nll_loss 4.002 | ppl 16.03 | bleu 22.45 | wps 3640.7 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.46
2020-12-14 11:07:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 4 @ 2244 updates, score 22.45) (writing took 3.461450031027198 seconds)
2020-12-14 11:07:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-14 11:07:36 | INFO | train | epoch 004 | symm_kl 0.699 | loss 3.678 | nll_loss 0.871 | ppl 1.83 | wps 20093.3 | ups 1.92 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 1.25e-05 | gnorm 1.319 | train_wall 257 | wall 1143
2020-12-14 11:07:36 | INFO | fairseq.trainer | begin training epoch 5
2020-12-14 11:07:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:07:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:07:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:07:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:08:08 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=0.682, loss=3.645, nll_loss=0.862, ppl=1.82, wps=12529.5, ups=1.2, wpb=10433.5, bsz=373.3, num_updates=2300, lr=1.25e-05, gnorm=1.289, train_wall=48, wall=1175
2020-12-14 11:08:55 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=0.689, loss=3.665, nll_loss=0.871, ppl=1.83, wps=22208.5, ups=2.13, wpb=10447.6, bsz=372.4, num_updates=2400, lr=1.25e-05, gnorm=1.309, train_wall=47, wall=1222
2020-12-14 11:09:41 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=0.689, loss=3.665, nll_loss=0.873, ppl=1.83, wps=22987.2, ups=2.18, wpb=10524.3, bsz=363.8, num_updates=2500, lr=1.25e-05, gnorm=1.301, train_wall=46, wall=1268
2020-12-14 11:10:27 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=0.698, loss=3.684, nll_loss=0.88, ppl=1.84, wps=22507.8, ups=2.16, wpb=10415.7, bsz=357.5, num_updates=2600, lr=1.25e-05, gnorm=1.311, train_wall=46, wall=1314
2020-12-14 11:11:13 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=0.676, loss=3.638, nll_loss=0.864, ppl=1.82, wps=22829.3, ups=2.16, wpb=10565, bsz=383, num_updates=2700, lr=1.25e-05, gnorm=1.275, train_wall=46, wall=1360
2020-12-14 11:12:00 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=0.678, loss=3.641, nll_loss=0.865, ppl=1.82, wps=22544.8, ups=2.15, wpb=10479.6, bsz=374.7, num_updates=2800, lr=1.25e-05, gnorm=1.284, train_wall=46, wall=1407
2020-12-14 11:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:12:24 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | loss 5.562 | nll_loss 3.994 | ppl 15.94 | bleu 22.46 | wps 4434.9 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.46
2020-12-14 11:12:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:12:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:12:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 5 @ 2805 updates, score 22.46) (writing took 5.299186710268259 seconds)
2020-12-14 11:12:29 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-14 11:12:29 | INFO | train | epoch 005 | symm_kl 0.686 | loss 3.658 | nll_loss 0.87 | ppl 1.83 | wps 20095.4 | ups 1.92 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 1.25e-05 | gnorm 1.297 | train_wall 260 | wall 1436
2020-12-14 11:12:29 | INFO | fairseq.trainer | begin training epoch 6
2020-12-14 11:12:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:12:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:13:16 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=0.679, loss=3.644, nll_loss=0.865, ppl=1.82, wps=13591.2, ups=1.32, wpb=10318.1, bsz=377.4, num_updates=2900, lr=1.25e-05, gnorm=1.315, train_wall=45, wall=1483
2020-12-14 11:14:02 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=0.676, loss=3.641, nll_loss=0.868, ppl=1.83, wps=23275.3, ups=2.18, wpb=10679.3, bsz=372.1, num_updates=3000, lr=1.25e-05, gnorm=1.264, train_wall=46, wall=1529
2020-12-14 11:14:47 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=0.674, loss=3.635, nll_loss=0.863, ppl=1.82, wps=23010.5, ups=2.2, wpb=10477.8, bsz=365.4, num_updates=3100, lr=1.25e-05, gnorm=1.284, train_wall=45, wall=1574
2020-12-14 11:15:33 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=0.684, loss=3.664, nll_loss=0.88, ppl=1.84, wps=22882.4, ups=2.18, wpb=10517.4, bsz=358, num_updates=3200, lr=1.25e-05, gnorm=1.289, train_wall=46, wall=1620
2020-12-14 11:16:19 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=0.661, loss=3.606, nll_loss=0.855, ppl=1.81, wps=22859, ups=2.17, wpb=10534.9, bsz=372, num_updates=3300, lr=1.25e-05, gnorm=1.261, train_wall=46, wall=1666
2020-12-14 11:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:16:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:16:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:17:11 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | loss 5.547 | nll_loss 3.979 | ppl 15.77 | bleu 22.53 | wps 4463.7 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.53
2020-12-14 11:17:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:17:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:17:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:17:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 6 @ 3366 updates, score 22.53) (writing took 5.165359744802117 seconds)
2020-12-14 11:17:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-14 11:17:16 | INFO | train | epoch 006 | symm_kl 0.675 | loss 3.641 | nll_loss 0.868 | ppl 1.83 | wps 20496.1 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 1.25e-05 | gnorm 1.282 | train_wall 255 | wall 1723
2020-12-14 11:17:16 | INFO | fairseq.trainer | begin training epoch 7
2020-12-14 11:17:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:17:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:17:35 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=0.676, loss=3.654, nll_loss=0.882, ppl=1.84, wps=13551.8, ups=1.32, wpb=10237, bsz=369, num_updates=3400, lr=1.25e-05, gnorm=1.286, train_wall=45, wall=1742
2020-12-14 11:18:20 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=0.672, loss=3.634, nll_loss=0.865, ppl=1.82, wps=22972.8, ups=2.19, wpb=10508.4, bsz=371.6, num_updates=3500, lr=1.25e-05, gnorm=1.277, train_wall=46, wall=1787
2020-12-14 11:19:06 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=0.669, loss=3.634, nll_loss=0.87, ppl=1.83, wps=22734.9, ups=2.19, wpb=10404.4, bsz=363.4, num_updates=3600, lr=1.25e-05, gnorm=1.277, train_wall=46, wall=1833
2020-12-14 11:19:52 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=0.668, loss=3.633, nll_loss=0.873, ppl=1.83, wps=22973, ups=2.2, wpb=10456.6, bsz=375.4, num_updates=3700, lr=1.25e-05, gnorm=1.266, train_wall=45, wall=1879
2020-12-14 11:20:38 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=0.668, loss=3.626, nll_loss=0.864, ppl=1.82, wps=22761.4, ups=2.17, wpb=10467.8, bsz=366.5, num_updates=3800, lr=1.25e-05, gnorm=1.278, train_wall=46, wall=1925
2020-12-14 11:21:24 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=0.658, loss=3.613, nll_loss=0.866, ppl=1.82, wps=23021.7, ups=2.15, wpb=10688.9, bsz=373.3, num_updates=3900, lr=1.25e-05, gnorm=1.237, train_wall=46, wall=1971
2020-12-14 11:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:21:59 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | loss 5.54 | nll_loss 3.975 | ppl 15.73 | bleu 22.4 | wps 4249.9 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.53
2020-12-14 11:21:59 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:22:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 7 @ 3927 updates, score 22.4) (writing took 3.0735454466193914 seconds)
2020-12-14 11:22:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-14 11:22:02 | INFO | train | epoch 007 | symm_kl 0.666 | loss 3.626 | nll_loss 0.868 | ppl 1.82 | wps 20556.2 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 1.25e-05 | gnorm 1.267 | train_wall 256 | wall 2009
2020-12-14 11:22:02 | INFO | fairseq.trainer | begin training epoch 8
2020-12-14 11:22:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:22:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:22:39 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=0.654, loss=3.604, nll_loss=0.864, ppl=1.82, wps=14182.7, ups=1.34, wpb=10590, bsz=375, num_updates=4000, lr=1.25e-05, gnorm=1.245, train_wall=45, wall=2046
2020-12-14 11:23:25 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=0.668, loss=3.628, nll_loss=0.865, ppl=1.82, wps=22941, ups=2.18, wpb=10504.7, bsz=358.9, num_updates=4100, lr=1.25e-05, gnorm=1.267, train_wall=46, wall=2092
2020-12-14 11:24:10 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=0.658, loss=3.613, nll_loss=0.865, ppl=1.82, wps=22612.4, ups=2.18, wpb=10367.5, bsz=366.4, num_updates=4200, lr=1.25e-05, gnorm=1.259, train_wall=46, wall=2137
2020-12-14 11:24:56 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=0.652, loss=3.599, nll_loss=0.86, ppl=1.82, wps=22621.4, ups=2.17, wpb=10416.1, bsz=389.5, num_updates=4300, lr=1.25e-05, gnorm=1.247, train_wall=46, wall=2183
2020-12-14 11:25:43 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=0.646, loss=3.591, nll_loss=0.863, ppl=1.82, wps=23114.1, ups=2.17, wpb=10648.7, bsz=379.6, num_updates=4400, lr=1.25e-05, gnorm=1.22, train_wall=46, wall=2230
2020-12-14 11:26:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:26:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:26:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:26:44 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | loss 5.535 | nll_loss 3.969 | ppl 15.66 | bleu 22.58 | wps 4542.6 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.58
2020-12-14 11:26:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 8 @ 4488 updates, score 22.58) (writing took 5.488973217085004 seconds)
2020-12-14 11:26:50 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-14 11:26:50 | INFO | train | epoch 008 | symm_kl 0.658 | loss 3.613 | nll_loss 0.867 | ppl 1.82 | wps 20448.5 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 1.25e-05 | gnorm 1.253 | train_wall 256 | wall 2297
2020-12-14 11:26:50 | INFO | fairseq.trainer | begin training epoch 9
2020-12-14 11:26:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:26:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:26:59 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=0.664, loss=3.633, nll_loss=0.879, ppl=1.84, wps=13587.7, ups=1.32, wpb=10320.4, bsz=352.8, num_updates=4500, lr=1.25e-05, gnorm=1.28, train_wall=46, wall=2305
2020-12-14 11:27:44 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=0.652, loss=3.595, nll_loss=0.856, ppl=1.81, wps=23261.3, ups=2.21, wpb=10532.6, bsz=374.2, num_updates=4600, lr=1.25e-05, gnorm=1.236, train_wall=45, wall=2351
2020-12-14 11:28:30 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=0.666, loss=3.636, nll_loss=0.877, ppl=1.84, wps=22753.5, ups=2.16, wpb=10528, bsz=345, num_updates=4700, lr=1.25e-05, gnorm=1.261, train_wall=46, wall=2397
2020-12-14 11:29:16 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=0.644, loss=3.589, nll_loss=0.862, ppl=1.82, wps=22864, ups=2.18, wpb=10473.1, bsz=377.1, num_updates=4800, lr=1.25e-05, gnorm=1.229, train_wall=46, wall=2443
2020-12-14 11:30:02 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=0.648, loss=3.596, nll_loss=0.862, ppl=1.82, wps=22708.6, ups=2.17, wpb=10483.1, bsz=368.3, num_updates=4900, lr=1.25e-05, gnorm=1.248, train_wall=46, wall=2489
2020-12-14 11:30:48 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=0.641, loss=3.584, nll_loss=0.862, ppl=1.82, wps=22825.4, ups=2.17, wpb=10514.7, bsz=388.6, num_updates=5000, lr=1.25e-05, gnorm=1.225, train_wall=46, wall=2535
2020-12-14 11:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:31:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:31:33 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | loss 5.529 | nll_loss 3.962 | ppl 15.59 | bleu 22.42 | wps 4196.8 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.58
2020-12-14 11:31:33 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:31:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 9 @ 5049 updates, score 22.42) (writing took 3.206224665045738 seconds)
2020-12-14 11:31:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-14 11:31:36 | INFO | train | epoch 009 | symm_kl 0.651 | loss 3.603 | nll_loss 0.865 | ppl 1.82 | wps 20506.5 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 1.25e-05 | gnorm 1.242 | train_wall 256 | wall 2583
2020-12-14 11:31:36 | INFO | fairseq.trainer | begin training epoch 10
2020-12-14 11:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:31:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:32:03 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=0.657, loss=3.621, nll_loss=0.875, ppl=1.83, wps=13833.6, ups=1.34, wpb=10353, bsz=356, num_updates=5100, lr=1.25e-05, gnorm=1.259, train_wall=45, wall=2610
2020-12-14 11:32:49 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=0.647, loss=3.599, nll_loss=0.869, ppl=1.83, wps=23161.5, ups=2.19, wpb=10577.3, bsz=365.6, num_updates=5200, lr=1.25e-05, gnorm=1.226, train_wall=45, wall=2656
2020-12-14 11:33:35 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=0.651, loss=3.604, nll_loss=0.866, ppl=1.82, wps=22852.3, ups=2.18, wpb=10501.8, bsz=363.6, num_updates=5300, lr=1.25e-05, gnorm=1.238, train_wall=46, wall=2702
2020-12-14 11:34:21 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=0.647, loss=3.599, nll_loss=0.868, ppl=1.82, wps=22716.6, ups=2.17, wpb=10450.1, bsz=375.6, num_updates=5400, lr=1.25e-05, gnorm=1.238, train_wall=46, wall=2748
2020-12-14 11:35:07 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=0.634, loss=3.569, nll_loss=0.858, ppl=1.81, wps=22756.1, ups=2.17, wpb=10472.1, bsz=373.6, num_updates=5500, lr=1.25e-05, gnorm=1.22, train_wall=46, wall=2794
2020-12-14 11:35:54 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=0.639, loss=3.582, nll_loss=0.865, ppl=1.82, wps=22349.2, ups=2.13, wpb=10516.7, bsz=381.2, num_updates=5600, lr=1.25e-05, gnorm=1.22, train_wall=47, wall=2841
2020-12-14 11:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:36:26 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | loss 5.526 | nll_loss 3.961 | ppl 15.57 | bleu 22.52 | wps 3475.7 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.58
2020-12-14 11:36:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:36:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:36:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.52) (writing took 3.738448865711689 seconds)
2020-12-14 11:36:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-12-14 11:36:30 | INFO | train | epoch 010 | symm_kl 0.644 | loss 3.592 | nll_loss 0.865 | ppl 1.82 | wps 20052.8 | ups 1.91 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 1.25e-05 | gnorm 1.233 | train_wall 257 | wall 2877
2020-12-14 11:36:30 | INFO | fairseq.trainer | begin training epoch 11
2020-12-14 11:36:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:36:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:37:17 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=0.647, loss=3.597, nll_loss=0.866, ppl=1.82, wps=12486.2, ups=1.21, wpb=10358.6, bsz=351.1, num_updates=5700, lr=1.25e-05, gnorm=1.245, train_wall=47, wall=2924
2020-12-14 11:38:04 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=0.63, loss=3.562, nll_loss=0.856, ppl=1.81, wps=22373.8, ups=2.12, wpb=10564, bsz=383.6, num_updates=5800, lr=1.25e-05, gnorm=1.208, train_wall=47, wall=2971
2020-12-14 11:38:50 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=0.648, loss=3.604, nll_loss=0.874, ppl=1.83, wps=22558.2, ups=2.17, wpb=10400.1, bsz=355.4, num_updates=5900, lr=1.25e-05, gnorm=1.234, train_wall=46, wall=3017
2020-12-14 11:39:36 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=0.648, loss=3.606, nll_loss=0.875, ppl=1.83, wps=22646.6, ups=2.18, wpb=10394.2, bsz=372.6, num_updates=6000, lr=1.25e-05, gnorm=1.241, train_wall=46, wall=3063
2020-12-14 11:40:22 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=0.629, loss=3.56, nll_loss=0.855, ppl=1.81, wps=23184.6, ups=2.18, wpb=10652.7, bsz=380.5, num_updates=6100, lr=1.25e-05, gnorm=1.213, train_wall=46, wall=3109
2020-12-14 11:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:40:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:40:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:40:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:40:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:40:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:40:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:41:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:41:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:41:16 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | loss 5.521 | nll_loss 3.955 | ppl 15.51 | bleu 22.48 | wps 4479.1 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 22.58
2020-12-14 11:41:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:41:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:41:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:41:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:41:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:41:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 11 @ 6171 updates, score 22.48) (writing took 3.215510904788971 seconds)
2020-12-14 11:41:19 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-12-14 11:41:19 | INFO | train | epoch 011 | symm_kl 0.639 | loss 3.584 | nll_loss 0.865 | ppl 1.82 | wps 20326.9 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 1.25e-05 | gnorm 1.225 | train_wall 259 | wall 3166
2020-12-14 11:41:19 | INFO | fairseq.trainer | begin training epoch 12
2020-12-14 11:41:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:41:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:41:35 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=0.633, loss=3.572, nll_loss=0.86, ppl=1.82, wps=14220.1, ups=1.36, wpb=10473.3, bsz=364.9, num_updates=6200, lr=1.25e-05, gnorm=1.215, train_wall=45, wall=3182
2020-12-14 11:42:21 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=0.634, loss=3.574, nll_loss=0.863, ppl=1.82, wps=22981, ups=2.21, wpb=10400.1, bsz=369, num_updates=6300, lr=1.25e-05, gnorm=1.216, train_wall=45, wall=3228
2020-12-14 11:43:06 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=0.641, loss=3.594, nll_loss=0.873, ppl=1.83, wps=22895.6, ups=2.19, wpb=10444.8, bsz=372, num_updates=6400, lr=1.25e-05, gnorm=1.227, train_wall=45, wall=3273
2020-12-14 11:43:53 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=0.623, loss=3.551, nll_loss=0.855, ppl=1.81, wps=22895.7, ups=2.15, wpb=10631.9, bsz=382.4, num_updates=6500, lr=1.25e-05, gnorm=1.191, train_wall=46, wall=3320
2020-12-14 11:44:39 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=0.643, loss=3.599, nll_loss=0.875, ppl=1.83, wps=22854.5, ups=2.17, wpb=10531, bsz=361.8, num_updates=6600, lr=1.25e-05, gnorm=1.223, train_wall=46, wall=3366
2020-12-14 11:45:25 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=0.628, loss=3.562, nll_loss=0.86, ppl=1.81, wps=22844.6, ups=2.18, wpb=10493.3, bsz=369.8, num_updates=6700, lr=1.25e-05, gnorm=1.205, train_wall=46, wall=3412
2020-12-14 11:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:45:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:45:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:45:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:45:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:45:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:45:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:46:03 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | loss 5.515 | nll_loss 3.949 | ppl 15.44 | bleu 22.64 | wps 4040.1 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 22.64
2020-12-14 11:46:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:46:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:46:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 12 @ 6732 updates, score 22.64) (writing took 5.410464968532324 seconds)
2020-12-14 11:46:08 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-12-14 11:46:08 | INFO | train | epoch 012 | symm_kl 0.634 | loss 3.575 | nll_loss 0.864 | ppl 1.82 | wps 20346.6 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 1.25e-05 | gnorm 1.213 | train_wall 256 | wall 3455
2020-12-14 11:46:08 | INFO | fairseq.trainer | begin training epoch 13
2020-12-14 11:46:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:46:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:46:42 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=0.635, loss=3.574, nll_loss=0.861, ppl=1.82, wps=13457, ups=1.3, wpb=10372.9, bsz=360.1, num_updates=6800, lr=1.25e-05, gnorm=1.228, train_wall=45, wall=3489
2020-12-14 11:47:28 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=0.637, loss=3.583, nll_loss=0.866, ppl=1.82, wps=23008.2, ups=2.18, wpb=10571.4, bsz=349, num_updates=6900, lr=1.25e-05, gnorm=1.222, train_wall=46, wall=3535
2020-12-14 11:48:14 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=0.628, loss=3.567, nll_loss=0.865, ppl=1.82, wps=22701.4, ups=2.15, wpb=10544.3, bsz=369.9, num_updates=7000, lr=1.25e-05, gnorm=1.199, train_wall=46, wall=3581
2020-12-14 11:49:00 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=0.617, loss=3.541, nll_loss=0.853, ppl=1.81, wps=22909.5, ups=2.18, wpb=10490.1, bsz=389.7, num_updates=7100, lr=1.25e-05, gnorm=1.203, train_wall=46, wall=3627
2020-12-14 11:49:46 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=0.627, loss=3.565, nll_loss=0.864, ppl=1.82, wps=22974.8, ups=2.19, wpb=10510.3, bsz=369.8, num_updates=7200, lr=1.25e-05, gnorm=1.197, train_wall=46, wall=3673
2020-12-14 11:50:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:50:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:50:50 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | loss 5.513 | nll_loss 3.946 | ppl 15.41 | bleu 22.63 | wps 4471.3 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 22.64
2020-12-14 11:50:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:50:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 13 @ 7293 updates, score 22.63) (writing took 3.2467638850212097 seconds)
2020-12-14 11:50:53 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-12-14 11:50:53 | INFO | train | epoch 013 | symm_kl 0.629 | loss 3.567 | nll_loss 0.863 | ppl 1.82 | wps 20649.5 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 1.25e-05 | gnorm 1.211 | train_wall 256 | wall 3740
2020-12-14 11:50:53 | INFO | fairseq.trainer | begin training epoch 14
2020-12-14 11:50:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:50:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:50:59 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=0.627, loss=3.569, nll_loss=0.868, ppl=1.82, wps=14010, ups=1.36, wpb=10327.2, bsz=373.9, num_updates=7300, lr=1.25e-05, gnorm=1.221, train_wall=45, wall=3746
2020-12-14 11:51:45 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=0.633, loss=3.577, nll_loss=0.865, ppl=1.82, wps=23414.2, ups=2.21, wpb=10590, bsz=367, num_updates=7400, lr=1.25e-05, gnorm=1.2, train_wall=45, wall=3792
2020-12-14 11:52:31 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=0.62, loss=3.547, nll_loss=0.854, ppl=1.81, wps=22853.3, ups=2.16, wpb=10574.3, bsz=374, num_updates=7500, lr=1.25e-05, gnorm=1.193, train_wall=46, wall=3838
2020-12-14 11:53:16 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=0.627, loss=3.564, nll_loss=0.863, ppl=1.82, wps=22834.5, ups=2.2, wpb=10386.9, bsz=357.6, num_updates=7600, lr=1.25e-05, gnorm=1.224, train_wall=45, wall=3883
2020-12-14 11:54:02 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=0.631, loss=3.577, nll_loss=0.871, ppl=1.83, wps=22564, ups=2.18, wpb=10338.9, bsz=367.2, num_updates=7700, lr=1.25e-05, gnorm=1.218, train_wall=46, wall=3929
2020-12-14 11:54:49 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=0.619, loss=3.55, nll_loss=0.858, ppl=1.81, wps=22897.3, ups=2.16, wpb=10594.9, bsz=368.6, num_updates=7800, lr=1.25e-05, gnorm=1.187, train_wall=46, wall=3976
2020-12-14 11:55:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 11:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:55:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 11:55:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 11:55:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 11:55:35 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | loss 5.513 | nll_loss 3.947 | ppl 15.42 | bleu 22.6 | wps 4488 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 22.64
2020-12-14 11:55:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 11:55:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:55:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:55:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 14 @ 7854 updates, score 22.6) (writing took 3.22310833632946 seconds)
2020-12-14 11:55:38 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-12-14 11:55:38 | INFO | train | epoch 014 | symm_kl 0.625 | loss 3.561 | nll_loss 0.862 | ppl 1.82 | wps 20608.3 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 1.25e-05 | gnorm 1.205 | train_wall 256 | wall 4025
2020-12-14 11:55:38 | INFO | fairseq.trainer | begin training epoch 15
2020-12-14 11:55:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 11:55:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 11:56:02 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=0.625, loss=3.565, nll_loss=0.868, ppl=1.82, wps=14072.2, ups=1.36, wpb=10380.4, bsz=382.1, num_updates=7900, lr=1.25e-05, gnorm=1.217, train_wall=45, wall=4049
2020-12-14 11:56:48 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=0.624, loss=3.564, nll_loss=0.866, ppl=1.82, wps=22916.8, ups=2.19, wpb=10471, bsz=365.8, num_updates=8000, lr=1.25e-05, gnorm=1.198, train_wall=45, wall=4095
2020-12-14 11:57:34 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=0.614, loss=3.535, nll_loss=0.852, ppl=1.8, wps=22809.6, ups=2.18, wpb=10485.3, bsz=382.9, num_updates=8100, lr=1.25e-05, gnorm=1.194, train_wall=46, wall=4141
2020-12-14 11:58:20 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=0.62, loss=3.551, nll_loss=0.859, ppl=1.81, wps=23037.8, ups=2.19, wpb=10526.2, bsz=362.5, num_updates=8200, lr=1.25e-05, gnorm=1.198, train_wall=45, wall=4187
2020-12-14 11:59:06 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=0.615, loss=3.547, nll_loss=0.862, ppl=1.82, wps=22901.6, ups=2.18, wpb=10527.6, bsz=374.3, num_updates=8300, lr=1.25e-05, gnorm=1.18, train_wall=46, wall=4233
2020-12-14 11:59:52 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=0.626, loss=3.571, nll_loss=0.871, ppl=1.83, wps=22675.4, ups=2.17, wpb=10468.7, bsz=364.2, num_updates=8400, lr=1.25e-05, gnorm=1.204, train_wall=46, wall=4279
2020-12-14 11:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:00:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:00:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:00:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:00:20 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | loss 5.512 | nll_loss 3.945 | ppl 15.4 | bleu 22.63 | wps 4517.2 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 22.64
2020-12-14 12:00:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:00:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:00:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 15 @ 8415 updates, score 22.63) (writing took 3.2828368488699198 seconds)
2020-12-14 12:00:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-12-14 12:00:23 | INFO | train | epoch 015 | symm_kl 0.62 | loss 3.553 | nll_loss 0.862 | ppl 1.82 | wps 20626 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 1.25e-05 | gnorm 1.196 | train_wall 256 | wall 4310
2020-12-14 12:00:23 | INFO | fairseq.trainer | begin training epoch 16
2020-12-14 12:00:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:00:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:01:06 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=0.612, loss=3.536, nll_loss=0.854, ppl=1.81, wps=14111.9, ups=1.36, wpb=10408.1, bsz=377.6, num_updates=8500, lr=1.25e-05, gnorm=1.19, train_wall=45, wall=4353
2020-12-14 12:01:51 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=0.62, loss=3.553, nll_loss=0.863, ppl=1.82, wps=22873.4, ups=2.19, wpb=10433.6, bsz=372.2, num_updates=8600, lr=1.25e-05, gnorm=1.2, train_wall=45, wall=4398
2020-12-14 12:02:37 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=0.619, loss=3.552, nll_loss=0.862, ppl=1.82, wps=23095.3, ups=2.19, wpb=10533.1, bsz=379.8, num_updates=8700, lr=1.25e-05, gnorm=1.188, train_wall=45, wall=4444
2020-12-14 12:03:23 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=0.605, loss=3.52, nll_loss=0.849, ppl=1.8, wps=22848.5, ups=2.17, wpb=10551.2, bsz=375.2, num_updates=8800, lr=1.25e-05, gnorm=1.168, train_wall=46, wall=4490
2020-12-14 12:04:09 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=0.618, loss=3.549, nll_loss=0.86, ppl=1.82, wps=22813.6, ups=2.17, wpb=10494.4, bsz=360.2, num_updates=8900, lr=1.25e-05, gnorm=1.197, train_wall=46, wall=4536
2020-12-14 12:04:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:04:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:04:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:04:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:04:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:04:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:04:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:05:05 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | loss 5.503 | nll_loss 3.937 | ppl 15.31 | bleu 22.62 | wps 4497.8 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 22.64
2020-12-14 12:05:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:05:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:05:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 16 @ 8976 updates, score 22.62) (writing took 3.443401737138629 seconds)
2020-12-14 12:05:09 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-12-14 12:05:09 | INFO | train | epoch 016 | symm_kl 0.617 | loss 3.548 | nll_loss 0.861 | ppl 1.82 | wps 20622.9 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 1.25e-05 | gnorm 1.192 | train_wall 256 | wall 4596
2020-12-14 12:05:09 | INFO | fairseq.trainer | begin training epoch 17
2020-12-14 12:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:05:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:05:23 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=0.617, loss=3.554, nll_loss=0.868, ppl=1.83, wps=14178.6, ups=1.36, wpb=10459.4, bsz=358.3, num_updates=9000, lr=1.25e-05, gnorm=1.199, train_wall=45, wall=4610
2020-12-14 12:06:08 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=0.615, loss=3.544, nll_loss=0.86, ppl=1.81, wps=23068.6, ups=2.2, wpb=10489.1, bsz=374.8, num_updates=9100, lr=1.25e-05, gnorm=1.193, train_wall=45, wall=4655
2020-12-14 12:06:54 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=0.603, loss=3.511, nll_loss=0.843, ppl=1.79, wps=22894, ups=2.16, wpb=10592.8, bsz=369.3, num_updates=9200, lr=1.25e-05, gnorm=1.174, train_wall=46, wall=4701
2020-12-14 12:07:40 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=0.612, loss=3.535, nll_loss=0.854, ppl=1.81, wps=22965.1, ups=2.19, wpb=10507.6, bsz=365.5, num_updates=9300, lr=1.25e-05, gnorm=1.184, train_wall=46, wall=4747
2020-12-14 12:08:26 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=0.614, loss=3.549, nll_loss=0.866, ppl=1.82, wps=22761.8, ups=2.19, wpb=10397.2, bsz=380.2, num_updates=9400, lr=1.25e-05, gnorm=1.193, train_wall=45, wall=4793
2020-12-14 12:09:12 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=0.622, loss=3.571, nll_loss=0.878, ppl=1.84, wps=22807, ups=2.17, wpb=10491.6, bsz=362.4, num_updates=9500, lr=1.25e-05, gnorm=1.199, train_wall=46, wall=4839
2020-12-14 12:09:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:09:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:09:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:09:51 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | loss 5.5 | nll_loss 3.935 | ppl 15.29 | bleu 22.63 | wps 4370.4 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 22.64
2020-12-14 12:09:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:09:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:09:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:09:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 17 @ 9537 updates, score 22.63) (writing took 3.092026563361287 seconds)
2020-12-14 12:09:54 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-12-14 12:09:54 | INFO | train | epoch 017 | symm_kl 0.613 | loss 3.542 | nll_loss 0.86 | ppl 1.82 | wps 20633.9 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 1.25e-05 | gnorm 1.189 | train_wall 256 | wall 4881
2020-12-14 12:09:54 | INFO | fairseq.trainer | begin training epoch 18
2020-12-14 12:09:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:09:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:10:26 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=0.614, loss=3.543, nll_loss=0.859, ppl=1.81, wps=14145.3, ups=1.36, wpb=10417.2, bsz=358.2, num_updates=9600, lr=1.25e-05, gnorm=1.194, train_wall=45, wall=4913
2020-12-14 12:11:11 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=0.609, loss=3.536, nll_loss=0.861, ppl=1.82, wps=23012.1, ups=2.19, wpb=10492.8, bsz=370, num_updates=9700, lr=1.25e-05, gnorm=1.184, train_wall=45, wall=4958
2020-12-14 12:11:57 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=0.608, loss=3.53, nll_loss=0.854, ppl=1.81, wps=22767.3, ups=2.18, wpb=10456, bsz=363.1, num_updates=9800, lr=1.25e-05, gnorm=1.188, train_wall=46, wall=5004
2020-12-14 12:12:43 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=0.607, loss=3.533, nll_loss=0.861, ppl=1.82, wps=22666.2, ups=2.18, wpb=10380.2, bsz=372.9, num_updates=9900, lr=1.25e-05, gnorm=1.191, train_wall=46, wall=5050
2020-12-14 12:13:29 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=0.605, loss=3.528, nll_loss=0.859, ppl=1.81, wps=23179.4, ups=2.18, wpb=10622, bsz=379.6, num_updates=10000, lr=1.25e-05, gnorm=1.156, train_wall=46, wall=5096
2020-12-14 12:14:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:14:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:14:35 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | loss 5.499 | nll_loss 3.934 | ppl 15.28 | bleu 22.64 | wps 4354.6 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 22.64
2020-12-14 12:14:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:14:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 18 @ 10098 updates, score 22.64) (writing took 5.309463530778885 seconds)
2020-12-14 12:14:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-12-14 12:14:41 | INFO | train | epoch 018 | symm_kl 0.609 | loss 3.535 | nll_loss 0.859 | ppl 1.81 | wps 20500.7 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 1.25e-05 | gnorm 1.18 | train_wall 255 | wall 5168
2020-12-14 12:14:41 | INFO | fairseq.trainer | begin training epoch 19
2020-12-14 12:14:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:14:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:14:45 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=0.613, loss=3.55, nll_loss=0.869, ppl=1.83, wps=13692.4, ups=1.31, wpb=10417.9, bsz=365.4, num_updates=10100, lr=1.25e-05, gnorm=1.184, train_wall=46, wall=5172
2020-12-14 12:15:30 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=0.604, loss=3.525, nll_loss=0.854, ppl=1.81, wps=23056.2, ups=2.21, wpb=10422.9, bsz=373.4, num_updates=10200, lr=1.25e-05, gnorm=1.176, train_wall=45, wall=5217
2020-12-14 12:16:16 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=0.596, loss=3.511, nll_loss=0.853, ppl=1.81, wps=23214.8, ups=2.18, wpb=10632.5, bsz=380, num_updates=10300, lr=1.25e-05, gnorm=1.146, train_wall=46, wall=5263
2020-12-14 12:17:02 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=0.608, loss=3.533, nll_loss=0.859, ppl=1.81, wps=22853.1, ups=2.18, wpb=10486.3, bsz=361.5, num_updates=10400, lr=1.25e-05, gnorm=1.189, train_wall=46, wall=5309
2020-12-14 12:17:48 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=0.613, loss=3.547, nll_loss=0.866, ppl=1.82, wps=22879.2, ups=2.18, wpb=10482.8, bsz=369.7, num_updates=10500, lr=1.25e-05, gnorm=1.183, train_wall=46, wall=5354
2020-12-14 12:18:33 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=0.603, loss=3.519, nll_loss=0.852, ppl=1.81, wps=22780.9, ups=2.18, wpb=10451.7, bsz=365.4, num_updates=10600, lr=1.25e-05, gnorm=1.186, train_wall=46, wall=5400
2020-12-14 12:19:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:19:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:19:21 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | loss 5.5 | nll_loss 3.934 | ppl 15.29 | bleu 22.75 | wps 4565.4 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 22.75
2020-12-14 12:19:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:19:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 19 @ 10659 updates, score 22.75) (writing took 5.405463242903352 seconds)
2020-12-14 12:19:26 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-12-14 12:19:26 | INFO | train | epoch 019 | symm_kl 0.605 | loss 3.53 | nll_loss 0.858 | ppl 1.81 | wps 20576 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 1.25e-05 | gnorm 1.182 | train_wall 255 | wall 5453
2020-12-14 12:19:26 | INFO | fairseq.trainer | begin training epoch 20
2020-12-14 12:19:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:19:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:19:48 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=0.604, loss=3.527, nll_loss=0.86, ppl=1.82, wps=14039.4, ups=1.34, wpb=10477.3, bsz=372.4, num_updates=10700, lr=1.25e-05, gnorm=1.195, train_wall=45, wall=5475
2020-12-14 12:20:34 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=0.608, loss=3.538, nll_loss=0.864, ppl=1.82, wps=23174.8, ups=2.2, wpb=10538.1, bsz=365.8, num_updates=10800, lr=1.25e-05, gnorm=1.174, train_wall=45, wall=5520
2020-12-14 12:21:20 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=0.599, loss=3.519, nll_loss=0.857, ppl=1.81, wps=22769.8, ups=2.17, wpb=10496, bsz=377, num_updates=10900, lr=1.25e-05, gnorm=1.168, train_wall=46, wall=5567
2020-12-14 12:22:06 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=0.607, loss=3.531, nll_loss=0.857, ppl=1.81, wps=22754, ups=2.16, wpb=10521.2, bsz=361.4, num_updates=11000, lr=1.25e-05, gnorm=1.182, train_wall=46, wall=5613
2020-12-14 12:22:52 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=0.601, loss=3.525, nll_loss=0.861, ppl=1.82, wps=22878.4, ups=2.19, wpb=10445.5, bsz=372.4, num_updates=11100, lr=1.25e-05, gnorm=1.18, train_wall=45, wall=5658
2020-12-14 12:23:37 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=0.605, loss=3.531, nll_loss=0.862, ppl=1.82, wps=22778.4, ups=2.19, wpb=10410.2, bsz=367.4, num_updates=11200, lr=1.25e-05, gnorm=1.196, train_wall=45, wall=5704
2020-12-14 12:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:23:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:23:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:23:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:23:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:23:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:24:08 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | loss 5.498 | nll_loss 3.932 | ppl 15.27 | bleu 22.55 | wps 4394.2 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 22.75
2020-12-14 12:24:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:24:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:24:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:24:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:24:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:24:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 20 @ 11220 updates, score 22.55) (writing took 2.8465541917830706 seconds)
2020-12-14 12:24:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-12-14 12:24:11 | INFO | train | epoch 020 | symm_kl 0.603 | loss 3.526 | nll_loss 0.859 | ppl 1.81 | wps 20650.2 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 1.25e-05 | gnorm 1.178 | train_wall 256 | wall 5738
2020-12-14 12:24:11 | INFO | fairseq.trainer | begin training epoch 21
2020-12-14 12:24:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:24:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:24:51 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=0.603, loss=3.53, nll_loss=0.863, ppl=1.82, wps=14013.3, ups=1.36, wpb=10315.2, bsz=376.1, num_updates=11300, lr=1.25e-05, gnorm=1.19, train_wall=45, wall=5778
2020-12-14 12:25:37 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=0.6, loss=3.521, nll_loss=0.858, ppl=1.81, wps=22977.7, ups=2.18, wpb=10526.7, bsz=359.6, num_updates=11400, lr=1.25e-05, gnorm=1.168, train_wall=46, wall=5824
2020-12-14 12:26:23 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=0.596, loss=3.508, nll_loss=0.85, ppl=1.8, wps=22899.6, ups=2.18, wpb=10514.2, bsz=377.1, num_updates=11500, lr=1.25e-05, gnorm=1.164, train_wall=46, wall=5870
2020-12-14 12:27:08 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=0.591, loss=3.498, nll_loss=0.847, ppl=1.8, wps=23105.6, ups=2.18, wpb=10592.6, bsz=375.9, num_updates=11600, lr=1.25e-05, gnorm=1.151, train_wall=46, wall=5915
2020-12-14 12:27:54 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=0.603, loss=3.529, nll_loss=0.863, ppl=1.82, wps=22893, ups=2.18, wpb=10501.6, bsz=373.3, num_updates=11700, lr=1.25e-05, gnorm=1.16, train_wall=46, wall=5961
2020-12-14 12:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:28:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:28:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:28:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:28:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:28:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:28:52 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | loss 5.495 | nll_loss 3.929 | ppl 15.23 | bleu 22.67 | wps 4553.8 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 22.75
2020-12-14 12:28:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:28:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:28:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:28:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 21 @ 11781 updates, score 22.67) (writing took 3.145384918898344 seconds)
2020-12-14 12:28:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-12-14 12:28:55 | INFO | train | epoch 021 | symm_kl 0.6 | loss 3.521 | nll_loss 0.858 | ppl 1.81 | wps 20686.4 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 1.25e-05 | gnorm 1.169 | train_wall 255 | wall 6022
2020-12-14 12:28:55 | INFO | fairseq.trainer | begin training epoch 22
2020-12-14 12:28:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:28:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:29:07 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=0.607, loss=3.538, nll_loss=0.867, ppl=1.82, wps=14400.6, ups=1.38, wpb=10469.7, bsz=353, num_updates=11800, lr=1.25e-05, gnorm=1.178, train_wall=45, wall=6034
2020-12-14 12:29:52 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=0.594, loss=3.506, nll_loss=0.852, ppl=1.8, wps=23338.8, ups=2.2, wpb=10589.6, bsz=376.8, num_updates=11900, lr=1.25e-05, gnorm=1.155, train_wall=45, wall=6079
2020-12-14 12:30:38 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=0.59, loss=3.499, nll_loss=0.851, ppl=1.8, wps=22842.4, ups=2.18, wpb=10493.2, bsz=374.8, num_updates=12000, lr=1.25e-05, gnorm=1.152, train_wall=46, wall=6125
2020-12-14 12:31:24 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=0.606, loss=3.538, nll_loss=0.868, ppl=1.82, wps=22834.2, ups=2.19, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1.25e-05, gnorm=1.175, train_wall=45, wall=6171
2020-12-14 12:32:10 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=0.593, loss=3.499, nll_loss=0.845, ppl=1.8, wps=22986.7, ups=2.18, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1.25e-05, gnorm=1.158, train_wall=46, wall=6217
2020-12-14 12:32:56 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=0.601, loss=3.53, nll_loss=0.867, ppl=1.82, wps=22690.6, ups=2.17, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1.25e-05, gnorm=1.17, train_wall=46, wall=6263
2020-12-14 12:33:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:33:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:33:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:33:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:33:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:33:37 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | loss 5.489 | nll_loss 3.924 | ppl 15.18 | bleu 22.63 | wps 4307.6 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 22.75
2020-12-14 12:33:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:33:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:33:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:33:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 22 @ 12342 updates, score 22.63) (writing took 3.197670765221119 seconds)
2020-12-14 12:33:40 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-12-14 12:33:40 | INFO | train | epoch 022 | symm_kl 0.598 | loss 3.517 | nll_loss 0.858 | ppl 1.81 | wps 20670.5 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.25e-05 | gnorm 1.163 | train_wall 255 | wall 6307
2020-12-14 12:33:40 | INFO | fairseq.trainer | begin training epoch 23
2020-12-14 12:33:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:33:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:34:10 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=0.612, loss=3.55, nll_loss=0.873, ppl=1.83, wps=14045.9, ups=1.35, wpb=10402.8, bsz=349, num_updates=12400, lr=1.25e-05, gnorm=1.189, train_wall=45, wall=6337
2020-12-14 12:34:56 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=0.59, loss=3.505, nll_loss=0.858, ppl=1.81, wps=22904.4, ups=2.18, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1.25e-05, gnorm=1.151, train_wall=46, wall=6383
2020-12-14 12:35:42 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=0.605, loss=3.536, nll_loss=0.865, ppl=1.82, wps=22653.6, ups=2.17, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1.25e-05, gnorm=1.19, train_wall=46, wall=6429
2020-12-14 12:36:28 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=0.589, loss=3.5, nll_loss=0.853, ppl=1.81, wps=22885.2, ups=2.16, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1.25e-05, gnorm=1.145, train_wall=46, wall=6475
2020-12-14 12:37:14 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=0.592, loss=3.507, nll_loss=0.856, ppl=1.81, wps=22455.2, ups=2.16, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1.25e-05, gnorm=1.155, train_wall=46, wall=6521
2020-12-14 12:38:00 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=0.587, loss=3.492, nll_loss=0.847, ppl=1.8, wps=22905.4, ups=2.18, wpb=10521.5, bsz=381, num_updates=12900, lr=1.25e-05, gnorm=1.156, train_wall=46, wall=6567
2020-12-14 12:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:38:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:38:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:38:23 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | loss 5.489 | nll_loss 3.923 | ppl 15.17 | bleu 22.63 | wps 4508.1 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 22.75
2020-12-14 12:38:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:38:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 23 @ 12903 updates, score 22.63) (writing took 3.325883038341999 seconds)
2020-12-14 12:38:26 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-12-14 12:38:26 | INFO | train | epoch 023 | symm_kl 0.595 | loss 3.512 | nll_loss 0.857 | ppl 1.81 | wps 20543.8 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.25e-05 | gnorm 1.164 | train_wall 257 | wall 6593
2020-12-14 12:38:26 | INFO | fairseq.trainer | begin training epoch 24
2020-12-14 12:38:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:38:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:39:15 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=0.591, loss=3.505, nll_loss=0.854, ppl=1.81, wps=14166.5, ups=1.34, wpb=10536, bsz=373.3, num_updates=13000, lr=1.25e-05, gnorm=1.15, train_wall=46, wall=6642
2020-12-14 12:40:01 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=0.595, loss=3.509, nll_loss=0.853, ppl=1.81, wps=22407.9, ups=2.15, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1.25e-05, gnorm=1.165, train_wall=46, wall=6688
2020-12-14 12:40:47 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=0.59, loss=3.502, nll_loss=0.853, ppl=1.81, wps=22895.4, ups=2.17, wpb=10536, bsz=369, num_updates=13200, lr=1.25e-05, gnorm=1.152, train_wall=46, wall=6734
2020-12-14 12:41:33 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=0.595, loss=3.517, nll_loss=0.862, ppl=1.82, wps=22584.1, ups=2.19, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1.25e-05, gnorm=1.165, train_wall=45, wall=6780
2020-12-14 12:42:19 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=0.596, loss=3.521, nll_loss=0.867, ppl=1.82, wps=22864.7, ups=2.18, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1.25e-05, gnorm=1.16, train_wall=46, wall=6826
2020-12-14 12:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:42:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:42:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:42:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:42:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:42:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:42:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:42:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:42:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:42:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:42:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:43:10 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | loss 5.487 | nll_loss 3.922 | ppl 15.16 | bleu 22.55 | wps 4529.5 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 22.75
2020-12-14 12:43:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:43:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:43:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:43:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:43:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 24 @ 13464 updates, score 22.55) (writing took 3.234998444095254 seconds)
2020-12-14 12:43:13 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-12-14 12:43:13 | INFO | train | epoch 024 | symm_kl 0.592 | loss 3.507 | nll_loss 0.856 | ppl 1.81 | wps 20518.1 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.25e-05 | gnorm 1.156 | train_wall 257 | wall 6880
2020-12-14 12:43:13 | INFO | fairseq.trainer | begin training epoch 25
2020-12-14 12:43:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:43:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:43:32 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=0.585, loss=3.488, nll_loss=0.845, ppl=1.8, wps=14329.6, ups=1.36, wpb=10574, bsz=365.8, num_updates=13500, lr=1.25e-05, gnorm=1.148, train_wall=46, wall=6899
2020-12-14 12:44:18 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=0.588, loss=3.495, nll_loss=0.847, ppl=1.8, wps=23174.6, ups=2.19, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1.25e-05, gnorm=1.15, train_wall=45, wall=6945
2020-12-14 12:45:05 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=0.59, loss=3.506, nll_loss=0.858, ppl=1.81, wps=21956.8, ups=2.12, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1.25e-05, gnorm=1.154, train_wall=47, wall=6992
2020-12-14 12:45:51 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=0.591, loss=3.506, nll_loss=0.857, ppl=1.81, wps=23030.1, ups=2.19, wpb=10508.8, bsz=363, num_updates=13800, lr=1.25e-05, gnorm=1.162, train_wall=45, wall=7038
2020-12-14 12:46:37 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=0.587, loss=3.505, nll_loss=0.861, ppl=1.82, wps=23167, ups=2.18, wpb=10632, bsz=381.2, num_updates=13900, lr=1.25e-05, gnorm=1.139, train_wall=46, wall=7084
2020-12-14 12:47:23 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=0.586, loss=3.498, nll_loss=0.856, ppl=1.81, wps=22575.9, ups=2.17, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1.25e-05, gnorm=1.156, train_wall=46, wall=7130
2020-12-14 12:47:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:47:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:47:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:47:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:47:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:47:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:47:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:47:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:47:57 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | loss 5.486 | nll_loss 3.921 | ppl 15.15 | bleu 22.67 | wps 4109.5 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 22.75
2020-12-14 12:47:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:47:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:48:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:48:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 25 @ 14025 updates, score 22.67) (writing took 3.5951460730284452 seconds)
2020-12-14 12:48:01 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-12-14 12:48:01 | INFO | train | epoch 025 | symm_kl 0.589 | loss 3.503 | nll_loss 0.855 | ppl 1.81 | wps 20408.7 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.25e-05 | gnorm 1.155 | train_wall 257 | wall 7168
2020-12-14 12:48:01 | INFO | fairseq.trainer | begin training epoch 26
2020-12-14 12:48:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:48:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:48:39 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=0.577, loss=3.473, nll_loss=0.843, ppl=1.79, wps=13723.3, ups=1.32, wpb=10395, bsz=387.4, num_updates=14100, lr=1.25e-05, gnorm=1.144, train_wall=45, wall=7206
2020-12-14 12:49:24 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=0.592, loss=3.504, nll_loss=0.853, ppl=1.81, wps=22799.1, ups=2.19, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1.25e-05, gnorm=1.164, train_wall=46, wall=7251
2020-12-14 12:50:10 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=0.579, loss=3.482, nll_loss=0.85, ppl=1.8, wps=23134.5, ups=2.17, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1.25e-05, gnorm=1.125, train_wall=46, wall=7297
2020-12-14 12:50:58 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=0.593, loss=3.513, nll_loss=0.862, ppl=1.82, wps=22280.9, ups=2.11, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1.25e-05, gnorm=1.151, train_wall=47, wall=7345
2020-12-14 12:51:44 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=0.59, loss=3.507, nll_loss=0.857, ppl=1.81, wps=22736.5, ups=2.16, wpb=10526, bsz=359.1, num_updates=14500, lr=1.25e-05, gnorm=1.158, train_wall=46, wall=7391
2020-12-14 12:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:52:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:52:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:52:45 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | loss 5.483 | nll_loss 3.919 | ppl 15.12 | bleu 22.51 | wps 4449.5 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 22.75
2020-12-14 12:52:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 26 @ 14586 updates, score 22.51) (writing took 3.4669988583773375 seconds)
2020-12-14 12:52:48 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-12-14 12:52:48 | INFO | train | epoch 026 | symm_kl 0.587 | loss 3.499 | nll_loss 0.855 | ppl 1.81 | wps 20483.3 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.25e-05 | gnorm 1.151 | train_wall 258 | wall 7455
2020-12-14 12:52:48 | INFO | fairseq.trainer | begin training epoch 27
2020-12-14 12:52:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:52:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:52:58 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=0.592, loss=3.516, nll_loss=0.867, ppl=1.82, wps=13941.9, ups=1.36, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1.25e-05, gnorm=1.175, train_wall=45, wall=7465
2020-12-14 12:53:43 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=0.591, loss=3.51, nll_loss=0.861, ppl=1.82, wps=23159.7, ups=2.2, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1.25e-05, gnorm=1.151, train_wall=45, wall=7510
2020-12-14 12:54:29 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=0.588, loss=3.502, nll_loss=0.855, ppl=1.81, wps=22980.6, ups=2.18, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1.25e-05, gnorm=1.155, train_wall=46, wall=7556
2020-12-14 12:55:18 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=0.581, loss=3.487, nll_loss=0.853, ppl=1.81, wps=21651.9, ups=2.06, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1.25e-05, gnorm=1.143, train_wall=48, wall=7605
2020-12-14 12:56:06 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=0.578, loss=3.48, nll_loss=0.849, ppl=1.8, wps=21633.1, ups=2.05, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1.25e-05, gnorm=1.132, train_wall=49, wall=7653
2020-12-14 12:56:52 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=0.584, loss=3.494, nll_loss=0.856, ppl=1.81, wps=22808.9, ups=2.18, wpb=10447, bsz=358.8, num_updates=15100, lr=1.25e-05, gnorm=1.151, train_wall=46, wall=7699
2020-12-14 12:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 12:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:57:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:57:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 12:57:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 12:57:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 12:57:35 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | loss 5.486 | nll_loss 3.922 | ppl 15.15 | bleu 22.57 | wps 4478.3 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 22.75
2020-12-14 12:57:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 12:57:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:57:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:57:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 27 @ 15147 updates, score 22.57) (writing took 3.2853879369795322 seconds)
2020-12-14 12:57:39 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-12-14 12:57:39 | INFO | train | epoch 027 | symm_kl 0.585 | loss 3.496 | nll_loss 0.856 | ppl 1.81 | wps 20242.4 | ups 1.93 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.25e-05 | gnorm 1.15 | train_wall 261 | wall 7746
2020-12-14 12:57:39 | INFO | fairseq.trainer | begin training epoch 28
2020-12-14 12:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 12:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 12:58:06 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=0.576, loss=3.474, nll_loss=0.844, ppl=1.8, wps=13982.2, ups=1.36, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1.25e-05, gnorm=1.153, train_wall=45, wall=7773
2020-12-14 12:58:51 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=0.586, loss=3.499, nll_loss=0.856, ppl=1.81, wps=23010.9, ups=2.2, wpb=10482.5, bsz=354, num_updates=15300, lr=1.25e-05, gnorm=1.151, train_wall=45, wall=7818
2020-12-14 12:59:37 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=0.579, loss=3.482, nll_loss=0.849, ppl=1.8, wps=22937.4, ups=2.17, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1.25e-05, gnorm=1.143, train_wall=46, wall=7864
2020-12-14 13:00:23 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=0.586, loss=3.495, nll_loss=0.853, ppl=1.81, wps=22928.8, ups=2.18, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1.25e-05, gnorm=1.146, train_wall=46, wall=7910
2020-12-14 13:01:09 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=0.584, loss=3.504, nll_loss=0.867, ppl=1.82, wps=22631.6, ups=2.17, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1.25e-05, gnorm=1.143, train_wall=46, wall=7956
2020-12-14 13:01:56 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=0.585, loss=3.505, nll_loss=0.865, ppl=1.82, wps=22767.2, ups=2.16, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1.25e-05, gnorm=1.14, train_wall=46, wall=8003
2020-12-14 13:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:02:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:02:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:02:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:02:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:02:21 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | loss 5.481 | nll_loss 3.916 | ppl 15.09 | bleu 22.55 | wps 4455.8 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 22.75
2020-12-14 13:02:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:02:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:02:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:02:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 28 @ 15708 updates, score 22.55) (writing took 3.2449801713228226 seconds)
2020-12-14 13:02:24 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-12-14 13:02:24 | INFO | train | epoch 028 | symm_kl 0.582 | loss 3.492 | nll_loss 0.855 | ppl 1.81 | wps 20611.8 | ups 1.97 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.25e-05 | gnorm 1.144 | train_wall 256 | wall 8031
2020-12-14 13:02:24 | INFO | fairseq.trainer | begin training epoch 29
2020-12-14 13:02:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:02:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:03:09 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=0.577, loss=3.48, nll_loss=0.851, ppl=1.8, wps=14359.3, ups=1.37, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1.25e-05, gnorm=1.141, train_wall=45, wall=8076
2020-12-14 13:03:57 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=0.58, loss=3.485, nll_loss=0.852, ppl=1.8, wps=21814.2, ups=2.07, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1.25e-05, gnorm=1.141, train_wall=48, wall=8124
2020-12-14 13:04:46 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=0.584, loss=3.499, nll_loss=0.859, ppl=1.81, wps=21600.5, ups=2.05, wpb=10552.2, bsz=366, num_updates=16000, lr=1.25e-05, gnorm=1.136, train_wall=49, wall=8173
2020-12-14 13:05:32 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=0.576, loss=3.476, nll_loss=0.849, ppl=1.8, wps=22629.1, ups=2.18, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1.25e-05, gnorm=1.144, train_wall=46, wall=8219
2020-12-14 13:06:20 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=0.583, loss=3.494, nll_loss=0.854, ppl=1.81, wps=21701.2, ups=2.09, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1.25e-05, gnorm=1.162, train_wall=48, wall=8267
2020-12-14 13:06:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:06:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:06:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:06:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:06:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:06:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:06:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:07:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:07:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:07:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:07:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:07:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:07:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:07:13 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | loss 5.48 | nll_loss 3.914 | ppl 15.08 | bleu 22.63 | wps 4503.7 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 22.75
2020-12-14 13:07:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:07:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:07:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:07:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:07:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:07:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 29 @ 16269 updates, score 22.63) (writing took 3.2554537001997232 seconds)
2020-12-14 13:07:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-12-14 13:07:16 | INFO | train | epoch 029 | symm_kl 0.581 | loss 3.489 | nll_loss 0.854 | ppl 1.81 | wps 20116.8 | ups 1.92 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.25e-05 | gnorm 1.143 | train_wall 263 | wall 8323
2020-12-14 13:07:16 | INFO | fairseq.trainer | begin training epoch 30
2020-12-14 13:07:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:07:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:07:34 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=0.579, loss=3.486, nll_loss=0.855, ppl=1.81, wps=14163, ups=1.36, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1.25e-05, gnorm=1.14, train_wall=46, wall=8341
2020-12-14 13:08:19 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=0.578, loss=3.482, nll_loss=0.851, ppl=1.8, wps=23206.3, ups=2.19, wpb=10585.6, bsz=366, num_updates=16400, lr=1.25e-05, gnorm=1.133, train_wall=45, wall=8386
2020-12-14 13:09:05 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=0.577, loss=3.482, nll_loss=0.853, ppl=1.81, wps=22626.5, ups=2.17, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1.25e-05, gnorm=1.138, train_wall=46, wall=8432
2020-12-14 13:09:51 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=0.587, loss=3.501, nll_loss=0.858, ppl=1.81, wps=22506.7, ups=2.17, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1.25e-05, gnorm=1.149, train_wall=46, wall=8478
2020-12-14 13:10:38 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=0.574, loss=3.475, nll_loss=0.85, ppl=1.8, wps=22949.2, ups=2.16, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1.25e-05, gnorm=1.12, train_wall=46, wall=8525
2020-12-14 13:11:24 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=0.577, loss=3.487, nll_loss=0.857, ppl=1.81, wps=22667.3, ups=2.15, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1.25e-05, gnorm=1.133, train_wall=46, wall=8571
2020-12-14 13:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:11:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:11:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:12:02 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | loss 5.476 | nll_loss 3.911 | ppl 15.05 | bleu 22.5 | wps 4282.2 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 22.75
2020-12-14 13:12:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:12:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 30 @ 16830 updates, score 22.5) (writing took 3.3248165994882584 seconds)
2020-12-14 13:12:05 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-12-14 13:12:05 | INFO | train | epoch 030 | symm_kl 0.579 | loss 3.485 | nll_loss 0.854 | ppl 1.81 | wps 20355.3 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.25e-05 | gnorm 1.137 | train_wall 258 | wall 8612
2020-12-14 13:12:05 | INFO | fairseq.trainer | begin training epoch 31
2020-12-14 13:12:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:12:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:12:40 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=0.584, loss=3.496, nll_loss=0.856, ppl=1.81, wps=13627.4, ups=1.31, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1.25e-05, gnorm=1.163, train_wall=46, wall=8647
2020-12-14 13:13:26 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=0.573, loss=3.473, nll_loss=0.848, ppl=1.8, wps=23024.8, ups=2.18, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1.25e-05, gnorm=1.121, train_wall=46, wall=8693
2020-12-14 13:14:13 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=0.574, loss=3.476, nll_loss=0.85, ppl=1.8, wps=22671.9, ups=2.16, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1.25e-05, gnorm=1.131, train_wall=46, wall=8740
2020-12-14 13:14:59 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=0.586, loss=3.504, nll_loss=0.862, ppl=1.82, wps=22471.9, ups=2.17, wpb=10374.8, bsz=340, num_updates=17200, lr=1.25e-05, gnorm=1.155, train_wall=46, wall=8786
2020-12-14 13:15:45 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=0.571, loss=3.468, nll_loss=0.849, ppl=1.8, wps=22759.2, ups=2.16, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1.25e-05, gnorm=1.122, train_wall=46, wall=8832
2020-12-14 13:16:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:16:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:16:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:16:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:16:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:16:52 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | loss 5.481 | nll_loss 3.915 | ppl 15.09 | bleu 22.61 | wps 3988.8 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 22.75
2020-12-14 13:16:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:16:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:16:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:16:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 31 @ 17391 updates, score 22.61) (writing took 3.2057772260159254 seconds)
2020-12-14 13:16:55 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-12-14 13:16:55 | INFO | train | epoch 031 | symm_kl 0.576 | loss 3.482 | nll_loss 0.853 | ppl 1.81 | wps 20292.1 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.25e-05 | gnorm 1.136 | train_wall 258 | wall 8902
2020-12-14 13:16:55 | INFO | fairseq.trainer | begin training epoch 32
2020-12-14 13:16:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:17:03 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=0.576, loss=3.485, nll_loss=0.857, ppl=1.81, wps=13374.6, ups=1.29, wpb=10367.8, bsz=370, num_updates=17400, lr=1.25e-05, gnorm=1.147, train_wall=47, wall=8910
2020-12-14 13:17:48 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=0.579, loss=3.488, nll_loss=0.855, ppl=1.81, wps=23198.8, ups=2.22, wpb=10453, bsz=358.9, num_updates=17500, lr=1.25e-05, gnorm=1.133, train_wall=45, wall=8955
2020-12-14 13:18:34 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=0.565, loss=3.453, nll_loss=0.841, ppl=1.79, wps=22949.6, ups=2.15, wpb=10691.6, bsz=379.4, num_updates=17600, lr=1.25e-05, gnorm=1.102, train_wall=46, wall=9001
2020-12-14 13:19:20 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=0.575, loss=3.475, nll_loss=0.849, ppl=1.8, wps=22876.8, ups=2.17, wpb=10541.6, bsz=365.9, num_updates=17700, lr=1.25e-05, gnorm=1.135, train_wall=46, wall=9047
2020-12-14 13:20:06 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=0.583, loss=3.496, nll_loss=0.859, ppl=1.81, wps=22666.7, ups=2.18, wpb=10411, bsz=347.3, num_updates=17800, lr=1.25e-05, gnorm=1.158, train_wall=46, wall=9093
2020-12-14 13:20:52 | INFO | train_inner | epoch 032:    509 / 561 symm_kl=0.573, loss=3.477, nll_loss=0.853, ppl=1.81, wps=22657.1, ups=2.18, wpb=10412.4, bsz=390.5, num_updates=17900, lr=1.25e-05, gnorm=1.132, train_wall=46, wall=9139
2020-12-14 13:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:21:42 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | loss 5.48 | nll_loss 3.915 | ppl 15.09 | bleu 22.67 | wps 3609.4 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 22.75
2020-12-14 13:21:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 32 @ 17952 updates, score 22.67) (writing took 3.315065611153841 seconds)
2020-12-14 13:21:45 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-12-14 13:21:45 | INFO | train | epoch 032 | symm_kl 0.575 | loss 3.479 | nll_loss 0.852 | ppl 1.81 | wps 20292.4 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.25e-05 | gnorm 1.133 | train_wall 257 | wall 9192
2020-12-14 13:21:45 | INFO | fairseq.trainer | begin training epoch 33
2020-12-14 13:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:21:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:21:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:22:10 | INFO | train_inner | epoch 033:     48 / 561 symm_kl=0.573, loss=3.482, nll_loss=0.86, ppl=1.82, wps=13411, ups=1.28, wpb=10480.2, bsz=382.2, num_updates=18000, lr=1.25e-05, gnorm=1.126, train_wall=45, wall=9217
2020-12-14 13:22:56 | INFO | train_inner | epoch 033:    148 / 561 symm_kl=0.565, loss=3.456, nll_loss=0.844, ppl=1.8, wps=22769.8, ups=2.17, wpb=10476.8, bsz=378.3, num_updates=18100, lr=1.25e-05, gnorm=1.118, train_wall=46, wall=9263
2020-12-14 13:23:42 | INFO | train_inner | epoch 033:    248 / 561 symm_kl=0.571, loss=3.471, nll_loss=0.85, ppl=1.8, wps=22916.4, ups=2.17, wpb=10542.1, bsz=374.1, num_updates=18200, lr=1.25e-05, gnorm=1.127, train_wall=46, wall=9309
2020-12-14 13:24:29 | INFO | train_inner | epoch 033:    348 / 561 symm_kl=0.576, loss=3.483, nll_loss=0.854, ppl=1.81, wps=22539.6, ups=2.16, wpb=10437.8, bsz=375, num_updates=18300, lr=1.25e-05, gnorm=1.139, train_wall=46, wall=9356
2020-12-14 13:25:15 | INFO | train_inner | epoch 033:    448 / 561 symm_kl=0.582, loss=3.491, nll_loss=0.854, ppl=1.81, wps=22530.6, ups=2.15, wpb=10483.6, bsz=349.7, num_updates=18400, lr=1.25e-05, gnorm=1.144, train_wall=46, wall=9402
2020-12-14 13:26:01 | INFO | train_inner | epoch 033:    548 / 561 symm_kl=0.571, loss=3.475, nll_loss=0.854, ppl=1.81, wps=22803.7, ups=2.18, wpb=10480.7, bsz=370.4, num_updates=18500, lr=1.25e-05, gnorm=1.123, train_wall=46, wall=9448
2020-12-14 13:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:26:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:26:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:26:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:26:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:26:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:26:28 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | loss 5.476 | nll_loss 3.911 | ppl 15.04 | bleu 22.7 | wps 4624 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 22.75
2020-12-14 13:26:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:26:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:26:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 33 @ 18513 updates, score 22.7) (writing took 3.419260609894991 seconds)
2020-12-14 13:26:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-12-14 13:26:32 | INFO | train | epoch 033 | symm_kl 0.573 | loss 3.476 | nll_loss 0.852 | ppl 1.8 | wps 20524.2 | ups 1.96 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.25e-05 | gnorm 1.131 | train_wall 257 | wall 9479
2020-12-14 13:26:32 | INFO | fairseq.trainer | begin training epoch 34
2020-12-14 13:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:27:14 | INFO | train_inner | epoch 034:     87 / 561 symm_kl=0.572, loss=3.475, nll_loss=0.852, ppl=1.8, wps=14401.1, ups=1.37, wpb=10500.6, bsz=376.6, num_updates=18600, lr=1.25e-05, gnorm=1.136, train_wall=45, wall=9521
2020-12-14 13:28:00 | INFO | train_inner | epoch 034:    187 / 561 symm_kl=0.582, loss=3.499, nll_loss=0.862, ppl=1.82, wps=22633.6, ups=2.17, wpb=10453.2, bsz=372.1, num_updates=18700, lr=1.25e-05, gnorm=1.133, train_wall=46, wall=9567
2020-12-14 13:28:48 | INFO | train_inner | epoch 034:    287 / 561 symm_kl=0.57, loss=3.471, nll_loss=0.852, ppl=1.81, wps=21862.1, ups=2.09, wpb=10473.3, bsz=368.2, num_updates=18800, lr=1.25e-05, gnorm=1.134, train_wall=48, wall=9615
2020-12-14 13:29:34 | INFO | train_inner | epoch 034:    387 / 561 symm_kl=0.564, loss=3.455, nll_loss=0.844, ppl=1.79, wps=22570.5, ups=2.17, wpb=10398.1, bsz=377.4, num_updates=18900, lr=1.25e-05, gnorm=1.118, train_wall=46, wall=9661
2020-12-14 13:30:21 | INFO | train_inner | epoch 034:    487 / 561 symm_kl=0.558, loss=3.436, nll_loss=0.834, ppl=1.78, wps=22989.6, ups=2.16, wpb=10664.6, bsz=372.5, num_updates=19000, lr=1.25e-05, gnorm=1.102, train_wall=46, wall=9708
2020-12-14 13:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:31:16 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | loss 5.479 | nll_loss 3.913 | ppl 15.07 | bleu 22.6 | wps 4565.6 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 22.75
2020-12-14 13:31:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:31:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:31:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:31:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 34 @ 19074 updates, score 22.6) (writing took 2.988036382943392 seconds)
2020-12-14 13:31:19 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-12-14 13:31:19 | INFO | train | epoch 034 | symm_kl 0.571 | loss 3.472 | nll_loss 0.851 | ppl 1.8 | wps 20450.8 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.25e-05 | gnorm 1.128 | train_wall 259 | wall 9766
2020-12-14 13:31:19 | INFO | fairseq.trainer | begin training epoch 35
2020-12-14 13:31:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:31:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:31:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:31:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:31:34 | INFO | train_inner | epoch 035:     26 / 561 symm_kl=0.576, loss=3.486, nll_loss=0.858, ppl=1.81, wps=14136.6, ups=1.36, wpb=10418.3, bsz=347, num_updates=19100, lr=1.25e-05, gnorm=1.144, train_wall=46, wall=9781
2020-12-14 13:32:20 | INFO | train_inner | epoch 035:    126 / 561 symm_kl=0.565, loss=3.46, nll_loss=0.848, ppl=1.8, wps=23026.9, ups=2.2, wpb=10451.9, bsz=365.3, num_updates=19200, lr=1.25e-05, gnorm=1.121, train_wall=45, wall=9827
2020-12-14 13:33:07 | INFO | train_inner | epoch 035:    226 / 561 symm_kl=0.566, loss=3.459, nll_loss=0.844, ppl=1.8, wps=22298.6, ups=2.13, wpb=10479.3, bsz=382.2, num_updates=19300, lr=1.25e-05, gnorm=1.125, train_wall=47, wall=9874
2020-12-14 13:33:53 | INFO | train_inner | epoch 035:    326 / 561 symm_kl=0.572, loss=3.475, nll_loss=0.853, ppl=1.81, wps=22589.7, ups=2.14, wpb=10548.4, bsz=368.7, num_updates=19400, lr=1.25e-05, gnorm=1.128, train_wall=46, wall=9920
2020-12-14 13:34:40 | INFO | train_inner | epoch 035:    426 / 561 symm_kl=0.57, loss=3.475, nll_loss=0.856, ppl=1.81, wps=22849.7, ups=2.17, wpb=10546.4, bsz=379.4, num_updates=19500, lr=1.25e-05, gnorm=1.115, train_wall=46, wall=9967
2020-12-14 13:35:25 | INFO | train_inner | epoch 035:    526 / 561 symm_kl=0.578, loss=3.491, nll_loss=0.861, ppl=1.82, wps=22851.2, ups=2.19, wpb=10412.2, bsz=355, num_updates=19600, lr=1.25e-05, gnorm=1.143, train_wall=45, wall=10012
2020-12-14 13:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:36:04 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | loss 5.474 | nll_loss 3.91 | ppl 15.04 | bleu 22.61 | wps 4248 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 22.75
2020-12-14 13:36:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:36:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:36:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:36:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:36:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:36:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 35 @ 19635 updates, score 22.61) (writing took 3.090849971398711 seconds)
2020-12-14 13:36:07 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-12-14 13:36:07 | INFO | train | epoch 035 | symm_kl 0.569 | loss 3.47 | nll_loss 0.852 | ppl 1.8 | wps 20454.4 | ups 1.95 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.25e-05 | gnorm 1.125 | train_wall 257 | wall 10054
2020-12-14 13:36:07 | INFO | fairseq.trainer | begin training epoch 36
2020-12-14 13:36:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:36:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:36:39 | INFO | train_inner | epoch 036:     65 / 561 symm_kl=0.568, loss=3.465, nll_loss=0.849, ppl=1.8, wps=13931.9, ups=1.35, wpb=10344.6, bsz=362.5, num_updates=19700, lr=1.25e-05, gnorm=1.131, train_wall=45, wall=10086
2020-12-14 13:37:25 | INFO | train_inner | epoch 036:    165 / 561 symm_kl=0.572, loss=3.476, nll_loss=0.853, ppl=1.81, wps=22829.4, ups=2.2, wpb=10399, bsz=361.7, num_updates=19800, lr=1.25e-05, gnorm=1.133, train_wall=45, wall=10132
2020-12-14 13:38:11 | INFO | train_inner | epoch 036:    265 / 561 symm_kl=0.569, loss=3.469, nll_loss=0.852, ppl=1.81, wps=22733.8, ups=2.16, wpb=10532.1, bsz=367.1, num_updates=19900, lr=1.25e-05, gnorm=1.13, train_wall=46, wall=10178
2020-12-14 13:38:59 | INFO | train_inner | epoch 036:    365 / 561 symm_kl=0.565, loss=3.461, nll_loss=0.848, ppl=1.8, wps=22117.4, ups=2.09, wpb=10583.1, bsz=386.2, num_updates=20000, lr=1.25e-05, gnorm=1.105, train_wall=48, wall=10226
2020-12-14 13:39:45 | INFO | train_inner | epoch 036:    465 / 561 symm_kl=0.564, loss=3.459, nll_loss=0.849, ppl=1.8, wps=23095.4, ups=2.16, wpb=10695.3, bsz=376, num_updates=20100, lr=1.25e-05, gnorm=1.102, train_wall=46, wall=10272
2020-12-14 13:40:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-14 13:40:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:40:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:40:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-14 13:40:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-14 13:40:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-14 13:40:52 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | loss 5.474 | nll_loss 3.909 | ppl 15.02 | bleu 22.69 | wps 4295.2 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 22.75
2020-12-14 13:40:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-14 13:40:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:40:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 36 @ 20196 updates, score 22.69) (writing took 3.343705613166094 seconds)
2020-12-14 13:40:56 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-12-14 13:40:56 | INFO | train | epoch 036 | symm_kl 0.568 | loss 3.466 | nll_loss 0.85 | ppl 1.8 | wps 20349.7 | ups 1.94 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.25e-05 | gnorm 1.122 | train_wall 259 | wall 10343
2020-12-14 13:40:56 | INFO | fairseq.trainer | begin training epoch 37
2020-12-14 13:40:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-14 13:40:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-14 13:41:01 | INFO | train_inner | epoch 037:      4 / 561 symm_kl=0.569, loss=3.471, nll_loss=0.854, ppl=1.81, wps=13646, ups=1.33, wpb=10275, bsz=363.2, num_updates=20200, lr=1.25e-05, gnorm=1.143, train_wall=46, wall=10348
2020-12-14 13:41:46 | INFO | train_inner | epoch 037:    104 / 561 symm_kl=0.571, loss=3.472, nll_loss=0.851, ppl=1.8, wps=22979.8, ups=2.2, wpb=10439.9, bsz=356.5, num_updates=20300, lr=1.25e-05, gnorm=1.129, train_wall=45, wall=10393
2020-12-14 13:42:32 | INFO | train_inner | epoch 037:    204 / 561 symm_kl=0.572, loss=3.477, nll_loss=0.855, ppl=1.81, wps=22622.7, ups=2.16, wpb=10457.5, bsz=360.4, num_updates=20400, lr=1.25e-05, gnorm=1.137, train_wall=46, wall=10439
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 252 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
