nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/zero_dropout
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07'
2021-01-06 22:12:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:12:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:13199
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:13199
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:13199
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-06 22:12:55 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-06 22:12:59 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:13199', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/zero_dropout', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-06 22:12:59 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-06 22:12:59 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-06 22:12:59 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-06 22:12:59 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-06 22:12:59 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-06 22:13:00 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-06 22:13:00 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-06 22:13:00 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-06 22:13:00 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-06 22:13:00 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-06 22:13:00 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-06 22:13:00 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-06 22:13:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-06 22:13:00 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-06 22:13:00 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-06 22:13:00 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-06 22:13:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-06 22:13:00 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-06 22:13:00 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-06 22:13:00 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-06 22:13:01 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-06 22:13:01 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-06 22:13:01 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-06 22:13:01 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-06 22:13:01 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-06 22:13:01 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-06 22:13:01 | INFO | fairseq.trainer | begin training epoch 1
2021-01-06 22:13:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:13:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:13:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:13:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:14:05 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=1.471, self_kl=0, self_cv=0, loss=4.78, nll_loss=0.846, ppl=1.8, wps=17719.9, ups=1.67, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.43e-06, gnorm=3.192, train_wall=60, wall=65
2021-01-06 22:15:05 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=1.369, self_kl=0, self_cv=0, loss=4.648, nll_loss=0.892, ppl=1.86, wps=17501.6, ups=1.65, wpb=10583.4, bsz=369.8, num_updates=200, lr=2.76e-06, gnorm=3.007, train_wall=60, wall=125
2021-01-06 22:16:07 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=1.21, self_kl=0, self_cv=0, loss=4.455, nll_loss=0.976, ppl=1.97, wps=16873.2, ups=1.63, wpb=10335, bsz=373, num_updates=300, lr=4.09e-06, gnorm=2.676, train_wall=61, wall=187
2021-01-06 22:17:09 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=1.077, self_kl=0, self_cv=0, loss=4.285, nll_loss=1.024, ppl=2.03, wps=17042.4, ups=1.61, wpb=10571.8, bsz=388.4, num_updates=400, lr=5.42e-06, gnorm=1.969, train_wall=62, wall=249
2021-01-06 22:18:10 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=1.009, self_kl=0, self_cv=0, loss=4.213, nll_loss=1.06, ppl=2.08, wps=16918.4, ups=1.63, wpb=10411.2, bsz=371.8, num_updates=500, lr=6.75e-06, gnorm=1.789, train_wall=61, wall=310
2021-01-06 22:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:18:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:18:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:18:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:18:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:18:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:18:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:18:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:18:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:18:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:19:09 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.483 | nll_loss 4.038 | ppl 16.42 | bleu 22.34 | wps 4539.8 | wpb 7508.5 | bsz 272.7 | num_updates 561
2021-01-06 22:19:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:19:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:19:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:19:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.34) (writing took 2.136285303160548 seconds)
2021-01-06 22:19:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-06 22:19:11 | INFO | train | epoch 001 | symm_kl 1.204 | self_kl 0 | self_cv 0 | loss 4.452 | nll_loss 0.975 | ppl 1.97 | wps 16056.4 | ups 1.53 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 7.5613e-06 | gnorm 2.438 | train_wall 342 | wall 371
2021-01-06 22:19:11 | INFO | fairseq.trainer | begin training epoch 2
2021-01-06 22:19:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:19:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:19:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:19:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:19:38 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=0.971, self_kl=0, self_cv=0, loss=4.185, nll_loss=1.095, ppl=2.14, wps=11818.6, ups=1.14, wpb=10345.6, bsz=358.4, num_updates=600, lr=8.08e-06, gnorm=1.689, train_wall=61, wall=398
2021-01-06 22:20:39 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=0.923, self_kl=0, self_cv=0, loss=4.134, nll_loss=1.117, ppl=2.17, wps=17219.4, ups=1.63, wpb=10532.9, bsz=366.4, num_updates=700, lr=9.41e-06, gnorm=1.582, train_wall=61, wall=459
2021-01-06 22:21:41 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=0.871, self_kl=0, self_cv=0, loss=4.058, nll_loss=1.11, ppl=2.16, wps=17038.6, ups=1.62, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.074e-05, gnorm=1.518, train_wall=61, wall=521
2021-01-06 22:22:42 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=0.846, self_kl=0, self_cv=0, loss=4.033, nll_loss=1.121, ppl=2.18, wps=17209.7, ups=1.63, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.207e-05, gnorm=1.486, train_wall=61, wall=582
2021-01-06 22:23:43 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=0.832, self_kl=0, self_cv=0, loss=4.028, nll_loss=1.137, ppl=2.2, wps=17055, ups=1.63, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.34e-05, gnorm=1.452, train_wall=61, wall=643
2021-01-06 22:24:45 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=0.814, self_kl=0, self_cv=0, loss=4.011, nll_loss=1.147, ppl=2.21, wps=16936.2, ups=1.62, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.473e-05, gnorm=1.43, train_wall=61, wall=705
2021-01-06 22:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:24:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:24:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:24:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:25:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:25:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:25:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:25:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:25:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:25:20 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.389 | nll_loss 3.957 | ppl 15.53 | bleu 22.33 | wps 4509.2 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.34
2021-01-06 22:25:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:25:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:25:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:25:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:25:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:25:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 2 @ 1122 updates, score 22.33) (writing took 2.8352598883211613 seconds)
2021-01-06 22:25:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-06 22:25:23 | INFO | train | epoch 002 | symm_kl 0.86 | self_kl 0 | self_cv 0 | loss 4.055 | nll_loss 1.124 | ppl 2.18 | wps 15836.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.50226e-05 | gnorm 1.504 | train_wall 343 | wall 743
2021-01-06 22:25:23 | INFO | fairseq.trainer | begin training epoch 3
2021-01-06 22:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:26:13 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=0.787, self_kl=0, self_cv=0, loss=3.963, nll_loss=1.135, ppl=2.2, wps=11878.4, ups=1.14, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.606e-05, gnorm=1.405, train_wall=60, wall=793
2021-01-06 22:27:14 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=0.773, self_kl=0, self_cv=0, loss=3.95, nll_loss=1.138, ppl=2.2, wps=16970.1, ups=1.63, wpb=10420.6, bsz=376, num_updates=1300, lr=1.739e-05, gnorm=1.378, train_wall=61, wall=854
2021-01-06 22:28:16 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=0.756, self_kl=0, self_cv=0, loss=3.921, nll_loss=1.131, ppl=2.19, wps=16955.9, ups=1.62, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.872e-05, gnorm=1.36, train_wall=62, wall=916
2021-01-06 22:29:18 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=0.749, self_kl=0, self_cv=0, loss=3.933, nll_loss=1.157, ppl=2.23, wps=17019.4, ups=1.63, wpb=10472.3, bsz=374.7, num_updates=1500, lr=2.005e-05, gnorm=1.341, train_wall=61, wall=977
2021-01-06 22:30:19 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=0.727, self_kl=0, self_cv=0, loss=3.883, nll_loss=1.132, ppl=2.19, wps=17298.2, ups=1.62, wpb=10650.7, bsz=373.4, num_updates=1600, lr=2.138e-05, gnorm=1.315, train_wall=61, wall=1039
2021-01-06 22:31:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:31:31 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.347 | nll_loss 3.907 | ppl 15 | bleu 22.46 | wps 4522.7 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.46
2021-01-06 22:31:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:31:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.46) (writing took 4.636543918401003 seconds)
2021-01-06 22:31:36 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-06 22:31:36 | INFO | train | epoch 003 | symm_kl 0.752 | self_kl 0 | self_cv 0 | loss 3.92 | nll_loss 1.137 | ppl 2.2 | wps 15760.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 2.24839e-05 | gnorm 1.352 | train_wall 343 | wall 1116
2021-01-06 22:31:36 | INFO | fairseq.trainer | begin training epoch 4
2021-01-06 22:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:31:49 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=0.722, self_kl=0, self_cv=0, loss=3.874, nll_loss=1.13, ppl=2.19, wps=11611.2, ups=1.11, wpb=10447.8, bsz=352, num_updates=1700, lr=2.271e-05, gnorm=1.331, train_wall=61, wall=1129
2021-01-06 22:32:50 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=0.717, self_kl=0, self_cv=0, loss=3.877, nll_loss=1.141, ppl=2.21, wps=17164.8, ups=1.64, wpb=10469.1, bsz=365.6, num_updates=1800, lr=2.404e-05, gnorm=1.32, train_wall=61, wall=1190
2021-01-06 22:33:52 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=0.707, self_kl=0, self_cv=0, loss=3.869, nll_loss=1.144, ppl=2.21, wps=16651, ups=1.62, wpb=10271.1, bsz=367.4, num_updates=1900, lr=2.537e-05, gnorm=1.308, train_wall=61, wall=1252
2021-01-06 22:34:53 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=0.693, self_kl=0, self_cv=0, loss=3.835, nll_loss=1.127, ppl=2.18, wps=17213.3, ups=1.63, wpb=10571.4, bsz=356.9, num_updates=2000, lr=2.67e-05, gnorm=1.289, train_wall=61, wall=1313
2021-01-06 22:35:55 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=0.679, self_kl=0, self_cv=0, loss=3.82, nll_loss=1.132, ppl=2.19, wps=17154, ups=1.63, wpb=10532.7, bsz=370.6, num_updates=2100, lr=2.803e-05, gnorm=1.26, train_wall=61, wall=1375
2021-01-06 22:36:56 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=0.667, self_kl=0, self_cv=0, loss=3.802, nll_loss=1.131, ppl=2.19, wps=17240.2, ups=1.62, wpb=10614.4, bsz=387.6, num_updates=2200, lr=2.936e-05, gnorm=1.231, train_wall=61, wall=1436
2021-01-06 22:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:37:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:37:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:37:43 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.319 | nll_loss 3.872 | ppl 14.64 | bleu 22.41 | wps 4933.3 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.46
2021-01-06 22:37:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:37:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 4 @ 2244 updates, score 22.41) (writing took 2.7342650461941957 seconds)
2021-01-06 22:37:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-06 22:37:46 | INFO | train | epoch 004 | symm_kl 0.69 | self_kl 0 | self_cv 0 | loss 3.835 | nll_loss 1.133 | ppl 2.19 | wps 15893.1 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 2.99452e-05 | gnorm 1.28 | train_wall 343 | wall 1486
2021-01-06 22:37:46 | INFO | fairseq.trainer | begin training epoch 5
2021-01-06 22:37:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:37:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:37:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:37:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:38:23 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=0.655, self_kl=0, self_cv=0, loss=3.775, nll_loss=1.118, ppl=2.17, wps=12035.6, ups=1.15, wpb=10433.5, bsz=373.3, num_updates=2300, lr=3.069e-05, gnorm=1.25, train_wall=61, wall=1523
2021-01-06 22:39:24 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=0.656, self_kl=0, self_cv=0, loss=3.787, nll_loss=1.128, ppl=2.19, wps=17000.9, ups=1.63, wpb=10447.6, bsz=372.4, num_updates=2400, lr=3.202e-05, gnorm=1.264, train_wall=61, wall=1584
2021-01-06 22:40:26 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=0.651, self_kl=0, self_cv=0, loss=3.781, nll_loss=1.131, ppl=2.19, wps=17111.4, ups=1.63, wpb=10524.3, bsz=363.8, num_updates=2500, lr=3.335e-05, gnorm=1.22, train_wall=61, wall=1646
2021-01-06 22:41:27 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=0.655, self_kl=0, self_cv=0, loss=3.802, nll_loss=1.149, ppl=2.22, wps=16989.8, ups=1.63, wpb=10415.7, bsz=357.5, num_updates=2600, lr=3.468e-05, gnorm=1.23, train_wall=61, wall=1707
2021-01-06 22:42:29 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=0.63, self_kl=0, self_cv=0, loss=3.745, nll_loss=1.122, ppl=2.18, wps=17212.9, ups=1.63, wpb=10565, bsz=383, num_updates=2700, lr=3.601e-05, gnorm=1.188, train_wall=61, wall=1768
2021-01-06 22:43:30 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=0.629, self_kl=0, self_cv=0, loss=3.746, nll_loss=1.125, ppl=2.18, wps=16967.7, ups=1.62, wpb=10479.6, bsz=374.7, num_updates=2800, lr=3.734e-05, gnorm=1.213, train_wall=62, wall=1830
2021-01-06 22:43:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:43:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:43:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:43:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:43:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:43:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:43:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:43:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:43:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:43:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:43:54 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.292 | nll_loss 3.839 | ppl 14.31 | bleu 22.55 | wps 4580.6 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.55
2021-01-06 22:43:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:43:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:43:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:43:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:43:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:43:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 5 @ 2805 updates, score 22.55) (writing took 4.706641830503941 seconds)
2021-01-06 22:43:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-06 22:43:59 | INFO | train | epoch 005 | symm_kl 0.645 | self_kl 0 | self_cv 0 | loss 3.773 | nll_loss 1.13 | ppl 2.19 | wps 15761.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 3.74065e-05 | gnorm 1.227 | train_wall 343 | wall 1859
2021-01-06 22:43:59 | INFO | fairseq.trainer | begin training epoch 6
2021-01-06 22:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:45:00 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=0.621, self_kl=0, self_cv=0, loss=3.732, nll_loss=1.121, ppl=2.17, wps=11524.7, ups=1.12, wpb=10318.1, bsz=377.4, num_updates=2900, lr=3.867e-05, gnorm=1.208, train_wall=60, wall=1920
2021-01-06 22:46:02 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=0.617, self_kl=0, self_cv=0, loss=3.728, nll_loss=1.122, ppl=2.18, wps=17136.6, ups=1.6, wpb=10679.3, bsz=372.1, num_updates=3000, lr=4e-05, gnorm=1.165, train_wall=62, wall=1982
2021-01-06 22:47:04 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=0.611, self_kl=0, self_cv=0, loss=3.72, nll_loss=1.121, ppl=2.17, wps=16997.4, ups=1.62, wpb=10477.8, bsz=365.4, num_updates=3100, lr=3.93496e-05, gnorm=1.194, train_wall=61, wall=2044
2021-01-06 22:48:05 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=0.614, self_kl=0, self_cv=0, loss=3.744, nll_loss=1.144, ppl=2.21, wps=17096.3, ups=1.63, wpb=10517.4, bsz=358, num_updates=3200, lr=3.87298e-05, gnorm=1.198, train_wall=61, wall=2105
2021-01-06 22:49:07 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=0.592, self_kl=0, self_cv=0, loss=3.683, nll_loss=1.108, ppl=2.16, wps=17188.6, ups=1.63, wpb=10534.9, bsz=372, num_updates=3300, lr=3.81385e-05, gnorm=1.168, train_wall=61, wall=2167
2021-01-06 22:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:50:08 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.807 | ppl 14 | bleu 22.57 | wps 4536.6 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.57
2021-01-06 22:50:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:50:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 6 @ 3366 updates, score 22.57) (writing took 4.933870391920209 seconds)
2021-01-06 22:50:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-06 22:50:13 | INFO | train | epoch 006 | symm_kl 0.61 | self_kl 0 | self_cv 0 | loss 3.723 | nll_loss 1.127 | ppl 2.18 | wps 15724.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 3.77627e-05 | gnorm 1.185 | train_wall 343 | wall 2233
2021-01-06 22:50:13 | INFO | fairseq.trainer | begin training epoch 7
2021-01-06 22:50:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:50:37 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=0.597, self_kl=0, self_cv=0, loss=3.722, nll_loss=1.145, ppl=2.21, wps=11381.8, ups=1.11, wpb=10237, bsz=369, num_updates=3400, lr=3.75735e-05, gnorm=1.159, train_wall=60, wall=2256
2021-01-06 22:51:39 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=0.594, self_kl=0, self_cv=0, loss=3.696, nll_loss=1.12, ppl=2.17, wps=16912.6, ups=1.61, wpb=10508.4, bsz=371.6, num_updates=3500, lr=3.70328e-05, gnorm=1.163, train_wall=62, wall=2319
2021-01-06 22:52:40 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=0.588, self_kl=0, self_cv=0, loss=3.691, nll_loss=1.123, ppl=2.18, wps=16989.1, ups=1.63, wpb=10404.4, bsz=363.4, num_updates=3600, lr=3.65148e-05, gnorm=1.155, train_wall=61, wall=2380
2021-01-06 22:53:41 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=0.585, self_kl=0, self_cv=0, loss=3.691, nll_loss=1.128, ppl=2.19, wps=17014, ups=1.63, wpb=10456.6, bsz=375.4, num_updates=3700, lr=3.6018e-05, gnorm=1.134, train_wall=61, wall=2441
2021-01-06 22:54:43 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=0.584, self_kl=0, self_cv=0, loss=3.682, nll_loss=1.119, ppl=2.17, wps=17066.7, ups=1.63, wpb=10467.8, bsz=366.5, num_updates=3800, lr=3.55409e-05, gnorm=1.148, train_wall=61, wall=2503
2021-01-06 22:55:44 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=0.572, self_kl=0, self_cv=0, loss=3.666, nll_loss=1.12, ppl=2.17, wps=17367.2, ups=1.62, wpb=10688.9, bsz=373.3, num_updates=3900, lr=3.50823e-05, gnorm=1.115, train_wall=61, wall=2564
2021-01-06 22:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 22:56:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:56:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:56:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 22:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 22:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 22:56:22 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.25 | nll_loss 3.789 | ppl 13.82 | bleu 22.61 | wps 4523.3 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.61
2021-01-06 22:56:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 22:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:56:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 7 @ 3927 updates, score 22.61) (writing took 4.874006116762757 seconds)
2021-01-06 22:56:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-06 22:56:27 | INFO | train | epoch 007 | symm_kl 0.584 | self_kl 0 | self_cv 0 | loss 3.683 | nll_loss 1.122 | ppl 2.18 | wps 15735 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 3.49615e-05 | gnorm 1.139 | train_wall 343 | wall 2607
2021-01-06 22:56:27 | INFO | fairseq.trainer | begin training epoch 8
2021-01-06 22:56:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 22:56:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 22:57:14 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=0.564, self_kl=0, self_cv=0, loss=3.645, nll_loss=1.108, ppl=2.16, wps=11785.7, ups=1.11, wpb=10590, bsz=375, num_updates=4000, lr=3.4641e-05, gnorm=1.094, train_wall=60, wall=2654
2021-01-06 22:58:16 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=0.577, self_kl=0, self_cv=0, loss=3.67, nll_loss=1.116, ppl=2.17, wps=17090.3, ups=1.63, wpb=10504.7, bsz=358.9, num_updates=4100, lr=3.4216e-05, gnorm=1.131, train_wall=61, wall=2715
2021-01-06 22:59:17 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=0.566, self_kl=0, self_cv=0, loss=3.653, nll_loss=1.113, ppl=2.16, wps=16927, ups=1.63, wpb=10367.5, bsz=366.4, num_updates=4200, lr=3.38062e-05, gnorm=1.143, train_wall=61, wall=2777
2021-01-06 23:00:18 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=0.559, self_kl=0, self_cv=0, loss=3.637, nll_loss=1.107, ppl=2.15, wps=16960.4, ups=1.63, wpb=10416.1, bsz=389.5, num_updates=4300, lr=3.34108e-05, gnorm=1.103, train_wall=61, wall=2838
2021-01-06 23:01:20 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=0.55, self_kl=0, self_cv=0, loss=3.627, nll_loss=1.11, ppl=2.16, wps=17217.9, ups=1.62, wpb=10648.7, bsz=379.6, num_updates=4400, lr=3.30289e-05, gnorm=1.084, train_wall=62, wall=2900
2021-01-06 23:02:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:02:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:02:35 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.246 | nll_loss 3.78 | ppl 13.73 | bleu 22.43 | wps 4497.6 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.61
2021-01-06 23:02:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:02:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 8 @ 4488 updates, score 22.43) (writing took 2.9431994557380676 seconds)
2021-01-06 23:02:38 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-06 23:02:38 | INFO | train | epoch 008 | symm_kl 0.564 | self_kl 0 | self_cv 0 | loss 3.652 | nll_loss 1.116 | ppl 2.17 | wps 15830 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 3.27035e-05 | gnorm 1.114 | train_wall 343 | wall 2978
2021-01-06 23:02:38 | INFO | fairseq.trainer | begin training epoch 9
2021-01-06 23:02:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:02:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:02:49 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=0.565, self_kl=0, self_cv=0, loss=3.669, nll_loss=1.135, ppl=2.2, wps=11656.7, ups=1.13, wpb=10320.4, bsz=352.8, num_updates=4500, lr=3.26599e-05, gnorm=1.124, train_wall=61, wall=2989
2021-01-06 23:03:49 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=0.551, self_kl=0, self_cv=0, loss=3.619, nll_loss=1.098, ppl=2.14, wps=17312.4, ups=1.64, wpb=10532.6, bsz=374.2, num_updates=4600, lr=3.23029e-05, gnorm=1.088, train_wall=61, wall=3049
2021-01-06 23:04:51 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=0.563, self_kl=0, self_cv=0, loss=3.661, nll_loss=1.126, ppl=2.18, wps=17038.2, ups=1.62, wpb=10528, bsz=345, num_updates=4700, lr=3.19574e-05, gnorm=1.105, train_wall=62, wall=3111
2021-01-06 23:05:53 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.611, nll_loss=1.105, ppl=2.15, wps=17053.3, ups=1.63, wpb=10473.1, bsz=377.1, num_updates=4800, lr=3.16228e-05, gnorm=1.085, train_wall=61, wall=3173
2021-01-06 23:06:54 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=0.546, self_kl=0, self_cv=0, loss=3.621, nll_loss=1.109, ppl=2.16, wps=16981.2, ups=1.62, wpb=10483.1, bsz=368.3, num_updates=4900, lr=3.12984e-05, gnorm=1.093, train_wall=62, wall=3234
2021-01-06 23:07:56 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.608, nll_loss=1.106, ppl=2.15, wps=16980.3, ups=1.61, wpb=10514.7, bsz=388.6, num_updates=5000, lr=3.09839e-05, gnorm=1.077, train_wall=62, wall=3296
2021-01-06 23:08:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:08:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:08:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:08:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:08:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:08:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:08:47 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.762 | ppl 13.57 | bleu 22.68 | wps 4592 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.68
2021-01-06 23:08:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:08:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:08:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:08:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 9 @ 5049 updates, score 22.68) (writing took 4.993667772039771 seconds)
2021-01-06 23:08:52 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-06 23:08:52 | INFO | train | epoch 009 | symm_kl 0.548 | self_kl 0 | self_cv 0 | loss 3.627 | nll_loss 1.111 | ppl 2.16 | wps 15718.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 3.08332e-05 | gnorm 1.092 | train_wall 344 | wall 3352
2021-01-06 23:08:52 | INFO | fairseq.trainer | begin training epoch 10
2021-01-06 23:08:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:08:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:09:26 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=0.548, self_kl=0, self_cv=0, loss=3.638, nll_loss=1.124, ppl=2.18, wps=11488.6, ups=1.11, wpb=10353, bsz=356, num_updates=5100, lr=3.06786e-05, gnorm=1.092, train_wall=61, wall=3386
2021-01-06 23:10:28 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=0.54, self_kl=0, self_cv=0, loss=3.614, nll_loss=1.109, ppl=2.16, wps=17060.3, ups=1.61, wpb=10577.3, bsz=365.6, num_updates=5200, lr=3.03822e-05, gnorm=1.066, train_wall=62, wall=3448
2021-01-06 23:11:30 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.619, nll_loss=1.109, ppl=2.16, wps=16967.5, ups=1.62, wpb=10501.8, bsz=363.6, num_updates=5300, lr=3.00942e-05, gnorm=1.082, train_wall=62, wall=3510
2021-01-06 23:12:32 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.613, nll_loss=1.112, ppl=2.16, wps=16907.1, ups=1.62, wpb=10450.1, bsz=375.6, num_updates=5400, lr=2.98142e-05, gnorm=1.063, train_wall=62, wall=3572
2021-01-06 23:13:34 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.582, nll_loss=1.097, ppl=2.14, wps=16996.7, ups=1.62, wpb=10472.1, bsz=373.6, num_updates=5500, lr=2.9542e-05, gnorm=1.05, train_wall=61, wall=3634
2021-01-06 23:14:35 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.598, nll_loss=1.106, ppl=2.15, wps=17082, ups=1.62, wpb=10516.7, bsz=381.2, num_updates=5600, lr=2.9277e-05, gnorm=1.065, train_wall=61, wall=3695
2021-01-06 23:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:14:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:14:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:14:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:14:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:14:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:14:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:14:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:14:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:14:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:14:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:15:03 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.224 | nll_loss 3.756 | ppl 13.51 | bleu 22.5 | wps 4602 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.68
2021-01-06 23:15:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:15:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:15:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:15:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.5) (writing took 2.9697200004011393 seconds)
2021-01-06 23:15:05 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-06 23:15:05 | INFO | train | epoch 010 | symm_kl 0.536 | self_kl 0 | self_cv 0 | loss 3.607 | nll_loss 1.107 | ppl 2.15 | wps 15763.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 2.92509e-05 | gnorm 1.067 | train_wall 345 | wall 3725
2021-01-06 23:15:05 | INFO | fairseq.trainer | begin training epoch 11
2021-01-06 23:15:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:15:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:15:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:15:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:16:03 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=0.536, self_kl=0, self_cv=0, loss=3.606, nll_loss=1.106, ppl=2.15, wps=11806.6, ups=1.14, wpb=10358.6, bsz=351.1, num_updates=5700, lr=2.90191e-05, gnorm=1.066, train_wall=60, wall=3783
2021-01-06 23:17:05 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.567, nll_loss=1.088, ppl=2.13, wps=17010.3, ups=1.61, wpb=10564, bsz=383.6, num_updates=5800, lr=2.87678e-05, gnorm=1.033, train_wall=62, wall=3845
2021-01-06 23:18:07 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=0.533, self_kl=0, self_cv=0, loss=3.611, nll_loss=1.118, ppl=2.17, wps=16927, ups=1.63, wpb=10400.1, bsz=355.4, num_updates=5900, lr=2.8523e-05, gnorm=1.06, train_wall=61, wall=3907
2021-01-06 23:19:09 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.613, nll_loss=1.123, ppl=2.18, wps=16777, ups=1.61, wpb=10394.2, bsz=372.6, num_updates=6000, lr=2.82843e-05, gnorm=1.044, train_wall=62, wall=3968
2021-01-06 23:20:10 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.565, nll_loss=1.089, ppl=2.13, wps=17281.6, ups=1.62, wpb=10652.7, bsz=380.5, num_updates=6100, lr=2.80515e-05, gnorm=1.02, train_wall=61, wall=4030
2021-01-06 23:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:20:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:20:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:20:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:20:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:20:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:20:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:21:15 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.218 | nll_loss 3.747 | ppl 13.43 | bleu 22.46 | wps 4536.5 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 22.68
2021-01-06 23:21:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:21:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:21:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:21:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 11 @ 6171 updates, score 22.46) (writing took 2.9406325574964285 seconds)
2021-01-06 23:21:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-06 23:21:18 | INFO | train | epoch 011 | symm_kl 0.526 | self_kl 0 | self_cv 0 | loss 3.59 | nll_loss 1.104 | ppl 2.15 | wps 15779.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 2.78896e-05 | gnorm 1.044 | train_wall 344 | wall 4098
2021-01-06 23:21:18 | INFO | fairseq.trainer | begin training epoch 12
2021-01-06 23:21:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:21:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:21:39 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.576, nll_loss=1.097, ppl=2.14, wps=11829.5, ups=1.13, wpb=10473.3, bsz=364.9, num_updates=6200, lr=2.78243e-05, gnorm=1.058, train_wall=61, wall=4119
2021-01-06 23:22:40 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.572, nll_loss=1.096, ppl=2.14, wps=16870.6, ups=1.62, wpb=10400.1, bsz=369, num_updates=6300, lr=2.76026e-05, gnorm=1.037, train_wall=61, wall=4180
2021-01-06 23:23:42 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=0.523, self_kl=0, self_cv=0, loss=3.592, nll_loss=1.11, ppl=2.16, wps=17030.5, ups=1.63, wpb=10444.8, bsz=372, num_updates=6400, lr=2.73861e-05, gnorm=1.059, train_wall=61, wall=4242
2021-01-06 23:24:44 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.552, nll_loss=1.085, ppl=2.12, wps=17179.4, ups=1.62, wpb=10631.9, bsz=382.4, num_updates=6500, lr=2.71746e-05, gnorm=1.021, train_wall=62, wall=4304
2021-01-06 23:25:46 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=0.523, self_kl=0, self_cv=0, loss=3.601, nll_loss=1.12, ppl=2.17, wps=16974.8, ups=1.61, wpb=10531, bsz=361.8, num_updates=6600, lr=2.6968e-05, gnorm=1.048, train_wall=62, wall=4366
2021-01-06 23:26:47 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=0.51, self_kl=0, self_cv=0, loss=3.56, nll_loss=1.095, ppl=2.14, wps=16984.3, ups=1.62, wpb=10493.3, bsz=369.8, num_updates=6700, lr=2.6766e-05, gnorm=1.018, train_wall=62, wall=4427
2021-01-06 23:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:27:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:27:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:27:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:27:28 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.209 | nll_loss 3.737 | ppl 13.33 | bleu 22.67 | wps 4585.8 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 22.68
2021-01-06 23:27:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:27:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 12 @ 6732 updates, score 22.67) (writing took 2.985180240124464 seconds)
2021-01-06 23:27:31 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-06 23:27:31 | INFO | train | epoch 012 | symm_kl 0.517 | self_kl 0 | self_cv 0 | loss 3.574 | nll_loss 1.1 | ppl 2.14 | wps 15761 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 2.67023e-05 | gnorm 1.04 | train_wall 345 | wall 4471
2021-01-06 23:27:31 | INFO | fairseq.trainer | begin training epoch 13
2021-01-06 23:27:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:27:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:28:16 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.572, nll_loss=1.097, ppl=2.14, wps=11737.8, ups=1.13, wpb=10372.9, bsz=360.1, num_updates=6800, lr=2.65684e-05, gnorm=1.054, train_wall=61, wall=4516
2021-01-06 23:29:18 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=0.518, self_kl=0, self_cv=0, loss=3.577, nll_loss=1.1, ppl=2.14, wps=17037, ups=1.61, wpb=10571.4, bsz=349, num_updates=6900, lr=2.63752e-05, gnorm=1.039, train_wall=62, wall=4578
2021-01-06 23:30:20 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.562, nll_loss=1.099, ppl=2.14, wps=17075.2, ups=1.62, wpb=10544.3, bsz=369.9, num_updates=7000, lr=2.61861e-05, gnorm=1.011, train_wall=62, wall=4640
2021-01-06 23:31:21 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.533, nll_loss=1.082, ppl=2.12, wps=16992, ups=1.62, wpb=10490.1, bsz=389.7, num_updates=7100, lr=2.60011e-05, gnorm=1.012, train_wall=62, wall=4701
2021-01-06 23:32:23 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=0.507, self_kl=0, self_cv=0, loss=3.559, nll_loss=1.098, ppl=2.14, wps=16982.7, ups=1.62, wpb=10510.3, bsz=369.8, num_updates=7200, lr=2.58199e-05, gnorm=1.02, train_wall=62, wall=4763
2021-01-06 23:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:33:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:33:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:33:42 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.205 | nll_loss 3.732 | ppl 13.29 | bleu 22.58 | wps 4525.8 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 22.68
2021-01-06 23:33:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:33:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 13 @ 7293 updates, score 22.58) (writing took 2.938264437019825 seconds)
2021-01-06 23:33:45 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-06 23:33:45 | INFO | train | epoch 013 | symm_kl 0.509 | self_kl 0 | self_cv 0 | loss 3.561 | nll_loss 1.097 | ppl 2.14 | wps 15742.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 2.56547e-05 | gnorm 1.026 | train_wall 345 | wall 4845
2021-01-06 23:33:45 | INFO | fairseq.trainer | begin training epoch 14
2021-01-06 23:33:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:33:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:33:52 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.563, nll_loss=1.105, ppl=2.15, wps=11573.1, ups=1.12, wpb=10327.2, bsz=373.9, num_updates=7300, lr=2.56424e-05, gnorm=1.032, train_wall=62, wall=4852
2021-01-06 23:34:54 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=0.511, self_kl=0, self_cv=0, loss=3.565, nll_loss=1.097, ppl=2.14, wps=17322.7, ups=1.64, wpb=10590, bsz=367, num_updates=7400, lr=2.54686e-05, gnorm=1.021, train_wall=61, wall=4914
2021-01-06 23:35:56 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.535, nll_loss=1.083, ppl=2.12, wps=17006.4, ups=1.61, wpb=10574.3, bsz=374, num_updates=7500, lr=2.52982e-05, gnorm=1.006, train_wall=62, wall=4976
2021-01-06 23:36:57 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.556, nll_loss=1.099, ppl=2.14, wps=16902.8, ups=1.63, wpb=10386.9, bsz=357.6, num_updates=7600, lr=2.51312e-05, gnorm=1.018, train_wall=61, wall=5037
2021-01-06 23:37:59 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.57, nll_loss=1.109, ppl=2.16, wps=16807, ups=1.63, wpb=10338.9, bsz=367.2, num_updates=7700, lr=2.49675e-05, gnorm=1.018, train_wall=61, wall=5099
2021-01-06 23:39:01 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.541, nll_loss=1.091, ppl=2.13, wps=17011.6, ups=1.61, wpb=10594.9, bsz=368.6, num_updates=7800, lr=2.48069e-05, gnorm=1.003, train_wall=62, wall=5161
2021-01-06 23:39:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:39:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:39:55 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.199 | nll_loss 3.726 | ppl 13.23 | bleu 22.76 | wps 4720.7 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 22.76
2021-01-06 23:39:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:40:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 14 @ 7854 updates, score 22.76) (writing took 4.949223101139069 seconds)
2021-01-06 23:40:00 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-06 23:40:00 | INFO | train | epoch 014 | symm_kl 0.502 | self_kl 0 | self_cv 0 | loss 3.551 | nll_loss 1.095 | ppl 2.14 | wps 15675.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 2.47215e-05 | gnorm 1.014 | train_wall 345 | wall 5220
2021-01-06 23:40:00 | INFO | fairseq.trainer | begin training epoch 15
2021-01-06 23:40:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:40:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:40:31 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.551, nll_loss=1.101, ppl=2.15, wps=11523.7, ups=1.11, wpb=10380.4, bsz=382.1, num_updates=7900, lr=2.46494e-05, gnorm=1.019, train_wall=61, wall=5251
2021-01-06 23:41:33 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.551, nll_loss=1.097, ppl=2.14, wps=16917.1, ups=1.62, wpb=10471, bsz=365.8, num_updates=8000, lr=2.44949e-05, gnorm=1.014, train_wall=62, wall=5313
2021-01-06 23:42:35 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.521, nll_loss=1.076, ppl=2.11, wps=16918.4, ups=1.61, wpb=10485.3, bsz=382.9, num_updates=8100, lr=2.43432e-05, gnorm=1.002, train_wall=62, wall=5375
2021-01-06 23:43:37 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.537, nll_loss=1.091, ppl=2.13, wps=16986.7, ups=1.61, wpb=10526.2, bsz=362.5, num_updates=8200, lr=2.41943e-05, gnorm=1.01, train_wall=62, wall=5437
2021-01-06 23:44:38 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.533, nll_loss=1.092, ppl=2.13, wps=17106.4, ups=1.62, wpb=10527.6, bsz=374.3, num_updates=8300, lr=2.40481e-05, gnorm=0.993, train_wall=61, wall=5498
2021-01-06 23:45:40 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.56, nll_loss=1.11, ppl=2.16, wps=17002.1, ups=1.62, wpb=10468.7, bsz=364.2, num_updates=8400, lr=2.39046e-05, gnorm=1.024, train_wall=61, wall=5560
2021-01-06 23:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:45:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:45:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:45:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:45:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:45:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:45:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:46:12 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.205 | nll_loss 3.727 | ppl 13.24 | bleu 22.69 | wps 4068 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 22.76
2021-01-06 23:46:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:46:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:46:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:46:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 15 @ 8415 updates, score 22.69) (writing took 2.9942820835858583 seconds)
2021-01-06 23:46:15 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-06 23:46:15 | INFO | train | epoch 015 | symm_kl 0.496 | self_kl 0 | self_cv 0 | loss 3.54 | nll_loss 1.092 | ppl 2.13 | wps 15671.9 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 2.38833e-05 | gnorm 1.009 | train_wall 345 | wall 5595
2021-01-06 23:46:15 | INFO | fairseq.trainer | begin training epoch 16
2021-01-06 23:46:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:46:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:47:11 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.519, nll_loss=1.08, ppl=2.11, wps=11505.6, ups=1.11, wpb=10408.1, bsz=377.6, num_updates=8500, lr=2.37635e-05, gnorm=0.993, train_wall=61, wall=5650
2021-01-06 23:48:12 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.536, nll_loss=1.091, ppl=2.13, wps=16855.2, ups=1.62, wpb=10433.6, bsz=372.2, num_updates=8600, lr=2.3625e-05, gnorm=1, train_wall=62, wall=5712
2021-01-06 23:49:14 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.536, nll_loss=1.091, ppl=2.13, wps=17157.6, ups=1.63, wpb=10533.1, bsz=379.8, num_updates=8700, lr=2.34888e-05, gnorm=1.003, train_wall=61, wall=5774
2021-01-06 23:50:16 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.074, ppl=2.11, wps=17029.7, ups=1.61, wpb=10551.2, bsz=375.2, num_updates=8800, lr=2.3355e-05, gnorm=0.998, train_wall=62, wall=5836
2021-01-06 23:51:17 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.535, nll_loss=1.092, ppl=2.13, wps=17069.2, ups=1.63, wpb=10494.4, bsz=360.2, num_updates=8900, lr=2.32234e-05, gnorm=1.014, train_wall=61, wall=5897
2021-01-06 23:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:52:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:52:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:52:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:52:25 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.197 | nll_loss 3.72 | ppl 13.18 | bleu 22.53 | wps 4542.4 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 22.76
2021-01-06 23:52:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 16 @ 8976 updates, score 22.53) (writing took 3.0836154092103243 seconds)
2021-01-06 23:52:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-06 23:52:28 | INFO | train | epoch 016 | symm_kl 0.491 | self_kl 0 | self_cv 0 | loss 3.531 | nll_loss 1.09 | ppl 2.13 | wps 15768.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 2.31249e-05 | gnorm 1.005 | train_wall 344 | wall 5968
2021-01-06 23:52:28 | INFO | fairseq.trainer | begin training epoch 17
2021-01-06 23:52:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:52:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:52:46 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.534, nll_loss=1.099, ppl=2.14, wps=11771.4, ups=1.13, wpb=10459.4, bsz=358.3, num_updates=9000, lr=2.3094e-05, gnorm=1.022, train_wall=61, wall=5986
2021-01-06 23:53:48 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.524, nll_loss=1.086, ppl=2.12, wps=16927.1, ups=1.61, wpb=10489.1, bsz=374.8, num_updates=9100, lr=2.29668e-05, gnorm=0.987, train_wall=62, wall=6048
2021-01-06 23:54:50 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.492, nll_loss=1.063, ppl=2.09, wps=17003.6, ups=1.61, wpb=10592.8, bsz=369.3, num_updates=9200, lr=2.28416e-05, gnorm=0.982, train_wall=62, wall=6110
2021-01-06 23:55:52 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.516, nll_loss=1.08, ppl=2.11, wps=17103.2, ups=1.63, wpb=10507.6, bsz=365.5, num_updates=9300, lr=2.27185e-05, gnorm=0.989, train_wall=61, wall=6172
2021-01-06 23:56:53 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.531, nll_loss=1.095, ppl=2.14, wps=16881.2, ups=1.62, wpb=10397.2, bsz=380.2, num_updates=9400, lr=2.25973e-05, gnorm=1.005, train_wall=61, wall=6233
2021-01-06 23:57:55 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=0.492, self_kl=0, self_cv=0, loss=3.554, nll_loss=1.114, ppl=2.16, wps=17127.3, ups=1.63, wpb=10491.6, bsz=362.4, num_updates=9500, lr=2.24781e-05, gnorm=1.002, train_wall=61, wall=6295
2021-01-06 23:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-06 23:58:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-06 23:58:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-06 23:58:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-06 23:58:38 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.194 | nll_loss 3.716 | ppl 13.14 | bleu 22.72 | wps 4613.6 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 22.76
2021-01-06 23:58:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-06 23:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:58:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:58:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 17 @ 9537 updates, score 22.72) (writing took 3.0587546080350876 seconds)
2021-01-06 23:58:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-06 23:58:42 | INFO | train | epoch 017 | symm_kl 0.486 | self_kl 0 | self_cv 0 | loss 3.523 | nll_loss 1.089 | ppl 2.13 | wps 15759.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 2.24344e-05 | gnorm 0.996 | train_wall 345 | wall 6341
2021-01-06 23:58:42 | INFO | fairseq.trainer | begin training epoch 18
2021-01-06 23:58:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-06 23:58:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-06 23:59:23 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.524, nll_loss=1.088, ppl=2.13, wps=11808.7, ups=1.13, wpb=10417.2, bsz=358.2, num_updates=9600, lr=2.23607e-05, gnorm=1.009, train_wall=61, wall=6383
2021-01-07 00:00:25 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.517, nll_loss=1.087, ppl=2.12, wps=16942.3, ups=1.61, wpb=10492.8, bsz=370, num_updates=9700, lr=2.22451e-05, gnorm=0.979, train_wall=62, wall=6445
2021-01-07 00:01:27 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.509, nll_loss=1.079, ppl=2.11, wps=16895.7, ups=1.62, wpb=10456, bsz=363.1, num_updates=9800, lr=2.21313e-05, gnorm=0.994, train_wall=62, wall=6507
2021-01-07 00:02:28 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.513, nll_loss=1.089, ppl=2.13, wps=16842.4, ups=1.62, wpb=10380.2, bsz=372.9, num_updates=9900, lr=2.20193e-05, gnorm=0.986, train_wall=61, wall=6568
2021-01-07 00:03:30 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.509, nll_loss=1.086, ppl=2.12, wps=17129, ups=1.61, wpb=10622, bsz=379.6, num_updates=10000, lr=2.19089e-05, gnorm=0.972, train_wall=62, wall=6630
2021-01-07 00:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:04:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:04:52 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.715 | ppl 13.13 | bleu 22.73 | wps 4567.5 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 22.76
2021-01-07 00:04:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:04:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:04:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 18 @ 10098 updates, score 22.73) (writing took 3.2295263092964888 seconds)
2021-01-07 00:04:55 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-07 00:04:55 | INFO | train | epoch 018 | symm_kl 0.481 | self_kl 0 | self_cv 0 | loss 3.515 | nll_loss 1.086 | ppl 2.12 | wps 15739 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 2.18023e-05 | gnorm 0.986 | train_wall 345 | wall 6715
2021-01-07 00:04:55 | INFO | fairseq.trainer | begin training epoch 19
2021-01-07 00:04:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:04:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:05:00 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=0.484, self_kl=0, self_cv=0, loss=3.53, nll_loss=1.1, ppl=2.14, wps=11657.7, ups=1.12, wpb=10417.9, bsz=365.4, num_updates=10100, lr=2.18002e-05, gnorm=0.992, train_wall=62, wall=6720
2021-01-07 00:06:01 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.501, nll_loss=1.076, ppl=2.11, wps=16983, ups=1.63, wpb=10422.9, bsz=373.4, num_updates=10200, lr=2.1693e-05, gnorm=0.971, train_wall=61, wall=6781
2021-01-07 00:07:03 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=0.469, self_kl=0, self_cv=0, loss=3.487, nll_loss=1.072, ppl=2.1, wps=17115.3, ups=1.61, wpb=10632.5, bsz=380, num_updates=10300, lr=2.15875e-05, gnorm=0.979, train_wall=62, wall=6843
2021-01-07 00:08:05 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.512, nll_loss=1.086, ppl=2.12, wps=17089.5, ups=1.63, wpb=10486.3, bsz=361.5, num_updates=10400, lr=2.14834e-05, gnorm=0.991, train_wall=61, wall=6904
2021-01-07 00:09:06 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.526, nll_loss=1.096, ppl=2.14, wps=16961, ups=1.62, wpb=10482.8, bsz=369.7, num_updates=10500, lr=2.13809e-05, gnorm=0.989, train_wall=62, wall=6966
2021-01-07 00:10:08 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.498, nll_loss=1.079, ppl=2.11, wps=16905.1, ups=1.62, wpb=10451.7, bsz=365.4, num_updates=10600, lr=2.12798e-05, gnorm=0.992, train_wall=62, wall=7028
2021-01-07 00:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:10:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:10:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:10:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:10:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:10:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:11:05 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.186 | nll_loss 3.71 | ppl 13.08 | bleu 22.82 | wps 4610.4 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 22.82
2021-01-07 00:11:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:11:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:11:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:11:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:11:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:11:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 19 @ 10659 updates, score 22.82) (writing took 4.982915345579386 seconds)
2021-01-07 00:11:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-07 00:11:10 | INFO | train | epoch 019 | symm_kl 0.477 | self_kl 0 | self_cv 0 | loss 3.507 | nll_loss 1.084 | ppl 2.12 | wps 15682.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 2.12208e-05 | gnorm 0.988 | train_wall 345 | wall 7090
2021-01-07 00:11:10 | INFO | fairseq.trainer | begin training epoch 20
2021-01-07 00:11:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:11:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:11:38 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.086, ppl=2.12, wps=11652, ups=1.11, wpb=10477.3, bsz=372.4, num_updates=10700, lr=2.11801e-05, gnorm=0.99, train_wall=61, wall=7118
2021-01-07 00:12:40 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.514, nll_loss=1.088, ppl=2.13, wps=17092.9, ups=1.62, wpb=10538.1, bsz=365.8, num_updates=10800, lr=2.10819e-05, gnorm=0.98, train_wall=61, wall=7180
2021-01-07 00:13:42 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=0.47, self_kl=0, self_cv=0, loss=3.492, nll_loss=1.078, ppl=2.11, wps=16910.8, ups=1.61, wpb=10496, bsz=377, num_updates=10900, lr=2.09849e-05, gnorm=0.976, train_wall=62, wall=7242
2021-01-07 00:14:43 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.506, nll_loss=1.082, ppl=2.12, wps=17171.8, ups=1.63, wpb=10521.2, bsz=361.4, num_updates=11000, lr=2.08893e-05, gnorm=0.986, train_wall=61, wall=7303
2021-01-07 00:15:45 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.501, nll_loss=1.087, ppl=2.13, wps=16998.5, ups=1.63, wpb=10445.5, bsz=372.4, num_updates=11100, lr=2.0795e-05, gnorm=0.979, train_wall=61, wall=7364
2021-01-07 00:16:46 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=0.474, self_kl=0, self_cv=0, loss=3.508, nll_loss=1.09, ppl=2.13, wps=16944.8, ups=1.63, wpb=10410.2, bsz=367.4, num_updates=11200, lr=2.0702e-05, gnorm=0.989, train_wall=61, wall=7426
2021-01-07 00:16:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:17:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:17:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:17:20 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.708 | ppl 13.07 | bleu 22.89 | wps 4555.2 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 22.89
2021-01-07 00:17:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:17:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:17:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:17:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 20 @ 11220 updates, score 22.89) (writing took 4.983526648953557 seconds)
2021-01-07 00:17:25 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-07 00:17:25 | INFO | train | epoch 020 | symm_kl 0.473 | self_kl 0 | self_cv 0 | loss 3.501 | nll_loss 1.083 | ppl 2.12 | wps 15707.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 2.06835e-05 | gnorm 0.979 | train_wall 344 | wall 7465
2021-01-07 00:17:25 | INFO | fairseq.trainer | begin training epoch 21
2021-01-07 00:17:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:17:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:18:16 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.088, ppl=2.13, wps=11424.2, ups=1.11, wpb=10315.2, bsz=376.1, num_updates=11300, lr=2.06102e-05, gnorm=0.99, train_wall=61, wall=7516
2021-01-07 00:19:18 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.495, nll_loss=1.081, ppl=2.12, wps=17032.6, ups=1.62, wpb=10526.7, bsz=359.6, num_updates=11400, lr=2.05196e-05, gnorm=0.957, train_wall=62, wall=7578
2021-01-07 00:20:20 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=0.467, self_kl=0, self_cv=0, loss=3.481, nll_loss=1.07, ppl=2.1, wps=16976.3, ups=1.61, wpb=10514.2, bsz=377.1, num_updates=11500, lr=2.04302e-05, gnorm=0.949, train_wall=62, wall=7640
2021-01-07 00:21:22 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=0.463, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.065, ppl=2.09, wps=17196.8, ups=1.62, wpb=10592.6, bsz=375.9, num_updates=11600, lr=2.03419e-05, gnorm=0.959, train_wall=61, wall=7702
2021-01-07 00:22:23 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.088, ppl=2.13, wps=16995.3, ups=1.62, wpb=10501.6, bsz=373.3, num_updates=11700, lr=2.02548e-05, gnorm=0.964, train_wall=62, wall=7763
2021-01-07 00:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:23:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:23:36 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.703 | ppl 13.03 | bleu 22.73 | wps 4020.6 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 22.89
2021-01-07 00:23:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:23:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 21 @ 11781 updates, score 22.73) (writing took 3.069677099585533 seconds)
2021-01-07 00:23:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-07 00:23:39 | INFO | train | epoch 021 | symm_kl 0.47 | self_kl 0 | self_cv 0 | loss 3.495 | nll_loss 1.081 | ppl 2.12 | wps 15688.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 2.0185e-05 | gnorm 0.966 | train_wall 344 | wall 7839
2021-01-07 00:23:39 | INFO | fairseq.trainer | begin training epoch 22
2021-01-07 00:23:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:23:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:23:54 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.513, nll_loss=1.094, ppl=2.13, wps=11545.3, ups=1.1, wpb=10469.7, bsz=353, num_updates=11800, lr=2.01688e-05, gnorm=0.979, train_wall=61, wall=7854
2021-01-07 00:24:55 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=0.465, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.07, ppl=2.1, wps=17314.3, ups=1.64, wpb=10589.6, bsz=376.8, num_updates=11900, lr=2.00839e-05, gnorm=0.949, train_wall=61, wall=7915
2021-01-07 00:25:57 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.068, ppl=2.1, wps=17027, ups=1.62, wpb=10493.2, bsz=374.8, num_updates=12000, lr=2e-05, gnorm=0.953, train_wall=61, wall=7977
2021-01-07 00:26:59 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.511, nll_loss=1.096, ppl=2.14, wps=16856, ups=1.62, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1.99172e-05, gnorm=0.976, train_wall=62, wall=8039
2021-01-07 00:28:00 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=0.466, self_kl=0, self_cv=0, loss=3.475, nll_loss=1.065, ppl=2.09, wps=17135, ups=1.63, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1.98354e-05, gnorm=0.971, train_wall=61, wall=8100
2021-01-07 00:29:01 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=0.468, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.094, ppl=2.13, wps=17024.4, ups=1.63, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1.97546e-05, gnorm=0.964, train_wall=61, wall=8161
2021-01-07 00:29:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:29:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:29:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:29:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:29:49 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.698 | ppl 12.98 | bleu 22.78 | wps 4424.2 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 22.89
2021-01-07 00:29:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:29:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 22 @ 12342 updates, score 22.78) (writing took 3.133662421256304 seconds)
2021-01-07 00:29:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-07 00:29:52 | INFO | train | epoch 022 | symm_kl 0.467 | self_kl 0 | self_cv 0 | loss 3.49 | nll_loss 1.08 | ppl 2.11 | wps 15785.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.9721e-05 | gnorm 0.965 | train_wall 343 | wall 8212
2021-01-07 00:29:52 | INFO | fairseq.trainer | begin training epoch 23
2021-01-07 00:29:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:29:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:30:30 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=0.477, self_kl=0, self_cv=0, loss=3.522, nll_loss=1.101, ppl=2.15, wps=11720.4, ups=1.13, wpb=10402.8, bsz=349, num_updates=12400, lr=1.96748e-05, gnorm=0.984, train_wall=60, wall=8250
2021-01-07 00:31:32 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.077, ppl=2.11, wps=16988.1, ups=1.61, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1.95959e-05, gnorm=0.941, train_wall=62, wall=8312
2021-01-07 00:32:34 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.509, nll_loss=1.092, ppl=2.13, wps=16920.8, ups=1.62, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1.9518e-05, gnorm=0.973, train_wall=61, wall=8374
2021-01-07 00:33:36 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.472, nll_loss=1.072, ppl=2.1, wps=17117.3, ups=1.62, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1.9441e-05, gnorm=0.95, train_wall=62, wall=8436
2021-01-07 00:34:38 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=0.461, self_kl=0, self_cv=0, loss=3.48, nll_loss=1.077, ppl=2.11, wps=16781.8, ups=1.61, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1.93649e-05, gnorm=0.948, train_wall=62, wall=8498
2021-01-07 00:35:39 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=0.458, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.065, ppl=2.09, wps=17091.2, ups=1.62, wpb=10521.5, bsz=381, num_updates=12900, lr=1.92897e-05, gnorm=0.965, train_wall=61, wall=8559
2021-01-07 00:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:35:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:35:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:35:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:35:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:35:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:36:02 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.697 | ppl 12.97 | bleu 22.7 | wps 4499.9 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 22.89
2021-01-07 00:36:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:36:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:36:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:36:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:36:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:36:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 23 @ 12903 updates, score 22.7) (writing took 3.117482600733638 seconds)
2021-01-07 00:36:05 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-07 00:36:05 | INFO | train | epoch 023 | symm_kl 0.464 | self_kl 0 | self_cv 0 | loss 3.484 | nll_loss 1.078 | ppl 2.11 | wps 15753.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.92875e-05 | gnorm 0.958 | train_wall 344 | wall 8585
2021-01-07 00:36:05 | INFO | fairseq.trainer | begin training epoch 24
2021-01-07 00:36:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:37:08 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.073, ppl=2.1, wps=11878.6, ups=1.13, wpb=10536, bsz=373.3, num_updates=13000, lr=1.92154e-05, gnorm=0.949, train_wall=61, wall=8648
2021-01-07 00:38:10 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=0.463, self_kl=0, self_cv=0, loss=3.479, nll_loss=1.073, ppl=2.1, wps=16912.9, ups=1.62, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1.91419e-05, gnorm=0.962, train_wall=61, wall=8710
2021-01-07 00:39:12 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.473, nll_loss=1.072, ppl=2.1, wps=16955.6, ups=1.61, wpb=10536, bsz=369, num_updates=13200, lr=1.90693e-05, gnorm=0.962, train_wall=62, wall=8772
2021-01-07 00:40:13 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=0.461, self_kl=0, self_cv=0, loss=3.486, nll_loss=1.085, ppl=2.12, wps=16827.2, ups=1.63, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1.89974e-05, gnorm=0.956, train_wall=61, wall=8833
2021-01-07 00:41:15 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=0.463, self_kl=0, self_cv=0, loss=3.495, nll_loss=1.093, ppl=2.13, wps=16988.8, ups=1.62, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1.89264e-05, gnorm=0.967, train_wall=61, wall=8895
2021-01-07 00:41:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:41:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:41:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:41:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:41:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:41:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:41:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:42:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:42:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:42:16 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.696 | ppl 12.96 | bleu 22.77 | wps 4523.7 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 22.89
2021-01-07 00:42:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:42:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:42:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:42:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 24 @ 13464 updates, score 22.77) (writing took 3.080345204100013 seconds)
2021-01-07 00:42:19 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-07 00:42:19 | INFO | train | epoch 024 | symm_kl 0.46 | self_kl 0 | self_cv 0 | loss 3.478 | nll_loss 1.077 | ppl 2.11 | wps 15754.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.88814e-05 | gnorm 0.956 | train_wall 344 | wall 8959
2021-01-07 00:42:19 | INFO | fairseq.trainer | begin training epoch 25
2021-01-07 00:42:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:42:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:42:44 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.459, nll_loss=1.061, ppl=2.09, wps=11863.2, ups=1.12, wpb=10574, bsz=365.8, num_updates=13500, lr=1.88562e-05, gnorm=0.948, train_wall=61, wall=8984
2021-01-07 00:43:46 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=0.458, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.066, ppl=2.09, wps=17107.6, ups=1.62, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1.87867e-05, gnorm=0.943, train_wall=62, wall=9045
2021-01-07 00:44:47 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.078, ppl=2.11, wps=16882.3, ups=1.63, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1.8718e-05, gnorm=0.945, train_wall=61, wall=9107
2021-01-07 00:45:48 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.078, ppl=2.11, wps=17132.8, ups=1.63, wpb=10508.8, bsz=363, num_updates=13800, lr=1.86501e-05, gnorm=0.958, train_wall=61, wall=9168
2021-01-07 00:46:50 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.081, ppl=2.12, wps=17289.1, ups=1.63, wpb=10632, bsz=381.2, num_updates=13900, lr=1.85829e-05, gnorm=0.938, train_wall=61, wall=9230
2021-01-07 00:47:51 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=0.454, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.074, ppl=2.11, wps=16950, ups=1.63, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1.85164e-05, gnorm=0.949, train_wall=61, wall=9291
2021-01-07 00:48:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:48:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:48:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:48:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:48:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:48:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:48:27 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.696 | ppl 12.96 | bleu 22.66 | wps 4541.3 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 22.89
2021-01-07 00:48:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:48:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 25 @ 14025 updates, score 22.66) (writing took 3.075310042127967 seconds)
2021-01-07 00:48:31 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-07 00:48:31 | INFO | train | epoch 025 | symm_kl 0.458 | self_kl 0 | self_cv 0 | loss 3.473 | nll_loss 1.075 | ppl 2.11 | wps 15816.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.84999e-05 | gnorm 0.949 | train_wall 343 | wall 9330
2021-01-07 00:48:31 | INFO | fairseq.trainer | begin training epoch 26
2021-01-07 00:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:48:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:49:19 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.056, ppl=2.08, wps=11769.5, ups=1.13, wpb=10395, bsz=387.4, num_updates=14100, lr=1.84506e-05, gnorm=0.937, train_wall=61, wall=9379
2021-01-07 00:50:21 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.473, nll_loss=1.071, ppl=2.1, wps=16935.3, ups=1.62, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1.83855e-05, gnorm=0.967, train_wall=61, wall=9441
2021-01-07 00:51:22 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=0.449, self_kl=0, self_cv=0, loss=3.452, nll_loss=1.064, ppl=2.09, wps=17409.6, ups=1.63, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1.83211e-05, gnorm=0.922, train_wall=61, wall=9502
2021-01-07 00:52:24 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.483, nll_loss=1.083, ppl=2.12, wps=17163.2, ups=1.63, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1.82574e-05, gnorm=0.97, train_wall=61, wall=9564
2021-01-07 00:53:25 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=0.457, self_kl=0, self_cv=0, loss=3.477, nll_loss=1.08, ppl=2.11, wps=17121.4, ups=1.63, wpb=10526, bsz=359.1, num_updates=14500, lr=1.81944e-05, gnorm=0.938, train_wall=61, wall=9625
2021-01-07 00:54:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 00:54:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 00:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 00:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 00:54:39 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.694 | ppl 12.95 | bleu 22.79 | wps 4608.4 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 22.89
2021-01-07 00:54:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 00:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 26 @ 14586 updates, score 22.79) (writing took 3.0940482523292303 seconds)
2021-01-07 00:54:42 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-07 00:54:42 | INFO | train | epoch 026 | symm_kl 0.455 | self_kl 0 | self_cv 0 | loss 3.469 | nll_loss 1.074 | ppl 2.11 | wps 15843 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.81406e-05 | gnorm 0.95 | train_wall 343 | wall 9702
2021-01-07 00:54:42 | INFO | fairseq.trainer | begin training epoch 27
2021-01-07 00:54:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 00:54:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 00:54:54 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=0.458, self_kl=0, self_cv=0, loss=3.486, nll_loss=1.091, ppl=2.13, wps=11630.7, ups=1.13, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1.81319e-05, gnorm=0.969, train_wall=61, wall=9713
2021-01-07 00:55:55 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=0.458, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.08, ppl=2.11, wps=17117, ups=1.63, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1.80702e-05, gnorm=0.949, train_wall=61, wall=9775
2021-01-07 00:56:57 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.074, ppl=2.11, wps=17038.1, ups=1.62, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1.8009e-05, gnorm=0.941, train_wall=62, wall=9837
2021-01-07 00:57:58 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=0.449, self_kl=0, self_cv=0, loss=3.456, nll_loss=1.071, ppl=2.1, wps=17049.4, ups=1.62, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1.79485e-05, gnorm=0.928, train_wall=61, wall=9898
2021-01-07 00:59:00 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.063, ppl=2.09, wps=17115.6, ups=1.62, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1.78885e-05, gnorm=0.929, train_wall=61, wall=9960
2021-01-07 01:00:02 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.463, nll_loss=1.073, ppl=2.1, wps=16963.6, ups=1.62, wpb=10447, bsz=358.8, num_updates=15100, lr=1.78292e-05, gnorm=0.948, train_wall=61, wall=10022
2021-01-07 01:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:00:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:00:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:00:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:00:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:00:52 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.693 | ppl 12.93 | bleu 22.86 | wps 4410.4 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 22.89
2021-01-07 01:00:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:00:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:00:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:00:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 27 @ 15147 updates, score 22.86) (writing took 3.110295783728361 seconds)
2021-01-07 01:00:55 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-07 01:00:55 | INFO | train | epoch 027 | symm_kl 0.453 | self_kl 0 | self_cv 0 | loss 3.465 | nll_loss 1.074 | ppl 2.11 | wps 15742 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.78015e-05 | gnorm 0.941 | train_wall 344 | wall 10075
2021-01-07 01:00:55 | INFO | fairseq.trainer | begin training epoch 28
2021-01-07 01:00:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:00:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:01:30 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.059, ppl=2.08, wps=11618.7, ups=1.13, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1.77705e-05, gnorm=0.949, train_wall=61, wall=10110
2021-01-07 01:02:32 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=0.453, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.073, ppl=2.1, wps=16931.5, ups=1.62, wpb=10482.5, bsz=354, num_updates=15300, lr=1.77123e-05, gnorm=0.945, train_wall=62, wall=10172
2021-01-07 01:03:34 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.45, nll_loss=1.064, ppl=2.09, wps=17133.4, ups=1.62, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1.76547e-05, gnorm=0.939, train_wall=61, wall=10234
2021-01-07 01:04:36 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=0.454, self_kl=0, self_cv=0, loss=3.463, nll_loss=1.069, ppl=2.1, wps=16941.6, ups=1.61, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1.75977e-05, gnorm=0.941, train_wall=62, wall=10296
2021-01-07 01:05:37 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=0.451, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.09, ppl=2.13, wps=16965.7, ups=1.63, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1.75412e-05, gnorm=0.952, train_wall=61, wall=10357
2021-01-07 01:06:39 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.474, nll_loss=1.086, ppl=2.12, wps=17097.8, ups=1.62, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1.74852e-05, gnorm=0.933, train_wall=62, wall=10419
2021-01-07 01:06:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:06:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:06:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:06:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:06:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:07:05 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.173 | nll_loss 3.688 | ppl 12.89 | bleu 22.84 | wps 4582.5 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 22.89
2021-01-07 01:07:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:07:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:07:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:07:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:07:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:07:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 28 @ 15708 updates, score 22.84) (writing took 3.071651818230748 seconds)
2021-01-07 01:07:08 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-07 01:07:08 | INFO | train | epoch 028 | symm_kl 0.45 | self_kl 0 | self_cv 0 | loss 3.46 | nll_loss 1.072 | ppl 2.1 | wps 15780.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.74808e-05 | gnorm 0.942 | train_wall 344 | wall 10448
2021-01-07 01:07:08 | INFO | fairseq.trainer | begin training epoch 29
2021-01-07 01:07:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:07:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:08:07 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.064, ppl=2.09, wps=11914.9, ups=1.13, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1.74298e-05, gnorm=0.934, train_wall=61, wall=10507
2021-01-07 01:09:09 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.07, ppl=2.1, wps=17096, ups=1.62, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1.73749e-05, gnorm=0.938, train_wall=61, wall=10569
2021-01-07 01:10:10 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=0.453, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.076, ppl=2.11, wps=17188.1, ups=1.63, wpb=10552.2, bsz=366, num_updates=16000, lr=1.73205e-05, gnorm=0.935, train_wall=61, wall=10630
2021-01-07 01:11:12 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.067, ppl=2.1, wps=16919, ups=1.63, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1.72666e-05, gnorm=0.94, train_wall=61, wall=10692
2021-01-07 01:12:13 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.072, ppl=2.1, wps=16826.7, ups=1.62, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1.72133e-05, gnorm=0.951, train_wall=61, wall=10753
2021-01-07 01:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:12:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:12:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:12:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:13:17 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.171 | nll_loss 3.688 | ppl 12.89 | bleu 22.73 | wps 4475.9 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 22.89
2021-01-07 01:13:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:13:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 29 @ 16269 updates, score 22.73) (writing took 2.9400113616138697 seconds)
2021-01-07 01:13:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-07 01:13:20 | INFO | train | epoch 029 | symm_kl 0.448 | self_kl 0 | self_cv 0 | loss 3.457 | nll_loss 1.071 | ppl 2.1 | wps 15796.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.71767e-05 | gnorm 0.939 | train_wall 344 | wall 10820
2021-01-07 01:13:20 | INFO | fairseq.trainer | begin training epoch 30
2021-01-07 01:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:13:42 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.453, nll_loss=1.071, ppl=2.1, wps=11754.3, ups=1.12, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1.71604e-05, gnorm=0.938, train_wall=61, wall=10842
2021-01-07 01:14:44 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.064, ppl=2.09, wps=17293.8, ups=1.63, wpb=10585.6, bsz=366, num_updates=16400, lr=1.7108e-05, gnorm=0.928, train_wall=61, wall=10903
2021-01-07 01:15:45 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.071, ppl=2.1, wps=16968.7, ups=1.63, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1.70561e-05, gnorm=0.934, train_wall=61, wall=10965
2021-01-07 01:16:47 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.468, nll_loss=1.079, ppl=2.11, wps=16843.4, ups=1.62, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1.70046e-05, gnorm=0.945, train_wall=62, wall=11027
2021-01-07 01:17:48 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=0.442, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.065, ppl=2.09, wps=17218.5, ups=1.62, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1.69536e-05, gnorm=0.917, train_wall=61, wall=11088
2021-01-07 01:18:50 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.454, nll_loss=1.074, ppl=2.1, wps=17015.5, ups=1.62, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1.69031e-05, gnorm=0.927, train_wall=62, wall=11150
2021-01-07 01:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:19:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:19:29 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.17 | nll_loss 3.685 | ppl 12.86 | bleu 22.9 | wps 4576.4 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 22.9
2021-01-07 01:19:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:19:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 30 @ 16830 updates, score 22.9) (writing took 4.9795633889734745 seconds)
2021-01-07 01:19:34 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-07 01:19:34 | INFO | train | epoch 030 | symm_kl 0.446 | self_kl 0 | self_cv 0 | loss 3.453 | nll_loss 1.07 | ppl 2.1 | wps 15720.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.6888e-05 | gnorm 0.933 | train_wall 344 | wall 11194
2021-01-07 01:19:34 | INFO | fairseq.trainer | begin training epoch 31
2021-01-07 01:19:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:19:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:20:20 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.075, ppl=2.11, wps=11577.4, ups=1.11, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1.6853e-05, gnorm=0.952, train_wall=60, wall=11240
2021-01-07 01:21:22 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=0.441, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.063, ppl=2.09, wps=17154.9, ups=1.62, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1.68034e-05, gnorm=0.919, train_wall=61, wall=11302
2021-01-07 01:22:24 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.065, ppl=2.09, wps=16943.2, ups=1.62, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1.67542e-05, gnorm=0.93, train_wall=62, wall=11364
2021-01-07 01:23:25 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.471, nll_loss=1.081, ppl=2.12, wps=16959.6, ups=1.63, wpb=10374.8, bsz=340, num_updates=17200, lr=1.67054e-05, gnorm=0.959, train_wall=61, wall=11425
2021-01-07 01:24:26 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.061, ppl=2.09, wps=17182.3, ups=1.63, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1.6657e-05, gnorm=0.919, train_wall=61, wall=11486
2021-01-07 01:25:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:25:43 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.174 | nll_loss 3.688 | ppl 12.89 | bleu 22.78 | wps 4534.3 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 22.9
2021-01-07 01:25:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 31 @ 17391 updates, score 22.78) (writing took 3.095263546332717 seconds)
2021-01-07 01:25:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-07 01:25:46 | INFO | train | epoch 031 | symm_kl 0.444 | self_kl 0 | self_cv 0 | loss 3.449 | nll_loss 1.069 | ppl 2.1 | wps 15815.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.66134e-05 | gnorm 0.932 | train_wall 343 | wall 11566
2021-01-07 01:25:46 | INFO | fairseq.trainer | begin training epoch 32
2021-01-07 01:25:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:25:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:25:55 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.453, nll_loss=1.075, ppl=2.11, wps=11665.3, ups=1.13, wpb=10367.8, bsz=370, num_updates=17400, lr=1.66091e-05, gnorm=0.936, train_wall=61, wall=11575
2021-01-07 01:26:56 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.455, nll_loss=1.071, ppl=2.1, wps=17070.3, ups=1.63, wpb=10453, bsz=358.9, num_updates=17500, lr=1.65616e-05, gnorm=0.924, train_wall=61, wall=11636
2021-01-07 01:27:58 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.05, ppl=2.07, wps=17327.3, ups=1.62, wpb=10691.6, bsz=379.4, num_updates=17600, lr=1.65145e-05, gnorm=0.914, train_wall=61, wall=11698
2021-01-07 01:28:59 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.063, ppl=2.09, wps=17151.2, ups=1.63, wpb=10541.6, bsz=365.9, num_updates=17700, lr=1.64677e-05, gnorm=0.924, train_wall=61, wall=11759
2021-01-07 01:30:01 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=0.449, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.079, ppl=2.11, wps=16885.4, ups=1.62, wpb=10411, bsz=347.3, num_updates=17800, lr=1.64214e-05, gnorm=0.944, train_wall=61, wall=11821
2021-01-07 01:31:03 | INFO | train_inner | epoch 032:    509 / 561 symm_kl=0.441, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.07, ppl=2.1, wps=16848.3, ups=1.62, wpb=10412.4, bsz=390.5, num_updates=17900, lr=1.63755e-05, gnorm=0.936, train_wall=62, wall=11883
2021-01-07 01:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:31:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:31:56 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.171 | nll_loss 3.686 | ppl 12.87 | bleu 22.79 | wps 4571.4 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 22.9
2021-01-07 01:31:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:31:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 32 @ 17952 updates, score 22.79) (writing took 3.097211442887783 seconds)
2021-01-07 01:31:59 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-07 01:31:59 | INFO | train | epoch 032 | symm_kl 0.442 | self_kl 0 | self_cv 0 | loss 3.446 | nll_loss 1.068 | ppl 2.1 | wps 15782.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.63517e-05 | gnorm 0.929 | train_wall 344 | wall 11939
2021-01-07 01:31:59 | INFO | fairseq.trainer | begin training epoch 33
2021-01-07 01:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:32:31 | INFO | train_inner | epoch 033:     48 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.075, ppl=2.11, wps=11837.3, ups=1.13, wpb=10480.2, bsz=382.2, num_updates=18000, lr=1.63299e-05, gnorm=0.917, train_wall=61, wall=11971
2021-01-07 01:33:33 | INFO | train_inner | epoch 033:    148 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.054, ppl=2.08, wps=17076.7, ups=1.63, wpb=10476.8, bsz=378.3, num_updates=18100, lr=1.62848e-05, gnorm=0.914, train_wall=61, wall=12033
2021-01-07 01:34:34 | INFO | train_inner | epoch 033:    248 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.062, ppl=2.09, wps=17089.6, ups=1.62, wpb=10542.1, bsz=374.1, num_updates=18200, lr=1.624e-05, gnorm=0.912, train_wall=61, wall=12094
2021-01-07 01:35:36 | INFO | train_inner | epoch 033:    348 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.449, nll_loss=1.07, ppl=2.1, wps=16969, ups=1.63, wpb=10437.8, bsz=375, num_updates=18300, lr=1.61955e-05, gnorm=0.938, train_wall=61, wall=12156
2021-01-07 01:36:38 | INFO | train_inner | epoch 033:    448 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.073, ppl=2.1, wps=16919.1, ups=1.61, wpb=10483.6, bsz=349.7, num_updates=18400, lr=1.61515e-05, gnorm=0.937, train_wall=62, wall=12218
2021-01-07 01:37:40 | INFO | train_inner | epoch 033:    548 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.071, ppl=2.1, wps=16959.2, ups=1.62, wpb=10480.7, bsz=370.4, num_updates=18500, lr=1.61077e-05, gnorm=0.923, train_wall=62, wall=12280
2021-01-07 01:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:38:09 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.168 | nll_loss 3.682 | ppl 12.83 | bleu 22.9 | wps 4543.5 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 22.9
2021-01-07 01:38:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:38:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 33 @ 18513 updates, score 22.9) (writing took 5.129855655133724 seconds)
2021-01-07 01:38:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-07 01:38:14 | INFO | train | epoch 033 | symm_kl 0.44 | self_kl 0 | self_cv 0 | loss 3.442 | nll_loss 1.067 | ppl 2.09 | wps 15687.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.61021e-05 | gnorm 0.925 | train_wall 344 | wall 12314
2021-01-07 01:38:14 | INFO | fairseq.trainer | begin training epoch 34
2021-01-07 01:38:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:38:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:39:10 | INFO | train_inner | epoch 034:     87 / 561 symm_kl=0.44, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.066, ppl=2.09, wps=11643.9, ups=1.11, wpb=10500.6, bsz=376.6, num_updates=18600, lr=1.60644e-05, gnorm=0.934, train_wall=60, wall=12370
2021-01-07 01:40:11 | INFO | train_inner | epoch 034:    187 / 561 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.082, ppl=2.12, wps=17001.8, ups=1.63, wpb=10453.2, bsz=372.1, num_updates=18700, lr=1.60214e-05, gnorm=0.938, train_wall=61, wall=12431
2021-01-07 01:41:13 | INFO | train_inner | epoch 034:    287 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.065, ppl=2.09, wps=16999.3, ups=1.62, wpb=10473.3, bsz=368.2, num_updates=18800, lr=1.59787e-05, gnorm=0.916, train_wall=61, wall=12493
2021-01-07 01:42:14 | INFO | train_inner | epoch 034:    387 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.057, ppl=2.08, wps=16946.5, ups=1.63, wpb=10398.1, bsz=377.4, num_updates=18900, lr=1.59364e-05, gnorm=0.914, train_wall=61, wall=12554
2021-01-07 01:43:16 | INFO | train_inner | epoch 034:    487 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.042, ppl=2.06, wps=17347.4, ups=1.63, wpb=10664.6, bsz=372.5, num_updates=19000, lr=1.58944e-05, gnorm=0.91, train_wall=61, wall=12616
2021-01-07 01:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:44:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:44:23 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.167 | nll_loss 3.682 | ppl 12.84 | bleu 22.78 | wps 4502.4 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 22.9
2021-01-07 01:44:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 34 @ 19074 updates, score 22.78) (writing took 3.1214924212545156 seconds)
2021-01-07 01:44:26 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-07 01:44:26 | INFO | train | epoch 034 | symm_kl 0.439 | self_kl 0 | self_cv 0 | loss 3.439 | nll_loss 1.066 | ppl 2.09 | wps 15810.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.58635e-05 | gnorm 0.922 | train_wall 343 | wall 12686
2021-01-07 01:44:26 | INFO | fairseq.trainer | begin training epoch 35
2021-01-07 01:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:44:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:44:45 | INFO | train_inner | epoch 035:     26 / 561 symm_kl=0.442, self_kl=0, self_cv=0, loss=3.452, nll_loss=1.076, ppl=2.11, wps=11695.8, ups=1.12, wpb=10418.3, bsz=347, num_updates=19100, lr=1.58527e-05, gnorm=0.926, train_wall=61, wall=12705
2021-01-07 01:45:46 | INFO | train_inner | epoch 035:    126 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.059, ppl=2.08, wps=17069.6, ups=1.63, wpb=10451.9, bsz=365.3, num_updates=19200, lr=1.58114e-05, gnorm=0.918, train_wall=61, wall=12766
2021-01-07 01:46:48 | INFO | train_inner | epoch 035:    226 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.423, nll_loss=1.054, ppl=2.08, wps=16999.1, ups=1.62, wpb=10479.3, bsz=382.2, num_updates=19300, lr=1.57704e-05, gnorm=0.914, train_wall=61, wall=12828
2021-01-07 01:47:49 | INFO | train_inner | epoch 035:    326 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.069, ppl=2.1, wps=17181.5, ups=1.63, wpb=10548.4, bsz=368.7, num_updates=19400, lr=1.57297e-05, gnorm=0.912, train_wall=61, wall=12889
2021-01-07 01:48:51 | INFO | train_inner | epoch 035:    426 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.071, ppl=2.1, wps=17152.3, ups=1.63, wpb=10546.4, bsz=379.4, num_updates=19500, lr=1.56893e-05, gnorm=0.908, train_wall=61, wall=12950
2021-01-07 01:49:52 | INFO | train_inner | epoch 035:    526 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.079, ppl=2.11, wps=16909.9, ups=1.62, wpb=10412.2, bsz=355, num_updates=19600, lr=1.56492e-05, gnorm=0.936, train_wall=61, wall=13012
2021-01-07 01:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:50:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:50:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:50:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:50:35 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.166 | nll_loss 3.681 | ppl 12.82 | bleu 22.9 | wps 4403.9 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 22.9
2021-01-07 01:50:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:50:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 35 @ 19635 updates, score 22.9) (writing took 5.156451912596822 seconds)
2021-01-07 01:50:40 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-07 01:50:40 | INFO | train | epoch 035 | symm_kl 0.437 | self_kl 0 | self_cv 0 | loss 3.436 | nll_loss 1.065 | ppl 2.09 | wps 15710.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.56353e-05 | gnorm 0.917 | train_wall 343 | wall 13060
2021-01-07 01:50:40 | INFO | fairseq.trainer | begin training epoch 36
2021-01-07 01:50:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:50:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:51:23 | INFO | train_inner | epoch 036:     65 / 561 symm_kl=0.436, self_kl=0, self_cv=0, loss=3.433, nll_loss=1.063, ppl=2.09, wps=11438.6, ups=1.11, wpb=10344.6, bsz=362.5, num_updates=19700, lr=1.56094e-05, gnorm=0.93, train_wall=60, wall=13103
2021-01-07 01:52:24 | INFO | train_inner | epoch 036:    165 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.067, ppl=2.1, wps=16926.2, ups=1.63, wpb=10399, bsz=361.7, num_updates=19800, lr=1.557e-05, gnorm=0.936, train_wall=61, wall=13164
2021-01-07 01:53:25 | INFO | train_inner | epoch 036:    265 / 561 symm_kl=0.436, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.066, ppl=2.09, wps=17150.6, ups=1.63, wpb=10532.1, bsz=367.1, num_updates=19900, lr=1.55308e-05, gnorm=0.919, train_wall=61, wall=13225
2021-01-07 01:54:27 | INFO | train_inner | epoch 036:    365 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.061, ppl=2.09, wps=17154.4, ups=1.62, wpb=10583.1, bsz=386.2, num_updates=20000, lr=1.54919e-05, gnorm=0.907, train_wall=61, wall=13287
2021-01-07 01:55:29 | INFO | train_inner | epoch 036:    465 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.061, ppl=2.09, wps=17356.4, ups=1.62, wpb=10695.3, bsz=376, num_updates=20100, lr=1.54533e-05, gnorm=0.913, train_wall=61, wall=13349
2021-01-07 01:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 01:56:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 01:56:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 01:56:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 01:56:49 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.165 | nll_loss 3.679 | ppl 12.81 | bleu 22.78 | wps 4526.2 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 22.9
2021-01-07 01:56:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 01:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 36 @ 20196 updates, score 22.78) (writing took 3.139543406665325 seconds)
2021-01-07 01:56:52 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-07 01:56:52 | INFO | train | epoch 036 | symm_kl 0.435 | self_kl 0 | self_cv 0 | loss 3.433 | nll_loss 1.064 | ppl 2.09 | wps 15821.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.54166e-05 | gnorm 0.922 | train_wall 343 | wall 13432
2021-01-07 01:56:52 | INFO | fairseq.trainer | begin training epoch 37
2021-01-07 01:56:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 01:56:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 01:56:58 | INFO | train_inner | epoch 037:      4 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.439, nll_loss=1.073, ppl=2.1, wps=11574.2, ups=1.13, wpb=10275, bsz=363.2, num_updates=20200, lr=1.5415e-05, gnorm=0.925, train_wall=61, wall=13437
2021-01-07 01:57:59 | INFO | train_inner | epoch 037:    104 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.065, ppl=2.09, wps=17061.7, ups=1.63, wpb=10439.9, bsz=356.5, num_updates=20300, lr=1.5377e-05, gnorm=0.918, train_wall=61, wall=13499
2021-01-07 01:59:00 | INFO | train_inner | epoch 037:    204 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.071, ppl=2.1, wps=17033.7, ups=1.63, wpb=10457.5, bsz=360.4, num_updates=20400, lr=1.53393e-05, gnorm=0.918, train_wall=61, wall=13560
2021-01-07 02:00:02 | INFO | train_inner | epoch 037:    304 / 561 symm_kl=0.428, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.055, ppl=2.08, wps=17096.4, ups=1.63, wpb=10499.2, bsz=371, num_updates=20500, lr=1.53018e-05, gnorm=0.903, train_wall=61, wall=13621
2021-01-07 02:01:03 | INFO | train_inner | epoch 037:    404 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.049, ppl=2.07, wps=17089.4, ups=1.62, wpb=10539.4, bsz=381.9, num_updates=20600, lr=1.52647e-05, gnorm=0.897, train_wall=61, wall=13683
2021-01-07 02:02:05 | INFO | train_inner | epoch 037:    504 / 561 symm_kl=0.44, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.078, ppl=2.11, wps=17128.6, ups=1.62, wpb=10546.3, bsz=368.3, num_updates=20700, lr=1.52277e-05, gnorm=0.928, train_wall=61, wall=13745
2021-01-07 02:02:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:02:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:02:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:03:01 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.165 | nll_loss 3.677 | ppl 12.79 | bleu 22.97 | wps 4582 | wpb 7508.5 | bsz 272.7 | num_updates 20757 | best_bleu 22.97
2021-01-07 02:03:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:03:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:03:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:03:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:03:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:03:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 37 @ 20757 updates, score 22.97) (writing took 5.155650915578008 seconds)
2021-01-07 02:03:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-07 02:03:06 | INFO | train | epoch 037 | symm_kl 0.434 | self_kl 0 | self_cv 0 | loss 3.429 | nll_loss 1.063 | ppl 2.09 | wps 15728 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 20757 | lr 1.52068e-05 | gnorm 0.914 | train_wall 343 | wall 13806
2021-01-07 02:03:06 | INFO | fairseq.trainer | begin training epoch 38
2021-01-07 02:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:03:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:03:35 | INFO | train_inner | epoch 038:     43 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.061, ppl=2.09, wps=11456.8, ups=1.11, wpb=10305.5, bsz=371.7, num_updates=20800, lr=1.51911e-05, gnorm=0.927, train_wall=60, wall=13835
2021-01-07 02:04:36 | INFO | train_inner | epoch 038:    143 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.054, ppl=2.08, wps=17067.2, ups=1.64, wpb=10382.8, bsz=381.5, num_updates=20900, lr=1.51547e-05, gnorm=0.911, train_wall=61, wall=13895
2021-01-07 02:05:37 | INFO | train_inner | epoch 038:    243 / 561 symm_kl=0.436, self_kl=0, self_cv=0, loss=3.441, nll_loss=1.072, ppl=2.1, wps=17225.8, ups=1.62, wpb=10607.4, bsz=373.3, num_updates=21000, lr=1.51186e-05, gnorm=0.925, train_wall=61, wall=13957
2021-01-07 02:06:39 | INFO | train_inner | epoch 038:    343 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.058, ppl=2.08, wps=17123.9, ups=1.63, wpb=10531.3, bsz=375.9, num_updates=21100, lr=1.50827e-05, gnorm=0.902, train_wall=61, wall=14019
2021-01-07 02:07:41 | INFO | train_inner | epoch 038:    443 / 561 symm_kl=0.434, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.072, ppl=2.1, wps=16975.4, ups=1.6, wpb=10578.1, bsz=360.2, num_updates=21200, lr=1.50471e-05, gnorm=0.905, train_wall=62, wall=14081
2021-01-07 02:08:43 | INFO | train_inner | epoch 038:    543 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.053, ppl=2.07, wps=16971.6, ups=1.62, wpb=10474.3, bsz=365.5, num_updates=21300, lr=1.50117e-05, gnorm=0.917, train_wall=62, wall=14143
2021-01-07 02:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:08:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:08:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:08:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:08:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:08:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:08:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:08:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:08:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:08:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:08:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:09:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:09:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:09:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:09:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:09:15 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.164 | nll_loss 3.679 | ppl 12.81 | bleu 22.93 | wps 4566.7 | wpb 7508.5 | bsz 272.7 | num_updates 21318 | best_bleu 22.97
2021-01-07 02:09:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:09:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:09:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:09:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:09:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:09:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 38 @ 21318 updates, score 22.93) (writing took 3.0941170677542686 seconds)
2021-01-07 02:09:18 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-07 02:09:18 | INFO | train | epoch 038 | symm_kl 0.432 | self_kl 0 | self_cv 0 | loss 3.427 | nll_loss 1.062 | ppl 2.09 | wps 15818.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21318 | lr 1.50054e-05 | gnorm 0.914 | train_wall 343 | wall 14178
2021-01-07 02:09:18 | INFO | fairseq.trainer | begin training epoch 39
2021-01-07 02:09:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:09:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:10:11 | INFO | train_inner | epoch 039:     82 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.058, ppl=2.08, wps=11787, ups=1.14, wpb=10369.8, bsz=363, num_updates=21400, lr=1.49766e-05, gnorm=0.91, train_wall=60, wall=14231
2021-01-07 02:11:12 | INFO | train_inner | epoch 039:    182 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.053, ppl=2.07, wps=17302.3, ups=1.63, wpb=10627.7, bsz=378.5, num_updates=21500, lr=1.49417e-05, gnorm=0.893, train_wall=61, wall=14292
2021-01-07 02:12:14 | INFO | train_inner | epoch 039:    282 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.053, ppl=2.08, wps=17016.9, ups=1.62, wpb=10491.2, bsz=374.5, num_updates=21600, lr=1.49071e-05, gnorm=0.907, train_wall=61, wall=14354
2021-01-07 02:13:15 | INFO | train_inner | epoch 039:    382 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.07, ppl=2.1, wps=17175.8, ups=1.62, wpb=10585.4, bsz=374.2, num_updates=21700, lr=1.48727e-05, gnorm=0.899, train_wall=61, wall=14415
2021-01-07 02:14:17 | INFO | train_inner | epoch 039:    482 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.071, ppl=2.1, wps=16894.1, ups=1.63, wpb=10386.4, bsz=370.4, num_updates=21800, lr=1.48386e-05, gnorm=0.924, train_wall=61, wall=14477
2021-01-07 02:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:15:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:15:27 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.165 | nll_loss 3.678 | ppl 12.8 | bleu 22.88 | wps 4557 | wpb 7508.5 | bsz 272.7 | num_updates 21879 | best_bleu 22.97
2021-01-07 02:15:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 39 @ 21879 updates, score 22.88) (writing took 3.078899512067437 seconds)
2021-01-07 02:15:30 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-07 02:15:30 | INFO | train | epoch 039 | symm_kl 0.431 | self_kl 0 | self_cv 0 | loss 3.424 | nll_loss 1.062 | ppl 2.09 | wps 15792.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21879 | lr 1.48118e-05 | gnorm 0.909 | train_wall 344 | wall 14550
2021-01-07 02:15:30 | INFO | fairseq.trainer | begin training epoch 40
2021-01-07 02:15:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:15:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:15:46 | INFO | train_inner | epoch 040:     21 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.065, ppl=2.09, wps=11674.8, ups=1.12, wpb=10399.8, bsz=356.1, num_updates=21900, lr=1.48047e-05, gnorm=0.917, train_wall=62, wall=14566
2021-01-07 02:16:47 | INFO | train_inner | epoch 040:    121 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.442, nll_loss=1.069, ppl=2.1, wps=17003.4, ups=1.64, wpb=10379.6, bsz=338.5, num_updates=22000, lr=1.4771e-05, gnorm=0.926, train_wall=61, wall=14627
2021-01-07 02:17:49 | INFO | train_inner | epoch 040:    221 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.059, ppl=2.08, wps=16979.9, ups=1.62, wpb=10463.9, bsz=383.7, num_updates=22100, lr=1.47375e-05, gnorm=0.899, train_wall=61, wall=14689
2021-01-07 02:18:50 | INFO | train_inner | epoch 040:    321 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.058, ppl=2.08, wps=17130.8, ups=1.62, wpb=10604.3, bsz=372.6, num_updates=22200, lr=1.47043e-05, gnorm=0.902, train_wall=62, wall=14750
2021-01-07 02:19:52 | INFO | train_inner | epoch 040:    421 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.057, ppl=2.08, wps=17213.9, ups=1.63, wpb=10543, bsz=368, num_updates=22300, lr=1.46713e-05, gnorm=0.912, train_wall=61, wall=14812
2021-01-07 02:20:54 | INFO | train_inner | epoch 040:    521 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.427, nll_loss=1.069, ppl=2.1, wps=17007.8, ups=1.62, wpb=10508.8, bsz=389.1, num_updates=22400, lr=1.46385e-05, gnorm=0.901, train_wall=62, wall=14873
2021-01-07 02:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:21:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:21:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:21:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:21:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:21:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:21:39 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.162 | nll_loss 3.674 | ppl 12.77 | bleu 22.94 | wps 4500.5 | wpb 7508.5 | bsz 272.7 | num_updates 22440 | best_bleu 22.97
2021-01-07 02:21:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:21:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:21:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:21:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 40 @ 22440 updates, score 22.94) (writing took 3.067877048626542 seconds)
2021-01-07 02:21:42 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-07 02:21:42 | INFO | train | epoch 040 | symm_kl 0.429 | self_kl 0 | self_cv 0 | loss 3.421 | nll_loss 1.061 | ppl 2.09 | wps 15797.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 22440 | lr 1.46254e-05 | gnorm 0.908 | train_wall 344 | wall 14922
2021-01-07 02:21:42 | INFO | fairseq.trainer | begin training epoch 41
2021-01-07 02:21:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:21:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:22:22 | INFO | train_inner | epoch 041:     60 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.057, ppl=2.08, wps=11762.7, ups=1.13, wpb=10385.6, bsz=373, num_updates=22500, lr=1.46059e-05, gnorm=0.9, train_wall=61, wall=14962
2021-01-07 02:23:23 | INFO | train_inner | epoch 041:    160 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.061, ppl=2.09, wps=17095.6, ups=1.63, wpb=10494.1, bsz=345.8, num_updates=22600, lr=1.45736e-05, gnorm=0.936, train_wall=61, wall=15023
2021-01-07 02:24:25 | INFO | train_inner | epoch 041:    260 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.041, ppl=2.06, wps=17165.9, ups=1.63, wpb=10557.1, bsz=380.2, num_updates=22700, lr=1.45414e-05, gnorm=0.898, train_wall=61, wall=15085
2021-01-07 02:25:26 | INFO | train_inner | epoch 041:    360 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.068, ppl=2.1, wps=17152, ups=1.64, wpb=10438.5, bsz=394.9, num_updates=22800, lr=1.45095e-05, gnorm=0.891, train_wall=61, wall=15145
2021-01-07 02:26:27 | INFO | train_inner | epoch 041:    460 / 561 symm_kl=0.434, self_kl=0, self_cv=0, loss=3.438, nll_loss=1.072, ppl=2.1, wps=17048.1, ups=1.63, wpb=10456.4, bsz=370.5, num_updates=22900, lr=1.44778e-05, gnorm=0.928, train_wall=61, wall=15207
2021-01-07 02:27:29 | INFO | train_inner | epoch 041:    560 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.059, ppl=2.08, wps=17035.7, ups=1.62, wpb=10540.6, bsz=353.5, num_updates=23000, lr=1.44463e-05, gnorm=0.918, train_wall=62, wall=15269
2021-01-07 02:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:27:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:27:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:27:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:27:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:27:51 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.164 | nll_loss 3.675 | ppl 12.77 | bleu 22.93 | wps 4478.4 | wpb 7508.5 | bsz 272.7 | num_updates 23001 | best_bleu 22.97
2021-01-07 02:27:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:27:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:27:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:27:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 41 @ 23001 updates, score 22.93) (writing took 3.1072728019207716 seconds)
2021-01-07 02:27:54 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-07 02:27:54 | INFO | train | epoch 041 | symm_kl 0.428 | self_kl 0 | self_cv 0 | loss 3.419 | nll_loss 1.06 | ppl 2.09 | wps 15830.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23001 | lr 1.4446e-05 | gnorm 0.913 | train_wall 343 | wall 15294
2021-01-07 02:27:54 | INFO | fairseq.trainer | begin training epoch 42
2021-01-07 02:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:27:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:28:57 | INFO | train_inner | epoch 042:     99 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.053, ppl=2.07, wps=11855.9, ups=1.13, wpb=10470.5, bsz=353.7, num_updates=23100, lr=1.4415e-05, gnorm=0.906, train_wall=60, wall=15357
2021-01-07 02:29:59 | INFO | train_inner | epoch 042:    199 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.065, ppl=2.09, wps=16881.8, ups=1.62, wpb=10395.3, bsz=364.5, num_updates=23200, lr=1.43839e-05, gnorm=0.918, train_wall=61, wall=15419
2021-01-07 02:31:00 | INFO | train_inner | epoch 042:    299 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.052, ppl=2.07, wps=17105.6, ups=1.63, wpb=10517.8, bsz=379.2, num_updates=23300, lr=1.4353e-05, gnorm=0.904, train_wall=61, wall=15480
2021-01-07 02:32:02 | INFO | train_inner | epoch 042:    399 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.06, ppl=2.09, wps=17258.3, ups=1.63, wpb=10619.2, bsz=381.1, num_updates=23400, lr=1.43223e-05, gnorm=0.894, train_wall=61, wall=15542
2021-01-07 02:33:03 | INFO | train_inner | epoch 042:    499 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.057, ppl=2.08, wps=16996.1, ups=1.63, wpb=10432.1, bsz=368, num_updates=23500, lr=1.42918e-05, gnorm=0.898, train_wall=61, wall=15603
2021-01-07 02:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:33:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:33:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:33:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:33:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:33:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:33:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:33:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:33:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:33:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:33:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:34:02 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.16 | nll_loss 3.674 | ppl 12.76 | bleu 22.9 | wps 4502.2 | wpb 7508.5 | bsz 272.7 | num_updates 23562 | best_bleu 22.97
2021-01-07 02:34:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:34:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:34:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:34:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:34:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:34:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 42 @ 23562 updates, score 22.9) (writing took 3.0642162449657917 seconds)
2021-01-07 02:34:05 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-07 02:34:05 | INFO | train | epoch 042 | symm_kl 0.426 | self_kl 0 | self_cv 0 | loss 3.417 | nll_loss 1.06 | ppl 2.08 | wps 15828.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23562 | lr 1.4273e-05 | gnorm 0.904 | train_wall 343 | wall 15665
2021-01-07 02:34:05 | INFO | fairseq.trainer | begin training epoch 43
2021-01-07 02:34:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:34:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:34:31 | INFO | train_inner | epoch 043:     38 / 561 symm_kl=0.428, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.068, ppl=2.1, wps=11693.6, ups=1.13, wpb=10325.9, bsz=362.3, num_updates=23600, lr=1.42615e-05, gnorm=0.913, train_wall=61, wall=15691
2021-01-07 02:35:33 | INFO | train_inner | epoch 043:    138 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.051, ppl=2.07, wps=17024, ups=1.62, wpb=10524.1, bsz=376.6, num_updates=23700, lr=1.42314e-05, gnorm=0.892, train_wall=62, wall=15753
2021-01-07 02:36:35 | INFO | train_inner | epoch 043:    238 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.062, ppl=2.09, wps=16957.8, ups=1.62, wpb=10450.7, bsz=352.5, num_updates=23800, lr=1.42014e-05, gnorm=0.915, train_wall=61, wall=15815
2021-01-07 02:37:36 | INFO | train_inner | epoch 043:    338 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.067, ppl=2.1, wps=17233.2, ups=1.63, wpb=10589.2, bsz=368.9, num_updates=23900, lr=1.41717e-05, gnorm=0.892, train_wall=61, wall=15876
2021-01-07 02:38:38 | INFO | train_inner | epoch 043:    438 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.063, ppl=2.09, wps=16927.7, ups=1.63, wpb=10396.9, bsz=384.3, num_updates=24000, lr=1.41421e-05, gnorm=0.893, train_wall=61, wall=15938
2021-01-07 02:39:40 | INFO | train_inner | epoch 043:    538 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.056, ppl=2.08, wps=17093.8, ups=1.62, wpb=10570.1, bsz=376.1, num_updates=24100, lr=1.41128e-05, gnorm=0.894, train_wall=62, wall=15999
2021-01-07 02:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:39:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:40:15 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.159 | nll_loss 3.671 | ppl 12.74 | bleu 22.92 | wps 4551.7 | wpb 7508.5 | bsz 272.7 | num_updates 24123 | best_bleu 22.97
2021-01-07 02:40:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:40:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:40:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:40:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:40:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 43 @ 24123 updates, score 22.92) (writing took 3.1264930069446564 seconds)
2021-01-07 02:40:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-07 02:40:18 | INFO | train | epoch 043 | symm_kl 0.425 | self_kl 0 | self_cv 0 | loss 3.414 | nll_loss 1.059 | ppl 2.08 | wps 15791.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 24123 | lr 1.4106e-05 | gnorm 0.901 | train_wall 344 | wall 16038
2021-01-07 02:40:18 | INFO | fairseq.trainer | begin training epoch 44
2021-01-07 02:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:41:08 | INFO | train_inner | epoch 044:     77 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.058, ppl=2.08, wps=11800.1, ups=1.13, wpb=10441.4, bsz=372.2, num_updates=24200, lr=1.40836e-05, gnorm=0.919, train_wall=61, wall=16088
2021-01-07 02:42:10 | INFO | train_inner | epoch 044:    177 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.035, ppl=2.05, wps=16952.2, ups=1.62, wpb=10486.7, bsz=363.9, num_updates=24300, lr=1.40546e-05, gnorm=0.892, train_wall=62, wall=16150
2021-01-07 02:43:12 | INFO | train_inner | epoch 044:    277 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.032, ppl=2.04, wps=17031.5, ups=1.62, wpb=10509.5, bsz=373.7, num_updates=24400, lr=1.40257e-05, gnorm=0.888, train_wall=61, wall=16211
2021-01-07 02:44:13 | INFO | train_inner | epoch 044:    377 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.41, nll_loss=1.061, ppl=2.09, wps=17107.8, ups=1.63, wpb=10504.8, bsz=386.4, num_updates=24500, lr=1.39971e-05, gnorm=0.889, train_wall=61, wall=16273
2021-01-07 02:45:14 | INFO | train_inner | epoch 044:    477 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.447, nll_loss=1.087, ppl=2.12, wps=17038.7, ups=1.63, wpb=10427.2, bsz=360.4, num_updates=24600, lr=1.39686e-05, gnorm=0.916, train_wall=61, wall=16334
2021-01-07 02:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:46:27 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.161 | nll_loss 3.671 | ppl 12.74 | bleu 23.01 | wps 4545.8 | wpb 7508.5 | bsz 272.7 | num_updates 24684 | best_bleu 23.01
2021-01-07 02:46:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:46:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 44 @ 24684 updates, score 23.01) (writing took 4.9144504722207785 seconds)
2021-01-07 02:46:32 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-07 02:46:32 | INFO | train | epoch 044 | symm_kl 0.424 | self_kl 0 | self_cv 0 | loss 3.411 | nll_loss 1.058 | ppl 2.08 | wps 15710.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 24684 | lr 1.39448e-05 | gnorm 0.903 | train_wall 344 | wall 16412
2021-01-07 02:46:32 | INFO | fairseq.trainer | begin training epoch 45
2021-01-07 02:46:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:46:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:46:45 | INFO | train_inner | epoch 045:     16 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.435, nll_loss=1.073, ppl=2.1, wps=11536.9, ups=1.1, wpb=10489.5, bsz=351.2, num_updates=24700, lr=1.39403e-05, gnorm=0.924, train_wall=61, wall=16425
2021-01-07 02:47:46 | INFO | train_inner | epoch 045:    116 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.054, ppl=2.08, wps=17050.7, ups=1.64, wpb=10419.3, bsz=373.3, num_updates=24800, lr=1.39122e-05, gnorm=0.885, train_wall=61, wall=16486
2021-01-07 02:48:48 | INFO | train_inner | epoch 045:    216 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.054, ppl=2.08, wps=17080.4, ups=1.63, wpb=10490.5, bsz=385.9, num_updates=24900, lr=1.38842e-05, gnorm=0.904, train_wall=61, wall=16548
2021-01-07 02:49:49 | INFO | train_inner | epoch 045:    316 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.434, nll_loss=1.075, ppl=2.11, wps=17016.2, ups=1.63, wpb=10437, bsz=359.9, num_updates=25000, lr=1.38564e-05, gnorm=0.911, train_wall=61, wall=16609
2021-01-07 02:50:51 | INFO | train_inner | epoch 045:    416 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.045, ppl=2.06, wps=17056.1, ups=1.62, wpb=10557.3, bsz=367.8, num_updates=25100, lr=1.38288e-05, gnorm=0.912, train_wall=62, wall=16671
2021-01-07 02:51:53 | INFO | train_inner | epoch 045:    516 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.054, ppl=2.08, wps=17067.6, ups=1.61, wpb=10577.4, bsz=369.7, num_updates=25200, lr=1.38013e-05, gnorm=0.887, train_wall=62, wall=16733
2021-01-07 02:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:52:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:52:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:52:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:52:41 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.161 | nll_loss 3.673 | ppl 12.75 | bleu 22.86 | wps 4549.3 | wpb 7508.5 | bsz 272.7 | num_updates 25245 | best_bleu 23.01
2021-01-07 02:52:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 45 @ 25245 updates, score 22.86) (writing took 3.059548592194915 seconds)
2021-01-07 02:52:44 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-07 02:52:44 | INFO | train | epoch 045 | symm_kl 0.423 | self_kl 0 | self_cv 0 | loss 3.409 | nll_loss 1.057 | ppl 2.08 | wps 15798.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 25245 | lr 1.3789e-05 | gnorm 0.901 | train_wall 344 | wall 16784
2021-01-07 02:52:44 | INFO | fairseq.trainer | begin training epoch 46
2021-01-07 02:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:53:21 | INFO | train_inner | epoch 046:     55 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.05, ppl=2.07, wps=11938.3, ups=1.14, wpb=10483.7, bsz=360.6, num_updates=25300, lr=1.3774e-05, gnorm=0.905, train_wall=60, wall=16821
2021-01-07 02:54:22 | INFO | train_inner | epoch 046:    155 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.06, ppl=2.09, wps=16735.1, ups=1.62, wpb=10304.7, bsz=379.3, num_updates=25400, lr=1.37469e-05, gnorm=0.913, train_wall=61, wall=16882
2021-01-07 02:55:24 | INFO | train_inner | epoch 046:    255 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.048, ppl=2.07, wps=17144.4, ups=1.62, wpb=10582.9, bsz=378.7, num_updates=25500, lr=1.37199e-05, gnorm=0.886, train_wall=62, wall=16944
2021-01-07 02:56:26 | INFO | train_inner | epoch 046:    355 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.058, ppl=2.08, wps=17227.5, ups=1.62, wpb=10612.9, bsz=365.6, num_updates=25600, lr=1.36931e-05, gnorm=0.909, train_wall=61, wall=17005
2021-01-07 02:57:27 | INFO | train_inner | epoch 046:    455 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.056, ppl=2.08, wps=17068.2, ups=1.63, wpb=10503, bsz=362.3, num_updates=25700, lr=1.36664e-05, gnorm=0.905, train_wall=61, wall=17067
2021-01-07 02:58:29 | INFO | train_inner | epoch 046:    555 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.064, ppl=2.09, wps=16897.7, ups=1.63, wpb=10397.9, bsz=371.5, num_updates=25800, lr=1.36399e-05, gnorm=0.895, train_wall=61, wall=17129
2021-01-07 02:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 02:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 02:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 02:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 02:58:53 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.161 | nll_loss 3.67 | ppl 12.73 | bleu 23.05 | wps 4534 | wpb 7508.5 | bsz 272.7 | num_updates 25806 | best_bleu 23.05
2021-01-07 02:58:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 02:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 46 @ 25806 updates, score 23.05) (writing took 5.156013702973723 seconds)
2021-01-07 02:58:59 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-07 02:58:59 | INFO | train | epoch 046 | symm_kl 0.422 | self_kl 0 | self_cv 0 | loss 3.407 | nll_loss 1.056 | ppl 2.08 | wps 15719.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 25806 | lr 1.36383e-05 | gnorm 0.902 | train_wall 343 | wall 17158
2021-01-07 02:58:59 | INFO | fairseq.trainer | begin training epoch 47
2021-01-07 02:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 02:59:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 02:59:59 | INFO | train_inner | epoch 047:     94 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.052, ppl=2.07, wps=11724.7, ups=1.11, wpb=10585.2, bsz=362.2, num_updates=25900, lr=1.36135e-05, gnorm=0.894, train_wall=61, wall=17219
2021-01-07 03:01:00 | INFO | train_inner | epoch 047:    194 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.059, ppl=2.08, wps=17040.8, ups=1.63, wpb=10442.5, bsz=350.3, num_updates=26000, lr=1.35873e-05, gnorm=0.919, train_wall=61, wall=17280
2021-01-07 03:02:02 | INFO | train_inner | epoch 047:    294 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.051, ppl=2.07, wps=17032.7, ups=1.62, wpb=10503.2, bsz=374.6, num_updates=26100, lr=1.35613e-05, gnorm=0.881, train_wall=61, wall=17342
2021-01-07 03:03:04 | INFO | train_inner | epoch 047:    394 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.056, ppl=2.08, wps=16950.6, ups=1.62, wpb=10454.3, bsz=371.8, num_updates=26200, lr=1.35354e-05, gnorm=0.898, train_wall=61, wall=17403
2021-01-07 03:04:06 | INFO | train_inner | epoch 047:    494 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.052, ppl=2.07, wps=16912.4, ups=1.61, wpb=10484.2, bsz=388.4, num_updates=26300, lr=1.35096e-05, gnorm=0.881, train_wall=62, wall=17465
2021-01-07 03:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:04:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:04:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:04:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:04:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:04:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:04:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:05:09 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.161 | nll_loss 3.672 | ppl 12.75 | bleu 22.85 | wps 4411.5 | wpb 7508.5 | bsz 272.7 | num_updates 26367 | best_bleu 23.05
2021-01-07 03:05:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:05:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:05:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:05:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 47 @ 26367 updates, score 22.85) (writing took 3.0741830486804247 seconds)
2021-01-07 03:05:12 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-07 03:05:12 | INFO | train | epoch 047 | symm_kl 0.42 | self_kl 0 | self_cv 0 | loss 3.405 | nll_loss 1.056 | ppl 2.08 | wps 15749.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 26367 | lr 1.34924e-05 | gnorm 0.897 | train_wall 344 | wall 17532
2021-01-07 03:05:12 | INFO | fairseq.trainer | begin training epoch 48
2021-01-07 03:05:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:05:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:05:35 | INFO | train_inner | epoch 048:     33 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.055, ppl=2.08, wps=11673.4, ups=1.12, wpb=10450.7, bsz=374.3, num_updates=26400, lr=1.3484e-05, gnorm=0.911, train_wall=61, wall=17555
2021-01-07 03:06:37 | INFO | train_inner | epoch 048:    133 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.065, ppl=2.09, wps=16925.4, ups=1.63, wpb=10402.3, bsz=369.9, num_updates=26500, lr=1.34585e-05, gnorm=0.902, train_wall=61, wall=17616
2021-01-07 03:07:38 | INFO | train_inner | epoch 048:    233 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.058, ppl=2.08, wps=17013.3, ups=1.62, wpb=10517.5, bsz=360.6, num_updates=26600, lr=1.34332e-05, gnorm=0.895, train_wall=62, wall=17678
2021-01-07 03:08:40 | INFO | train_inner | epoch 048:    333 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.064, ppl=2.09, wps=16919.4, ups=1.62, wpb=10463.7, bsz=354.2, num_updates=26700, lr=1.3408e-05, gnorm=0.905, train_wall=62, wall=17740
2021-01-07 03:09:43 | INFO | train_inner | epoch 048:    433 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.047, ppl=2.07, wps=16770.4, ups=1.59, wpb=10537.4, bsz=385, num_updates=26800, lr=1.3383e-05, gnorm=0.891, train_wall=63, wall=17803
2021-01-07 03:10:45 | INFO | train_inner | epoch 048:    533 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.053, ppl=2.07, wps=16902.3, ups=1.61, wpb=10468.4, bsz=375.1, num_updates=26900, lr=1.33581e-05, gnorm=0.896, train_wall=62, wall=17865
2021-01-07 03:11:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:11:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:11:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:11:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:11:23 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.158 | nll_loss 3.67 | ppl 12.73 | bleu 22.95 | wps 4517.5 | wpb 7508.5 | bsz 272.7 | num_updates 26928 | best_bleu 23.05
2021-01-07 03:11:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:11:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:11:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:11:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 48 @ 26928 updates, score 22.95) (writing took 3.070353504270315 seconds)
2021-01-07 03:11:26 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-07 03:11:26 | INFO | train | epoch 048 | symm_kl 0.419 | self_kl 0 | self_cv 0 | loss 3.403 | nll_loss 1.055 | ppl 2.08 | wps 15704.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 26928 | lr 1.33511e-05 | gnorm 0.897 | train_wall 346 | wall 17906
2021-01-07 03:11:26 | INFO | fairseq.trainer | begin training epoch 49
2021-01-07 03:11:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:11:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:12:14 | INFO | train_inner | epoch 049:     72 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.045, ppl=2.06, wps=11816.5, ups=1.13, wpb=10466.8, bsz=374.3, num_updates=27000, lr=1.33333e-05, gnorm=0.895, train_wall=61, wall=17953
2021-01-07 03:13:15 | INFO | train_inner | epoch 049:    172 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.054, ppl=2.08, wps=16794.8, ups=1.61, wpb=10408.2, bsz=375.4, num_updates=27100, lr=1.33087e-05, gnorm=0.891, train_wall=62, wall=18015
2021-01-07 03:14:17 | INFO | train_inner | epoch 049:    272 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.069, ppl=2.1, wps=17007.6, ups=1.62, wpb=10517.2, bsz=356.2, num_updates=27200, lr=1.32842e-05, gnorm=0.9, train_wall=62, wall=18077
2021-01-07 03:15:19 | INFO | train_inner | epoch 049:    372 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.058, ppl=2.08, wps=16849.9, ups=1.62, wpb=10381.6, bsz=372.5, num_updates=27300, lr=1.32599e-05, gnorm=0.892, train_wall=61, wall=18139
2021-01-07 03:16:20 | INFO | train_inner | epoch 049:    472 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.046, ppl=2.07, wps=17167.4, ups=1.63, wpb=10545.9, bsz=360.4, num_updates=27400, lr=1.32357e-05, gnorm=0.885, train_wall=61, wall=18200
2021-01-07 03:17:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:17:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:17:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:17:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:17:36 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.158 | nll_loss 3.669 | ppl 12.72 | bleu 22.87 | wps 4536.6 | wpb 7508.5 | bsz 272.7 | num_updates 27489 | best_bleu 23.05
2021-01-07 03:17:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:17:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 49 @ 27489 updates, score 22.87) (writing took 3.129592824727297 seconds)
2021-01-07 03:17:39 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-07 03:17:39 | INFO | train | epoch 049 | symm_kl 0.418 | self_kl 0 | self_cv 0 | loss 3.401 | nll_loss 1.055 | ppl 2.08 | wps 15767.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 27489 | lr 1.32142e-05 | gnorm 0.892 | train_wall 344 | wall 18279
2021-01-07 03:17:39 | INFO | fairseq.trainer | begin training epoch 50
2021-01-07 03:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:17:49 | INFO | train_inner | epoch 050:     11 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.054, ppl=2.08, wps=11800.8, ups=1.12, wpb=10504.5, bsz=380.2, num_updates=27500, lr=1.32116e-05, gnorm=0.889, train_wall=61, wall=18289
2021-01-07 03:18:50 | INFO | train_inner | epoch 050:    111 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.056, ppl=2.08, wps=17264.7, ups=1.64, wpb=10546.8, bsz=356.6, num_updates=27600, lr=1.31876e-05, gnorm=0.9, train_wall=61, wall=18350
2021-01-07 03:19:52 | INFO | train_inner | epoch 050:    211 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.047, ppl=2.07, wps=16986, ups=1.63, wpb=10440.7, bsz=360.7, num_updates=27700, lr=1.31638e-05, gnorm=0.887, train_wall=61, wall=18412
2021-01-07 03:20:54 | INFO | train_inner | epoch 050:    311 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.052, ppl=2.07, wps=17084.4, ups=1.62, wpb=10539.4, bsz=374.1, num_updates=27800, lr=1.31401e-05, gnorm=0.882, train_wall=61, wall=18474
2021-01-07 03:21:55 | INFO | train_inner | epoch 050:    411 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.048, ppl=2.07, wps=17052.9, ups=1.63, wpb=10476, bsz=379.4, num_updates=27900, lr=1.31165e-05, gnorm=0.883, train_wall=61, wall=18535
2021-01-07 03:22:57 | INFO | train_inner | epoch 050:    511 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.055, ppl=2.08, wps=17083.6, ups=1.62, wpb=10527.3, bsz=380.4, num_updates=28000, lr=1.30931e-05, gnorm=0.887, train_wall=61, wall=18597
2021-01-07 03:23:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:23:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:23:48 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.16 | nll_loss 3.668 | ppl 12.71 | bleu 22.85 | wps 4542.2 | wpb 7508.5 | bsz 272.7 | num_updates 28050 | best_bleu 23.05
2021-01-07 03:23:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:23:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:23:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:23:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 50 @ 28050 updates, score 22.85) (writing took 3.109665809199214 seconds)
2021-01-07 03:23:51 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-07 03:23:51 | INFO | train | epoch 050 | symm_kl 0.418 | self_kl 0 | self_cv 0 | loss 3.399 | nll_loss 1.054 | ppl 2.08 | wps 15812.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 28050 | lr 1.30814e-05 | gnorm 0.89 | train_wall 343 | wall 18651
2021-01-07 03:23:51 | INFO | fairseq.trainer | begin training epoch 51
2021-01-07 03:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:23:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:24:25 | INFO | train_inner | epoch 051:     50 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.072, ppl=2.1, wps=11822.8, ups=1.14, wpb=10389.2, bsz=352.9, num_updates=28100, lr=1.30698e-05, gnorm=0.905, train_wall=60, wall=18684
2021-01-07 03:25:26 | INFO | train_inner | epoch 051:    150 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.046, ppl=2.07, wps=17005.2, ups=1.62, wpb=10509.5, bsz=378.9, num_updates=28200, lr=1.30466e-05, gnorm=0.888, train_wall=62, wall=18746
2021-01-07 03:26:28 | INFO | train_inner | epoch 051:    250 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.067, ppl=2.09, wps=17010.7, ups=1.63, wpb=10455.1, bsz=374, num_updates=28300, lr=1.30235e-05, gnorm=0.899, train_wall=61, wall=18808
2021-01-07 03:27:29 | INFO | train_inner | epoch 051:    350 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.057, ppl=2.08, wps=17023, ups=1.63, wpb=10446.7, bsz=371.6, num_updates=28400, lr=1.30005e-05, gnorm=0.889, train_wall=61, wall=18869
2021-01-07 03:28:31 | INFO | train_inner | epoch 051:    450 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.043, ppl=2.06, wps=16917.8, ups=1.61, wpb=10520.1, bsz=360.2, num_updates=28500, lr=1.29777e-05, gnorm=0.879, train_wall=62, wall=18931
2021-01-07 03:29:33 | INFO | train_inner | epoch 051:    550 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.047, ppl=2.07, wps=16994.8, ups=1.61, wpb=10528.9, bsz=374.8, num_updates=28600, lr=1.2955e-05, gnorm=0.885, train_wall=62, wall=18993
2021-01-07 03:29:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:29:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:29:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:29:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:29:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:29:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:29:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:30:01 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.156 | nll_loss 3.666 | ppl 12.7 | bleu 22.93 | wps 4471.9 | wpb 7508.5 | bsz 272.7 | num_updates 28611 | best_bleu 23.05
2021-01-07 03:30:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:30:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:30:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:30:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:30:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:30:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 51 @ 28611 updates, score 22.93) (writing took 3.1375084407627583 seconds)
2021-01-07 03:30:05 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-07 03:30:05 | INFO | train | epoch 051 | symm_kl 0.416 | self_kl 0 | self_cv 0 | loss 3.397 | nll_loss 1.053 | ppl 2.07 | wps 15759.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 28611 | lr 1.29525e-05 | gnorm 0.889 | train_wall 344 | wall 19024
2021-01-07 03:30:05 | INFO | fairseq.trainer | begin training epoch 52
2021-01-07 03:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:31:02 | INFO | train_inner | epoch 052:     89 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.047, ppl=2.07, wps=11877.8, ups=1.13, wpb=10531.1, bsz=358.2, num_updates=28700, lr=1.29324e-05, gnorm=0.895, train_wall=61, wall=19082
2021-01-07 03:32:04 | INFO | train_inner | epoch 052:    189 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.055, ppl=2.08, wps=16979.7, ups=1.61, wpb=10525.3, bsz=379.4, num_updates=28800, lr=1.29099e-05, gnorm=0.901, train_wall=62, wall=19144
2021-01-07 03:33:06 | INFO | train_inner | epoch 052:    289 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.075, ppl=2.11, wps=16973.2, ups=1.62, wpb=10459.8, bsz=346.9, num_updates=28900, lr=1.28876e-05, gnorm=0.908, train_wall=61, wall=19206
2021-01-07 03:34:08 | INFO | train_inner | epoch 052:    389 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.029, ppl=2.04, wps=16931.4, ups=1.61, wpb=10505.1, bsz=372.6, num_updates=29000, lr=1.28654e-05, gnorm=0.876, train_wall=62, wall=19268
2021-01-07 03:35:09 | INFO | train_inner | epoch 052:    489 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.415, nll_loss=1.065, ppl=2.09, wps=16760.4, ups=1.62, wpb=10334.4, bsz=367.1, num_updates=29100, lr=1.28432e-05, gnorm=0.906, train_wall=61, wall=19329
2021-01-07 03:35:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:35:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:35:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:35:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:35:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:35:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:35:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:35:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:35:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:35:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:36:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:36:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:36:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:36:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:36:15 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.155 | nll_loss 3.666 | ppl 12.69 | bleu 23 | wps 4485.3 | wpb 7508.5 | bsz 272.7 | num_updates 29172 | best_bleu 23.05
2021-01-07 03:36:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:36:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:36:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:36:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 52 @ 29172 updates, score 23.0) (writing took 3.1139841079711914 seconds)
2021-01-07 03:36:18 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-07 03:36:18 | INFO | train | epoch 052 | symm_kl 0.415 | self_kl 0 | self_cv 0 | loss 3.395 | nll_loss 1.053 | ppl 2.07 | wps 15743.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 29172 | lr 1.28274e-05 | gnorm 0.895 | train_wall 345 | wall 19398
2021-01-07 03:36:18 | INFO | fairseq.trainer | begin training epoch 53
2021-01-07 03:36:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:36:38 | INFO | train_inner | epoch 053:     28 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.035, ppl=2.05, wps=11891.3, ups=1.13, wpb=10565.3, bsz=394.1, num_updates=29200, lr=1.28212e-05, gnorm=0.876, train_wall=61, wall=19418
2021-01-07 03:37:39 | INFO | train_inner | epoch 053:    128 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.057, ppl=2.08, wps=17258.3, ups=1.64, wpb=10549.4, bsz=362.6, num_updates=29300, lr=1.27993e-05, gnorm=0.906, train_wall=61, wall=19479
2021-01-07 03:38:41 | INFO | train_inner | epoch 053:    228 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.404, nll_loss=1.055, ppl=2.08, wps=16923.2, ups=1.63, wpb=10404.1, bsz=355.6, num_updates=29400, lr=1.27775e-05, gnorm=0.907, train_wall=61, wall=19541
2021-01-07 03:39:43 | INFO | train_inner | epoch 053:    328 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.039, ppl=2.06, wps=17005.1, ups=1.61, wpb=10550, bsz=379.8, num_updates=29500, lr=1.27559e-05, gnorm=0.882, train_wall=62, wall=19603
2021-01-07 03:40:45 | INFO | train_inner | epoch 053:    428 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.042, ppl=2.06, wps=16900.5, ups=1.61, wpb=10490.5, bsz=387.4, num_updates=29600, lr=1.27343e-05, gnorm=0.879, train_wall=62, wall=19665
2021-01-07 03:41:46 | INFO | train_inner | epoch 053:    528 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.068, ppl=2.1, wps=17050.9, ups=1.63, wpb=10444.8, bsz=366.7, num_updates=29700, lr=1.27128e-05, gnorm=0.895, train_wall=61, wall=19726
2021-01-07 03:42:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:42:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:42:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:42:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:42:28 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.156 | nll_loss 3.666 | ppl 12.69 | bleu 22.88 | wps 4517.9 | wpb 7508.5 | bsz 272.7 | num_updates 29733 | best_bleu 23.05
2021-01-07 03:42:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:42:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:42:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:42:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 53 @ 29733 updates, score 22.88) (writing took 3.0730780847370625 seconds)
2021-01-07 03:42:31 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-07 03:42:31 | INFO | train | epoch 053 | symm_kl 0.415 | self_kl 0 | self_cv 0 | loss 3.394 | nll_loss 1.053 | ppl 2.07 | wps 15784.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 29733 | lr 1.27058e-05 | gnorm 0.893 | train_wall 344 | wall 19771
2021-01-07 03:42:31 | INFO | fairseq.trainer | begin training epoch 54
2021-01-07 03:42:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:42:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:43:15 | INFO | train_inner | epoch 054:     67 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.056, ppl=2.08, wps=11802.6, ups=1.13, wpb=10441.9, bsz=359.6, num_updates=29800, lr=1.26915e-05, gnorm=0.893, train_wall=61, wall=19815
2021-01-07 03:44:17 | INFO | train_inner | epoch 054:    167 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.049, ppl=2.07, wps=16934.5, ups=1.61, wpb=10526.1, bsz=368.2, num_updates=29900, lr=1.26702e-05, gnorm=0.878, train_wall=62, wall=19877
2021-01-07 03:45:19 | INFO | train_inner | epoch 054:    267 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.06, ppl=2.08, wps=16959.6, ups=1.62, wpb=10491.1, bsz=372.2, num_updates=30000, lr=1.26491e-05, gnorm=0.899, train_wall=62, wall=19939
2021-01-07 03:46:21 | INFO | train_inner | epoch 054:    367 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.042, ppl=2.06, wps=17025.2, ups=1.61, wpb=10584.5, bsz=376.3, num_updates=30100, lr=1.26281e-05, gnorm=0.877, train_wall=62, wall=20001
2021-01-07 03:47:23 | INFO | train_inner | epoch 054:    467 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.056, ppl=2.08, wps=16963.5, ups=1.62, wpb=10493.2, bsz=368.6, num_updates=30200, lr=1.26072e-05, gnorm=0.896, train_wall=62, wall=20063
2021-01-07 03:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:48:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:48:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:48:42 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.155 | nll_loss 3.666 | ppl 12.69 | bleu 22.94 | wps 4592.8 | wpb 7508.5 | bsz 272.7 | num_updates 30294 | best_bleu 23.05
2021-01-07 03:48:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:48:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 54 @ 30294 updates, score 22.94) (writing took 3.081270184367895 seconds)
2021-01-07 03:48:45 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-07 03:48:45 | INFO | train | epoch 054 | symm_kl 0.413 | self_kl 0 | self_cv 0 | loss 3.391 | nll_loss 1.051 | ppl 2.07 | wps 15723.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 30294 | lr 1.25876e-05 | gnorm 0.889 | train_wall 346 | wall 20145
2021-01-07 03:48:45 | INFO | fairseq.trainer | begin training epoch 55
2021-01-07 03:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:48:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:48:52 | INFO | train_inner | epoch 055:      6 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.053, ppl=2.07, wps=11577.2, ups=1.12, wpb=10303.8, bsz=364.4, num_updates=30300, lr=1.25863e-05, gnorm=0.895, train_wall=62, wall=20152
2021-01-07 03:49:53 | INFO | train_inner | epoch 055:    106 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.061, ppl=2.09, wps=17185.7, ups=1.63, wpb=10511.4, bsz=376.5, num_updates=30400, lr=1.25656e-05, gnorm=0.881, train_wall=61, wall=20213
2021-01-07 03:50:55 | INFO | train_inner | epoch 055:    206 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.047, ppl=2.07, wps=16947.6, ups=1.61, wpb=10500.2, bsz=383.4, num_updates=30500, lr=1.2545e-05, gnorm=0.869, train_wall=62, wall=20275
2021-01-07 03:51:57 | INFO | train_inner | epoch 055:    306 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.049, ppl=2.07, wps=16964.4, ups=1.61, wpb=10505.5, bsz=358.1, num_updates=30600, lr=1.25245e-05, gnorm=0.893, train_wall=62, wall=20337
2021-01-07 03:52:59 | INFO | train_inner | epoch 055:    406 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.046, ppl=2.06, wps=17053.1, ups=1.61, wpb=10591.9, bsz=368.2, num_updates=30700, lr=1.25041e-05, gnorm=0.877, train_wall=62, wall=20399
2021-01-07 03:54:00 | INFO | train_inner | epoch 055:    506 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.052, ppl=2.07, wps=17043.2, ups=1.64, wpb=10394.2, bsz=368.7, num_updates=30800, lr=1.24838e-05, gnorm=0.901, train_wall=61, wall=20460
2021-01-07 03:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 03:54:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:54:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:54:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:54:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:54:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:54:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 03:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 03:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 03:54:55 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.153 | nll_loss 3.664 | ppl 12.68 | bleu 22.99 | wps 4532.7 | wpb 7508.5 | bsz 272.7 | num_updates 30855 | best_bleu 23.05
2021-01-07 03:54:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 03:54:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:54:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:54:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:54:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:54:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 55 @ 30855 updates, score 22.99) (writing took 3.121626438573003 seconds)
2021-01-07 03:54:58 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-07 03:54:58 | INFO | train | epoch 055 | symm_kl 0.413 | self_kl 0 | self_cv 0 | loss 3.39 | nll_loss 1.051 | ppl 2.07 | wps 15761.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 30855 | lr 1.24726e-05 | gnorm 0.887 | train_wall 344 | wall 20518
2021-01-07 03:54:58 | INFO | fairseq.trainer | begin training epoch 56
2021-01-07 03:54:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 03:55:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 03:55:28 | INFO | train_inner | epoch 056:     45 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.057, ppl=2.08, wps=11708.5, ups=1.13, wpb=10331.1, bsz=356.5, num_updates=30900, lr=1.24635e-05, gnorm=0.907, train_wall=60, wall=20548
2021-01-07 03:56:30 | INFO | train_inner | epoch 056:    145 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.05, ppl=2.07, wps=17077.2, ups=1.61, wpb=10597, bsz=377, num_updates=31000, lr=1.24434e-05, gnorm=0.873, train_wall=62, wall=20610
2021-01-07 03:57:32 | INFO | train_inner | epoch 056:    245 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.037, ppl=2.05, wps=17114.8, ups=1.62, wpb=10582.9, bsz=408.2, num_updates=31100, lr=1.24234e-05, gnorm=0.866, train_wall=62, wall=20672
2021-01-07 03:58:33 | INFO | train_inner | epoch 056:    345 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.052, ppl=2.07, wps=16918.2, ups=1.63, wpb=10400.7, bsz=344.2, num_updates=31200, lr=1.24035e-05, gnorm=0.904, train_wall=61, wall=20733
2021-01-07 03:59:35 | INFO | train_inner | epoch 056:    445 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.05, ppl=2.07, wps=16773.3, ups=1.61, wpb=10398.5, bsz=371.8, num_updates=31300, lr=1.23836e-05, gnorm=0.93, train_wall=62, wall=20795
2021-01-07 04:00:37 | INFO | train_inner | epoch 056:    545 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.057, ppl=2.08, wps=17129, ups=1.62, wpb=10556.1, bsz=363.9, num_updates=31400, lr=1.23639e-05, gnorm=0.879, train_wall=61, wall=20857
2021-01-07 04:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:00:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:00:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:01:08 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.155 | nll_loss 3.665 | ppl 12.69 | bleu 22.9 | wps 4493.5 | wpb 7508.5 | bsz 272.7 | num_updates 31416 | best_bleu 23.05
2021-01-07 04:01:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:01:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:01:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:01:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 56 @ 31416 updates, score 22.9) (writing took 3.112998180091381 seconds)
2021-01-07 04:01:11 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-07 04:01:11 | INFO | train | epoch 056 | symm_kl 0.412 | self_kl 0 | self_cv 0 | loss 3.388 | nll_loss 1.051 | ppl 2.07 | wps 15751.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 31416 | lr 1.23608e-05 | gnorm 0.893 | train_wall 345 | wall 20891
2021-01-07 04:01:11 | INFO | fairseq.trainer | begin training epoch 57
2021-01-07 04:01:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:01:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:02:05 | INFO | train_inner | epoch 057:     84 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.052, ppl=2.07, wps=11779.9, ups=1.13, wpb=10411.4, bsz=373.3, num_updates=31500, lr=1.23443e-05, gnorm=0.89, train_wall=61, wall=20945
2021-01-07 04:03:08 | INFO | train_inner | epoch 057:    184 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.058, ppl=2.08, wps=16812.6, ups=1.6, wpb=10476.3, bsz=375.3, num_updates=31600, lr=1.23247e-05, gnorm=0.889, train_wall=62, wall=21008
2021-01-07 04:04:10 | INFO | train_inner | epoch 057:    284 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.049, ppl=2.07, wps=16796.4, ups=1.61, wpb=10418.2, bsz=348.9, num_updates=31700, lr=1.23053e-05, gnorm=0.888, train_wall=62, wall=21070
2021-01-07 04:05:12 | INFO | train_inner | epoch 057:    384 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.048, ppl=2.07, wps=16917.4, ups=1.61, wpb=10518.8, bsz=377.6, num_updates=31800, lr=1.22859e-05, gnorm=0.881, train_wall=62, wall=21132
2021-01-07 04:06:14 | INFO | train_inner | epoch 057:    484 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.051, ppl=2.07, wps=17107.9, ups=1.61, wpb=10597.3, bsz=371.9, num_updates=31900, lr=1.22666e-05, gnorm=0.87, train_wall=62, wall=21194
2021-01-07 04:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:07:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:07:22 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.15 | nll_loss 3.659 | ppl 12.63 | bleu 23 | wps 4502.3 | wpb 7508.5 | bsz 272.7 | num_updates 31977 | best_bleu 23.05
2021-01-07 04:07:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:07:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 57 @ 31977 updates, score 23.0) (writing took 3.1015945095568895 seconds)
2021-01-07 04:07:25 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-07 04:07:25 | INFO | train | epoch 057 | symm_kl 0.411 | self_kl 0 | self_cv 0 | loss 3.387 | nll_loss 1.05 | ppl 2.07 | wps 15715.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 31977 | lr 1.22519e-05 | gnorm 0.883 | train_wall 345 | wall 21265
2021-01-07 04:07:25 | INFO | fairseq.trainer | begin training epoch 58
2021-01-07 04:07:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:07:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:07:43 | INFO | train_inner | epoch 058:     23 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.043, ppl=2.06, wps=11789.6, ups=1.13, wpb=10448.7, bsz=359.8, num_updates=32000, lr=1.22474e-05, gnorm=0.881, train_wall=61, wall=21282
2021-01-07 04:08:44 | INFO | train_inner | epoch 058:    123 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.034, ppl=2.05, wps=17197.3, ups=1.63, wpb=10578.5, bsz=386.2, num_updates=32100, lr=1.22284e-05, gnorm=0.865, train_wall=61, wall=21344
2021-01-07 04:09:46 | INFO | train_inner | epoch 058:    223 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.408, nll_loss=1.066, ppl=2.09, wps=16812.3, ups=1.62, wpb=10363.2, bsz=372.5, num_updates=32200, lr=1.22094e-05, gnorm=0.894, train_wall=61, wall=21406
2021-01-07 04:10:48 | INFO | train_inner | epoch 058:    323 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.044, ppl=2.06, wps=16889.6, ups=1.6, wpb=10573, bsz=359.3, num_updates=32300, lr=1.21904e-05, gnorm=0.879, train_wall=62, wall=21468
2021-01-07 04:11:50 | INFO | train_inner | epoch 058:    423 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.401, nll_loss=1.059, ppl=2.08, wps=16889.3, ups=1.62, wpb=10422.2, bsz=360.6, num_updates=32400, lr=1.21716e-05, gnorm=0.89, train_wall=61, wall=21530
2021-01-07 04:12:51 | INFO | train_inner | epoch 058:    523 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.055, ppl=2.08, wps=17037.6, ups=1.63, wpb=10446.5, bsz=361.1, num_updates=32500, lr=1.21529e-05, gnorm=0.871, train_wall=61, wall=21591
2021-01-07 04:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:13:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:13:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:13:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:13:36 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.152 | nll_loss 3.662 | ppl 12.66 | bleu 22.95 | wps 4450.5 | wpb 7508.5 | bsz 272.7 | num_updates 32538 | best_bleu 23.05
2021-01-07 04:13:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:13:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:13:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 58 @ 32538 updates, score 22.95) (writing took 3.091573180630803 seconds)
2021-01-07 04:13:39 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-07 04:13:39 | INFO | train | epoch 058 | symm_kl 0.41 | self_kl 0 | self_cv 0 | loss 3.385 | nll_loss 1.05 | ppl 2.07 | wps 15730.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 32538 | lr 1.21458e-05 | gnorm 0.877 | train_wall 345 | wall 21639
2021-01-07 04:13:39 | INFO | fairseq.trainer | begin training epoch 59
2021-01-07 04:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:14:20 | INFO | train_inner | epoch 059:     62 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.052, ppl=2.07, wps=11804.1, ups=1.13, wpb=10453.1, bsz=379.3, num_updates=32600, lr=1.21342e-05, gnorm=0.863, train_wall=60, wall=21680
2021-01-07 04:15:21 | INFO | train_inner | epoch 059:    162 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.041, ppl=2.06, wps=17121.5, ups=1.62, wpb=10539.2, bsz=364.2, num_updates=32700, lr=1.21157e-05, gnorm=0.891, train_wall=61, wall=21741
2021-01-07 04:16:23 | INFO | train_inner | epoch 059:    262 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.043, ppl=2.06, wps=16842.3, ups=1.61, wpb=10439.9, bsz=364.1, num_updates=32800, lr=1.20972e-05, gnorm=0.884, train_wall=62, wall=21803
2021-01-07 04:17:25 | INFO | train_inner | epoch 059:    362 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.06, ppl=2.09, wps=16912.5, ups=1.63, wpb=10395.9, bsz=351, num_updates=32900, lr=1.20788e-05, gnorm=0.896, train_wall=61, wall=21865
2021-01-07 04:18:27 | INFO | train_inner | epoch 059:    462 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.031, ppl=2.04, wps=17209.1, ups=1.62, wpb=10653.2, bsz=400.8, num_updates=33000, lr=1.20605e-05, gnorm=0.856, train_wall=62, wall=21927
2021-01-07 04:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:19:49 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.152 | nll_loss 3.66 | ppl 12.64 | bleu 23.02 | wps 4583.6 | wpb 7508.5 | bsz 272.7 | num_updates 33099 | best_bleu 23.05
2021-01-07 04:19:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 59 @ 33099 updates, score 23.02) (writing took 3.098611092194915 seconds)
2021-01-07 04:19:52 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-07 04:19:52 | INFO | train | epoch 059 | symm_kl 0.409 | self_kl 0 | self_cv 0 | loss 3.383 | nll_loss 1.049 | ppl 2.07 | wps 15773 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 33099 | lr 1.20424e-05 | gnorm 0.881 | train_wall 344 | wall 22012
2021-01-07 04:19:52 | INFO | fairseq.trainer | begin training epoch 60
2021-01-07 04:19:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:19:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:19:56 | INFO | train_inner | epoch 060:      1 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.062, ppl=2.09, wps=11670, ups=1.12, wpb=10421, bsz=369.7, num_updates=33100, lr=1.20422e-05, gnorm=0.882, train_wall=62, wall=22016
2021-01-07 04:20:57 | INFO | train_inner | epoch 060:    101 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.042, ppl=2.06, wps=17412.9, ups=1.65, wpb=10558, bsz=375.6, num_updates=33200, lr=1.20241e-05, gnorm=0.873, train_wall=60, wall=22077
2021-01-07 04:21:59 | INFO | train_inner | epoch 060:    201 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.4, nll_loss=1.058, ppl=2.08, wps=16814.1, ups=1.62, wpb=10401.3, bsz=359.1, num_updates=33300, lr=1.2006e-05, gnorm=0.892, train_wall=62, wall=22138
2021-01-07 04:23:00 | INFO | train_inner | epoch 060:    301 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.046, ppl=2.06, wps=17005.9, ups=1.62, wpb=10476.6, bsz=373.2, num_updates=33400, lr=1.1988e-05, gnorm=0.883, train_wall=61, wall=22200
2021-01-07 04:24:02 | INFO | train_inner | epoch 060:    401 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.054, ppl=2.08, wps=17049.5, ups=1.62, wpb=10552.6, bsz=366.6, num_updates=33500, lr=1.19701e-05, gnorm=0.889, train_wall=62, wall=22262
2021-01-07 04:25:04 | INFO | train_inner | epoch 060:    501 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.044, ppl=2.06, wps=16879.6, ups=1.61, wpb=10479.3, bsz=376.6, num_updates=33600, lr=1.19523e-05, gnorm=0.875, train_wall=62, wall=22324
2021-01-07 04:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:26:03 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.154 | nll_loss 3.664 | ppl 12.68 | bleu 22.92 | wps 4500.6 | wpb 7508.5 | bsz 272.7 | num_updates 33660 | best_bleu 23.05
2021-01-07 04:26:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:26:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 60 @ 33660 updates, score 22.92) (writing took 3.1098859645426273 seconds)
2021-01-07 04:26:06 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-07 04:26:06 | INFO | train | epoch 060 | symm_kl 0.408 | self_kl 0 | self_cv 0 | loss 3.382 | nll_loss 1.048 | ppl 2.07 | wps 15750.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 33660 | lr 1.19416e-05 | gnorm 0.885 | train_wall 345 | wall 22386
2021-01-07 04:26:06 | INFO | fairseq.trainer | begin training epoch 61
2021-01-07 04:26:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:26:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:26:33 | INFO | train_inner | epoch 061:     40 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.039, ppl=2.05, wps=11780.6, ups=1.13, wpb=10470.8, bsz=371.7, num_updates=33700, lr=1.19345e-05, gnorm=0.888, train_wall=61, wall=22413
2021-01-07 04:27:35 | INFO | train_inner | epoch 061:    140 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.052, ppl=2.07, wps=16894.1, ups=1.62, wpb=10423.5, bsz=371.8, num_updates=33800, lr=1.19169e-05, gnorm=0.875, train_wall=61, wall=22475
2021-01-07 04:28:37 | INFO | train_inner | epoch 061:    240 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.049, ppl=2.07, wps=16878.1, ups=1.61, wpb=10466.2, bsz=380.2, num_updates=33900, lr=1.18993e-05, gnorm=0.886, train_wall=62, wall=22537
2021-01-07 04:29:39 | INFO | train_inner | epoch 061:    340 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.057, ppl=2.08, wps=16894.6, ups=1.61, wpb=10463.9, bsz=356.2, num_updates=34000, lr=1.18818e-05, gnorm=0.892, train_wall=62, wall=22599
2021-01-07 04:30:41 | INFO | train_inner | epoch 061:    440 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.036, ppl=2.05, wps=17175, ups=1.61, wpb=10642.9, bsz=368.1, num_updates=34100, lr=1.18643e-05, gnorm=0.864, train_wall=62, wall=22661
2021-01-07 04:31:43 | INFO | train_inner | epoch 061:    540 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.045, ppl=2.06, wps=16882.1, ups=1.62, wpb=10444.1, bsz=371.7, num_updates=34200, lr=1.1847e-05, gnorm=0.896, train_wall=62, wall=22722
2021-01-07 04:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:31:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:31:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:31:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:31:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:32:16 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.152 | nll_loss 3.663 | ppl 12.66 | bleu 22.86 | wps 4533.9 | wpb 7508.5 | bsz 272.7 | num_updates 34221 | best_bleu 23.05
2021-01-07 04:32:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:32:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:32:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 61 @ 34221 updates, score 22.86) (writing took 3.100734943524003 seconds)
2021-01-07 04:32:20 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-07 04:32:20 | INFO | train | epoch 061 | symm_kl 0.407 | self_kl 0 | self_cv 0 | loss 3.38 | nll_loss 1.048 | ppl 2.07 | wps 15726.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 34221 | lr 1.18433e-05 | gnorm 0.883 | train_wall 345 | wall 22760
2021-01-07 04:32:20 | INFO | fairseq.trainer | begin training epoch 62
2021-01-07 04:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:32:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:32:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:33:11 | INFO | train_inner | epoch 062:     79 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.055, ppl=2.08, wps=11836.1, ups=1.13, wpb=10437.3, bsz=367.2, num_updates=34300, lr=1.18297e-05, gnorm=0.886, train_wall=60, wall=22811
2021-01-07 04:34:13 | INFO | train_inner | epoch 062:    179 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.035, ppl=2.05, wps=16887.5, ups=1.61, wpb=10489, bsz=366.9, num_updates=34400, lr=1.18125e-05, gnorm=0.882, train_wall=62, wall=22873
2021-01-07 04:35:14 | INFO | train_inner | epoch 062:    279 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.049, ppl=2.07, wps=17107.6, ups=1.62, wpb=10532.2, bsz=370.1, num_updates=34500, lr=1.17954e-05, gnorm=0.868, train_wall=61, wall=22934
2021-01-07 04:36:16 | INFO | train_inner | epoch 062:    379 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.045, ppl=2.06, wps=16948.9, ups=1.62, wpb=10486.1, bsz=382.8, num_updates=34600, lr=1.17783e-05, gnorm=0.874, train_wall=62, wall=22996
2021-01-07 04:37:18 | INFO | train_inner | epoch 062:    479 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.053, ppl=2.08, wps=16870.4, ups=1.62, wpb=10399.9, bsz=357.1, num_updates=34700, lr=1.17613e-05, gnorm=0.882, train_wall=61, wall=23058
2021-01-07 04:38:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:38:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:38:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:38:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:38:30 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.154 | nll_loss 3.664 | ppl 12.68 | bleu 22.9 | wps 4510.7 | wpb 7508.5 | bsz 272.7 | num_updates 34782 | best_bleu 23.05
2021-01-07 04:38:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:38:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 62 @ 34782 updates, score 22.9) (writing took 3.2373580057173967 seconds)
2021-01-07 04:38:33 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-07 04:38:33 | INFO | train | epoch 062 | symm_kl 0.407 | self_kl 0 | self_cv 0 | loss 3.378 | nll_loss 1.047 | ppl 2.07 | wps 15747.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 34782 | lr 1.17474e-05 | gnorm 0.876 | train_wall 345 | wall 23133
2021-01-07 04:38:33 | INFO | fairseq.trainer | begin training epoch 63
2021-01-07 04:38:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:38:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:38:47 | INFO | train_inner | epoch 063:     18 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.042, ppl=2.06, wps=11738.4, ups=1.12, wpb=10483.3, bsz=366, num_updates=34800, lr=1.17444e-05, gnorm=0.877, train_wall=61, wall=23147
2021-01-07 04:39:48 | INFO | train_inner | epoch 063:    118 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.045, ppl=2.06, wps=17437.5, ups=1.63, wpb=10670.8, bsz=364.8, num_updates=34900, lr=1.17276e-05, gnorm=0.877, train_wall=61, wall=23208
2021-01-07 04:40:50 | INFO | train_inner | epoch 063:    218 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.046, ppl=2.06, wps=16999, ups=1.62, wpb=10481.1, bsz=372.8, num_updates=35000, lr=1.17108e-05, gnorm=0.889, train_wall=61, wall=23270
2021-01-07 04:41:52 | INFO | train_inner | epoch 063:    318 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.041, ppl=2.06, wps=16946, ups=1.61, wpb=10497.3, bsz=383.2, num_updates=35100, lr=1.16941e-05, gnorm=0.875, train_wall=62, wall=23332
2021-01-07 04:42:54 | INFO | train_inner | epoch 063:    418 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.057, ppl=2.08, wps=16812.5, ups=1.62, wpb=10385, bsz=363.8, num_updates=35200, lr=1.16775e-05, gnorm=0.871, train_wall=62, wall=23394
2021-01-07 04:43:56 | INFO | train_inner | epoch 063:    518 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.058, ppl=2.08, wps=16729.2, ups=1.6, wpb=10433, bsz=380.1, num_updates=35300, lr=1.16609e-05, gnorm=0.902, train_wall=62, wall=23456
2021-01-07 04:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:44:43 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.15 | nll_loss 3.658 | ppl 12.62 | bleu 23.03 | wps 4545.2 | wpb 7508.5 | bsz 272.7 | num_updates 35343 | best_bleu 23.05
2021-01-07 04:44:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:44:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 63 @ 35343 updates, score 23.03) (writing took 3.124397773295641 seconds)
2021-01-07 04:44:47 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-07 04:44:47 | INFO | train | epoch 063 | symm_kl 0.406 | self_kl 0 | self_cv 0 | loss 3.377 | nll_loss 1.046 | ppl 2.07 | wps 15743.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 35343 | lr 1.16538e-05 | gnorm 0.882 | train_wall 345 | wall 23507
2021-01-07 04:44:47 | INFO | fairseq.trainer | begin training epoch 64
2021-01-07 04:44:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:44:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:45:24 | INFO | train_inner | epoch 064:     57 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.025, ppl=2.04, wps=11894.9, ups=1.13, wpb=10499.9, bsz=366.3, num_updates=35400, lr=1.16445e-05, gnorm=0.876, train_wall=60, wall=23544
2021-01-07 04:46:26 | INFO | train_inner | epoch 064:    157 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.048, ppl=2.07, wps=16889.2, ups=1.61, wpb=10487, bsz=366.4, num_updates=35500, lr=1.1628e-05, gnorm=0.888, train_wall=62, wall=23606
2021-01-07 04:47:28 | INFO | train_inner | epoch 064:    257 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.049, ppl=2.07, wps=16955.2, ups=1.62, wpb=10474.1, bsz=362.2, num_updates=35600, lr=1.16117e-05, gnorm=0.881, train_wall=62, wall=23668
2021-01-07 04:48:30 | INFO | train_inner | epoch 064:    357 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.051, ppl=2.07, wps=17139.8, ups=1.62, wpb=10589.6, bsz=368.4, num_updates=35700, lr=1.15954e-05, gnorm=0.871, train_wall=62, wall=23730
2021-01-07 04:49:32 | INFO | train_inner | epoch 064:    457 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.048, ppl=2.07, wps=16836.5, ups=1.62, wpb=10413.9, bsz=365.5, num_updates=35800, lr=1.15792e-05, gnorm=0.898, train_wall=62, wall=23792
2021-01-07 04:50:34 | INFO | train_inner | epoch 064:    557 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.041, ppl=2.06, wps=16968.4, ups=1.62, wpb=10487.9, bsz=378.7, num_updates=35900, lr=1.15631e-05, gnorm=0.868, train_wall=62, wall=23854
2021-01-07 04:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:50:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:50:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:50:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:50:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:50:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:50:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:50:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:50:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:50:57 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.153 | nll_loss 3.661 | ppl 12.65 | bleu 22.98 | wps 4608.8 | wpb 7508.5 | bsz 272.7 | num_updates 35904 | best_bleu 23.05
2021-01-07 04:50:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:50:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:50:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:51:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:51:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:51:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 64 @ 35904 updates, score 22.98) (writing took 3.069789983332157 seconds)
2021-01-07 04:51:00 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-07 04:51:00 | INFO | train | epoch 064 | symm_kl 0.405 | self_kl 0 | self_cv 0 | loss 3.375 | nll_loss 1.046 | ppl 2.06 | wps 15751.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 35904 | lr 1.15624e-05 | gnorm 0.882 | train_wall 345 | wall 23880
2021-01-07 04:51:00 | INFO | fairseq.trainer | begin training epoch 65
2021-01-07 04:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:51:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:52:02 | INFO | train_inner | epoch 065:     96 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.383, nll_loss=1.052, ppl=2.07, wps=11739.4, ups=1.14, wpb=10335.8, bsz=360.5, num_updates=36000, lr=1.1547e-05, gnorm=0.889, train_wall=60, wall=23942
2021-01-07 04:53:04 | INFO | train_inner | epoch 065:    196 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.044, ppl=2.06, wps=16910.7, ups=1.61, wpb=10512.1, bsz=360.4, num_updates=36100, lr=1.1531e-05, gnorm=0.865, train_wall=62, wall=24004
2021-01-07 04:54:06 | INFO | train_inner | epoch 065:    296 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.054, ppl=2.08, wps=16767.9, ups=1.61, wpb=10413.8, bsz=379.9, num_updates=36200, lr=1.15151e-05, gnorm=0.885, train_wall=62, wall=24066
2021-01-07 04:55:08 | INFO | train_inner | epoch 065:    396 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.359, nll_loss=1.036, ppl=2.05, wps=17182.3, ups=1.63, wpb=10570, bsz=388.1, num_updates=36300, lr=1.14992e-05, gnorm=0.866, train_wall=61, wall=24127
2021-01-07 04:56:09 | INFO | train_inner | epoch 065:    496 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.045, ppl=2.06, wps=17033.1, ups=1.62, wpb=10506.4, bsz=363.1, num_updates=36400, lr=1.14834e-05, gnorm=0.867, train_wall=61, wall=24189
2021-01-07 04:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 04:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:56:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:56:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:56:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:56:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 04:56:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 04:56:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 04:57:10 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.152 | nll_loss 3.66 | ppl 12.64 | bleu 22.94 | wps 4577 | wpb 7508.5 | bsz 272.7 | num_updates 36465 | best_bleu 23.05
2021-01-07 04:57:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 04:57:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:57:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:57:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:57:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:57:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 65 @ 36465 updates, score 22.94) (writing took 3.1724031548947096 seconds)
2021-01-07 04:57:13 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-07 04:57:13 | INFO | train | epoch 065 | symm_kl 0.404 | self_kl 0 | self_cv 0 | loss 3.374 | nll_loss 1.046 | ppl 2.06 | wps 15753.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 36465 | lr 1.14731e-05 | gnorm 0.875 | train_wall 345 | wall 24253
2021-01-07 04:57:13 | INFO | fairseq.trainer | begin training epoch 66
2021-01-07 04:57:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 04:57:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 04:57:38 | INFO | train_inner | epoch 066:     35 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.053, ppl=2.08, wps=11832.1, ups=1.13, wpb=10445.1, bsz=367.7, num_updates=36500, lr=1.14676e-05, gnorm=0.892, train_wall=61, wall=24277
2021-01-07 04:58:39 | INFO | train_inner | epoch 066:    135 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.036, ppl=2.05, wps=17078.3, ups=1.63, wpb=10495.9, bsz=377.7, num_updates=36600, lr=1.1452e-05, gnorm=0.868, train_wall=61, wall=24339
2021-01-07 04:59:41 | INFO | train_inner | epoch 066:    235 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.368, nll_loss=1.041, ppl=2.06, wps=16869.8, ups=1.61, wpb=10465, bsz=376.7, num_updates=36700, lr=1.14364e-05, gnorm=0.866, train_wall=62, wall=24401
2021-01-07 05:00:43 | INFO | train_inner | epoch 066:    335 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.345, nll_loss=1.022, ppl=2.03, wps=16989.7, ups=1.6, wpb=10611.2, bsz=367, num_updates=36800, lr=1.14208e-05, gnorm=0.874, train_wall=62, wall=24463
2021-01-07 05:01:45 | INFO | train_inner | epoch 066:    435 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.051, ppl=2.07, wps=17065.7, ups=1.62, wpb=10555.1, bsz=360.2, num_updates=36900, lr=1.14053e-05, gnorm=0.867, train_wall=62, wall=24525
2021-01-07 05:02:47 | INFO | train_inner | epoch 066:    535 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.052, ppl=2.07, wps=16867.8, ups=1.62, wpb=10443.3, bsz=371, num_updates=37000, lr=1.13899e-05, gnorm=0.866, train_wall=62, wall=24587
2021-01-07 05:03:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:03:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:03:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:03:24 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.151 | nll_loss 3.659 | ppl 12.63 | bleu 22.9 | wps 4515.7 | wpb 7508.5 | bsz 272.7 | num_updates 37026 | best_bleu 23.05
2021-01-07 05:03:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:03:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 66 @ 37026 updates, score 22.9) (writing took 3.183737013489008 seconds)
2021-01-07 05:03:27 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-07 05:03:27 | INFO | train | epoch 066 | symm_kl 0.404 | self_kl 0 | self_cv 0 | loss 3.372 | nll_loss 1.044 | ppl 2.06 | wps 15719.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 37026 | lr 1.13859e-05 | gnorm 0.873 | train_wall 345 | wall 24627
2021-01-07 05:03:27 | INFO | fairseq.trainer | begin training epoch 67
2021-01-07 05:03:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:03:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:04:16 | INFO | train_inner | epoch 067:     74 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.064, ppl=2.09, wps=11751.8, ups=1.13, wpb=10394.5, bsz=361.8, num_updates=37100, lr=1.13745e-05, gnorm=0.891, train_wall=61, wall=24676
2021-01-07 05:05:17 | INFO | train_inner | epoch 067:    174 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.049, ppl=2.07, wps=17011.1, ups=1.62, wpb=10514.9, bsz=370.6, num_updates=37200, lr=1.13592e-05, gnorm=0.872, train_wall=62, wall=24737
2021-01-07 05:06:19 | INFO | train_inner | epoch 067:    274 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.039, ppl=2.06, wps=17125.8, ups=1.61, wpb=10617.9, bsz=370.6, num_updates=37300, lr=1.1344e-05, gnorm=0.863, train_wall=62, wall=24799
2021-01-07 05:07:21 | INFO | train_inner | epoch 067:    374 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.028, ppl=2.04, wps=17028.9, ups=1.63, wpb=10439, bsz=372.6, num_updates=37400, lr=1.13288e-05, gnorm=0.871, train_wall=61, wall=24861
2021-01-07 05:08:23 | INFO | train_inner | epoch 067:    474 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.048, ppl=2.07, wps=16960.2, ups=1.62, wpb=10477.9, bsz=364.2, num_updates=37500, lr=1.13137e-05, gnorm=0.884, train_wall=62, wall=24922
2021-01-07 05:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:09:37 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.151 | nll_loss 3.661 | ppl 12.65 | bleu 23 | wps 4598 | wpb 7508.5 | bsz 272.7 | num_updates 37587 | best_bleu 23.05
2021-01-07 05:09:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 67 @ 37587 updates, score 23.0) (writing took 3.2226748410612345 seconds)
2021-01-07 05:09:40 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-07 05:09:40 | INFO | train | epoch 067 | symm_kl 0.403 | self_kl 0 | self_cv 0 | loss 3.371 | nll_loss 1.044 | ppl 2.06 | wps 15786.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 37587 | lr 1.13006e-05 | gnorm 0.873 | train_wall 344 | wall 25000
2021-01-07 05:09:40 | INFO | fairseq.trainer | begin training epoch 68
2021-01-07 05:09:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:09:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:09:51 | INFO | train_inner | epoch 068:     13 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.048, ppl=2.07, wps=11690.5, ups=1.13, wpb=10359.2, bsz=371.4, num_updates=37600, lr=1.12987e-05, gnorm=0.869, train_wall=61, wall=25011
2021-01-07 05:10:52 | INFO | train_inner | epoch 068:    113 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.036, ppl=2.05, wps=17020.9, ups=1.63, wpb=10424.8, bsz=377, num_updates=37700, lr=1.12837e-05, gnorm=0.883, train_wall=61, wall=25072
2021-01-07 05:11:54 | INFO | train_inner | epoch 068:    213 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.03, ppl=2.04, wps=17037.2, ups=1.62, wpb=10507.7, bsz=381.4, num_updates=37800, lr=1.12687e-05, gnorm=0.858, train_wall=61, wall=25134
2021-01-07 05:12:56 | INFO | train_inner | epoch 068:    313 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.394, nll_loss=1.061, ppl=2.09, wps=16933.6, ups=1.62, wpb=10461.3, bsz=368, num_updates=37900, lr=1.12538e-05, gnorm=0.898, train_wall=62, wall=25196
2021-01-07 05:13:58 | INFO | train_inner | epoch 068:    413 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.046, ppl=2.07, wps=17074, ups=1.62, wpb=10522.8, bsz=366.1, num_updates=38000, lr=1.1239e-05, gnorm=0.872, train_wall=61, wall=25257
2021-01-07 05:14:59 | INFO | train_inner | epoch 068:    513 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.051, ppl=2.07, wps=17082.2, ups=1.62, wpb=10524.8, bsz=360.5, num_updates=38100, lr=1.12243e-05, gnorm=0.875, train_wall=61, wall=25319
2021-01-07 05:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:15:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:15:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:15:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:15:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:15:50 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.153 | nll_loss 3.662 | ppl 12.66 | bleu 22.96 | wps 4519.9 | wpb 7508.5 | bsz 272.7 | num_updates 38148 | best_bleu 23.05
2021-01-07 05:15:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:15:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:15:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 68 @ 38148 updates, score 22.96) (writing took 3.1358791440725327 seconds)
2021-01-07 05:15:53 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-07 05:15:53 | INFO | train | epoch 068 | symm_kl 0.402 | self_kl 0 | self_cv 0 | loss 3.37 | nll_loss 1.044 | ppl 2.06 | wps 15767.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 38148 | lr 1.12172e-05 | gnorm 0.876 | train_wall 344 | wall 25373
2021-01-07 05:15:53 | INFO | fairseq.trainer | begin training epoch 69
2021-01-07 05:15:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:15:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:16:28 | INFO | train_inner | epoch 069:     52 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.051, ppl=2.07, wps=11736.7, ups=1.13, wpb=10378.4, bsz=358.9, num_updates=38200, lr=1.12096e-05, gnorm=0.882, train_wall=61, wall=25407
2021-01-07 05:17:29 | INFO | train_inner | epoch 069:    152 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.357, nll_loss=1.039, ppl=2.06, wps=16918.2, ups=1.61, wpb=10478.3, bsz=379.9, num_updates=38300, lr=1.11949e-05, gnorm=0.862, train_wall=62, wall=25469
2021-01-07 05:18:31 | INFO | train_inner | epoch 069:    252 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.032, ppl=2.05, wps=17170.6, ups=1.62, wpb=10626.7, bsz=373.1, num_updates=38400, lr=1.11803e-05, gnorm=0.878, train_wall=62, wall=25531
2021-01-07 05:19:33 | INFO | train_inner | epoch 069:    352 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.041, ppl=2.06, wps=17163.8, ups=1.62, wpb=10563.5, bsz=371.5, num_updates=38500, lr=1.11658e-05, gnorm=0.863, train_wall=61, wall=25593
2021-01-07 05:20:35 | INFO | train_inner | epoch 069:    452 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.047, ppl=2.07, wps=16892, ups=1.62, wpb=10445.1, bsz=360, num_updates=38600, lr=1.11513e-05, gnorm=0.877, train_wall=62, wall=25655
2021-01-07 05:21:36 | INFO | train_inner | epoch 069:    552 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.05, ppl=2.07, wps=16990.4, ups=1.63, wpb=10413.8, bsz=372.5, num_updates=38700, lr=1.11369e-05, gnorm=0.884, train_wall=61, wall=25716
2021-01-07 05:21:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:21:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:21:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:21:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:21:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:21:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:22:03 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.656 | ppl 12.61 | bleu 22.98 | wps 4540.3 | wpb 7508.5 | bsz 272.7 | num_updates 38709 | best_bleu 23.05
2021-01-07 05:22:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:22:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:22:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:22:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:22:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:22:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 69 @ 38709 updates, score 22.98) (writing took 3.0984916500747204 seconds)
2021-01-07 05:22:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-07 05:22:06 | INFO | train | epoch 069 | symm_kl 0.402 | self_kl 0 | self_cv 0 | loss 3.368 | nll_loss 1.044 | ppl 2.06 | wps 15778.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 38709 | lr 1.11356e-05 | gnorm 0.876 | train_wall 344 | wall 25746
2021-01-07 05:22:06 | INFO | fairseq.trainer | begin training epoch 70
2021-01-07 05:22:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:22:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:23:05 | INFO | train_inner | epoch 070:     91 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.033, ppl=2.05, wps=11847.4, ups=1.13, wpb=10486.8, bsz=372.2, num_updates=38800, lr=1.11226e-05, gnorm=0.864, train_wall=61, wall=25804
2021-01-07 05:24:06 | INFO | train_inner | epoch 070:    191 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.039, ppl=2.05, wps=16990.2, ups=1.62, wpb=10507.8, bsz=369.6, num_updates=38900, lr=1.11083e-05, gnorm=0.881, train_wall=62, wall=25866
2021-01-07 05:25:08 | INFO | train_inner | epoch 070:    291 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.047, ppl=2.07, wps=16988.7, ups=1.62, wpb=10473, bsz=366.5, num_updates=39000, lr=1.1094e-05, gnorm=0.876, train_wall=61, wall=25928
2021-01-07 05:26:10 | INFO | train_inner | epoch 070:    391 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.035, ppl=2.05, wps=17187.5, ups=1.62, wpb=10587.3, bsz=378.6, num_updates=39100, lr=1.10798e-05, gnorm=0.854, train_wall=61, wall=25990
2021-01-07 05:27:11 | INFO | train_inner | epoch 070:    491 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.056, ppl=2.08, wps=16932.1, ups=1.62, wpb=10437.4, bsz=363.8, num_updates=39200, lr=1.10657e-05, gnorm=0.872, train_wall=61, wall=26051
2021-01-07 05:27:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:27:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:27:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:27:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:27:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:27:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:27:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:28:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:28:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:28:16 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.149 | nll_loss 3.658 | ppl 12.63 | bleu 22.95 | wps 4546.3 | wpb 7508.5 | bsz 272.7 | num_updates 39270 | best_bleu 23.05
2021-01-07 05:28:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:28:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:28:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:28:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:28:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:28:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 70 @ 39270 updates, score 22.95) (writing took 3.0828532241284847 seconds)
2021-01-07 05:28:19 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-07 05:28:19 | INFO | train | epoch 070 | symm_kl 0.401 | self_kl 0 | self_cv 0 | loss 3.367 | nll_loss 1.043 | ppl 2.06 | wps 15760.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 39270 | lr 1.10558e-05 | gnorm 0.869 | train_wall 345 | wall 26119
2021-01-07 05:28:19 | INFO | fairseq.trainer | begin training epoch 71
2021-01-07 05:28:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:28:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:28:40 | INFO | train_inner | epoch 071:     30 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.059, ppl=2.08, wps=11639.1, ups=1.13, wpb=10331.6, bsz=354.9, num_updates=39300, lr=1.10516e-05, gnorm=0.883, train_wall=61, wall=26140
2021-01-07 05:29:41 | INFO | train_inner | epoch 071:    130 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.371, nll_loss=1.05, ppl=2.07, wps=17265.9, ups=1.63, wpb=10597.5, bsz=369.4, num_updates=39400, lr=1.10375e-05, gnorm=0.849, train_wall=61, wall=26201
2021-01-07 05:30:43 | INFO | train_inner | epoch 071:    230 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.033, ppl=2.05, wps=17028.6, ups=1.63, wpb=10467.1, bsz=374.8, num_updates=39500, lr=1.10236e-05, gnorm=0.853, train_wall=61, wall=26263
2021-01-07 05:31:44 | INFO | train_inner | epoch 071:    330 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.381, nll_loss=1.053, ppl=2.08, wps=17090.3, ups=1.63, wpb=10490, bsz=367.7, num_updates=39600, lr=1.10096e-05, gnorm=0.87, train_wall=61, wall=26324
2021-01-07 05:32:46 | INFO | train_inner | epoch 071:    430 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.048, ppl=2.07, wps=17153.7, ups=1.63, wpb=10531.8, bsz=373.4, num_updates=39700, lr=1.09958e-05, gnorm=0.86, train_wall=61, wall=26386
2021-01-07 05:33:48 | INFO | train_inner | epoch 071:    530 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.025, ppl=2.03, wps=16881, ups=1.62, wpb=10440.2, bsz=385.8, num_updates=39800, lr=1.09819e-05, gnorm=0.878, train_wall=62, wall=26447
2021-01-07 05:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:34:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:34:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:34:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:34:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:34:28 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.151 | nll_loss 3.658 | ppl 12.62 | bleu 23.07 | wps 4521.6 | wpb 7508.5 | bsz 272.7 | num_updates 39831 | best_bleu 23.07
2021-01-07 05:34:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:34:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:34:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:34:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 71 @ 39831 updates, score 23.07) (writing took 5.163358520716429 seconds)
2021-01-07 05:34:33 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-07 05:34:33 | INFO | train | epoch 071 | symm_kl 0.4 | self_kl 0 | self_cv 0 | loss 3.365 | nll_loss 1.043 | ppl 2.06 | wps 15719.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 39831 | lr 1.09777e-05 | gnorm 0.866 | train_wall 343 | wall 26493
2021-01-07 05:34:33 | INFO | fairseq.trainer | begin training epoch 72
2021-01-07 05:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:35:18 | INFO | train_inner | epoch 072:     69 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.043, ppl=2.06, wps=11370, ups=1.1, wpb=10295.3, bsz=368.1, num_updates=39900, lr=1.09682e-05, gnorm=0.876, train_wall=61, wall=26538
2021-01-07 05:36:20 | INFO | train_inner | epoch 072:    169 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.045, ppl=2.06, wps=17129.2, ups=1.62, wpb=10544.8, bsz=363.3, num_updates=40000, lr=1.09545e-05, gnorm=0.865, train_wall=61, wall=26600
2021-01-07 05:37:21 | INFO | train_inner | epoch 072:    269 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.053, ppl=2.08, wps=17132.6, ups=1.62, wpb=10549.6, bsz=365.5, num_updates=40100, lr=1.09408e-05, gnorm=0.871, train_wall=61, wall=26661
2021-01-07 05:38:23 | INFO | train_inner | epoch 072:    369 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.035, ppl=2.05, wps=17097.4, ups=1.62, wpb=10581, bsz=367.8, num_updates=40200, lr=1.09272e-05, gnorm=0.861, train_wall=62, wall=26723
2021-01-07 05:39:25 | INFO | train_inner | epoch 072:    469 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.034, ppl=2.05, wps=16751.4, ups=1.62, wpb=10356.6, bsz=379.3, num_updates=40300, lr=1.09136e-05, gnorm=0.868, train_wall=62, wall=26785
2021-01-07 05:40:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:40:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:40:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:40:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:40:43 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.656 | ppl 12.61 | bleu 23.05 | wps 4587.6 | wpb 7508.5 | bsz 272.7 | num_updates 40392 | best_bleu 23.07
2021-01-07 05:40:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:40:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 72 @ 40392 updates, score 23.05) (writing took 3.098314957693219 seconds)
2021-01-07 05:40:46 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-07 05:40:46 | INFO | train | epoch 072 | symm_kl 0.4 | self_kl 0 | self_cv 0 | loss 3.364 | nll_loss 1.042 | ppl 2.06 | wps 15758.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 40392 | lr 1.09012e-05 | gnorm 0.866 | train_wall 345 | wall 26866
2021-01-07 05:40:46 | INFO | fairseq.trainer | begin training epoch 73
2021-01-07 05:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:40:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:40:54 | INFO | train_inner | epoch 073:      8 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.354, nll_loss=1.037, ppl=2.05, wps=11742.5, ups=1.12, wpb=10502.2, bsz=359.5, num_updates=40400, lr=1.09001e-05, gnorm=0.865, train_wall=62, wall=26874
2021-01-07 05:41:56 | INFO | train_inner | epoch 073:    108 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.049, ppl=2.07, wps=17126.2, ups=1.64, wpb=10470.1, bsz=370.7, num_updates=40500, lr=1.08866e-05, gnorm=0.864, train_wall=61, wall=26935
2021-01-07 05:42:57 | INFO | train_inner | epoch 073:    208 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.033, ppl=2.05, wps=16804.6, ups=1.62, wpb=10361.4, bsz=355.9, num_updates=40600, lr=1.08732e-05, gnorm=0.884, train_wall=61, wall=26997
2021-01-07 05:43:59 | INFO | train_inner | epoch 073:    308 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.362, nll_loss=1.044, ppl=2.06, wps=16904.2, ups=1.62, wpb=10459.3, bsz=374.9, num_updates=40700, lr=1.08598e-05, gnorm=0.873, train_wall=62, wall=27059
2021-01-07 05:45:01 | INFO | train_inner | epoch 073:    408 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.025, ppl=2.04, wps=16973.2, ups=1.6, wpb=10585.1, bsz=389, num_updates=40800, lr=1.08465e-05, gnorm=0.857, train_wall=62, wall=27121
2021-01-07 05:46:03 | INFO | train_inner | epoch 073:    508 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.053, ppl=2.07, wps=17017.9, ups=1.62, wpb=10535.8, bsz=366.4, num_updates=40900, lr=1.08333e-05, gnorm=0.862, train_wall=62, wall=27183
2021-01-07 05:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:46:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:46:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:46:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:46:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:46:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:46:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:46:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:46:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:46:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:46:57 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.15 | nll_loss 3.657 | ppl 12.62 | bleu 23.04 | wps 4545.2 | wpb 7508.5 | bsz 272.7 | num_updates 40953 | best_bleu 23.07
2021-01-07 05:46:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:46:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:47:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:47:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 73 @ 40953 updates, score 23.04) (writing took 3.1061360146850348 seconds)
2021-01-07 05:47:00 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-07 05:47:00 | INFO | train | epoch 073 | symm_kl 0.399 | self_kl 0 | self_cv 0 | loss 3.363 | nll_loss 1.042 | ppl 2.06 | wps 15718.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 40953 | lr 1.08262e-05 | gnorm 0.87 | train_wall 346 | wall 27240
2021-01-07 05:47:00 | INFO | fairseq.trainer | begin training epoch 74
2021-01-07 05:47:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:47:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:47:32 | INFO | train_inner | epoch 074:     47 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.052, ppl=2.07, wps=11777.3, ups=1.12, wpb=10470.2, bsz=365.2, num_updates=41000, lr=1.082e-05, gnorm=0.878, train_wall=61, wall=27272
2021-01-07 05:48:34 | INFO | train_inner | epoch 074:    147 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.035, ppl=2.05, wps=17171, ups=1.62, wpb=10603.1, bsz=366.6, num_updates=41100, lr=1.08069e-05, gnorm=0.857, train_wall=62, wall=27334
2021-01-07 05:49:36 | INFO | train_inner | epoch 074:    247 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.037, ppl=2.05, wps=17002.2, ups=1.62, wpb=10516.6, bsz=373.8, num_updates=41200, lr=1.07937e-05, gnorm=0.863, train_wall=62, wall=27396
2021-01-07 05:50:37 | INFO | train_inner | epoch 074:    347 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.042, ppl=2.06, wps=17156.3, ups=1.62, wpb=10574, bsz=366, num_updates=41300, lr=1.07807e-05, gnorm=0.868, train_wall=61, wall=27457
2021-01-07 05:51:39 | INFO | train_inner | epoch 074:    447 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.045, ppl=2.06, wps=16822.6, ups=1.63, wpb=10310.5, bsz=370.5, num_updates=41400, lr=1.07676e-05, gnorm=0.865, train_wall=61, wall=27519
2021-01-07 05:52:40 | INFO | train_inner | epoch 074:    547 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.368, nll_loss=1.045, ppl=2.06, wps=16946.3, ups=1.62, wpb=10446.4, bsz=371, num_updates=41500, lr=1.07547e-05, gnorm=0.883, train_wall=61, wall=27580
2021-01-07 05:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:52:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:52:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:52:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:52:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:52:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:52:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:52:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:53:11 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.655 | ppl 12.6 | bleu 23.04 | wps 4402.1 | wpb 7508.5 | bsz 272.7 | num_updates 41514 | best_bleu 23.07
2021-01-07 05:53:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:53:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:53:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:53:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:53:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:53:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 74 @ 41514 updates, score 23.04) (writing took 3.1298164762556553 seconds)
2021-01-07 05:53:14 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-07 05:53:14 | INFO | train | epoch 074 | symm_kl 0.399 | self_kl 0 | self_cv 0 | loss 3.363 | nll_loss 1.042 | ppl 2.06 | wps 15746 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 41514 | lr 1.07528e-05 | gnorm 0.868 | train_wall 344 | wall 27614
2021-01-07 05:53:14 | INFO | fairseq.trainer | begin training epoch 75
2021-01-07 05:53:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:53:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:54:09 | INFO | train_inner | epoch 075:     86 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.022, ppl=2.03, wps=11737.9, ups=1.12, wpb=10435.6, bsz=376.5, num_updates=41600, lr=1.07417e-05, gnorm=0.848, train_wall=61, wall=27669
2021-01-07 05:55:11 | INFO | train_inner | epoch 075:    186 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.034, ppl=2.05, wps=17248.8, ups=1.62, wpb=10634, bsz=369.7, num_updates=41700, lr=1.07288e-05, gnorm=0.856, train_wall=61, wall=27731
2021-01-07 05:56:13 | INFO | train_inner | epoch 075:    286 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.052, ppl=2.07, wps=17166.9, ups=1.62, wpb=10590.4, bsz=359.5, num_updates=41800, lr=1.0716e-05, gnorm=0.875, train_wall=61, wall=27793
2021-01-07 05:57:14 | INFO | train_inner | epoch 075:    386 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.044, ppl=2.06, wps=16861, ups=1.64, wpb=10310.9, bsz=368.2, num_updates=41900, lr=1.07032e-05, gnorm=0.88, train_wall=61, wall=27854
2021-01-07 05:58:15 | INFO | train_inner | epoch 075:    486 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.053, ppl=2.07, wps=16923.3, ups=1.63, wpb=10402, bsz=369.2, num_updates=42000, lr=1.06904e-05, gnorm=0.872, train_wall=61, wall=27915
2021-01-07 05:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 05:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 05:59:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 05:59:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 05:59:23 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.15 | nll_loss 3.66 | ppl 12.64 | bleu 22.92 | wps 4553.4 | wpb 7508.5 | bsz 272.7 | num_updates 42075 | best_bleu 23.07
2021-01-07 05:59:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 05:59:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 75 @ 42075 updates, score 22.92) (writing took 3.133184067904949 seconds)
2021-01-07 05:59:26 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-07 05:59:26 | INFO | train | epoch 075 | symm_kl 0.398 | self_kl 0 | self_cv 0 | loss 3.361 | nll_loss 1.041 | ppl 2.06 | wps 15816.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 42075 | lr 1.06809e-05 | gnorm 0.866 | train_wall 343 | wall 27986
2021-01-07 05:59:26 | INFO | fairseq.trainer | begin training epoch 76
2021-01-07 05:59:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 05:59:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 05:59:44 | INFO | train_inner | epoch 076:     25 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.031, ppl=2.04, wps=11793, ups=1.12, wpb=10483, bsz=380.7, num_updates=42100, lr=1.06777e-05, gnorm=0.863, train_wall=61, wall=28004
2021-01-07 06:00:46 | INFO | train_inner | epoch 076:    125 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.038, ppl=2.05, wps=17087.8, ups=1.62, wpb=10537.4, bsz=384.6, num_updates=42200, lr=1.06651e-05, gnorm=0.852, train_wall=61, wall=28066
2021-01-07 06:01:47 | INFO | train_inner | epoch 076:    225 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.357, nll_loss=1.04, ppl=2.06, wps=17223.1, ups=1.63, wpb=10582.1, bsz=387.6, num_updates=42300, lr=1.06525e-05, gnorm=0.87, train_wall=61, wall=28127
2021-01-07 06:02:49 | INFO | train_inner | epoch 076:    325 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.033, ppl=2.05, wps=16968.9, ups=1.62, wpb=10458.3, bsz=352.1, num_updates=42400, lr=1.06399e-05, gnorm=0.872, train_wall=61, wall=28189
2021-01-07 06:03:51 | INFO | train_inner | epoch 076:    425 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.047, ppl=2.07, wps=16922, ups=1.62, wpb=10444.2, bsz=351.7, num_updates=42500, lr=1.06274e-05, gnorm=0.877, train_wall=62, wall=28251
2021-01-07 06:04:53 | INFO | train_inner | epoch 076:    525 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.047, ppl=2.07, wps=16988.6, ups=1.62, wpb=10514.8, bsz=371.4, num_updates=42600, lr=1.06149e-05, gnorm=0.86, train_wall=62, wall=28312
2021-01-07 06:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:05:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:05:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:05:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:05:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:05:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:05:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:05:36 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.654 | ppl 12.58 | bleu 23.04 | wps 4478.4 | wpb 7508.5 | bsz 272.7 | num_updates 42636 | best_bleu 23.07
2021-01-07 06:05:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:05:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 76 @ 42636 updates, score 23.04) (writing took 3.1100941747426987 seconds)
2021-01-07 06:05:39 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-07 06:05:39 | INFO | train | epoch 076 | symm_kl 0.397 | self_kl 0 | self_cv 0 | loss 3.36 | nll_loss 1.04 | ppl 2.06 | wps 15755.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 42636 | lr 1.06104e-05 | gnorm 0.867 | train_wall 344 | wall 28359
2021-01-07 06:05:39 | INFO | fairseq.trainer | begin training epoch 77
2021-01-07 06:05:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:05:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:05:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:05:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:06:21 | INFO | train_inner | epoch 077:     64 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.035, ppl=2.05, wps=11666.7, ups=1.13, wpb=10301, bsz=362.2, num_updates=42700, lr=1.06025e-05, gnorm=0.893, train_wall=60, wall=28401
2021-01-07 06:07:23 | INFO | train_inner | epoch 077:    164 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.028, ppl=2.04, wps=17025.2, ups=1.61, wpb=10546.4, bsz=376.5, num_updates=42800, lr=1.05901e-05, gnorm=0.845, train_wall=62, wall=28463
2021-01-07 06:08:25 | INFO | train_inner | epoch 077:    264 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.045, ppl=2.06, wps=17017, ups=1.62, wpb=10517.6, bsz=359.5, num_updates=42900, lr=1.05777e-05, gnorm=0.878, train_wall=62, wall=28524
2021-01-07 06:09:26 | INFO | train_inner | epoch 077:    364 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.357, nll_loss=1.036, ppl=2.05, wps=16982.7, ups=1.62, wpb=10469.6, bsz=368.8, num_updates=43000, lr=1.05654e-05, gnorm=0.874, train_wall=61, wall=28586
2021-01-07 06:10:28 | INFO | train_inner | epoch 077:    464 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.38, nll_loss=1.056, ppl=2.08, wps=17036.2, ups=1.62, wpb=10518.3, bsz=364.5, num_updates=43100, lr=1.05531e-05, gnorm=0.866, train_wall=62, wall=28648
2021-01-07 06:11:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:11:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:11:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:11:49 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.654 | ppl 12.59 | bleu 23.02 | wps 4575 | wpb 7508.5 | bsz 272.7 | num_updates 43197 | best_bleu 23.07
2021-01-07 06:11:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:11:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 77 @ 43197 updates, score 23.02) (writing took 3.039419136941433 seconds)
2021-01-07 06:11:52 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-07 06:11:52 | INFO | train | epoch 077 | symm_kl 0.397 | self_kl 0 | self_cv 0 | loss 3.359 | nll_loss 1.04 | ppl 2.06 | wps 15775.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 43197 | lr 1.05413e-05 | gnorm 0.868 | train_wall 344 | wall 28732
2021-01-07 06:11:52 | INFO | fairseq.trainer | begin training epoch 78
2021-01-07 06:11:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:11:57 | INFO | train_inner | epoch 078:      3 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.048, ppl=2.07, wps=11717.5, ups=1.12, wpb=10421.2, bsz=375.9, num_updates=43200, lr=1.05409e-05, gnorm=0.868, train_wall=61, wall=28737
2021-01-07 06:12:58 | INFO | train_inner | epoch 078:    103 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.047, ppl=2.07, wps=17104.7, ups=1.65, wpb=10381.6, bsz=367.8, num_updates=43300, lr=1.05287e-05, gnorm=0.87, train_wall=60, wall=28798
2021-01-07 06:13:59 | INFO | train_inner | epoch 078:    203 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.358, nll_loss=1.04, ppl=2.06, wps=17105, ups=1.62, wpb=10541.1, bsz=371.2, num_updates=43400, lr=1.05166e-05, gnorm=0.868, train_wall=61, wall=28859
2021-01-07 06:15:01 | INFO | train_inner | epoch 078:    303 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.032, ppl=2.04, wps=17098.4, ups=1.63, wpb=10506.2, bsz=367.8, num_updates=43500, lr=1.05045e-05, gnorm=0.879, train_wall=61, wall=28921
2021-01-07 06:16:02 | INFO | train_inner | epoch 078:    403 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.037, ppl=2.05, wps=17199.1, ups=1.63, wpb=10573, bsz=375.5, num_updates=43600, lr=1.04925e-05, gnorm=0.866, train_wall=61, wall=28982
2021-01-07 06:17:04 | INFO | train_inner | epoch 078:    503 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.045, ppl=2.06, wps=17087.9, ups=1.63, wpb=10494.6, bsz=375, num_updates=43700, lr=1.04804e-05, gnorm=0.872, train_wall=61, wall=29043
2021-01-07 06:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:17:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:17:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:17:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:17:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:17:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:18:00 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.149 | nll_loss 3.655 | ppl 12.6 | bleu 23.1 | wps 4571.4 | wpb 7508.5 | bsz 272.7 | num_updates 43758 | best_bleu 23.1
2021-01-07 06:18:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:18:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:18:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:18:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:18:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:18:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_best.pt (epoch 78 @ 43758 updates, score 23.1) (writing took 5.148491572588682 seconds)
2021-01-07 06:18:05 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-07 06:18:05 | INFO | train | epoch 078 | symm_kl 0.396 | self_kl 0 | self_cv 0 | loss 3.357 | nll_loss 1.039 | ppl 2.06 | wps 15737.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 43758 | lr 1.04735e-05 | gnorm 0.873 | train_wall 343 | wall 29105
2021-01-07 06:18:05 | INFO | fairseq.trainer | begin training epoch 79
2021-01-07 06:18:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:18:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:18:33 | INFO | train_inner | epoch 079:     42 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.034, ppl=2.05, wps=11526.6, ups=1.11, wpb=10363.7, bsz=353.7, num_updates=43800, lr=1.04685e-05, gnorm=0.868, train_wall=60, wall=29133
2021-01-07 06:19:35 | INFO | train_inner | epoch 079:    142 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.36, nll_loss=1.041, ppl=2.06, wps=17039.8, ups=1.63, wpb=10473.3, bsz=369.6, num_updates=43900, lr=1.04565e-05, gnorm=0.881, train_wall=61, wall=29195
2021-01-07 06:20:36 | INFO | train_inner | epoch 079:    242 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.033, ppl=2.05, wps=17131.5, ups=1.63, wpb=10520.9, bsz=368.5, num_updates=44000, lr=1.04447e-05, gnorm=0.859, train_wall=61, wall=29256
2021-01-07 06:21:38 | INFO | train_inner | epoch 079:    342 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.033, ppl=2.05, wps=17083.3, ups=1.62, wpb=10548.2, bsz=383.3, num_updates=44100, lr=1.04328e-05, gnorm=0.85, train_wall=62, wall=29318
2021-01-07 06:22:39 | INFO | train_inner | epoch 079:    442 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.368, nll_loss=1.045, ppl=2.06, wps=16977.9, ups=1.63, wpb=10413, bsz=355.9, num_updates=44200, lr=1.0421e-05, gnorm=0.875, train_wall=61, wall=29379
2021-01-07 06:23:41 | INFO | train_inner | epoch 079:    542 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.049, ppl=2.07, wps=17017.1, ups=1.61, wpb=10557.3, bsz=377.7, num_updates=44300, lr=1.04092e-05, gnorm=0.854, train_wall=62, wall=29441
2021-01-07 06:23:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:23:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:23:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:23:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:23:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:23:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:23:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:24:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:24:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:24:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:24:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:24:14 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.654 | ppl 12.59 | bleu 22.96 | wps 4505.4 | wpb 7508.5 | bsz 272.7 | num_updates 44319 | best_bleu 23.1
2021-01-07 06:24:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:24:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:24:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:24:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:24:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:24:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 79 @ 44319 updates, score 22.96) (writing took 3.0116734504699707 seconds)
2021-01-07 06:24:17 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-07 06:24:17 | INFO | train | epoch 079 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.356 | nll_loss 1.039 | ppl 2.06 | wps 15817.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 44319 | lr 1.0407e-05 | gnorm 0.864 | train_wall 343 | wall 29477
2021-01-07 06:24:17 | INFO | fairseq.trainer | begin training epoch 80
2021-01-07 06:24:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:24:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:25:10 | INFO | train_inner | epoch 080:     81 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.028, ppl=2.04, wps=11791.7, ups=1.13, wpb=10456.9, bsz=376.2, num_updates=44400, lr=1.03975e-05, gnorm=0.865, train_wall=61, wall=29530
2021-01-07 06:26:12 | INFO | train_inner | epoch 080:    181 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.36, nll_loss=1.041, ppl=2.06, wps=16803.9, ups=1.62, wpb=10400.2, bsz=371.1, num_updates=44500, lr=1.03858e-05, gnorm=0.874, train_wall=62, wall=29592
2021-01-07 06:27:14 | INFO | train_inner | epoch 080:    281 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.037, ppl=2.05, wps=17148.4, ups=1.62, wpb=10593.7, bsz=368.4, num_updates=44600, lr=1.03742e-05, gnorm=0.85, train_wall=62, wall=29654
2021-01-07 06:28:15 | INFO | train_inner | epoch 080:    381 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.047, ppl=2.07, wps=17198.7, ups=1.62, wpb=10608.4, bsz=367, num_updates=44700, lr=1.03626e-05, gnorm=0.865, train_wall=61, wall=29715
2021-01-07 06:29:17 | INFO | train_inner | epoch 080:    481 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.032, ppl=2.04, wps=16939.5, ups=1.63, wpb=10408.4, bsz=357.6, num_updates=44800, lr=1.0351e-05, gnorm=0.853, train_wall=61, wall=29777
2021-01-07 06:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:30:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:30:27 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.654 | ppl 12.59 | bleu 23.04 | wps 4576.7 | wpb 7508.5 | bsz 272.7 | num_updates 44880 | best_bleu 23.1
2021-01-07 06:30:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:30:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 80 @ 44880 updates, score 23.04) (writing took 3.0713082514703274 seconds)
2021-01-07 06:30:30 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-07 06:30:30 | INFO | train | epoch 080 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.355 | nll_loss 1.039 | ppl 2.05 | wps 15766.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 44880 | lr 1.03418e-05 | gnorm 0.862 | train_wall 344 | wall 29850
2021-01-07 06:30:30 | INFO | fairseq.trainer | begin training epoch 81
2021-01-07 06:30:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:30:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:30:46 | INFO | train_inner | epoch 081:     20 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.358, nll_loss=1.043, ppl=2.06, wps=11723.8, ups=1.13, wpb=10396.4, bsz=377.8, num_updates=44900, lr=1.03395e-05, gnorm=0.88, train_wall=61, wall=29866
2021-01-07 06:31:47 | INFO | train_inner | epoch 081:    120 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.037, ppl=2.05, wps=17155.6, ups=1.64, wpb=10468.5, bsz=373.5, num_updates=45000, lr=1.0328e-05, gnorm=0.853, train_wall=61, wall=29927
2021-01-07 06:32:48 | INFO | train_inner | epoch 081:    220 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.354, nll_loss=1.039, ppl=2.05, wps=17097.1, ups=1.63, wpb=10503.9, bsz=370.8, num_updates=45100, lr=1.03165e-05, gnorm=0.859, train_wall=61, wall=29988
2021-01-07 06:33:50 | INFO | train_inner | epoch 081:    320 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.042, ppl=2.06, wps=16990.1, ups=1.63, wpb=10441.1, bsz=382.6, num_updates=45200, lr=1.03051e-05, gnorm=0.875, train_wall=61, wall=30049
2021-01-07 06:34:51 | INFO | train_inner | epoch 081:    420 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.033, ppl=2.05, wps=17256.5, ups=1.63, wpb=10596.7, bsz=361.8, num_updates=45300, lr=1.02937e-05, gnorm=0.858, train_wall=61, wall=30111
2021-01-07 06:35:53 | INFO | train_inner | epoch 081:    520 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.359, nll_loss=1.043, ppl=2.06, wps=16999.9, ups=1.62, wpb=10490.6, bsz=372.4, num_updates=45400, lr=1.02824e-05, gnorm=0.853, train_wall=62, wall=30173
2021-01-07 06:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:36:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:36:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:36:39 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.656 | ppl 12.61 | bleu 23 | wps 4518.8 | wpb 7508.5 | bsz 272.7 | num_updates 45441 | best_bleu 23.1
2021-01-07 06:36:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:36:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:36:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:36:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 81 @ 45441 updates, score 23.0) (writing took 3.038717208430171 seconds)
2021-01-07 06:36:42 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-07 06:36:42 | INFO | train | epoch 081 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.354 | nll_loss 1.039 | ppl 2.05 | wps 15815.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 45441 | lr 1.02777e-05 | gnorm 0.864 | train_wall 343 | wall 30222
2021-01-07 06:36:42 | INFO | fairseq.trainer | begin training epoch 82
2021-01-07 06:36:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:36:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:37:21 | INFO | train_inner | epoch 082:     59 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.043, ppl=2.06, wps=11770.4, ups=1.13, wpb=10409.9, bsz=349.3, num_updates=45500, lr=1.02711e-05, gnorm=0.883, train_wall=61, wall=30261
2021-01-07 06:38:23 | INFO | train_inner | epoch 082:    159 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.038, ppl=2.05, wps=17019.5, ups=1.63, wpb=10465.7, bsz=374.6, num_updates=45600, lr=1.02598e-05, gnorm=0.862, train_wall=61, wall=30323
2021-01-07 06:39:24 | INFO | train_inner | epoch 082:    259 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.358, nll_loss=1.04, ppl=2.06, wps=17058.3, ups=1.62, wpb=10547.3, bsz=363.9, num_updates=45700, lr=1.02486e-05, gnorm=0.866, train_wall=62, wall=30384
2021-01-07 06:40:26 | INFO | train_inner | epoch 082:    359 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.031, ppl=2.04, wps=17260, ups=1.63, wpb=10602, bsz=367.5, num_updates=45800, lr=1.02374e-05, gnorm=0.877, train_wall=61, wall=30446
2021-01-07 06:41:27 | INFO | train_inner | epoch 082:    459 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.368, nll_loss=1.054, ppl=2.08, wps=16722.8, ups=1.63, wpb=10279.6, bsz=367.6, num_updates=45900, lr=1.02262e-05, gnorm=0.874, train_wall=61, wall=30507
2021-01-07 06:42:29 | INFO | train_inner | epoch 082:    559 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.024, ppl=2.03, wps=17203.8, ups=1.63, wpb=10568.6, bsz=386.6, num_updates=46000, lr=1.02151e-05, gnorm=0.843, train_wall=61, wall=30569
2021-01-07 06:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:42:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:42:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:42:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:42:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:42:51 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.654 | ppl 12.59 | bleu 22.95 | wps 4620.2 | wpb 7508.5 | bsz 272.7 | num_updates 46002 | best_bleu 23.1
2021-01-07 06:42:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:42:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:42:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 82 @ 46002 updates, score 22.95) (writing took 3.1121142338961363 seconds)
2021-01-07 06:42:54 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-07 06:42:54 | INFO | train | epoch 082 | symm_kl 0.394 | self_kl 0 | self_cv 0 | loss 3.352 | nll_loss 1.038 | ppl 2.05 | wps 15825.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 46002 | lr 1.02149e-05 | gnorm 0.866 | train_wall 343 | wall 30594
2021-01-07 06:42:54 | INFO | fairseq.trainer | begin training epoch 83
2021-01-07 06:42:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:42:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:43:57 | INFO | train_inner | epoch 083:     98 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.36, nll_loss=1.042, ppl=2.06, wps=11833.4, ups=1.14, wpb=10394, bsz=358.3, num_updates=46100, lr=1.0204e-05, gnorm=0.859, train_wall=60, wall=30656
2021-01-07 06:44:58 | INFO | train_inner | epoch 083:    198 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.05, ppl=2.07, wps=16881.7, ups=1.63, wpb=10378.9, bsz=367.9, num_updates=46200, lr=1.01929e-05, gnorm=0.872, train_wall=61, wall=30718
2021-01-07 06:45:59 | INFO | train_inner | epoch 083:    298 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.031, ppl=2.04, wps=17206.4, ups=1.63, wpb=10566.5, bsz=366, num_updates=46300, lr=1.01819e-05, gnorm=0.861, train_wall=61, wall=30779
2021-01-07 06:47:01 | INFO | train_inner | epoch 083:    398 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.038, ppl=2.05, wps=16995.8, ups=1.63, wpb=10419, bsz=377.3, num_updates=46400, lr=1.0171e-05, gnorm=0.854, train_wall=61, wall=30841
2021-01-07 06:48:02 | INFO | train_inner | epoch 083:    498 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.348, nll_loss=1.037, ppl=2.05, wps=17218.5, ups=1.63, wpb=10569.6, bsz=385, num_updates=46500, lr=1.016e-05, gnorm=0.851, train_wall=61, wall=30902
2021-01-07 06:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:48:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:48:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:48:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:48:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:48:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:48:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:49:04 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.149 | nll_loss 3.656 | ppl 12.61 | bleu 22.98 | wps 4060.5 | wpb 7508.5 | bsz 272.7 | num_updates 46563 | best_bleu 23.1
2021-01-07 06:49:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:49:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:49:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:49:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:49:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:49:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 83 @ 46563 updates, score 22.98) (writing took 3.0645196363329887 seconds)
2021-01-07 06:49:07 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-07 06:49:07 | INFO | train | epoch 083 | symm_kl 0.393 | self_kl 0 | self_cv 0 | loss 3.351 | nll_loss 1.037 | ppl 2.05 | wps 15755.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 46563 | lr 1.01531e-05 | gnorm 0.86 | train_wall 343 | wall 30967
2021-01-07 06:49:07 | INFO | fairseq.trainer | begin training epoch 84
2021-01-07 06:49:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:49:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:49:33 | INFO | train_inner | epoch 084:     37 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.023, ppl=2.03, wps=11658.3, ups=1.1, wpb=10551.7, bsz=368.1, num_updates=46600, lr=1.01491e-05, gnorm=0.86, train_wall=61, wall=30993
2021-01-07 06:50:34 | INFO | train_inner | epoch 084:    137 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.039, ppl=2.05, wps=16982.2, ups=1.63, wpb=10394.7, bsz=362.2, num_updates=46700, lr=1.01382e-05, gnorm=0.866, train_wall=61, wall=31054
2021-01-07 06:51:36 | INFO | train_inner | epoch 084:    237 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.044, ppl=2.06, wps=16984.2, ups=1.62, wpb=10508.4, bsz=361.6, num_updates=46800, lr=1.01274e-05, gnorm=0.88, train_wall=62, wall=31116
2021-01-07 06:52:37 | INFO | train_inner | epoch 084:    337 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.345, nll_loss=1.032, ppl=2.04, wps=17056.9, ups=1.63, wpb=10450.1, bsz=373.6, num_updates=46900, lr=1.01166e-05, gnorm=0.863, train_wall=61, wall=31177
2021-01-07 06:53:39 | INFO | train_inner | epoch 084:    437 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.348, nll_loss=1.037, ppl=2.05, wps=17037.7, ups=1.62, wpb=10517.8, bsz=380.4, num_updates=47000, lr=1.01058e-05, gnorm=0.864, train_wall=62, wall=31239
2021-01-07 06:54:41 | INFO | train_inner | epoch 084:    537 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.03, ppl=2.04, wps=17045.7, ups=1.61, wpb=10554.9, bsz=367, num_updates=47100, lr=1.00951e-05, gnorm=0.857, train_wall=62, wall=31301
2021-01-07 06:54:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 06:54:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:54:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:54:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:54:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:54:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:54:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 06:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 06:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 06:55:17 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.149 | nll_loss 3.659 | ppl 12.63 | bleu 22.8 | wps 4513.5 | wpb 7508.5 | bsz 272.7 | num_updates 47124 | best_bleu 23.1
2021-01-07 06:55:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 06:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:55:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:55:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:55:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:55:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 84 @ 47124 updates, score 22.8) (writing took 3.0924518946558237 seconds)
2021-01-07 06:55:20 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-07 06:55:20 | INFO | train | epoch 084 | symm_kl 0.393 | self_kl 0 | self_cv 0 | loss 3.351 | nll_loss 1.038 | ppl 2.05 | wps 15781.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 47124 | lr 1.00925e-05 | gnorm 0.865 | train_wall 344 | wall 31340
2021-01-07 06:55:20 | INFO | fairseq.trainer | begin training epoch 85
2021-01-07 06:55:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 06:55:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 06:56:09 | INFO | train_inner | epoch 085:     76 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.027, ppl=2.04, wps=11913.2, ups=1.13, wpb=10521.4, bsz=388.8, num_updates=47200, lr=1.00844e-05, gnorm=0.84, train_wall=60, wall=31389
2021-01-07 06:57:10 | INFO | train_inner | epoch 085:    176 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.031, ppl=2.04, wps=16985.9, ups=1.63, wpb=10424.7, bsz=364.1, num_updates=47300, lr=1.00737e-05, gnorm=0.867, train_wall=61, wall=31450
2021-01-07 06:58:12 | INFO | train_inner | epoch 085:    276 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.03, ppl=2.04, wps=17312.9, ups=1.63, wpb=10641.3, bsz=372.8, num_updates=47400, lr=1.00631e-05, gnorm=0.86, train_wall=61, wall=31512
2021-01-07 06:59:13 | INFO | train_inner | epoch 085:    376 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.05, ppl=2.07, wps=16996.9, ups=1.63, wpb=10412, bsz=369.3, num_updates=47500, lr=1.00525e-05, gnorm=0.869, train_wall=61, wall=31573
2021-01-07 07:00:15 | INFO | train_inner | epoch 085:    476 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.054, ppl=2.08, wps=16994.1, ups=1.63, wpb=10454.5, bsz=352.3, num_updates=47600, lr=1.00419e-05, gnorm=0.864, train_wall=61, wall=31635
2021-01-07 07:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:01:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:01:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:01:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:01:28 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.149 | nll_loss 3.656 | ppl 12.61 | bleu 23.04 | wps 4590.9 | wpb 7508.5 | bsz 272.7 | num_updates 47685 | best_bleu 23.1
2021-01-07 07:01:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:01:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 85 @ 47685 updates, score 23.04) (writing took 3.0817115511745214 seconds)
2021-01-07 07:01:31 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-07 07:01:31 | INFO | train | epoch 085 | symm_kl 0.392 | self_kl 0 | self_cv 0 | loss 3.35 | nll_loss 1.037 | ppl 2.05 | wps 15855.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 47685 | lr 1.0033e-05 | gnorm 0.861 | train_wall 343 | wall 31711
2021-01-07 07:01:31 | INFO | fairseq.trainer | begin training epoch 86
2021-01-07 07:01:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:01:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:01:43 | INFO | train_inner | epoch 086:     15 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.045, ppl=2.06, wps=11765.5, ups=1.13, wpb=10414.5, bsz=365.8, num_updates=47700, lr=1.00314e-05, gnorm=0.884, train_wall=61, wall=31723
2021-01-07 07:02:44 | INFO | train_inner | epoch 086:    115 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.026, ppl=2.04, wps=17188.7, ups=1.64, wpb=10504.5, bsz=369.2, num_updates=47800, lr=1.00209e-05, gnorm=0.854, train_wall=61, wall=31784
2021-01-07 07:03:46 | INFO | train_inner | epoch 086:    215 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.017, ppl=2.02, wps=17183, ups=1.63, wpb=10571.3, bsz=370, num_updates=47900, lr=1.00104e-05, gnorm=0.859, train_wall=61, wall=31846
2021-01-07 07:04:47 | INFO | train_inner | epoch 086:    315 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.055, ppl=2.08, wps=16741.2, ups=1.62, wpb=10311, bsz=359.4, num_updates=48000, lr=1e-05, gnorm=0.867, train_wall=61, wall=31907
2021-01-07 07:05:49 | INFO | train_inner | epoch 086:    415 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.052, ppl=2.07, wps=17141.8, ups=1.62, wpb=10556, bsz=363.5, num_updates=48100, lr=9.9896e-06, gnorm=0.867, train_wall=61, wall=31969
2021-01-07 07:06:51 | INFO | train_inner | epoch 086:    515 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.03, ppl=2.04, wps=16974.7, ups=1.61, wpb=10539.6, bsz=394.6, num_updates=48200, lr=9.97923e-06, gnorm=0.848, train_wall=62, wall=32031
2021-01-07 07:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:07:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:07:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:07:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:07:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:07:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:07:41 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.653 | ppl 12.58 | bleu 22.98 | wps 4502.9 | wpb 7508.5 | bsz 272.7 | num_updates 48246 | best_bleu 23.1
2021-01-07 07:07:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 86 @ 48246 updates, score 22.98) (writing took 3.0745186842978 seconds)
2021-01-07 07:07:44 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-07 07:07:44 | INFO | train | epoch 086 | symm_kl 0.392 | self_kl 0 | self_cv 0 | loss 3.348 | nll_loss 1.036 | ppl 2.05 | wps 15770.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 48246 | lr 9.97447e-06 | gnorm 0.865 | train_wall 344 | wall 32084
2021-01-07 07:07:44 | INFO | fairseq.trainer | begin training epoch 87
2021-01-07 07:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:07:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:07:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:08:19 | INFO | train_inner | epoch 087:     54 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.022, ppl=2.03, wps=11865.1, ups=1.13, wpb=10484.6, bsz=357, num_updates=48300, lr=9.9689e-06, gnorm=0.872, train_wall=60, wall=32119
2021-01-07 07:09:21 | INFO | train_inner | epoch 087:    154 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.052, ppl=2.07, wps=16605.5, ups=1.63, wpb=10217.7, bsz=374, num_updates=48400, lr=9.95859e-06, gnorm=0.866, train_wall=61, wall=32181
2021-01-07 07:10:23 | INFO | train_inner | epoch 087:    254 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.018, ppl=2.03, wps=17184.1, ups=1.62, wpb=10607.1, bsz=361.4, num_updates=48500, lr=9.94832e-06, gnorm=0.858, train_wall=62, wall=32243
2021-01-07 07:11:24 | INFO | train_inner | epoch 087:    354 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.04, ppl=2.06, wps=17094.3, ups=1.62, wpb=10544.2, bsz=364.4, num_updates=48600, lr=9.93808e-06, gnorm=0.865, train_wall=61, wall=32304
2021-01-07 07:12:26 | INFO | train_inner | epoch 087:    454 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.039, ppl=2.05, wps=17107.9, ups=1.62, wpb=10541.1, bsz=376.4, num_updates=48700, lr=9.92787e-06, gnorm=0.847, train_wall=61, wall=32366
2021-01-07 07:13:27 | INFO | train_inner | epoch 087:    554 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.044, ppl=2.06, wps=17148, ups=1.63, wpb=10528.8, bsz=381.8, num_updates=48800, lr=9.91769e-06, gnorm=0.848, train_wall=61, wall=32427
2021-01-07 07:13:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:13:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:13:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:13:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:13:53 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.655 | ppl 12.59 | bleu 22.97 | wps 4493.5 | wpb 7508.5 | bsz 272.7 | num_updates 48807 | best_bleu 23.1
2021-01-07 07:13:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:13:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:13:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:13:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 87 @ 48807 updates, score 22.97) (writing took 3.1125638484954834 seconds)
2021-01-07 07:13:56 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-07 07:13:56 | INFO | train | epoch 087 | symm_kl 0.391 | self_kl 0 | self_cv 0 | loss 3.348 | nll_loss 1.036 | ppl 2.05 | wps 15795.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 48807 | lr 9.91698e-06 | gnorm 0.858 | train_wall 344 | wall 32456
2021-01-07 07:13:56 | INFO | fairseq.trainer | begin training epoch 88
2021-01-07 07:13:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:13:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:14:56 | INFO | train_inner | epoch 088:     93 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.039, ppl=2.05, wps=11747.9, ups=1.13, wpb=10372.7, bsz=363.2, num_updates=48900, lr=9.90755e-06, gnorm=0.86, train_wall=60, wall=32516
2021-01-07 07:15:57 | INFO | train_inner | epoch 088:    193 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.026, ppl=2.04, wps=16858.9, ups=1.62, wpb=10376.6, bsz=358.2, num_updates=49000, lr=9.89743e-06, gnorm=0.856, train_wall=61, wall=32577
2021-01-07 07:16:59 | INFO | train_inner | epoch 088:    293 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.027, ppl=2.04, wps=17098.6, ups=1.62, wpb=10577.9, bsz=398.9, num_updates=49100, lr=9.88735e-06, gnorm=0.831, train_wall=62, wall=32639
2021-01-07 07:18:00 | INFO | train_inner | epoch 088:    393 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.047, ppl=2.07, wps=17063.6, ups=1.63, wpb=10445.9, bsz=344.1, num_updates=49200, lr=9.8773e-06, gnorm=0.891, train_wall=61, wall=32700
2021-01-07 07:19:02 | INFO | train_inner | epoch 088:    493 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.044, ppl=2.06, wps=17083.5, ups=1.63, wpb=10502.6, bsz=377.1, num_updates=49300, lr=9.86727e-06, gnorm=0.865, train_wall=61, wall=32762
2021-01-07 07:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:19:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:19:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:19:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:19:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:19:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:19:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:19:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:20:05 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.147 | nll_loss 3.654 | ppl 12.59 | bleu 22.97 | wps 4485.5 | wpb 7508.5 | bsz 272.7 | num_updates 49368 | best_bleu 23.1
2021-01-07 07:20:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:20:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:20:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:20:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:20:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:20:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 88 @ 49368 updates, score 22.97) (writing took 2.941855024546385 seconds)
2021-01-07 07:20:08 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-07 07:20:08 | INFO | train | epoch 088 | symm_kl 0.391 | self_kl 0 | self_cv 0 | loss 3.346 | nll_loss 1.036 | ppl 2.05 | wps 15817.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 49368 | lr 9.86048e-06 | gnorm 0.858 | train_wall 343 | wall 32828
2021-01-07 07:20:08 | INFO | fairseq.trainer | begin training epoch 89
2021-01-07 07:20:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:20:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:20:30 | INFO | train_inner | epoch 089:     32 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.036, ppl=2.05, wps=11943.5, ups=1.13, wpb=10574.7, bsz=380.6, num_updates=49400, lr=9.85728e-06, gnorm=0.847, train_wall=61, wall=32850
2021-01-07 07:21:31 | INFO | train_inner | epoch 089:    132 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.028, ppl=2.04, wps=16922.5, ups=1.63, wpb=10356.2, bsz=364.6, num_updates=49500, lr=9.84732e-06, gnorm=0.85, train_wall=61, wall=32911
2021-01-07 07:22:34 | INFO | train_inner | epoch 089:    232 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.026, ppl=2.04, wps=16950.7, ups=1.61, wpb=10526.3, bsz=365.7, num_updates=49600, lr=9.83739e-06, gnorm=0.865, train_wall=62, wall=32974
2021-01-07 07:23:35 | INFO | train_inner | epoch 089:    332 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.042, ppl=2.06, wps=17034.1, ups=1.62, wpb=10531.2, bsz=379.8, num_updates=49700, lr=9.82749e-06, gnorm=0.849, train_wall=62, wall=33035
2021-01-07 07:24:37 | INFO | train_inner | epoch 089:    432 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.037, ppl=2.05, wps=17110.2, ups=1.62, wpb=10552.4, bsz=357.4, num_updates=49800, lr=9.81761e-06, gnorm=0.855, train_wall=61, wall=33097
2021-01-07 07:25:38 | INFO | train_inner | epoch 089:    532 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.05, ppl=2.07, wps=17007.3, ups=1.63, wpb=10439.3, bsz=363.8, num_updates=49900, lr=9.80777e-06, gnorm=0.887, train_wall=61, wall=33158
2021-01-07 07:25:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:25:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:25:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:25:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:25:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:25:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:25:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:26:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:26:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:26:17 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.144 | nll_loss 3.652 | ppl 12.57 | bleu 22.94 | wps 4532.2 | wpb 7508.5 | bsz 272.7 | num_updates 49929 | best_bleu 23.1
2021-01-07 07:26:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:26:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:26:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:26:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:26:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:26:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 89 @ 49929 updates, score 22.94) (writing took 2.9817484989762306 seconds)
2021-01-07 07:26:20 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-07 07:26:20 | INFO | train | epoch 089 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.345 | nll_loss 1.036 | ppl 2.05 | wps 15782.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 49929 | lr 9.80492e-06 | gnorm 0.86 | train_wall 344 | wall 33200
2021-01-07 07:26:20 | INFO | fairseq.trainer | begin training epoch 90
2021-01-07 07:26:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:26:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:27:07 | INFO | train_inner | epoch 090:     71 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.026, ppl=2.04, wps=11874.8, ups=1.14, wpb=10456.7, bsz=367.8, num_updates=50000, lr=9.79796e-06, gnorm=0.86, train_wall=60, wall=33246
2021-01-07 07:28:09 | INFO | train_inner | epoch 090:    171 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.027, ppl=2.04, wps=16892, ups=1.61, wpb=10492.2, bsz=353.5, num_updates=50100, lr=9.78818e-06, gnorm=0.861, train_wall=62, wall=33309
2021-01-07 07:29:11 | INFO | train_inner | epoch 090:    271 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.341, nll_loss=1.035, ppl=2.05, wps=17033.8, ups=1.62, wpb=10537.9, bsz=386.1, num_updates=50200, lr=9.77842e-06, gnorm=0.839, train_wall=62, wall=33370
2021-01-07 07:30:12 | INFO | train_inner | epoch 090:    371 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.049, ppl=2.07, wps=16816.3, ups=1.61, wpb=10419.6, bsz=366.2, num_updates=50300, lr=9.7687e-06, gnorm=0.859, train_wall=62, wall=33432
2021-01-07 07:31:14 | INFO | train_inner | epoch 090:    471 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.029, ppl=2.04, wps=16901.4, ups=1.62, wpb=10434.1, bsz=370.4, num_updates=50400, lr=9.759e-06, gnorm=0.849, train_wall=62, wall=33494
2021-01-07 07:32:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:32:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:32:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:32:31 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.652 | ppl 12.57 | bleu 22.89 | wps 4535.4 | wpb 7508.5 | bsz 272.7 | num_updates 50490 | best_bleu 23.1
2021-01-07 07:32:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:32:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 90 @ 50490 updates, score 22.89) (writing took 2.9536869190633297 seconds)
2021-01-07 07:32:34 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-07 07:32:34 | INFO | train | epoch 090 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.345 | nll_loss 1.035 | ppl 2.05 | wps 15725.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 50490 | lr 9.7503e-06 | gnorm 0.852 | train_wall 345 | wall 33574
2021-01-07 07:32:34 | INFO | fairseq.trainer | begin training epoch 91
2021-01-07 07:32:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:32:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:32:44 | INFO | train_inner | epoch 091:     10 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.04, ppl=2.06, wps=11759.8, ups=1.12, wpb=10516.1, bsz=383.2, num_updates=50500, lr=9.74933e-06, gnorm=0.844, train_wall=62, wall=33584
2021-01-07 07:33:45 | INFO | train_inner | epoch 091:    110 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.038, ppl=2.05, wps=17000.9, ups=1.63, wpb=10453.1, bsz=379.8, num_updates=50600, lr=9.7397e-06, gnorm=0.843, train_wall=61, wall=33645
2021-01-07 07:34:47 | INFO | train_inner | epoch 091:    210 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.345, nll_loss=1.035, ppl=2.05, wps=16921.6, ups=1.62, wpb=10441.5, bsz=368.3, num_updates=50700, lr=9.73009e-06, gnorm=0.857, train_wall=62, wall=33707
2021-01-07 07:35:49 | INFO | train_inner | epoch 091:    310 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.044, ppl=2.06, wps=16939.7, ups=1.62, wpb=10470.4, bsz=371.4, num_updates=50800, lr=9.7205e-06, gnorm=0.863, train_wall=62, wall=33769
2021-01-07 07:36:51 | INFO | train_inner | epoch 091:    410 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.015, ppl=2.02, wps=17156.5, ups=1.61, wpb=10633.1, bsz=363.4, num_updates=50900, lr=9.71095e-06, gnorm=0.86, train_wall=62, wall=33831
2021-01-07 07:37:52 | INFO | train_inner | epoch 091:    510 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.035, ppl=2.05, wps=17111.5, ups=1.63, wpb=10520, bsz=367, num_updates=51000, lr=9.70143e-06, gnorm=0.852, train_wall=61, wall=33892
2021-01-07 07:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:38:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:38:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:38:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:38:45 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.653 | ppl 12.58 | bleu 22.95 | wps 4519 | wpb 7508.5 | bsz 272.7 | num_updates 51051 | best_bleu 23.1
2021-01-07 07:38:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:38:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:38:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:38:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 91 @ 51051 updates, score 22.95) (writing took 3.0047217290848494 seconds)
2021-01-07 07:38:48 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-07 07:38:48 | INFO | train | epoch 091 | symm_kl 0.389 | self_kl 0 | self_cv 0 | loss 3.343 | nll_loss 1.035 | ppl 2.05 | wps 15752.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 51051 | lr 9.69658e-06 | gnorm 0.857 | train_wall 345 | wall 33948
2021-01-07 07:38:48 | INFO | fairseq.trainer | begin training epoch 92
2021-01-07 07:38:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:38:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:39:20 | INFO | train_inner | epoch 092:     49 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.039, ppl=2.05, wps=11715.6, ups=1.13, wpb=10331.8, bsz=364.4, num_updates=51100, lr=9.69193e-06, gnorm=0.873, train_wall=61, wall=33980
2021-01-07 07:40:22 | INFO | train_inner | epoch 092:    149 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.027, ppl=2.04, wps=17145, ups=1.63, wpb=10546.3, bsz=378.9, num_updates=51200, lr=9.68246e-06, gnorm=0.842, train_wall=61, wall=34042
2021-01-07 07:41:23 | INFO | train_inner | epoch 092:    249 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.046, ppl=2.07, wps=16936.6, ups=1.62, wpb=10432.2, bsz=347.2, num_updates=51300, lr=9.67302e-06, gnorm=0.868, train_wall=61, wall=34103
2021-01-07 07:42:25 | INFO | train_inner | epoch 092:    349 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.033, ppl=2.05, wps=16981.7, ups=1.62, wpb=10466.6, bsz=372.2, num_updates=51400, lr=9.6636e-06, gnorm=0.851, train_wall=61, wall=34165
2021-01-07 07:43:27 | INFO | train_inner | epoch 092:    449 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.028, ppl=2.04, wps=17192.1, ups=1.62, wpb=10588, bsz=382.6, num_updates=51500, lr=9.65422e-06, gnorm=0.837, train_wall=61, wall=34227
2021-01-07 07:44:28 | INFO | train_inner | epoch 092:    549 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.04, ppl=2.06, wps=16989.8, ups=1.62, wpb=10467.8, bsz=376.6, num_updates=51600, lr=9.64486e-06, gnorm=0.861, train_wall=61, wall=34288
2021-01-07 07:44:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:44:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:44:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:44:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:44:57 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.148 | nll_loss 3.655 | ppl 12.6 | bleu 22.91 | wps 4512.7 | wpb 7508.5 | bsz 272.7 | num_updates 51612 | best_bleu 23.1
2021-01-07 07:44:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:44:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:44:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:45:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:45:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:45:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 92 @ 51612 updates, score 22.91) (writing took 2.9422168880701065 seconds)
2021-01-07 07:45:00 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-07 07:45:00 | INFO | train | epoch 092 | symm_kl 0.389 | self_kl 0 | self_cv 0 | loss 3.342 | nll_loss 1.034 | ppl 2.05 | wps 15802.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 51612 | lr 9.64374e-06 | gnorm 0.855 | train_wall 344 | wall 34320
2021-01-07 07:45:00 | INFO | fairseq.trainer | begin training epoch 93
2021-01-07 07:45:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:45:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:45:57 | INFO | train_inner | epoch 093:     88 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.334, nll_loss=1.029, ppl=2.04, wps=11745, ups=1.13, wpb=10401.8, bsz=371.8, num_updates=51700, lr=9.63552e-06, gnorm=0.857, train_wall=61, wall=34377
2021-01-07 07:46:59 | INFO | train_inner | epoch 093:    188 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.021, ppl=2.03, wps=16821.3, ups=1.61, wpb=10424.8, bsz=366.7, num_updates=51800, lr=9.62622e-06, gnorm=0.85, train_wall=62, wall=34439
2021-01-07 07:48:00 | INFO | train_inner | epoch 093:    288 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.041, ppl=2.06, wps=16771.5, ups=1.62, wpb=10337.8, bsz=370.7, num_updates=51900, lr=9.61694e-06, gnorm=0.885, train_wall=61, wall=34500
2021-01-07 07:49:02 | INFO | train_inner | epoch 093:    388 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.05, ppl=2.07, wps=17154.1, ups=1.63, wpb=10556.2, bsz=357.7, num_updates=52000, lr=9.60769e-06, gnorm=0.862, train_wall=61, wall=34562
2021-01-07 07:50:04 | INFO | train_inner | epoch 093:    488 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.024, ppl=2.03, wps=17244.5, ups=1.62, wpb=10641.7, bsz=366, num_updates=52100, lr=9.59846e-06, gnorm=0.846, train_wall=62, wall=34624
2021-01-07 07:50:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:51:10 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.652 | ppl 12.57 | bleu 23.07 | wps 4604.5 | wpb 7508.5 | bsz 272.7 | num_updates 52173 | best_bleu 23.1
2021-01-07 07:51:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:51:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:51:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:51:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:51:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:51:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 93 @ 52173 updates, score 23.07) (writing took 2.892687728628516 seconds)
2021-01-07 07:51:13 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-07 07:51:13 | INFO | train | epoch 093 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.342 | nll_loss 1.034 | ppl 2.05 | wps 15781.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 52173 | lr 9.59175e-06 | gnorm 0.858 | train_wall 345 | wall 34692
2021-01-07 07:51:13 | INFO | fairseq.trainer | begin training epoch 94
2021-01-07 07:51:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:51:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:51:32 | INFO | train_inner | epoch 094:     27 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.027, ppl=2.04, wps=11910.6, ups=1.13, wpb=10523, bsz=383, num_updates=52200, lr=9.58927e-06, gnorm=0.866, train_wall=61, wall=34712
2021-01-07 07:52:33 | INFO | train_inner | epoch 094:    127 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.025, ppl=2.03, wps=17115, ups=1.64, wpb=10446.3, bsz=361.7, num_updates=52300, lr=9.58009e-06, gnorm=0.846, train_wall=61, wall=34773
2021-01-07 07:53:35 | INFO | train_inner | epoch 094:    227 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.037, ppl=2.05, wps=17163.1, ups=1.61, wpb=10673, bsz=368.2, num_updates=52400, lr=9.57095e-06, gnorm=0.854, train_wall=62, wall=34835
2021-01-07 07:54:37 | INFO | train_inner | epoch 094:    327 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.033, ppl=2.05, wps=17008.1, ups=1.62, wpb=10472.4, bsz=371.3, num_updates=52500, lr=9.56183e-06, gnorm=0.857, train_wall=61, wall=34897
2021-01-07 07:55:38 | INFO | train_inner | epoch 094:    427 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.06, ppl=2.09, wps=16870.7, ups=1.63, wpb=10370.7, bsz=349.8, num_updates=52600, lr=9.55274e-06, gnorm=0.872, train_wall=61, wall=34958
2021-01-07 07:56:40 | INFO | train_inner | epoch 094:    527 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.032, ppl=2.04, wps=16918.7, ups=1.62, wpb=10439.9, bsz=385.8, num_updates=52700, lr=9.54367e-06, gnorm=0.85, train_wall=62, wall=35020
2021-01-07 07:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 07:57:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:57:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:57:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 07:57:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 07:57:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 07:57:22 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.652 | ppl 12.57 | bleu 23.08 | wps 4553.9 | wpb 7508.5 | bsz 272.7 | num_updates 52734 | best_bleu 23.1
2021-01-07 07:57:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 07:57:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:57:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 94 @ 52734 updates, score 23.08) (writing took 2.9294162653386593 seconds)
2021-01-07 07:57:25 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-07 07:57:25 | INFO | train | epoch 094 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.341 | nll_loss 1.034 | ppl 2.05 | wps 15797.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 52734 | lr 9.54059e-06 | gnorm 0.86 | train_wall 344 | wall 35065
2021-01-07 07:57:25 | INFO | fairseq.trainer | begin training epoch 95
2021-01-07 07:57:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 07:57:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 07:58:08 | INFO | train_inner | epoch 095:     66 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.023, ppl=2.03, wps=11857.4, ups=1.13, wpb=10488.1, bsz=372.7, num_updates=52800, lr=9.53463e-06, gnorm=0.877, train_wall=61, wall=35108
2021-01-07 07:59:10 | INFO | train_inner | epoch 095:    166 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.043, ppl=2.06, wps=16794.8, ups=1.62, wpb=10385.3, bsz=367.7, num_updates=52900, lr=9.52561e-06, gnorm=0.858, train_wall=62, wall=35170
2021-01-07 08:00:12 | INFO | train_inner | epoch 095:    266 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.024, ppl=2.03, wps=16938.3, ups=1.62, wpb=10440.6, bsz=369.8, num_updates=53000, lr=9.51662e-06, gnorm=0.858, train_wall=61, wall=35232
2021-01-07 08:01:14 | INFO | train_inner | epoch 095:    366 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.033, ppl=2.05, wps=16950.9, ups=1.61, wpb=10545.6, bsz=362, num_updates=53100, lr=9.50765e-06, gnorm=0.843, train_wall=62, wall=35294
2021-01-07 08:02:16 | INFO | train_inner | epoch 095:    466 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.027, ppl=2.04, wps=17046.3, ups=1.61, wpb=10602.8, bsz=385.2, num_updates=53200, lr=9.49871e-06, gnorm=0.828, train_wall=62, wall=35356
2021-01-07 08:03:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:03:37 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.651 | ppl 12.56 | bleu 23.08 | wps 4516.7 | wpb 7508.5 | bsz 272.7 | num_updates 53295 | best_bleu 23.1
2021-01-07 08:03:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:03:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 95 @ 53295 updates, score 23.08) (writing took 2.950469372794032 seconds)
2021-01-07 08:03:40 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-07 08:03:40 | INFO | train | epoch 095 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.34 | nll_loss 1.033 | ppl 2.05 | wps 15695.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 53295 | lr 9.49024e-06 | gnorm 0.858 | train_wall 346 | wall 35439
2021-01-07 08:03:40 | INFO | fairseq.trainer | begin training epoch 96
2021-01-07 08:03:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:03:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:03:46 | INFO | train_inner | epoch 096:      5 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.042, ppl=2.06, wps=11647.5, ups=1.12, wpb=10427, bsz=364.4, num_updates=53300, lr=9.4898e-06, gnorm=0.894, train_wall=62, wall=35446
2021-01-07 08:04:47 | INFO | train_inner | epoch 096:    105 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.328, nll_loss=1.025, ppl=2.04, wps=17520.5, ups=1.65, wpb=10638.5, bsz=378.6, num_updates=53400, lr=9.48091e-06, gnorm=0.842, train_wall=61, wall=35506
2021-01-07 08:05:48 | INFO | train_inner | epoch 096:    205 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.028, ppl=2.04, wps=16982.9, ups=1.62, wpb=10514.3, bsz=376.9, num_updates=53500, lr=9.47204e-06, gnorm=0.86, train_wall=62, wall=35568
2021-01-07 08:06:50 | INFO | train_inner | epoch 096:    305 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.031, ppl=2.04, wps=16880.9, ups=1.63, wpb=10380.1, bsz=370.6, num_updates=53600, lr=9.4632e-06, gnorm=0.88, train_wall=61, wall=35630
2021-01-07 08:07:51 | INFO | train_inner | epoch 096:    405 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.053, ppl=2.08, wps=16996.1, ups=1.63, wpb=10452.4, bsz=369.2, num_updates=53700, lr=9.45439e-06, gnorm=0.874, train_wall=61, wall=35691
2021-01-07 08:08:53 | INFO | train_inner | epoch 096:    505 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.027, ppl=2.04, wps=16957.4, ups=1.62, wpb=10479.3, bsz=350.8, num_updates=53800, lr=9.4456e-06, gnorm=0.85, train_wall=62, wall=35753
2021-01-07 08:09:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:09:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:09:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:09:49 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.654 | ppl 12.59 | bleu 22.95 | wps 4508.2 | wpb 7508.5 | bsz 272.7 | num_updates 53856 | best_bleu 23.1
2021-01-07 08:09:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:09:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 96 @ 53856 updates, score 22.95) (writing took 2.9131371304392815 seconds)
2021-01-07 08:09:52 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-07 08:09:52 | INFO | train | epoch 096 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.339 | nll_loss 1.033 | ppl 2.05 | wps 15794.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 53856 | lr 9.44069e-06 | gnorm 0.859 | train_wall 344 | wall 35812
2021-01-07 08:09:52 | INFO | fairseq.trainer | begin training epoch 97
2021-01-07 08:09:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:09:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:10:22 | INFO | train_inner | epoch 097:     44 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.04, ppl=2.06, wps=11859.4, ups=1.13, wpb=10478.1, bsz=383.6, num_updates=53900, lr=9.43683e-06, gnorm=0.859, train_wall=61, wall=35842
2021-01-07 08:11:24 | INFO | train_inner | epoch 097:    144 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.037, ppl=2.05, wps=16997.1, ups=1.62, wpb=10521, bsz=379.8, num_updates=54000, lr=9.42809e-06, gnorm=0.833, train_wall=62, wall=35903
2021-01-07 08:12:25 | INFO | train_inner | epoch 097:    244 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.039, ppl=2.05, wps=16818.8, ups=1.62, wpb=10389.1, bsz=350, num_updates=54100, lr=9.41937e-06, gnorm=0.873, train_wall=62, wall=35965
2021-01-07 08:13:27 | INFO | train_inner | epoch 097:    344 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.323, nll_loss=1.025, ppl=2.03, wps=17216, ups=1.62, wpb=10621.6, bsz=381.2, num_updates=54200, lr=9.41068e-06, gnorm=0.836, train_wall=61, wall=36027
2021-01-07 08:14:29 | INFO | train_inner | epoch 097:    444 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.029, ppl=2.04, wps=16942.6, ups=1.62, wpb=10444.2, bsz=366.3, num_updates=54300, lr=9.40201e-06, gnorm=0.863, train_wall=61, wall=36089
2021-01-07 08:15:31 | INFO | train_inner | epoch 097:    544 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.021, ppl=2.03, wps=16789.6, ups=1.61, wpb=10459.4, bsz=363.5, num_updates=54400, lr=9.39336e-06, gnorm=0.855, train_wall=62, wall=36151
2021-01-07 08:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:15:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:15:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:15:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:15:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:15:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:15:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:15:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:15:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:15:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:16:03 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.147 | nll_loss 3.654 | ppl 12.59 | bleu 22.94 | wps 4436.1 | wpb 7508.5 | bsz 272.7 | num_updates 54417 | best_bleu 23.1
2021-01-07 08:16:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:16:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:16:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:16:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:16:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:16:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 97 @ 54417 updates, score 22.94) (writing took 2.9609637558460236 seconds)
2021-01-07 08:16:06 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-07 08:16:06 | INFO | train | epoch 097 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.338 | nll_loss 1.032 | ppl 2.05 | wps 15722.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 54417 | lr 9.3919e-06 | gnorm 0.856 | train_wall 345 | wall 36186
2021-01-07 08:16:06 | INFO | fairseq.trainer | begin training epoch 98
2021-01-07 08:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:17:00 | INFO | train_inner | epoch 098:     83 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.024, ppl=2.03, wps=11702.4, ups=1.13, wpb=10400.4, bsz=371, num_updates=54500, lr=9.38474e-06, gnorm=0.858, train_wall=61, wall=36240
2021-01-07 08:18:02 | INFO | train_inner | epoch 098:    183 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.034, ppl=2.05, wps=16934.9, ups=1.62, wpb=10448.3, bsz=379.5, num_updates=54600, lr=9.37614e-06, gnorm=0.869, train_wall=61, wall=36301
2021-01-07 08:19:03 | INFO | train_inner | epoch 098:    283 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.043, ppl=2.06, wps=17020.5, ups=1.63, wpb=10451.5, bsz=370.4, num_updates=54700, lr=9.36757e-06, gnorm=0.844, train_wall=61, wall=36363
2021-01-07 08:20:04 | INFO | train_inner | epoch 098:    383 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.337, nll_loss=1.03, ppl=2.04, wps=17066.2, ups=1.63, wpb=10491.7, bsz=365.6, num_updates=54800, lr=9.35902e-06, gnorm=0.85, train_wall=61, wall=36424
2021-01-07 08:21:06 | INFO | train_inner | epoch 098:    483 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.028, ppl=2.04, wps=17146.1, ups=1.62, wpb=10560, bsz=363.8, num_updates=54900, lr=9.35049e-06, gnorm=0.841, train_wall=61, wall=36486
2021-01-07 08:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:21:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:21:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:21:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:21:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:22:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:22:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:22:16 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.651 | ppl 12.57 | bleu 22.96 | wps 4532.8 | wpb 7508.5 | bsz 272.7 | num_updates 54978 | best_bleu 23.1
2021-01-07 08:22:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:22:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 98 @ 54978 updates, score 22.96) (writing took 2.919532399624586 seconds)
2021-01-07 08:22:19 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-07 08:22:19 | INFO | train | epoch 098 | symm_kl 0.386 | self_kl 0 | self_cv 0 | loss 3.337 | nll_loss 1.032 | ppl 2.05 | wps 15786.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 54978 | lr 9.34386e-06 | gnorm 0.853 | train_wall 344 | wall 36558
2021-01-07 08:22:19 | INFO | fairseq.trainer | begin training epoch 99
2021-01-07 08:22:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:22:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:22:35 | INFO | train_inner | epoch 099:     22 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.034, ppl=2.05, wps=11722.1, ups=1.12, wpb=10437.8, bsz=366.3, num_updates=55000, lr=9.34199e-06, gnorm=0.865, train_wall=61, wall=36575
2021-01-07 08:23:37 | INFO | train_inner | epoch 099:    122 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.334, nll_loss=1.031, ppl=2.04, wps=16968.1, ups=1.62, wpb=10493.9, bsz=373.4, num_updates=55100, lr=9.33351e-06, gnorm=0.85, train_wall=62, wall=36637
2021-01-07 08:24:39 | INFO | train_inner | epoch 099:    222 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.025, ppl=2.03, wps=16928.2, ups=1.61, wpb=10520.9, bsz=373.9, num_updates=55200, lr=9.32505e-06, gnorm=0.855, train_wall=62, wall=36699
2021-01-07 08:25:41 | INFO | train_inner | epoch 099:    322 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.037, ppl=2.05, wps=16834.1, ups=1.61, wpb=10472.1, bsz=358, num_updates=55300, lr=9.31661e-06, gnorm=0.864, train_wall=62, wall=36761
2021-01-07 08:26:43 | INFO | train_inner | epoch 099:    422 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.041, ppl=2.06, wps=16796.8, ups=1.62, wpb=10380.4, bsz=362.1, num_updates=55400, lr=9.3082e-06, gnorm=0.867, train_wall=62, wall=36823
2021-01-07 08:27:45 | INFO | train_inner | epoch 099:    522 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.029, ppl=2.04, wps=17116.4, ups=1.61, wpb=10614.4, bsz=372.7, num_updates=55500, lr=9.29981e-06, gnorm=0.829, train_wall=62, wall=36885
2021-01-07 08:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:28:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:28:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:28:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:28:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:28:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:28:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:28:30 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.652 | ppl 12.57 | bleu 22.87 | wps 4528.5 | wpb 7508.5 | bsz 272.7 | num_updates 55539 | best_bleu 23.1
2021-01-07 08:28:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:28:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:28:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:28:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 99 @ 55539 updates, score 22.87) (writing took 2.9042990561574697 seconds)
2021-01-07 08:28:33 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-07 08:28:33 | INFO | train | epoch 099 | symm_kl 0.386 | self_kl 0 | self_cv 0 | loss 3.336 | nll_loss 1.032 | ppl 2.04 | wps 15699.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 55539 | lr 9.29655e-06 | gnorm 0.853 | train_wall 346 | wall 36933
2021-01-07 08:28:33 | INFO | fairseq.trainer | begin training epoch 100
2021-01-07 08:28:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:28:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:29:13 | INFO | train_inner | epoch 100:     61 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.023, ppl=2.03, wps=11874.2, ups=1.13, wpb=10473, bsz=379, num_updates=55600, lr=9.29144e-06, gnorm=0.838, train_wall=61, wall=36973
2021-01-07 08:30:15 | INFO | train_inner | epoch 100:    161 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.013, ppl=2.02, wps=17124.8, ups=1.62, wpb=10592.1, bsz=384.8, num_updates=55700, lr=9.2831e-06, gnorm=0.84, train_wall=62, wall=37035
2021-01-07 08:31:17 | INFO | train_inner | epoch 100:    261 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.337, nll_loss=1.034, ppl=2.05, wps=17037.3, ups=1.61, wpb=10559.2, bsz=375.3, num_updates=55800, lr=9.27478e-06, gnorm=0.848, train_wall=62, wall=37097
2021-01-07 08:32:19 | INFO | train_inner | epoch 100:    361 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.323, nll_loss=1.02, ppl=2.03, wps=16961.2, ups=1.62, wpb=10489.3, bsz=363.7, num_updates=55900, lr=9.26648e-06, gnorm=0.84, train_wall=62, wall=37159
2021-01-07 08:33:21 | INFO | train_inner | epoch 100:    461 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.061, ppl=2.09, wps=16693.4, ups=1.61, wpb=10366.2, bsz=351.1, num_updates=56000, lr=9.2582e-06, gnorm=0.871, train_wall=62, wall=37221
2021-01-07 08:34:23 | INFO | train_inner | epoch 100:    561 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.043, ppl=2.06, wps=16690.5, ups=1.61, wpb=10397.8, bsz=372.5, num_updates=56100, lr=9.24995e-06, gnorm=0.856, train_wall=62, wall=37283
2021-01-07 08:34:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:34:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:34:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:34:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:34:44 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.147 | nll_loss 3.653 | ppl 12.58 | bleu 22.89 | wps 4545.9 | wpb 7508.5 | bsz 272.7 | num_updates 56100 | best_bleu 23.1
2021-01-07 08:34:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:34:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 100 @ 56100 updates, score 22.89) (writing took 2.9604347832500935 seconds)
2021-01-07 08:34:47 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-07 08:34:47 | INFO | train | epoch 100 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.335 | nll_loss 1.032 | ppl 2.04 | wps 15716.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 56100 | lr 9.24995e-06 | gnorm 0.849 | train_wall 346 | wall 37307
2021-01-07 08:34:47 | INFO | fairseq.trainer | begin training epoch 101
2021-01-07 08:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:34:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:34:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:35:52 | INFO | train_inner | epoch 101:    100 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.309, nll_loss=1.016, ppl=2.02, wps=11771.3, ups=1.13, wpb=10416.7, bsz=376.1, num_updates=56200, lr=9.24171e-06, gnorm=0.849, train_wall=61, wall=37372
2021-01-07 08:36:54 | INFO | train_inner | epoch 101:    200 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.033, ppl=2.05, wps=16817, ups=1.6, wpb=10505.2, bsz=360.7, num_updates=56300, lr=9.2335e-06, gnorm=0.86, train_wall=62, wall=37434
2021-01-07 08:37:56 | INFO | train_inner | epoch 101:    300 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.035, ppl=2.05, wps=17132.5, ups=1.62, wpb=10549.9, bsz=369.2, num_updates=56400, lr=9.22531e-06, gnorm=0.877, train_wall=61, wall=37496
2021-01-07 08:38:57 | INFO | train_inner | epoch 101:    400 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.027, ppl=2.04, wps=17153.8, ups=1.62, wpb=10559.7, bsz=369.2, num_updates=56500, lr=9.21714e-06, gnorm=0.856, train_wall=61, wall=37557
2021-01-07 08:39:59 | INFO | train_inner | epoch 101:    500 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.042, ppl=2.06, wps=16978.6, ups=1.63, wpb=10418, bsz=373.3, num_updates=56600, lr=9.209e-06, gnorm=0.869, train_wall=61, wall=37619
2021-01-07 08:40:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:40:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:40:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:40:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:40:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:40:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:40:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:40:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:40:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:40:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:40:58 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.653 | ppl 12.58 | bleu 22.97 | wps 4579.6 | wpb 7508.5 | bsz 272.7 | num_updates 56661 | best_bleu 23.1
2021-01-07 08:40:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:40:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:40:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:41:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:41:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:41:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 101 @ 56661 updates, score 22.97) (writing took 2.943215236067772 seconds)
2021-01-07 08:41:00 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-07 08:41:00 | INFO | train | epoch 101 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.334 | nll_loss 1.031 | ppl 2.04 | wps 15759.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 56661 | lr 9.20404e-06 | gnorm 0.861 | train_wall 345 | wall 37680
2021-01-07 08:41:00 | INFO | fairseq.trainer | begin training epoch 102
2021-01-07 08:41:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:41:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:41:27 | INFO | train_inner | epoch 102:     39 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.032, ppl=2.04, wps=11789.3, ups=1.13, wpb=10449.9, bsz=368.6, num_updates=56700, lr=9.20087e-06, gnorm=0.851, train_wall=61, wall=37707
2021-01-07 08:42:29 | INFO | train_inner | epoch 102:    139 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.345, nll_loss=1.037, ppl=2.05, wps=17061.1, ups=1.62, wpb=10510.8, bsz=369.1, num_updates=56800, lr=9.19277e-06, gnorm=0.857, train_wall=61, wall=37769
2021-01-07 08:43:31 | INFO | train_inner | epoch 102:    239 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.323, nll_loss=1.023, ppl=2.03, wps=16756.2, ups=1.6, wpb=10446.7, bsz=374.1, num_updates=56900, lr=9.18469e-06, gnorm=0.845, train_wall=62, wall=37831
2021-01-07 08:44:33 | INFO | train_inner | epoch 102:    339 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.032, ppl=2.05, wps=16752.1, ups=1.61, wpb=10379.8, bsz=371.6, num_updates=57000, lr=9.17663e-06, gnorm=0.848, train_wall=62, wall=37893
2021-01-07 08:45:36 | INFO | train_inner | epoch 102:    439 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.016, ppl=2.02, wps=16802.1, ups=1.6, wpb=10476.7, bsz=369.4, num_updates=57100, lr=9.16859e-06, gnorm=0.843, train_wall=62, wall=37956
2021-01-07 08:46:38 | INFO | train_inner | epoch 102:    539 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.045, ppl=2.06, wps=16954.6, ups=1.6, wpb=10624, bsz=361.5, num_updates=57200, lr=9.16057e-06, gnorm=0.853, train_wall=62, wall=38018
2021-01-07 08:46:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:46:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:46:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:46:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:46:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:47:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:47:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:47:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:47:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:47:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:47:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:47:13 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.647 | ppl 12.53 | bleu 23.02 | wps 4592.5 | wpb 7508.5 | bsz 272.7 | num_updates 57222 | best_bleu 23.1
2021-01-07 08:47:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:47:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:47:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:47:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 102 @ 57222 updates, score 23.02) (writing took 2.9460205398499966 seconds)
2021-01-07 08:47:16 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-07 08:47:16 | INFO | train | epoch 102 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.333 | nll_loss 1.031 | ppl 2.04 | wps 15660.2 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 57222 | lr 9.15881e-06 | gnorm 0.849 | train_wall 347 | wall 38056
2021-01-07 08:47:16 | INFO | fairseq.trainer | begin training epoch 103
2021-01-07 08:47:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:47:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:48:07 | INFO | train_inner | epoch 103:     78 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.028, ppl=2.04, wps=11843, ups=1.13, wpb=10480.8, bsz=382.8, num_updates=57300, lr=9.15258e-06, gnorm=0.854, train_wall=61, wall=38107
2021-01-07 08:49:09 | INFO | train_inner | epoch 103:    178 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.025, ppl=2.04, wps=16598.6, ups=1.6, wpb=10384.9, bsz=361.6, num_updates=57400, lr=9.1446e-06, gnorm=0.857, train_wall=62, wall=38169
2021-01-07 08:50:12 | INFO | train_inner | epoch 103:    278 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.025, ppl=2.04, wps=16835.8, ups=1.6, wpb=10494.5, bsz=361.1, num_updates=57500, lr=9.13664e-06, gnorm=0.843, train_wall=62, wall=38232
2021-01-07 08:51:15 | INFO | train_inner | epoch 103:    378 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.326, nll_loss=1.026, ppl=2.04, wps=16835.1, ups=1.58, wpb=10621.6, bsz=372.3, num_updates=57600, lr=9.12871e-06, gnorm=0.845, train_wall=63, wall=38295
2021-01-07 08:52:17 | INFO | train_inner | epoch 103:    478 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.042, ppl=2.06, wps=16882.2, ups=1.6, wpb=10524.3, bsz=366.1, num_updates=57700, lr=9.1208e-06, gnorm=0.85, train_wall=62, wall=38357
2021-01-07 08:53:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:53:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:53:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:53:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:53:30 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.143 | nll_loss 3.651 | ppl 12.56 | bleu 22.94 | wps 4611.5 | wpb 7508.5 | bsz 272.7 | num_updates 57783 | best_bleu 23.1
2021-01-07 08:53:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:53:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 103 @ 57783 updates, score 22.94) (writing took 2.9491877015680075 seconds)
2021-01-07 08:53:33 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-07 08:53:33 | INFO | train | epoch 103 | symm_kl 0.384 | self_kl 0 | self_cv 0 | loss 3.333 | nll_loss 1.031 | ppl 2.04 | wps 15610.7 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 57783 | lr 9.11424e-06 | gnorm 0.852 | train_wall 349 | wall 38433
2021-01-07 08:53:33 | INFO | fairseq.trainer | begin training epoch 104
2021-01-07 08:53:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:53:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:53:46 | INFO | train_inner | epoch 104:     17 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.341, nll_loss=1.037, ppl=2.05, wps=11652.7, ups=1.12, wpb=10385.8, bsz=379.3, num_updates=57800, lr=9.1129e-06, gnorm=0.86, train_wall=62, wall=38446
2021-01-07 08:54:48 | INFO | train_inner | epoch 104:    117 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.037, ppl=2.05, wps=17033.9, ups=1.62, wpb=10484.8, bsz=358.2, num_updates=57900, lr=9.10503e-06, gnorm=0.851, train_wall=61, wall=38508
2021-01-07 08:55:50 | INFO | train_inner | epoch 104:    217 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.012, ppl=2.02, wps=16962.5, ups=1.61, wpb=10540.3, bsz=374.9, num_updates=58000, lr=9.09718e-06, gnorm=0.851, train_wall=62, wall=38570
2021-01-07 08:56:52 | INFO | train_inner | epoch 104:    317 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.038, ppl=2.05, wps=17027, ups=1.63, wpb=10474.1, bsz=373.2, num_updates=58100, lr=9.08934e-06, gnorm=0.845, train_wall=61, wall=38631
2021-01-07 08:57:53 | INFO | train_inner | epoch 104:    417 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.037, ppl=2.05, wps=16926.5, ups=1.62, wpb=10459.7, bsz=370.1, num_updates=58200, lr=9.08153e-06, gnorm=0.857, train_wall=62, wall=38693
2021-01-07 08:58:56 | INFO | train_inner | epoch 104:    517 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.033, ppl=2.05, wps=16863.3, ups=1.61, wpb=10491.1, bsz=373, num_updates=58300, lr=9.07374e-06, gnorm=0.847, train_wall=62, wall=38755
2021-01-07 08:59:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 08:59:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:59:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:59:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:59:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 08:59:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 08:59:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 08:59:44 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.144 | nll_loss 3.652 | ppl 12.57 | bleu 22.89 | wps 4617.6 | wpb 7508.5 | bsz 272.7 | num_updates 58344 | best_bleu 23.1
2021-01-07 08:59:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 08:59:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:59:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 08:59:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 104 @ 58344 updates, score 22.89) (writing took 2.9928431157022715 seconds)
2021-01-07 08:59:47 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-07 08:59:47 | INFO | train | epoch 104 | symm_kl 0.384 | self_kl 0 | self_cv 0 | loss 3.332 | nll_loss 1.03 | ppl 2.04 | wps 15723.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 58344 | lr 9.07032e-06 | gnorm 0.85 | train_wall 346 | wall 38807
2021-01-07 08:59:47 | INFO | fairseq.trainer | begin training epoch 105
2021-01-07 08:59:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 08:59:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:00:24 | INFO | train_inner | epoch 105:     56 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.028, ppl=2.04, wps=11741.6, ups=1.13, wpb=10358.9, bsz=355.6, num_updates=58400, lr=9.06597e-06, gnorm=0.85, train_wall=61, wall=38844
2021-01-07 09:01:26 | INFO | train_inner | epoch 105:    156 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.025, ppl=2.03, wps=16904, ups=1.6, wpb=10539.8, bsz=392.9, num_updates=58500, lr=9.05822e-06, gnorm=0.836, train_wall=62, wall=38906
2021-01-07 09:02:28 | INFO | train_inner | epoch 105:    256 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.029, ppl=2.04, wps=16955, ups=1.62, wpb=10467.5, bsz=356.3, num_updates=58600, lr=9.05048e-06, gnorm=0.861, train_wall=62, wall=38968
2021-01-07 09:03:30 | INFO | train_inner | epoch 105:    356 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.027, ppl=2.04, wps=16957.1, ups=1.62, wpb=10493.4, bsz=360.9, num_updates=58700, lr=9.04277e-06, gnorm=0.851, train_wall=62, wall=39030
2021-01-07 09:04:32 | INFO | train_inner | epoch 105:    456 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.341, nll_loss=1.038, ppl=2.05, wps=17143.3, ups=1.62, wpb=10605.8, bsz=375.3, num_updates=58800, lr=9.03508e-06, gnorm=0.844, train_wall=62, wall=39092
2021-01-07 09:05:34 | INFO | train_inner | epoch 105:    556 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.326, nll_loss=1.027, ppl=2.04, wps=16863.5, ups=1.61, wpb=10472.5, bsz=374.4, num_updates=58900, lr=9.02741e-06, gnorm=0.837, train_wall=62, wall=39154
2021-01-07 09:05:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:05:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:05:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:05:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:05:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:05:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:05:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:05:57 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.647 | ppl 12.53 | bleu 22.97 | wps 4650 | wpb 7508.5 | bsz 272.7 | num_updates 58905 | best_bleu 23.1
2021-01-07 09:05:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:05:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:05:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:06:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:06:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 105 @ 58905 updates, score 22.97) (writing took 2.914929986000061 seconds)
2021-01-07 09:06:00 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-07 09:06:00 | INFO | train | epoch 105 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.331 | nll_loss 1.03 | ppl 2.04 | wps 15742.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 58905 | lr 9.02702e-06 | gnorm 0.847 | train_wall 346 | wall 39180
2021-01-07 09:06:00 | INFO | fairseq.trainer | begin training epoch 106
2021-01-07 09:06:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:06:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:07:02 | INFO | train_inner | epoch 106:     95 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.026, ppl=2.04, wps=11779.4, ups=1.14, wpb=10356, bsz=363.8, num_updates=59000, lr=9.01975e-06, gnorm=0.847, train_wall=61, wall=39242
2021-01-07 09:08:03 | INFO | train_inner | epoch 106:    195 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.033, ppl=2.05, wps=16947.3, ups=1.62, wpb=10488.8, bsz=375.1, num_updates=59100, lr=9.01212e-06, gnorm=0.844, train_wall=62, wall=39303
2021-01-07 09:09:05 | INFO | train_inner | epoch 106:    295 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.309, nll_loss=1.013, ppl=2.02, wps=16944, ups=1.61, wpb=10496.6, bsz=377.3, num_updates=59200, lr=9.0045e-06, gnorm=0.84, train_wall=62, wall=39365
2021-01-07 09:10:08 | INFO | train_inner | epoch 106:    395 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.359, nll_loss=1.051, ppl=2.07, wps=16873.9, ups=1.61, wpb=10484, bsz=360, num_updates=59300, lr=8.99691e-06, gnorm=0.863, train_wall=62, wall=39427
2021-01-07 09:11:10 | INFO | train_inner | epoch 106:    495 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.308, nll_loss=1.013, ppl=2.02, wps=16832.7, ups=1.6, wpb=10550, bsz=373.9, num_updates=59400, lr=8.98933e-06, gnorm=0.845, train_wall=62, wall=39490
2021-01-07 09:11:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:11:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:11:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:11:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:11:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:11:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:11:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:12:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:12:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:12:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:12:12 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.145 | nll_loss 3.65 | ppl 12.56 | bleu 22.97 | wps 4509.7 | wpb 7508.5 | bsz 272.7 | num_updates 59466 | best_bleu 23.1
2021-01-07 09:12:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:12:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:12:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:12:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:12:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:12:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 106 @ 59466 updates, score 22.97) (writing took 2.9407795537263155 seconds)
2021-01-07 09:12:15 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-07 09:12:15 | INFO | train | epoch 106 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.33 | nll_loss 1.029 | ppl 2.04 | wps 15697.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 59466 | lr 8.98434e-06 | gnorm 0.846 | train_wall 346 | wall 39555
2021-01-07 09:12:15 | INFO | fairseq.trainer | begin training epoch 107
2021-01-07 09:12:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:12:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:12:39 | INFO | train_inner | epoch 107:     34 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.026, ppl=2.04, wps=11878.4, ups=1.13, wpb=10520.1, bsz=368, num_updates=59500, lr=8.98177e-06, gnorm=0.835, train_wall=61, wall=39579
2021-01-07 09:13:41 | INFO | train_inner | epoch 107:    134 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.04, ppl=2.06, wps=16976.6, ups=1.62, wpb=10495.2, bsz=351.2, num_updates=59600, lr=8.97424e-06, gnorm=0.858, train_wall=62, wall=39641
2021-01-07 09:14:43 | INFO | train_inner | epoch 107:    234 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.322, nll_loss=1.022, ppl=2.03, wps=17007.6, ups=1.61, wpb=10537, bsz=386.2, num_updates=59700, lr=8.96672e-06, gnorm=0.849, train_wall=62, wall=39703
2021-01-07 09:15:44 | INFO | train_inner | epoch 107:    334 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.026, ppl=2.04, wps=16878.3, ups=1.62, wpb=10429.7, bsz=378.9, num_updates=59800, lr=8.95922e-06, gnorm=0.853, train_wall=62, wall=39764
2021-01-07 09:16:46 | INFO | train_inner | epoch 107:    434 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.047, ppl=2.07, wps=16915, ups=1.62, wpb=10418.8, bsz=362.9, num_updates=59900, lr=8.95173e-06, gnorm=0.869, train_wall=61, wall=39826
2021-01-07 09:17:48 | INFO | train_inner | epoch 107:    534 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.033, ppl=2.05, wps=16831, ups=1.61, wpb=10467.5, bsz=366, num_updates=60000, lr=8.94427e-06, gnorm=0.855, train_wall=62, wall=39888
2021-01-07 09:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:18:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:18:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:18:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:18:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:18:25 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.646 | ppl 12.52 | bleu 22.95 | wps 4638.4 | wpb 7508.5 | bsz 272.7 | num_updates 60027 | best_bleu 23.1
2021-01-07 09:18:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:18:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:18:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 107 @ 60027 updates, score 22.95) (writing took 2.9372186679393053 seconds)
2021-01-07 09:18:28 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-07 09:18:28 | INFO | train | epoch 107 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.33 | nll_loss 1.03 | ppl 2.04 | wps 15754.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 60027 | lr 8.94226e-06 | gnorm 0.854 | train_wall 345 | wall 39928
2021-01-07 09:18:28 | INFO | fairseq.trainer | begin training epoch 108
2021-01-07 09:18:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:18:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:19:16 | INFO | train_inner | epoch 108:     73 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.298, nll_loss=1.008, ppl=2.01, wps=12068.2, ups=1.14, wpb=10594, bsz=385.7, num_updates=60100, lr=8.93683e-06, gnorm=0.83, train_wall=61, wall=39976
2021-01-07 09:20:18 | INFO | train_inner | epoch 108:    173 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.326, nll_loss=1.026, ppl=2.04, wps=16837.1, ups=1.62, wpb=10396.6, bsz=357.2, num_updates=60200, lr=8.9294e-06, gnorm=0.857, train_wall=62, wall=40038
2021-01-07 09:21:19 | INFO | train_inner | epoch 108:    273 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.035, ppl=2.05, wps=16858.2, ups=1.62, wpb=10410.2, bsz=350.9, num_updates=60300, lr=8.92199e-06, gnorm=0.862, train_wall=62, wall=40099
2021-01-07 09:22:21 | INFO | train_inner | epoch 108:    373 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.326, nll_loss=1.025, ppl=2.04, wps=16928.5, ups=1.61, wpb=10493.9, bsz=380.9, num_updates=60400, lr=8.91461e-06, gnorm=0.841, train_wall=62, wall=40161
2021-01-07 09:23:24 | INFO | train_inner | epoch 108:    473 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.03, ppl=2.04, wps=16869.7, ups=1.61, wpb=10509.5, bsz=358.2, num_updates=60500, lr=8.90724e-06, gnorm=0.853, train_wall=62, wall=40224
2021-01-07 09:24:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:24:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:24:39 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.648 | ppl 12.53 | bleu 22.86 | wps 4629.5 | wpb 7508.5 | bsz 272.7 | num_updates 60588 | best_bleu 23.1
2021-01-07 09:24:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 108 @ 60588 updates, score 22.86) (writing took 2.91267860122025 seconds)
2021-01-07 09:24:42 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-07 09:24:42 | INFO | train | epoch 108 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.328 | nll_loss 1.029 | ppl 2.04 | wps 15740 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 60588 | lr 8.90076e-06 | gnorm 0.847 | train_wall 346 | wall 40302
2021-01-07 09:24:42 | INFO | fairseq.trainer | begin training epoch 109
2021-01-07 09:24:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:24:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:24:52 | INFO | train_inner | epoch 109:     12 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.043, ppl=2.06, wps=11743.6, ups=1.13, wpb=10406.5, bsz=387.7, num_updates=60600, lr=8.89988e-06, gnorm=0.844, train_wall=61, wall=40312
2021-01-07 09:25:54 | INFO | train_inner | epoch 109:    112 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.031, ppl=2.04, wps=17115.2, ups=1.62, wpb=10535.5, bsz=379.2, num_updates=60700, lr=8.89255e-06, gnorm=0.848, train_wall=61, wall=40374
2021-01-07 09:26:56 | INFO | train_inner | epoch 109:    212 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.038, ppl=2.05, wps=16827.6, ups=1.62, wpb=10409.3, bsz=363.3, num_updates=60800, lr=8.88523e-06, gnorm=0.858, train_wall=62, wall=40436
2021-01-07 09:27:57 | INFO | train_inner | epoch 109:    312 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.021, ppl=2.03, wps=17080.1, ups=1.62, wpb=10513.5, bsz=366.7, num_updates=60900, lr=8.87794e-06, gnorm=0.858, train_wall=61, wall=40497
2021-01-07 09:28:59 | INFO | train_inner | epoch 109:    412 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.023, ppl=2.03, wps=16944.4, ups=1.61, wpb=10508.6, bsz=369.4, num_updates=61000, lr=8.87066e-06, gnorm=0.852, train_wall=62, wall=40559
2021-01-07 09:30:01 | INFO | train_inner | epoch 109:    512 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.026, ppl=2.04, wps=16951.2, ups=1.61, wpb=10524.9, bsz=375.1, num_updates=61100, lr=8.86339e-06, gnorm=0.843, train_wall=62, wall=40621
2021-01-07 09:30:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:30:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:30:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:30:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:30:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:30:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:30:53 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.146 | nll_loss 3.652 | ppl 12.57 | bleu 22.85 | wps 4638.3 | wpb 7508.5 | bsz 272.7 | num_updates 61149 | best_bleu 23.1
2021-01-07 09:30:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:30:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:30:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 109 @ 61149 updates, score 22.85) (writing took 2.9478484876453876 seconds)
2021-01-07 09:30:56 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-07 09:30:56 | INFO | train | epoch 109 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.327 | nll_loss 1.028 | ppl 2.04 | wps 15743.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 61149 | lr 8.85984e-06 | gnorm 0.853 | train_wall 346 | wall 40675
2021-01-07 09:30:56 | INFO | fairseq.trainer | begin training epoch 110
2021-01-07 09:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:31:29 | INFO | train_inner | epoch 110:     51 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.022, ppl=2.03, wps=11840.1, ups=1.14, wpb=10408.9, bsz=363, num_updates=61200, lr=8.85615e-06, gnorm=0.84, train_wall=61, wall=40709
2021-01-07 09:32:32 | INFO | train_inner | epoch 110:    151 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.031, ppl=2.04, wps=16860.7, ups=1.61, wpb=10498.6, bsz=358.2, num_updates=61300, lr=8.84892e-06, gnorm=0.841, train_wall=62, wall=40772
2021-01-07 09:33:33 | INFO | train_inner | epoch 110:    251 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.014, ppl=2.02, wps=17132, ups=1.62, wpb=10600.3, bsz=362.2, num_updates=61400, lr=8.84171e-06, gnorm=0.847, train_wall=62, wall=40833
2021-01-07 09:34:35 | INFO | train_inner | epoch 110:    351 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.038, ppl=2.05, wps=16798, ups=1.63, wpb=10325.5, bsz=372.1, num_updates=61500, lr=8.83452e-06, gnorm=0.856, train_wall=61, wall=40895
2021-01-07 09:35:37 | INFO | train_inner | epoch 110:    451 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.04, ppl=2.06, wps=16940.3, ups=1.61, wpb=10510.1, bsz=383, num_updates=61600, lr=8.82735e-06, gnorm=0.843, train_wall=62, wall=40957
2021-01-07 09:36:39 | INFO | train_inner | epoch 110:    551 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.323, nll_loss=1.026, ppl=2.04, wps=17040.2, ups=1.61, wpb=10591.7, bsz=375, num_updates=61700, lr=8.82019e-06, gnorm=0.839, train_wall=62, wall=41019
2021-01-07 09:36:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:36:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:36:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:36:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:36:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:36:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:36:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:37:06 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.143 | nll_loss 3.648 | ppl 12.54 | bleu 22.98 | wps 4563.7 | wpb 7508.5 | bsz 272.7 | num_updates 61710 | best_bleu 23.1
2021-01-07 09:37:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:37:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:37:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:37:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 110 @ 61710 updates, score 22.98) (writing took 2.9282589610666037 seconds)
2021-01-07 09:37:09 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-07 09:37:09 | INFO | train | epoch 110 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.327 | nll_loss 1.028 | ppl 2.04 | wps 15737.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 61710 | lr 8.81948e-06 | gnorm 0.845 | train_wall 345 | wall 41049
2021-01-07 09:37:09 | INFO | fairseq.trainer | begin training epoch 111
2021-01-07 09:37:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:37:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:38:07 | INFO | train_inner | epoch 111:     90 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.021, ppl=2.03, wps=11980.2, ups=1.13, wpb=10577.6, bsz=371.8, num_updates=61800, lr=8.81305e-06, gnorm=0.858, train_wall=61, wall=41107
2021-01-07 09:39:10 | INFO | train_inner | epoch 111:    190 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.035, ppl=2.05, wps=16976.1, ups=1.61, wpb=10573.8, bsz=373.4, num_updates=61900, lr=8.80593e-06, gnorm=0.828, train_wall=62, wall=41170
2021-01-07 09:40:12 | INFO | train_inner | epoch 111:    290 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.337, nll_loss=1.037, ppl=2.05, wps=16838.8, ups=1.61, wpb=10437.5, bsz=357.4, num_updates=62000, lr=8.79883e-06, gnorm=0.838, train_wall=62, wall=41232
2021-01-07 09:41:14 | INFO | train_inner | epoch 111:    390 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.026, ppl=2.04, wps=16852.8, ups=1.61, wpb=10438.8, bsz=369, num_updates=62100, lr=8.79174e-06, gnorm=0.857, train_wall=62, wall=41294
2021-01-07 09:42:16 | INFO | train_inner | epoch 111:    490 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.021, ppl=2.03, wps=16782, ups=1.61, wpb=10444.3, bsz=363.4, num_updates=62200, lr=8.78467e-06, gnorm=0.848, train_wall=62, wall=41356
2021-01-07 09:43:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:43:21 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.648 | ppl 12.54 | bleu 23.02 | wps 4577.5 | wpb 7508.5 | bsz 272.7 | num_updates 62271 | best_bleu 23.1
2021-01-07 09:43:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:43:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 111 @ 62271 updates, score 23.02) (writing took 2.920280708000064 seconds)
2021-01-07 09:43:24 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-07 09:43:24 | INFO | train | epoch 111 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.326 | nll_loss 1.028 | ppl 2.04 | wps 15701.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 62271 | lr 8.77966e-06 | gnorm 0.848 | train_wall 346 | wall 41424
2021-01-07 09:43:24 | INFO | fairseq.trainer | begin training epoch 112
2021-01-07 09:43:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:43:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:43:45 | INFO | train_inner | epoch 112:     29 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.029, ppl=2.04, wps=11772.6, ups=1.13, wpb=10444, bsz=378.3, num_updates=62300, lr=8.77762e-06, gnorm=0.856, train_wall=61, wall=41445
2021-01-07 09:44:46 | INFO | train_inner | epoch 112:    129 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.023, ppl=2.03, wps=17309.6, ups=1.64, wpb=10569.4, bsz=368.6, num_updates=62400, lr=8.77058e-06, gnorm=0.854, train_wall=61, wall=41506
2021-01-07 09:45:47 | INFO | train_inner | epoch 112:    229 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.021, ppl=2.03, wps=16868.9, ups=1.62, wpb=10410.1, bsz=373, num_updates=62500, lr=8.76356e-06, gnorm=0.847, train_wall=62, wall=41567
2021-01-07 09:46:48 | INFO | train_inner | epoch 112:    329 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.038, ppl=2.05, wps=16977.5, ups=1.64, wpb=10342.3, bsz=352.3, num_updates=62600, lr=8.75656e-06, gnorm=0.85, train_wall=61, wall=41628
2021-01-07 09:47:50 | INFO | train_inner | epoch 112:    429 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.022, ppl=2.03, wps=17074.7, ups=1.62, wpb=10542.9, bsz=370.4, num_updates=62700, lr=8.74957e-06, gnorm=0.838, train_wall=62, wall=41690
2021-01-07 09:48:52 | INFO | train_inner | epoch 112:    529 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.027, ppl=2.04, wps=17138.3, ups=1.62, wpb=10588.7, bsz=386.8, num_updates=62800, lr=8.7426e-06, gnorm=0.834, train_wall=62, wall=41752
2021-01-07 09:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:49:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:49:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:49:33 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.648 | ppl 12.54 | bleu 22.98 | wps 4614.3 | wpb 7508.5 | bsz 272.7 | num_updates 62832 | best_bleu 23.1
2021-01-07 09:49:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:49:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 112 @ 62832 updates, score 22.98) (writing took 2.9672201722860336 seconds)
2021-01-07 09:49:36 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-07 09:49:36 | INFO | train | epoch 112 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.325 | nll_loss 1.028 | ppl 2.04 | wps 15819.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 62832 | lr 8.74038e-06 | gnorm 0.845 | train_wall 344 | wall 41796
2021-01-07 09:49:36 | INFO | fairseq.trainer | begin training epoch 113
2021-01-07 09:49:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:49:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:49:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:49:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:50:20 | INFO | train_inner | epoch 113:     68 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.043, ppl=2.06, wps=11766.4, ups=1.13, wpb=10391.5, bsz=362.3, num_updates=62900, lr=8.73565e-06, gnorm=0.847, train_wall=61, wall=41840
2021-01-07 09:51:22 | INFO | train_inner | epoch 113:    168 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.313, nll_loss=1.021, ppl=2.03, wps=17058.4, ups=1.62, wpb=10551, bsz=376.7, num_updates=63000, lr=8.72872e-06, gnorm=0.835, train_wall=62, wall=41902
2021-01-07 09:52:24 | INFO | train_inner | epoch 113:    268 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.304, nll_loss=1.013, ppl=2.02, wps=16945.8, ups=1.61, wpb=10547.2, bsz=392.5, num_updates=63100, lr=8.7218e-06, gnorm=0.825, train_wall=62, wall=41964
2021-01-07 09:53:26 | INFO | train_inner | epoch 113:    368 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.016, ppl=2.02, wps=17003.7, ups=1.61, wpb=10553.5, bsz=368.2, num_updates=63200, lr=8.71489e-06, gnorm=0.839, train_wall=62, wall=42026
2021-01-07 09:54:28 | INFO | train_inner | epoch 113:    468 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.031, ppl=2.04, wps=16972.1, ups=1.63, wpb=10419.6, bsz=368.2, num_updates=63300, lr=8.70801e-06, gnorm=0.842, train_wall=61, wall=42088
2021-01-07 09:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 09:55:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 09:55:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 09:55:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 09:55:46 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.14 | nll_loss 3.646 | ppl 12.51 | bleu 22.98 | wps 4587.9 | wpb 7508.5 | bsz 272.7 | num_updates 63393 | best_bleu 23.1
2021-01-07 09:55:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 09:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 113 @ 63393 updates, score 22.98) (writing took 2.9326031040400267 seconds)
2021-01-07 09:55:49 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-07 09:55:49 | INFO | train | epoch 113 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.324 | nll_loss 1.027 | ppl 2.04 | wps 15752.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 63393 | lr 8.70162e-06 | gnorm 0.843 | train_wall 345 | wall 42169
2021-01-07 09:55:49 | INFO | fairseq.trainer | begin training epoch 114
2021-01-07 09:55:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 09:55:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 09:55:56 | INFO | train_inner | epoch 114:      7 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.047, ppl=2.07, wps=11668.2, ups=1.13, wpb=10353.9, bsz=349.5, num_updates=63400, lr=8.70114e-06, gnorm=0.878, train_wall=61, wall=42176
2021-01-07 09:56:57 | INFO | train_inner | epoch 114:    107 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.041, ppl=2.06, wps=17310.2, ups=1.64, wpb=10544.9, bsz=354.9, num_updates=63500, lr=8.69428e-06, gnorm=0.856, train_wall=61, wall=42237
2021-01-07 09:57:59 | INFO | train_inner | epoch 114:    207 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.043, ppl=2.06, wps=16782.3, ups=1.62, wpb=10390.9, bsz=355.6, num_updates=63600, lr=8.68744e-06, gnorm=0.864, train_wall=62, wall=42299
2021-01-07 09:59:01 | INFO | train_inner | epoch 114:    307 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.316, nll_loss=1.021, ppl=2.03, wps=16990.3, ups=1.61, wpb=10550.5, bsz=380.5, num_updates=63700, lr=8.68062e-06, gnorm=0.834, train_wall=62, wall=42361
2021-01-07 10:00:03 | INFO | train_inner | epoch 114:    407 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.032, ppl=2.04, wps=16924.6, ups=1.63, wpb=10383, bsz=352.6, num_updates=63800, lr=8.67382e-06, gnorm=0.864, train_wall=61, wall=42423
2021-01-07 10:01:05 | INFO | train_inner | epoch 114:    507 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.286, nll_loss=1.005, ppl=2.01, wps=17078.7, ups=1.61, wpb=10584.4, bsz=397, num_updates=63900, lr=8.66703e-06, gnorm=0.825, train_wall=62, wall=42485
2021-01-07 10:01:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:01:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:01:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:01:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:01:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:01:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:01:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:01:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:01:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:01:58 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.141 | nll_loss 3.648 | ppl 12.54 | bleu 23.01 | wps 4659.8 | wpb 7508.5 | bsz 272.7 | num_updates 63954 | best_bleu 23.1
2021-01-07 10:01:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:01:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:02:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:02:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:02:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 114 @ 63954 updates, score 23.01) (writing took 2.947753706946969 seconds)
2021-01-07 10:02:01 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-07 10:02:01 | INFO | train | epoch 114 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.323 | nll_loss 1.026 | ppl 2.04 | wps 15791.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 63954 | lr 8.66337e-06 | gnorm 0.847 | train_wall 344 | wall 42541
2021-01-07 10:02:01 | INFO | fairseq.trainer | begin training epoch 115
2021-01-07 10:02:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:02:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:02:32 | INFO | train_inner | epoch 115:     46 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.31, nll_loss=1.021, ppl=2.03, wps=11883.9, ups=1.14, wpb=10419.5, bsz=383.5, num_updates=64000, lr=8.66025e-06, gnorm=0.838, train_wall=61, wall=42572
2021-01-07 10:03:34 | INFO | train_inner | epoch 115:    146 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.025, ppl=2.04, wps=17019.9, ups=1.62, wpb=10512.7, bsz=373, num_updates=64100, lr=8.6535e-06, gnorm=0.838, train_wall=62, wall=42634
2021-01-07 10:04:36 | INFO | train_inner | epoch 115:    246 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.328, nll_loss=1.031, ppl=2.04, wps=16980.3, ups=1.62, wpb=10505.5, bsz=369, num_updates=64200, lr=8.64675e-06, gnorm=0.833, train_wall=62, wall=42696
2021-01-07 10:05:38 | INFO | train_inner | epoch 115:    346 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.316, nll_loss=1.016, ppl=2.02, wps=16889.2, ups=1.61, wpb=10467.7, bsz=351.8, num_updates=64300, lr=8.64003e-06, gnorm=0.863, train_wall=62, wall=42758
2021-01-07 10:06:40 | INFO | train_inner | epoch 115:    446 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.313, nll_loss=1.018, ppl=2.03, wps=16973.2, ups=1.62, wpb=10463, bsz=375.5, num_updates=64400, lr=8.63332e-06, gnorm=0.837, train_wall=61, wall=42820
2021-01-07 10:07:42 | INFO | train_inner | epoch 115:    546 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.032, ppl=2.05, wps=16897.3, ups=1.6, wpb=10533.9, bsz=376.4, num_updates=64500, lr=8.62662e-06, gnorm=0.839, train_wall=62, wall=42882
2021-01-07 10:07:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:07:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:07:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:07:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:07:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:07:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:07:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:08:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:08:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:08:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:08:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:08:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:08:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:08:13 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.648 | ppl 12.54 | bleu 23.08 | wps 4325.1 | wpb 7508.5 | bsz 272.7 | num_updates 64515 | best_bleu 23.1
2021-01-07 10:08:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:08:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:08:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:08:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 115 @ 64515 updates, score 23.08) (writing took 3.049566460773349 seconds)
2021-01-07 10:08:16 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-07 10:08:16 | INFO | train | epoch 115 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.323 | nll_loss 1.026 | ppl 2.04 | wps 15699.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 64515 | lr 8.62562e-06 | gnorm 0.844 | train_wall 345 | wall 42916
2021-01-07 10:08:16 | INFO | fairseq.trainer | begin training epoch 116
2021-01-07 10:08:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:08:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:09:11 | INFO | train_inner | epoch 116:     85 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.304, nll_loss=1.015, ppl=2.02, wps=11774.6, ups=1.12, wpb=10490.9, bsz=371.4, num_updates=64600, lr=8.61994e-06, gnorm=0.852, train_wall=61, wall=42971
2021-01-07 10:10:14 | INFO | train_inner | epoch 116:    185 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.019, ppl=2.03, wps=17017.6, ups=1.6, wpb=10627.5, bsz=379.8, num_updates=64700, lr=8.61328e-06, gnorm=0.828, train_wall=62, wall=43033
2021-01-07 10:11:15 | INFO | train_inner | epoch 116:    285 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.028, ppl=2.04, wps=17031.8, ups=1.63, wpb=10458.3, bsz=364.9, num_updates=64800, lr=8.60663e-06, gnorm=0.854, train_wall=61, wall=43095
2021-01-07 10:12:17 | INFO | train_inner | epoch 116:    385 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.031, ppl=2.04, wps=16709.6, ups=1.62, wpb=10313.1, bsz=356.6, num_updates=64900, lr=8.6e-06, gnorm=0.865, train_wall=62, wall=43157
2021-01-07 10:13:18 | INFO | train_inner | epoch 116:    485 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.031, ppl=2.04, wps=17115.9, ups=1.62, wpb=10558.7, bsz=373.2, num_updates=65000, lr=8.59338e-06, gnorm=0.846, train_wall=61, wall=43218
2021-01-07 10:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:14:26 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.144 | nll_loss 3.649 | ppl 12.55 | bleu 22.92 | wps 4487.1 | wpb 7508.5 | bsz 272.7 | num_updates 65076 | best_bleu 23.1
2021-01-07 10:14:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 116 @ 65076 updates, score 22.92) (writing took 2.932823970913887 seconds)
2021-01-07 10:14:29 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-07 10:14:29 | INFO | train | epoch 116 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.322 | nll_loss 1.026 | ppl 2.04 | wps 15749.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 65076 | lr 8.58836e-06 | gnorm 0.846 | train_wall 345 | wall 43289
2021-01-07 10:14:29 | INFO | fairseq.trainer | begin training epoch 117
2021-01-07 10:14:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:14:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:14:47 | INFO | train_inner | epoch 117:     24 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.04, ppl=2.06, wps=11758.4, ups=1.13, wpb=10433.2, bsz=364.9, num_updates=65100, lr=8.58678e-06, gnorm=0.836, train_wall=61, wall=43307
2021-01-07 10:15:48 | INFO | train_inner | epoch 117:    124 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.334, nll_loss=1.033, ppl=2.05, wps=16975.9, ups=1.63, wpb=10409.8, bsz=361.1, num_updates=65200, lr=8.58019e-06, gnorm=0.854, train_wall=61, wall=43368
2021-01-07 10:16:50 | INFO | train_inner | epoch 117:    224 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.303, nll_loss=1.014, ppl=2.02, wps=17191.9, ups=1.62, wpb=10633.3, bsz=378.2, num_updates=65300, lr=8.57362e-06, gnorm=0.82, train_wall=62, wall=43430
2021-01-07 10:17:52 | INFO | train_inner | epoch 117:    324 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.03, ppl=2.04, wps=17125.7, ups=1.63, wpb=10513.5, bsz=373.8, num_updates=65400, lr=8.56706e-06, gnorm=0.846, train_wall=61, wall=43492
2021-01-07 10:18:54 | INFO | train_inner | epoch 117:    424 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.316, nll_loss=1.019, ppl=2.03, wps=16662.7, ups=1.62, wpb=10312.1, bsz=366.6, num_updates=65500, lr=8.56052e-06, gnorm=0.856, train_wall=62, wall=43553
2021-01-07 10:19:55 | INFO | train_inner | epoch 117:    524 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.027, ppl=2.04, wps=17178.5, ups=1.62, wpb=10586.1, bsz=376.6, num_updates=65600, lr=8.55399e-06, gnorm=0.841, train_wall=61, wall=43615
2021-01-07 10:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:20:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:20:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:20:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:20:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:20:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:20:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:20:41 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.141 | nll_loss 3.646 | ppl 12.52 | bleu 23.01 | wps 4096.7 | wpb 7508.5 | bsz 272.7 | num_updates 65637 | best_bleu 23.1
2021-01-07 10:20:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:20:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:20:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 117 @ 65637 updates, score 23.01) (writing took 2.9202321600168943 seconds)
2021-01-07 10:20:44 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-07 10:20:44 | INFO | train | epoch 117 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.322 | nll_loss 1.026 | ppl 2.04 | wps 15713.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 65637 | lr 8.55158e-06 | gnorm 0.846 | train_wall 344 | wall 43664
2021-01-07 10:20:44 | INFO | fairseq.trainer | begin training epoch 118
2021-01-07 10:20:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:20:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:21:25 | INFO | train_inner | epoch 118:     63 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.031, ppl=2.04, wps=11683.6, ups=1.11, wpb=10507.2, bsz=371.1, num_updates=65700, lr=8.54748e-06, gnorm=0.853, train_wall=61, wall=43705
2021-01-07 10:22:26 | INFO | train_inner | epoch 118:    163 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.313, nll_loss=1.021, ppl=2.03, wps=17056.4, ups=1.63, wpb=10468.5, bsz=364.8, num_updates=65800, lr=8.54098e-06, gnorm=0.841, train_wall=61, wall=43766
2021-01-07 10:23:28 | INFO | train_inner | epoch 118:    263 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.032, ppl=2.04, wps=17084.4, ups=1.62, wpb=10547.3, bsz=352.7, num_updates=65900, lr=8.5345e-06, gnorm=0.841, train_wall=62, wall=43828
2021-01-07 10:24:29 | INFO | train_inner | epoch 118:    363 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.328, nll_loss=1.028, ppl=2.04, wps=16864.4, ups=1.63, wpb=10317.5, bsz=377.6, num_updates=66000, lr=8.52803e-06, gnorm=0.877, train_wall=61, wall=43889
2021-01-07 10:25:31 | INFO | train_inner | epoch 118:    463 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.026, ppl=2.04, wps=16853.2, ups=1.61, wpb=10461.9, bsz=371.4, num_updates=66100, lr=8.52158e-06, gnorm=0.849, train_wall=62, wall=43951
2021-01-07 10:26:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:26:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:26:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:26:53 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.143 | nll_loss 3.648 | ppl 12.54 | bleu 23.04 | wps 4430 | wpb 7508.5 | bsz 272.7 | num_updates 66198 | best_bleu 23.1
2021-01-07 10:26:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:26:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:26:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:26:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 118 @ 66198 updates, score 23.04) (writing took 2.9615438412874937 seconds)
2021-01-07 10:26:56 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-07 10:26:56 | INFO | train | epoch 118 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.321 | nll_loss 1.025 | ppl 2.04 | wps 15783.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 66198 | lr 8.51527e-06 | gnorm 0.848 | train_wall 344 | wall 44036
2021-01-07 10:26:56 | INFO | fairseq.trainer | begin training epoch 119
2021-01-07 10:26:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:26:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:27:01 | INFO | train_inner | epoch 119:      2 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.024, ppl=2.03, wps=11801.4, ups=1.12, wpb=10538.9, bsz=381, num_updates=66200, lr=8.51514e-06, gnorm=0.838, train_wall=61, wall=44041
2021-01-07 10:28:01 | INFO | train_inner | epoch 119:    102 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.321, nll_loss=1.025, ppl=2.04, wps=17280.3, ups=1.65, wpb=10468.9, bsz=361.2, num_updates=66300, lr=8.50871e-06, gnorm=0.852, train_wall=60, wall=44101
2021-01-07 10:29:03 | INFO | train_inner | epoch 119:    202 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.026, ppl=2.04, wps=17021.4, ups=1.63, wpb=10470, bsz=361.1, num_updates=66400, lr=8.5023e-06, gnorm=0.843, train_wall=61, wall=44163
2021-01-07 10:30:05 | INFO | train_inner | epoch 119:    302 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.023, ppl=2.03, wps=17041.8, ups=1.62, wpb=10504.7, bsz=354.4, num_updates=66500, lr=8.49591e-06, gnorm=0.864, train_wall=61, wall=44224
2021-01-07 10:31:06 | INFO | train_inner | epoch 119:    402 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.037, ppl=2.05, wps=16996.2, ups=1.62, wpb=10500.5, bsz=387, num_updates=66600, lr=8.48953e-06, gnorm=0.831, train_wall=62, wall=44286
2021-01-07 10:32:08 | INFO | train_inner | epoch 119:    502 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.018, ppl=2.03, wps=16911.4, ups=1.62, wpb=10435.2, bsz=377, num_updates=66700, lr=8.48316e-06, gnorm=0.835, train_wall=62, wall=44348
2021-01-07 10:32:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:32:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:32:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:32:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:32:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:32:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:32:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:32:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:32:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:32:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:33:08 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.14 | nll_loss 3.647 | ppl 12.53 | bleu 22.89 | wps 4031.3 | wpb 7508.5 | bsz 272.7 | num_updates 66759 | best_bleu 23.1
2021-01-07 10:33:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:33:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:33:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:33:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 119 @ 66759 updates, score 22.89) (writing took 3.0727234687656164 seconds)
2021-01-07 10:33:11 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-07 10:33:11 | INFO | train | epoch 119 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.32 | nll_loss 1.025 | ppl 2.04 | wps 15707.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 66759 | lr 8.47941e-06 | gnorm 0.845 | train_wall 344 | wall 44411
2021-01-07 10:33:11 | INFO | fairseq.trainer | begin training epoch 120
2021-01-07 10:33:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:33:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:33:39 | INFO | train_inner | epoch 120:     41 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.019, ppl=2.03, wps=11550.1, ups=1.1, wpb=10483.7, bsz=370.9, num_updates=66800, lr=8.47681e-06, gnorm=0.855, train_wall=61, wall=44439
2021-01-07 10:34:40 | INFO | train_inner | epoch 120:    141 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.02, ppl=2.03, wps=17232, ups=1.62, wpb=10607.1, bsz=374.2, num_updates=66900, lr=8.47047e-06, gnorm=0.85, train_wall=61, wall=44500
2021-01-07 10:35:42 | INFO | train_inner | epoch 120:    241 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.313, nll_loss=1.02, ppl=2.03, wps=17025.3, ups=1.63, wpb=10456.1, bsz=366.8, num_updates=67000, lr=8.46415e-06, gnorm=0.853, train_wall=61, wall=44562
2021-01-07 10:36:43 | INFO | train_inner | epoch 120:    341 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.018, ppl=2.03, wps=16927.5, ups=1.62, wpb=10444.4, bsz=357.8, num_updates=67100, lr=8.45784e-06, gnorm=0.851, train_wall=61, wall=44623
2021-01-07 10:37:45 | INFO | train_inner | epoch 120:    441 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.028, ppl=2.04, wps=16974.5, ups=1.63, wpb=10424.8, bsz=368.3, num_updates=67200, lr=8.45154e-06, gnorm=0.847, train_wall=61, wall=44685
2021-01-07 10:38:46 | INFO | train_inner | epoch 120:    541 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.041, ppl=2.06, wps=17300.7, ups=1.63, wpb=10605.6, bsz=375.4, num_updates=67300, lr=8.44526e-06, gnorm=0.844, train_wall=61, wall=44746
2021-01-07 10:38:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:38:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:39:20 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.141 | nll_loss 3.645 | ppl 12.51 | bleu 23.07 | wps 4573 | wpb 7508.5 | bsz 272.7 | num_updates 67320 | best_bleu 23.1
2021-01-07 10:39:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:39:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:39:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:39:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:39:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:39:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 120 @ 67320 updates, score 23.07) (writing took 3.085867116227746 seconds)
2021-01-07 10:39:23 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-07 10:39:23 | INFO | train | epoch 120 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.32 | nll_loss 1.025 | ppl 2.04 | wps 15812.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 67320 | lr 8.44401e-06 | gnorm 0.852 | train_wall 343 | wall 44783
2021-01-07 10:39:23 | INFO | fairseq.trainer | begin training epoch 121
2021-01-07 10:39:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:39:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:40:14 | INFO | train_inner | epoch 121:     80 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.314, nll_loss=1.022, ppl=2.03, wps=11732.9, ups=1.13, wpb=10360, bsz=377.4, num_updates=67400, lr=8.43899e-06, gnorm=0.849, train_wall=61, wall=44834
2021-01-07 10:41:16 | INFO | train_inner | epoch 121:    180 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.328, nll_loss=1.029, ppl=2.04, wps=16957.2, ups=1.62, wpb=10438.5, bsz=365, num_updates=67500, lr=8.43274e-06, gnorm=0.85, train_wall=61, wall=44896
2021-01-07 10:42:17 | INFO | train_inner | epoch 121:    280 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.018, ppl=2.03, wps=17072.8, ups=1.63, wpb=10495.8, bsz=381.8, num_updates=67600, lr=8.4265e-06, gnorm=0.836, train_wall=61, wall=44957
2021-01-07 10:43:20 | INFO | train_inner | epoch 121:    380 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.306, nll_loss=1.018, ppl=2.03, wps=17049, ups=1.61, wpb=10614.3, bsz=373, num_updates=67700, lr=8.42028e-06, gnorm=0.817, train_wall=62, wall=45020
2021-01-07 10:44:21 | INFO | train_inner | epoch 121:    480 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.337, nll_loss=1.037, ppl=2.05, wps=16874.8, ups=1.63, wpb=10379.1, bsz=374.7, num_updates=67800, lr=8.41406e-06, gnorm=0.86, train_wall=61, wall=45081
2021-01-07 10:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:45:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:45:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:45:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:45:35 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.14 | nll_loss 3.647 | ppl 12.53 | bleu 22.94 | wps 4028.9 | wpb 7508.5 | bsz 272.7 | num_updates 67881 | best_bleu 23.1
2021-01-07 10:45:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 121 @ 67881 updates, score 22.94) (writing took 3.1844291891902685 seconds)
2021-01-07 10:45:38 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-07 10:45:38 | INFO | train | epoch 121 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.319 | nll_loss 1.025 | ppl 2.04 | wps 15663.9 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 67881 | lr 8.40904e-06 | gnorm 0.84 | train_wall 345 | wall 45158
2021-01-07 10:45:38 | INFO | fairseq.trainer | begin training epoch 122
2021-01-07 10:45:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:45:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:45:53 | INFO | train_inner | epoch 122:     19 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.314, nll_loss=1.022, ppl=2.03, wps=11504.4, ups=1.09, wpb=10543.3, bsz=353.2, num_updates=67900, lr=8.40787e-06, gnorm=0.832, train_wall=62, wall=45173
2021-01-07 10:46:54 | INFO | train_inner | epoch 122:    119 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.035, ppl=2.05, wps=17063.7, ups=1.63, wpb=10469.1, bsz=367.1, num_updates=68000, lr=8.40168e-06, gnorm=0.838, train_wall=61, wall=45234
2021-01-07 10:47:56 | INFO | train_inner | epoch 122:    219 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.321, nll_loss=1.026, ppl=2.04, wps=17164.7, ups=1.62, wpb=10627.5, bsz=372, num_updates=68100, lr=8.39551e-06, gnorm=0.835, train_wall=62, wall=45296
2021-01-07 10:48:58 | INFO | train_inner | epoch 122:    319 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.306, nll_loss=1.018, ppl=2.03, wps=17022.3, ups=1.62, wpb=10475.4, bsz=381, num_updates=68200, lr=8.38935e-06, gnorm=0.833, train_wall=61, wall=45358
2021-01-07 10:49:59 | INFO | train_inner | epoch 122:    419 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.018, ppl=2.03, wps=17137.1, ups=1.62, wpb=10560.7, bsz=379.2, num_updates=68300, lr=8.38321e-06, gnorm=0.85, train_wall=61, wall=45419
2021-01-07 10:51:01 | INFO | train_inner | epoch 122:    519 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.038, ppl=2.05, wps=16884.8, ups=1.62, wpb=10416.8, bsz=347.6, num_updates=68400, lr=8.37708e-06, gnorm=0.857, train_wall=61, wall=45481
2021-01-07 10:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:51:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:51:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:51:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:51:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:51:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:51:47 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.14 | nll_loss 3.646 | ppl 12.52 | bleu 22.95 | wps 4607.2 | wpb 7508.5 | bsz 272.7 | num_updates 68442 | best_bleu 23.1
2021-01-07 10:51:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:51:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:51:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:51:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 122 @ 68442 updates, score 22.95) (writing took 3.1029359474778175 seconds)
2021-01-07 10:51:51 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2021-01-07 10:51:51 | INFO | train | epoch 122 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.318 | nll_loss 1.025 | ppl 2.03 | wps 15790 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 68442 | lr 8.37451e-06 | gnorm 0.844 | train_wall 344 | wall 45531
2021-01-07 10:51:51 | INFO | fairseq.trainer | begin training epoch 123
2021-01-07 10:51:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:51:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:52:29 | INFO | train_inner | epoch 123:     58 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.025, ppl=2.04, wps=11723.5, ups=1.14, wpb=10306, bsz=367.7, num_updates=68500, lr=8.37096e-06, gnorm=0.851, train_wall=60, wall=45569
2021-01-07 10:53:30 | INFO | train_inner | epoch 123:    158 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.321, nll_loss=1.024, ppl=2.03, wps=17054, ups=1.63, wpb=10458.7, bsz=364.2, num_updates=68600, lr=8.36486e-06, gnorm=0.848, train_wall=61, wall=45630
2021-01-07 10:54:32 | INFO | train_inner | epoch 123:    258 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.019, ppl=2.03, wps=17214.4, ups=1.63, wpb=10583.4, bsz=376.8, num_updates=68700, lr=8.35877e-06, gnorm=0.835, train_wall=61, wall=45692
2021-01-07 10:55:33 | INFO | train_inner | epoch 123:    358 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.028, ppl=2.04, wps=17049.8, ups=1.63, wpb=10491.3, bsz=374.3, num_updates=68800, lr=8.35269e-06, gnorm=0.84, train_wall=61, wall=45753
2021-01-07 10:56:35 | INFO | train_inner | epoch 123:    458 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.314, nll_loss=1.024, ppl=2.03, wps=17006.5, ups=1.62, wpb=10495.3, bsz=372.2, num_updates=68900, lr=8.34663e-06, gnorm=0.849, train_wall=62, wall=45815
2021-01-07 10:57:36 | INFO | train_inner | epoch 123:    558 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.317, nll_loss=1.022, ppl=2.03, wps=17050.4, ups=1.63, wpb=10468.9, bsz=362.9, num_updates=69000, lr=8.34058e-06, gnorm=0.837, train_wall=61, wall=45876
2021-01-07 10:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 10:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:57:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 10:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 10:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 10:57:59 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.647 | ppl 12.53 | bleu 23.07 | wps 4640.9 | wpb 7508.5 | bsz 272.7 | num_updates 69003 | best_bleu 23.1
2021-01-07 10:57:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 10:58:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:58:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:58:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 123 @ 69003 updates, score 23.07) (writing took 3.0780213810503483 seconds)
2021-01-07 10:58:02 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2021-01-07 10:58:02 | INFO | train | epoch 123 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.317 | nll_loss 1.024 | ppl 2.03 | wps 15827.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 69003 | lr 8.3404e-06 | gnorm 0.841 | train_wall 343 | wall 45902
2021-01-07 10:58:02 | INFO | fairseq.trainer | begin training epoch 124
2021-01-07 10:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:58:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 10:58:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 10:59:04 | INFO | train_inner | epoch 124:     97 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.302, nll_loss=1.013, ppl=2.02, wps=12014.1, ups=1.14, wpb=10563.2, bsz=370.6, num_updates=69100, lr=8.33454e-06, gnorm=0.836, train_wall=61, wall=45964
2021-01-07 11:00:06 | INFO | train_inner | epoch 124:    197 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.023, ppl=2.03, wps=17095, ups=1.63, wpb=10503.8, bsz=367.7, num_updates=69200, lr=8.32851e-06, gnorm=0.857, train_wall=61, wall=46026
2021-01-07 11:01:07 | INFO | train_inner | epoch 124:    297 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.303, nll_loss=1.019, ppl=2.03, wps=17074.8, ups=1.63, wpb=10488.1, bsz=373, num_updates=69300, lr=8.3225e-06, gnorm=0.823, train_wall=61, wall=46087
2021-01-07 11:02:09 | INFO | train_inner | epoch 124:    397 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.027, ppl=2.04, wps=16904.8, ups=1.62, wpb=10455.3, bsz=373.1, num_updates=69400, lr=8.31651e-06, gnorm=0.84, train_wall=62, wall=46149
2021-01-07 11:03:11 | INFO | train_inner | epoch 124:    497 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.028, ppl=2.04, wps=16904.7, ups=1.61, wpb=10484.8, bsz=373.7, num_updates=69500, lr=8.31052e-06, gnorm=0.834, train_wall=62, wall=46211
2021-01-07 11:03:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 11:03:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:03:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:03:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:03:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:03:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:03:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:03:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:03:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:03:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:04:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:04:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:04:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:04:14 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.141 | nll_loss 3.646 | ppl 12.52 | bleu 23.02 | wps 4028.7 | wpb 7508.5 | bsz 272.7 | num_updates 69564 | best_bleu 23.1
2021-01-07 11:04:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 11:04:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:04:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:04:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:04:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:04:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 124 @ 69564 updates, score 23.02) (writing took 3.1174935437738895 seconds)
2021-01-07 11:04:17 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2021-01-07 11:04:17 | INFO | train | epoch 124 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.317 | nll_loss 1.025 | ppl 2.03 | wps 15688.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 69564 | lr 8.3067e-06 | gnorm 0.841 | train_wall 344 | wall 46277
2021-01-07 11:04:17 | INFO | fairseq.trainer | begin training epoch 125
2021-01-07 11:04:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:04:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:04:42 | INFO | train_inner | epoch 125:     36 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.032, ppl=2.05, wps=11407.3, ups=1.1, wpb=10378.5, bsz=358.7, num_updates=69600, lr=8.30455e-06, gnorm=0.854, train_wall=61, wall=46302
2021-01-07 11:05:43 | INFO | train_inner | epoch 125:    136 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.024, ppl=2.03, wps=16919.3, ups=1.63, wpb=10390.2, bsz=368.1, num_updates=69700, lr=8.29859e-06, gnorm=0.84, train_wall=61, wall=46363
2021-01-07 11:06:45 | INFO | train_inner | epoch 125:    236 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.317, nll_loss=1.024, ppl=2.03, wps=17027.6, ups=1.62, wpb=10505.2, bsz=377, num_updates=69800, lr=8.29264e-06, gnorm=0.835, train_wall=61, wall=46425
2021-01-07 11:07:47 | INFO | train_inner | epoch 125:    336 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.021, ppl=2.03, wps=17190.8, ups=1.62, wpb=10584.2, bsz=357.4, num_updates=69900, lr=8.28671e-06, gnorm=0.851, train_wall=61, wall=46487
2021-01-07 11:08:48 | INFO | train_inner | epoch 125:    436 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.027, ppl=2.04, wps=17043.6, ups=1.62, wpb=10513.1, bsz=380.5, num_updates=70000, lr=8.28079e-06, gnorm=0.829, train_wall=61, wall=46548
2021-01-07 11:09:50 | INFO | train_inner | epoch 125:    536 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.317, nll_loss=1.025, ppl=2.03, wps=16974.7, ups=1.62, wpb=10504.4, bsz=363.2, num_updates=70100, lr=8.27488e-06, gnorm=0.838, train_wall=62, wall=46610
2021-01-07 11:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 11:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:10:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:10:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:10:27 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.141 | nll_loss 3.645 | ppl 12.51 | bleu 22.94 | wps 4601.6 | wpb 7508.5 | bsz 272.7 | num_updates 70125 | best_bleu 23.1
2021-01-07 11:10:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 11:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:10:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:10:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 125 @ 70125 updates, score 22.94) (writing took 3.0743473153561354 seconds)
2021-01-07 11:10:30 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2021-01-07 11:10:30 | INFO | train | epoch 125 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.316 | nll_loss 1.024 | ppl 2.03 | wps 15779 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 70125 | lr 8.2734e-06 | gnorm 0.839 | train_wall 344 | wall 46650
2021-01-07 11:10:30 | INFO | fairseq.trainer | begin training epoch 126
2021-01-07 11:10:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:10:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:11:19 | INFO | train_inner | epoch 126:     75 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.303, nll_loss=1.017, ppl=2.02, wps=11834.7, ups=1.13, wpb=10462.3, bsz=384.6, num_updates=70200, lr=8.26898e-06, gnorm=0.838, train_wall=61, wall=46699
2021-01-07 11:12:20 | INFO | train_inner | epoch 126:    175 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.304, nll_loss=1.015, ppl=2.02, wps=17252.7, ups=1.63, wpb=10603.6, bsz=381.5, num_updates=70300, lr=8.2631e-06, gnorm=0.818, train_wall=61, wall=46760
2021-01-07 11:13:22 | INFO | train_inner | epoch 126:    275 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.022, ppl=2.03, wps=16904.5, ups=1.61, wpb=10473.9, bsz=369.4, num_updates=70400, lr=8.25723e-06, gnorm=0.835, train_wall=62, wall=46822
2021-01-07 11:14:24 | INFO | train_inner | epoch 126:    375 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.348, nll_loss=1.046, ppl=2.07, wps=16764.1, ups=1.62, wpb=10348.3, bsz=361.8, num_updates=70500, lr=8.25137e-06, gnorm=0.849, train_wall=62, wall=46884
2021-01-07 11:15:26 | INFO | train_inner | epoch 126:    475 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.304, nll_loss=1.014, ppl=2.02, wps=16781.2, ups=1.61, wpb=10418.3, bsz=367.7, num_updates=70600, lr=8.24552e-06, gnorm=0.854, train_wall=62, wall=46946
2021-01-07 11:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 11:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:16:40 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.143 | nll_loss 3.648 | ppl 12.54 | bleu 22.92 | wps 4548.7 | wpb 7508.5 | bsz 272.7 | num_updates 70686 | best_bleu 23.1
2021-01-07 11:16:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 11:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 126 @ 70686 updates, score 22.92) (writing took 3.104298759251833 seconds)
2021-01-07 11:16:43 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2021-01-07 11:16:43 | INFO | train | epoch 126 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.315 | nll_loss 1.023 | ppl 2.03 | wps 15743.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 70686 | lr 8.24051e-06 | gnorm 0.841 | train_wall 345 | wall 47023
2021-01-07 11:16:43 | INFO | fairseq.trainer | begin training epoch 127
2021-01-07 11:16:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:16:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:16:55 | INFO | train_inner | epoch 127:     14 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.024, ppl=2.03, wps=11791.8, ups=1.12, wpb=10526.9, bsz=356.7, num_updates=70700, lr=8.23969e-06, gnorm=0.854, train_wall=62, wall=47035
2021-01-07 11:17:57 | INFO | train_inner | epoch 127:    114 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.029, ppl=2.04, wps=16886.4, ups=1.63, wpb=10381.8, bsz=354.9, num_updates=70800, lr=8.23387e-06, gnorm=0.852, train_wall=61, wall=47097
2021-01-07 11:18:59 | INFO | train_inner | epoch 127:    214 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.019, ppl=2.03, wps=16921.9, ups=1.61, wpb=10507.1, bsz=379.6, num_updates=70900, lr=8.22806e-06, gnorm=0.852, train_wall=62, wall=47159
2021-01-07 11:20:01 | INFO | train_inner | epoch 127:    314 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.299, nll_loss=1.013, ppl=2.02, wps=16993.6, ups=1.61, wpb=10528.8, bsz=372.4, num_updates=71000, lr=8.22226e-06, gnorm=0.831, train_wall=62, wall=47221
2021-01-07 11:21:03 | INFO | train_inner | epoch 127:    414 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.317, nll_loss=1.028, ppl=2.04, wps=17015.3, ups=1.62, wpb=10513.1, bsz=379.4, num_updates=71100, lr=8.21648e-06, gnorm=0.832, train_wall=62, wall=47282
2021-01-07 11:22:05 | INFO | train_inner | epoch 127:    514 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.023, ppl=2.03, wps=16940.7, ups=1.61, wpb=10520.8, bsz=356.9, num_updates=71200, lr=8.21071e-06, gnorm=0.851, train_wall=62, wall=47345
2021-01-07 11:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 11:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:22:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:22:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 11:22:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 11:22:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 11:22:55 | INFO | valid | epoch 127 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.142 | nll_loss 3.648 | ppl 12.53 | bleu 22.93 | wps 4563.8 | wpb 7508.5 | bsz 272.7 | num_updates 71247 | best_bleu 23.1
2021-01-07 11:22:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 11:22:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:22:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:22:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:22:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:22:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/zero_dropout/checkpoint_last.pt (epoch 127 @ 71247 updates, score 22.93) (writing took 3.0940992794930935 seconds)
2021-01-07 11:22:58 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2021-01-07 11:22:58 | INFO | train | epoch 127 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.315 | nll_loss 1.023 | ppl 2.03 | wps 15708.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 71247 | lr 8.208e-06 | gnorm 0.844 | train_wall 346 | wall 47398
2021-01-07 11:22:58 | INFO | fairseq.trainer | begin training epoch 128
2021-01-07 11:22:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 11:23:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 11:23:33 | INFO | train_inner | epoch 128:     53 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.299, nll_loss=1.014, ppl=2.02, wps=11862.4, ups=1.13, wpb=10524, bsz=389.6, num_updates=71300, lr=8.20495e-06, gnorm=0.837, train_wall=61, wall=47433
2021-01-07 11:24:35 | INFO | train_inner | epoch 128:    153 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.035, ppl=2.05, wps=16939.6, ups=1.62, wpb=10427.3, bsz=347.7, num_updates=71400, lr=8.1992e-06, gnorm=0.86, train_wall=61, wall=47495
2021-01-07 11:25:37 | INFO | train_inner | epoch 128:    253 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.316, nll_loss=1.022, ppl=2.03, wps=16731, ups=1.6, wpb=10450, bsz=360.7, num_updates=71500, lr=8.19346e-06, gnorm=0.847, train_wall=62, wall=47557
2021-01-07 11:26:39 | INFO | train_inner | epoch 128:    353 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.303, nll_loss=1.018, ppl=2.02, wps=17117.5, ups=1.61, wpb=10606, bsz=391.2, num_updates=71600, lr=8.18774e-06, gnorm=0.837, train_wall=62, wall=47619
2021-01-07 11:27:41 | INFO | train_inner | epoch 128:    453 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.025, ppl=2.03, wps=16875.8, ups=1.62, wpb=10448.5, bsz=358.1, num_updates=71700, lr=8.18203e-06, gnorm=0.848, train_wall=62, wall=47681
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 798 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
