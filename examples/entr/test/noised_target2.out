nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/change_p
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 -p 0.4'
2021-01-07 12:00:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:10367
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10367
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10367
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-07 12:00:49 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-07 12:00:53 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10367', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/change_p', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-07 12:00:53 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-07 12:00:53 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-07 12:00:53 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-07 12:00:53 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-07 12:00:53 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-07 12:00:54 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-07 12:00:54 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-07 12:00:54 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-07 12:00:54 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-07 12:00:54 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-07 12:00:54 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-07 12:00:54 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-07 12:00:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-07 12:00:54 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-07 12:00:54 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-07 12:00:54 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-07 12:00:54 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-07 12:00:54 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-07 12:00:54 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-07 12:00:55 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-07 12:00:55 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 2 @ 561 updates)
2021-01-07 12:00:55 | INFO | fairseq.trainer | loading train data for epoch 2
2021-01-07 12:00:55 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-07 12:00:55 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-07 12:00:55 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-07 12:00:56 | INFO | fairseq.trainer | begin training epoch 2
2021-01-07 12:00:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:00:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:00:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:01:23 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=0.913, self_kl=0, self_cv=0, loss=4.094, nll_loss=1.083, ppl=2.12, wps=12017.8, ups=1.16, wpb=10345.6, bsz=358.4, num_updates=600, lr=8.08e-06, gnorm=1.636, train_wall=61, wall=0
2021-01-07 12:02:23 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=0.794, self_kl=0, self_cv=0, loss=3.899, nll_loss=1.015, ppl=2.02, wps=17499.8, ups=1.66, wpb=10532.9, bsz=366.4, num_updates=700, lr=9.41e-06, gnorm=1.504, train_wall=60, wall=0
2021-01-07 12:03:24 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=0.76, self_kl=0, self_cv=0, loss=3.841, nll_loss=0.998, ppl=2, wps=17154.1, ups=1.63, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.074e-05, gnorm=1.454, train_wall=61, wall=0
2021-01-07 12:04:26 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=0.745, self_kl=0, self_cv=0, loss=3.825, nll_loss=1.006, ppl=2.01, wps=17076.4, ups=1.62, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.207e-05, gnorm=1.424, train_wall=62, wall=0
2021-01-07 12:05:27 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=0.736, self_kl=0, self_cv=0, loss=3.827, nll_loss=1.021, ppl=2.03, wps=17064.8, ups=1.63, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.34e-05, gnorm=1.388, train_wall=61, wall=0
2021-01-07 12:06:29 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=0.723, self_kl=0, self_cv=0, loss=3.815, nll_loss=1.031, ppl=2.04, wps=16905.8, ups=1.62, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.473e-05, gnorm=1.375, train_wall=62, wall=0
2021-01-07 12:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:07:04 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.449 | nll_loss 3.953 | ppl 15.49 | bleu 22.54 | wps 4345.3 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.54
2021-01-07 12:07:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:07:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 2 @ 1122 updates, score 22.54) (writing took 4.622311541810632 seconds)
2021-01-07 12:07:09 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-07 12:07:09 | INFO | train | epoch 002 | symm_kl 0.978 | self_kl 0 | self_cv 0 | loss 4.147 | nll_loss 0.996 | ppl 1.99 | wps 15931.4 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.50226e-05 | gnorm 1.936 | train_wall 685 | wall 0
2021-01-07 12:07:09 | INFO | fairseq.trainer | begin training epoch 3
2021-01-07 12:07:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:07:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:07:59 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=0.701, self_kl=0, self_cv=0, loss=3.774, nll_loss=1.02, ppl=2.03, wps=11575.5, ups=1.11, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.606e-05, gnorm=1.361, train_wall=60, wall=0
2021-01-07 12:09:01 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=0.692, self_kl=0, self_cv=0, loss=3.77, nll_loss=1.028, ppl=2.04, wps=16953.7, ups=1.63, wpb=10420.6, bsz=376, num_updates=1300, lr=1.739e-05, gnorm=1.327, train_wall=61, wall=0
2021-01-07 12:10:02 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=0.678, self_kl=0, self_cv=0, loss=3.745, nll_loss=1.023, ppl=2.03, wps=17014.4, ups=1.62, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.872e-05, gnorm=1.296, train_wall=61, wall=0
2021-01-07 12:11:04 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=0.675, self_kl=0, self_cv=0, loss=3.762, nll_loss=1.048, ppl=2.07, wps=16960.7, ups=1.62, wpb=10472.3, bsz=374.7, num_updates=1500, lr=2.005e-05, gnorm=1.296, train_wall=62, wall=0
2021-01-07 12:12:06 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=0.653, self_kl=0, self_cv=0, loss=3.717, nll_loss=1.03, ppl=2.04, wps=17244.1, ups=1.62, wpb=10650.7, bsz=373.4, num_updates=1600, lr=2.138e-05, gnorm=1.254, train_wall=62, wall=0
2021-01-07 12:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:12:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:12:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:12:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:13:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:13:18 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.402 | nll_loss 3.903 | ppl 14.96 | bleu 22.58 | wps 4516 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.58
2021-01-07 12:13:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:13:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:13:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:13:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.58) (writing took 4.597845584154129 seconds)
2021-01-07 12:13:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-07 12:13:23 | INFO | train | epoch 003 | symm_kl 0.674 | self_kl 0 | self_cv 0 | loss 3.746 | nll_loss 1.03 | ppl 2.04 | wps 15739.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 2.24839e-05 | gnorm 1.3 | train_wall 343 | wall 0
2021-01-07 12:13:23 | INFO | fairseq.trainer | begin training epoch 4
2021-01-07 12:13:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:13:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:13:36 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=0.65, self_kl=0, self_cv=0, loss=3.71, nll_loss=1.028, ppl=2.04, wps=11553.5, ups=1.11, wpb=10447.8, bsz=352, num_updates=1700, lr=2.271e-05, gnorm=1.284, train_wall=61, wall=0
2021-01-07 12:14:37 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=0.647, self_kl=0, self_cv=0, loss=3.715, nll_loss=1.039, ppl=2.05, wps=17212.8, ups=1.64, wpb=10469.1, bsz=365.6, num_updates=1800, lr=2.404e-05, gnorm=1.261, train_wall=61, wall=0
2021-01-07 12:15:39 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=0.64, self_kl=0, self_cv=0, loss=3.712, nll_loss=1.045, ppl=2.06, wps=16676.2, ups=1.62, wpb=10271.1, bsz=367.4, num_updates=1900, lr=2.537e-05, gnorm=1.26, train_wall=61, wall=0
2021-01-07 12:16:40 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=0.627, self_kl=0, self_cv=0, loss=3.681, nll_loss=1.032, ppl=2.04, wps=17211.5, ups=1.63, wpb=10571.4, bsz=356.9, num_updates=2000, lr=2.67e-05, gnorm=1.227, train_wall=61, wall=0
2021-01-07 12:17:42 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=0.616, self_kl=0, self_cv=0, loss=3.672, nll_loss=1.039, ppl=2.05, wps=17153.9, ups=1.63, wpb=10532.7, bsz=370.6, num_updates=2100, lr=2.803e-05, gnorm=1.206, train_wall=61, wall=0
2021-01-07 12:18:43 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=0.607, self_kl=0, self_cv=0, loss=3.66, nll_loss=1.041, ppl=2.06, wps=17212.6, ups=1.62, wpb=10614.4, bsz=387.6, num_updates=2200, lr=2.936e-05, gnorm=1.174, train_wall=61, wall=0
2021-01-07 12:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:19:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:19:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:19:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:19:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:19:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:19:31 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.373 | nll_loss 3.872 | ppl 14.64 | bleu 22.65 | wps 4639.2 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.65
2021-01-07 12:19:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:19:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:19:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 4 @ 2244 updates, score 22.65) (writing took 4.661038029938936 seconds)
2021-01-07 12:19:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-07 12:19:36 | INFO | train | epoch 004 | symm_kl 0.625 | self_kl 0 | self_cv 0 | loss 3.683 | nll_loss 1.038 | ppl 2.05 | wps 15769.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 2.99452e-05 | gnorm 1.223 | train_wall 343 | wall 0
2021-01-07 12:19:36 | INFO | fairseq.trainer | begin training epoch 5
2021-01-07 12:19:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:19:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:20:13 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=0.595, self_kl=0, self_cv=0, loss=3.634, nll_loss=1.029, ppl=2.04, wps=11652.8, ups=1.12, wpb=10433.5, bsz=373.3, num_updates=2300, lr=3.069e-05, gnorm=1.173, train_wall=60, wall=0
2021-01-07 12:21:14 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=0.597, self_kl=0, self_cv=0, loss=3.647, nll_loss=1.04, ppl=2.06, wps=16944.4, ups=1.62, wpb=10447.6, bsz=372.4, num_updates=2400, lr=3.202e-05, gnorm=1.194, train_wall=61, wall=0
2021-01-07 12:22:16 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=0.593, self_kl=0, self_cv=0, loss=3.644, nll_loss=1.044, ppl=2.06, wps=17141.8, ups=1.63, wpb=10524.3, bsz=363.8, num_updates=2500, lr=3.335e-05, gnorm=1.161, train_wall=61, wall=0
2021-01-07 12:23:18 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=0.597, self_kl=0, self_cv=0, loss=3.663, nll_loss=1.06, ppl=2.08, wps=16861.6, ups=1.62, wpb=10415.7, bsz=357.5, num_updates=2600, lr=3.468e-05, gnorm=1.182, train_wall=62, wall=0
2021-01-07 12:24:20 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=0.575, self_kl=0, self_cv=0, loss=3.615, nll_loss=1.039, ppl=2.05, wps=17038, ups=1.61, wpb=10565, bsz=383, num_updates=2700, lr=3.601e-05, gnorm=1.154, train_wall=62, wall=0
2021-01-07 12:25:21 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=0.574, self_kl=0, self_cv=0, loss=3.616, nll_loss=1.043, ppl=2.06, wps=16940, ups=1.62, wpb=10479.6, bsz=374.7, num_updates=2800, lr=3.734e-05, gnorm=1.153, train_wall=62, wall=0
2021-01-07 12:25:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:25:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:25:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:25:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:25:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:25:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:25:45 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.349 | nll_loss 3.845 | ppl 14.37 | bleu 22.68 | wps 4559.2 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.68
2021-01-07 12:25:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:25:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:25:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:25:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 5 @ 2805 updates, score 22.68) (writing took 4.721613874658942 seconds)
2021-01-07 12:25:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-07 12:25:50 | INFO | train | epoch 005 | symm_kl 0.588 | self_kl 0 | self_cv 0 | loss 3.637 | nll_loss 1.043 | ppl 2.06 | wps 15704.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 3.74065e-05 | gnorm 1.17 | train_wall 344 | wall 0
2021-01-07 12:25:50 | INFO | fairseq.trainer | begin training epoch 6
2021-01-07 12:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:26:51 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=0.568, self_kl=0, self_cv=0, loss=3.604, nll_loss=1.038, ppl=2.05, wps=11505.7, ups=1.12, wpb=10318.1, bsz=377.4, num_updates=2900, lr=3.867e-05, gnorm=1.145, train_wall=60, wall=0
2021-01-07 12:27:53 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=0.564, self_kl=0, self_cv=0, loss=3.601, nll_loss=1.041, ppl=2.06, wps=17274.1, ups=1.62, wpb=10679.3, bsz=372.1, num_updates=3000, lr=4e-05, gnorm=1.12, train_wall=62, wall=0
2021-01-07 12:28:55 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=0.56, self_kl=0, self_cv=0, loss=3.595, nll_loss=1.041, ppl=2.06, wps=17013.3, ups=1.62, wpb=10477.8, bsz=365.4, num_updates=3100, lr=3.93496e-05, gnorm=1.128, train_wall=61, wall=0
2021-01-07 12:29:56 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=0.563, self_kl=0, self_cv=0, loss=3.62, nll_loss=1.064, ppl=2.09, wps=17234.9, ups=1.64, wpb=10517.4, bsz=358, num_updates=3200, lr=3.87298e-05, gnorm=1.126, train_wall=61, wall=0
2021-01-07 12:30:57 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=0.543, self_kl=0, self_cv=0, loss=3.564, nll_loss=1.034, ppl=2.05, wps=17070.2, ups=1.62, wpb=10534.9, bsz=372, num_updates=3300, lr=3.81385e-05, gnorm=1.102, train_wall=62, wall=0
2021-01-07 12:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:31:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:31:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:31:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:31:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:31:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:31:58 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.323 | nll_loss 3.815 | ppl 14.08 | bleu 22.84 | wps 4817.1 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.84
2021-01-07 12:31:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:31:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:31:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:32:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:32:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:32:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 6 @ 3366 updates, score 22.84) (writing took 4.669365780428052 seconds)
2021-01-07 12:32:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-07 12:32:03 | INFO | train | epoch 006 | symm_kl 0.559 | self_kl 0 | self_cv 0 | loss 3.599 | nll_loss 1.047 | ppl 2.07 | wps 15791.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 3.77627e-05 | gnorm 1.123 | train_wall 343 | wall 0
2021-01-07 12:32:03 | INFO | fairseq.trainer | begin training epoch 7
2021-01-07 12:32:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:32:26 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=0.55, self_kl=0, self_cv=0, loss=3.604, nll_loss=1.069, ppl=2.1, wps=11501.2, ups=1.12, wpb=10237, bsz=369, num_updates=3400, lr=3.75735e-05, gnorm=1.123, train_wall=61, wall=0
2021-01-07 12:33:28 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=0.544, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.042, ppl=2.06, wps=17064.5, ups=1.62, wpb=10508.4, bsz=371.6, num_updates=3500, lr=3.70328e-05, gnorm=1.105, train_wall=61, wall=0
2021-01-07 12:34:29 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=0.541, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.048, ppl=2.07, wps=16938.1, ups=1.63, wpb=10404.4, bsz=363.4, num_updates=3600, lr=3.65148e-05, gnorm=1.097, train_wall=61, wall=0
2021-01-07 12:35:31 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=0.538, self_kl=0, self_cv=0, loss=3.575, nll_loss=1.053, ppl=2.07, wps=16980.9, ups=1.62, wpb=10456.6, bsz=375.4, num_updates=3700, lr=3.6018e-05, gnorm=1.085, train_wall=61, wall=0
2021-01-07 12:36:33 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=0.537, self_kl=0, self_cv=0, loss=3.566, nll_loss=1.044, ppl=2.06, wps=16966.1, ups=1.62, wpb=10467.8, bsz=366.5, num_updates=3800, lr=3.55409e-05, gnorm=1.099, train_wall=61, wall=0
2021-01-07 12:37:34 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=0.527, self_kl=0, self_cv=0, loss=3.554, nll_loss=1.048, ppl=2.07, wps=17345.1, ups=1.62, wpb=10688.9, bsz=373.3, num_updates=3900, lr=3.50823e-05, gnorm=1.069, train_wall=61, wall=0
2021-01-07 12:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:37:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:37:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:37:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:37:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:37:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:37:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:37:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:37:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:37:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:38:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:38:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:38:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:38:10 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.313 | nll_loss 3.805 | ppl 13.98 | bleu 22.71 | wps 4990.4 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.84
2021-01-07 12:38:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:38:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 7 @ 3927 updates, score 22.71) (writing took 2.9088753014802933 seconds)
2021-01-07 12:38:13 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-07 12:38:13 | INFO | train | epoch 007 | symm_kl 0.536 | self_kl 0 | self_cv 0 | loss 3.567 | nll_loss 1.047 | ppl 2.07 | wps 15868.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 3.49615e-05 | gnorm 1.091 | train_wall 344 | wall 0
2021-01-07 12:38:13 | INFO | fairseq.trainer | begin training epoch 8
2021-01-07 12:38:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:38:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:39:01 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=0.519, self_kl=0, self_cv=0, loss=3.534, nll_loss=1.037, ppl=2.05, wps=12196.2, ups=1.15, wpb=10590, bsz=375, num_updates=4000, lr=3.4641e-05, gnorm=1.054, train_wall=61, wall=0
2021-01-07 12:40:02 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=0.531, self_kl=0, self_cv=0, loss=3.555, nll_loss=1.041, ppl=2.06, wps=17101.1, ups=1.63, wpb=10504.7, bsz=358.9, num_updates=4100, lr=3.4216e-05, gnorm=1.073, train_wall=61, wall=0
2021-01-07 12:41:04 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=0.521, self_kl=0, self_cv=0, loss=3.543, nll_loss=1.042, ppl=2.06, wps=16930.3, ups=1.63, wpb=10367.5, bsz=366.4, num_updates=4200, lr=3.38062e-05, gnorm=1.066, train_wall=61, wall=0
2021-01-07 12:42:05 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=0.515, self_kl=0, self_cv=0, loss=3.53, nll_loss=1.037, ppl=2.05, wps=16986.2, ups=1.63, wpb=10416.1, bsz=389.5, num_updates=4300, lr=3.34108e-05, gnorm=1.065, train_wall=61, wall=0
2021-01-07 12:43:07 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.522, nll_loss=1.041, ppl=2.06, wps=17302.8, ups=1.62, wpb=10648.7, bsz=379.6, num_updates=4400, lr=3.30289e-05, gnorm=1.033, train_wall=61, wall=0
2021-01-07 12:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:44:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:44:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:44:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:44:22 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.303 | nll_loss 3.793 | ppl 13.86 | bleu 22.58 | wps 4563.8 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.84
2021-01-07 12:44:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 8 @ 4488 updates, score 22.58) (writing took 2.978109873831272 seconds)
2021-01-07 12:44:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-07 12:44:25 | INFO | train | epoch 008 | symm_kl 0.52 | self_kl 0 | self_cv 0 | loss 3.542 | nll_loss 1.044 | ppl 2.06 | wps 15838.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 3.27035e-05 | gnorm 1.062 | train_wall 343 | wall 0
2021-01-07 12:44:25 | INFO | fairseq.trainer | begin training epoch 9
2021-01-07 12:44:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:44:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:44:35 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=0.521, self_kl=0, self_cv=0, loss=3.56, nll_loss=1.063, ppl=2.09, wps=11670.7, ups=1.13, wpb=10320.4, bsz=352.8, num_updates=4500, lr=3.26599e-05, gnorm=1.082, train_wall=61, wall=0
2021-01-07 12:45:36 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=0.509, self_kl=0, self_cv=0, loss=3.513, nll_loss=1.027, ppl=2.04, wps=17368.6, ups=1.65, wpb=10532.6, bsz=374.2, num_updates=4600, lr=3.23029e-05, gnorm=1.032, train_wall=60, wall=0
2021-01-07 12:46:37 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.553, nll_loss=1.055, ppl=2.08, wps=17186.9, ups=1.63, wpb=10528, bsz=345, num_updates=4700, lr=3.19574e-05, gnorm=1.062, train_wall=61, wall=0
2021-01-07 12:47:38 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=0.501, self_kl=0, self_cv=0, loss=3.509, nll_loss=1.037, ppl=2.05, wps=17031.4, ups=1.63, wpb=10473.1, bsz=377.1, num_updates=4800, lr=3.16228e-05, gnorm=1.029, train_wall=61, wall=0
2021-01-07 12:48:40 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=0.505, self_kl=0, self_cv=0, loss=3.517, nll_loss=1.04, ppl=2.06, wps=17141.1, ups=1.64, wpb=10483.1, bsz=368.3, num_updates=4900, lr=3.12984e-05, gnorm=1.037, train_wall=61, wall=0
2021-01-07 12:49:41 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=0.498, self_kl=0, self_cv=0, loss=3.506, nll_loss=1.039, ppl=2.06, wps=17035.3, ups=1.62, wpb=10514.7, bsz=388.6, num_updates=5000, lr=3.09839e-05, gnorm=1.026, train_wall=62, wall=0
2021-01-07 12:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:50:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:50:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:50:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:50:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:50:31 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.292 | nll_loss 3.779 | ppl 13.73 | bleu 22.85 | wps 5099.2 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.85
2021-01-07 12:50:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:50:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:50:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:50:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 9 @ 5049 updates, score 22.85) (writing took 4.722238857299089 seconds)
2021-01-07 12:50:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-07 12:50:35 | INFO | train | epoch 009 | symm_kl 0.507 | self_kl 0 | self_cv 0 | loss 3.523 | nll_loss 1.042 | ppl 2.06 | wps 15866.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 3.08332e-05 | gnorm 1.039 | train_wall 342 | wall 0
2021-01-07 12:50:35 | INFO | fairseq.trainer | begin training epoch 10
2021-01-07 12:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:51:09 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=0.508, self_kl=0, self_cv=0, loss=3.535, nll_loss=1.055, ppl=2.08, wps=11773.1, ups=1.14, wpb=10353, bsz=356, num_updates=5100, lr=3.06786e-05, gnorm=1.042, train_wall=60, wall=0
2021-01-07 12:52:11 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.512, nll_loss=1.042, ppl=2.06, wps=17199.9, ups=1.63, wpb=10577.3, bsz=365.6, num_updates=5200, lr=3.03822e-05, gnorm=1.025, train_wall=61, wall=0
2021-01-07 12:53:13 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=0.504, self_kl=0, self_cv=0, loss=3.517, nll_loss=1.041, ppl=2.06, wps=16948.4, ups=1.61, wpb=10501.8, bsz=363.6, num_updates=5300, lr=3.00942e-05, gnorm=1.034, train_wall=62, wall=0
2021-01-07 12:54:14 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.512, nll_loss=1.044, ppl=2.06, wps=16903.3, ups=1.62, wpb=10450.1, bsz=375.6, num_updates=5400, lr=2.98142e-05, gnorm=1.028, train_wall=62, wall=0
2021-01-07 12:55:16 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=0.487, self_kl=0, self_cv=0, loss=3.484, nll_loss=1.032, ppl=2.05, wps=16939.2, ups=1.62, wpb=10472.1, bsz=373.6, num_updates=5500, lr=2.9542e-05, gnorm=1.015, train_wall=62, wall=0
2021-01-07 12:56:18 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=0.491, self_kl=0, self_cv=0, loss=3.497, nll_loss=1.04, ppl=2.06, wps=17041.8, ups=1.62, wpb=10516.7, bsz=381.2, num_updates=5600, lr=2.9277e-05, gnorm=1.017, train_wall=62, wall=0
2021-01-07 12:56:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 12:56:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:56:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:56:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 12:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 12:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 12:56:45 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.284 | nll_loss 3.774 | ppl 13.68 | bleu 22.63 | wps 4633.6 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.85
2021-01-07 12:56:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 12:56:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:56:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:56:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.63) (writing took 2.8829874880611897 seconds)
2021-01-07 12:56:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-07 12:56:48 | INFO | train | epoch 010 | symm_kl 0.497 | self_kl 0 | self_cv 0 | loss 3.506 | nll_loss 1.04 | ppl 2.06 | wps 15788 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 2.92509e-05 | gnorm 1.026 | train_wall 344 | wall 0
2021-01-07 12:56:48 | INFO | fairseq.trainer | begin training epoch 11
2021-01-07 12:56:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 12:56:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 12:57:45 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=0.496, self_kl=0, self_cv=0, loss=3.504, nll_loss=1.038, ppl=2.05, wps=11866.6, ups=1.15, wpb=10358.6, bsz=351.1, num_updates=5700, lr=2.90191e-05, gnorm=1.033, train_wall=60, wall=0
2021-01-07 12:58:47 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=0.482, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.023, ppl=2.03, wps=17103.3, ups=1.62, wpb=10564, bsz=383.6, num_updates=5800, lr=2.87678e-05, gnorm=0.996, train_wall=62, wall=0
2021-01-07 12:59:48 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.511, nll_loss=1.051, ppl=2.07, wps=16942.9, ups=1.63, wpb=10400.1, bsz=355.4, num_updates=5900, lr=2.8523e-05, gnorm=1.024, train_wall=61, wall=0
2021-01-07 13:00:50 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=0.493, self_kl=0, self_cv=0, loss=3.514, nll_loss=1.055, ppl=2.08, wps=16817.7, ups=1.62, wpb=10394.2, bsz=372.6, num_updates=6000, lr=2.82843e-05, gnorm=1.01, train_wall=62, wall=0
2021-01-07 13:01:52 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.026, ppl=2.04, wps=17273.9, ups=1.62, wpb=10652.7, bsz=380.5, num_updates=6100, lr=2.80515e-05, gnorm=0.997, train_wall=61, wall=0
2021-01-07 13:02:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:02:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:02:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:02:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:02:56 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.763 | ppl 13.58 | bleu 22.76 | wps 4652.9 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 22.85
2021-01-07 13:02:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:02:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:02:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:02:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:02:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 11 @ 6171 updates, score 22.76) (writing took 2.9481149911880493 seconds)
2021-01-07 13:02:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-07 13:02:59 | INFO | train | epoch 011 | symm_kl 0.488 | self_kl 0 | self_cv 0 | loss 3.492 | nll_loss 1.039 | ppl 2.05 | wps 15830.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 2.78896e-05 | gnorm 1.01 | train_wall 343 | wall 0
2021-01-07 13:02:59 | INFO | fairseq.trainer | begin training epoch 12
2021-01-07 13:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:03:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:03:20 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=0.483, self_kl=0, self_cv=0, loss=3.48, nll_loss=1.033, ppl=2.05, wps=11915.1, ups=1.14, wpb=10473.3, bsz=364.9, num_updates=6200, lr=2.78243e-05, gnorm=1.002, train_wall=61, wall=0
2021-01-07 13:04:21 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.476, nll_loss=1.032, ppl=2.04, wps=17112.7, ups=1.65, wpb=10400.1, bsz=369, num_updates=6300, lr=2.76026e-05, gnorm=0.991, train_wall=61, wall=0
2021-01-07 13:05:22 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.496, nll_loss=1.045, ppl=2.06, wps=16984.6, ups=1.63, wpb=10444.8, bsz=372, num_updates=6400, lr=2.73861e-05, gnorm=1, train_wall=61, wall=0
2021-01-07 13:06:24 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.457, nll_loss=1.022, ppl=2.03, wps=17300.4, ups=1.63, wpb=10631.9, bsz=382.4, num_updates=6500, lr=2.71746e-05, gnorm=0.974, train_wall=61, wall=0
2021-01-07 13:07:25 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=0.486, self_kl=0, self_cv=0, loss=3.503, nll_loss=1.054, ppl=2.08, wps=17212.3, ups=1.63, wpb=10531, bsz=361.8, num_updates=6600, lr=2.6968e-05, gnorm=1.004, train_wall=61, wall=0
2021-01-07 13:08:26 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.467, nll_loss=1.032, ppl=2.05, wps=17065, ups=1.63, wpb=10493.3, bsz=369.8, num_updates=6700, lr=2.6766e-05, gnorm=0.984, train_wall=61, wall=0
2021-01-07 13:08:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:08:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:08:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:08:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:08:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:09:08 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.266 | nll_loss 3.754 | ppl 13.5 | bleu 22.86 | wps 4243 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 22.86
2021-01-07 13:09:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:09:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:09:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:09:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:09:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:09:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 12 @ 6732 updates, score 22.86) (writing took 4.8526694644242525 seconds)
2021-01-07 13:09:13 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-07 13:09:13 | INFO | train | epoch 012 | symm_kl 0.48 | self_kl 0 | self_cv 0 | loss 3.479 | nll_loss 1.036 | ppl 2.05 | wps 15727.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 2.67023e-05 | gnorm 0.992 | train_wall 342 | wall 0
2021-01-07 13:09:13 | INFO | fairseq.trainer | begin training epoch 13
2021-01-07 13:09:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:09:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:09:57 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.475, nll_loss=1.032, ppl=2.04, wps=11399.4, ups=1.1, wpb=10372.9, bsz=360.1, num_updates=6800, lr=2.65684e-05, gnorm=1.008, train_wall=60, wall=0
2021-01-07 13:10:58 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=0.481, self_kl=0, self_cv=0, loss=3.481, nll_loss=1.035, ppl=2.05, wps=17266.3, ups=1.63, wpb=10571.4, bsz=349, num_updates=6900, lr=2.63752e-05, gnorm=1.005, train_wall=61, wall=0
2021-01-07 13:12:00 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.469, nll_loss=1.036, ppl=2.05, wps=17116.2, ups=1.62, wpb=10544.3, bsz=369.9, num_updates=7000, lr=2.61861e-05, gnorm=0.982, train_wall=61, wall=0
2021-01-07 13:13:02 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=0.463, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.021, ppl=2.03, wps=16965.7, ups=1.62, wpb=10490.1, bsz=389.7, num_updates=7100, lr=2.60011e-05, gnorm=0.995, train_wall=62, wall=0
2021-01-07 13:14:03 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=0.472, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.035, ppl=2.05, wps=17097.5, ups=1.63, wpb=10510.3, bsz=369.8, num_updates=7200, lr=2.58199e-05, gnorm=0.975, train_wall=61, wall=0
2021-01-07 13:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:15:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:15:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:15:22 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.258 | nll_loss 3.747 | ppl 13.42 | bleu 22.74 | wps 4594.4 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 22.86
2021-01-07 13:15:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:15:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 13 @ 7293 updates, score 22.74) (writing took 2.904809070751071 seconds)
2021-01-07 13:15:25 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-07 13:15:25 | INFO | train | epoch 013 | symm_kl 0.474 | self_kl 0 | self_cv 0 | loss 3.468 | nll_loss 1.034 | ppl 2.05 | wps 15832.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 2.56547e-05 | gnorm 0.992 | train_wall 343 | wall 0
2021-01-07 13:15:25 | INFO | fairseq.trainer | begin training epoch 14
2021-01-07 13:15:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:15:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:15:32 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=0.471, self_kl=0, self_cv=0, loss=3.472, nll_loss=1.043, ppl=2.06, wps=11623.5, ups=1.13, wpb=10327.2, bsz=373.9, num_updates=7300, lr=2.56424e-05, gnorm=0.989, train_wall=61, wall=0
2021-01-07 13:16:33 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.033, ppl=2.05, wps=17279.1, ups=1.63, wpb=10590, bsz=367, num_updates=7400, lr=2.54686e-05, gnorm=0.976, train_wall=61, wall=0
2021-01-07 13:17:36 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=0.465, self_kl=0, self_cv=0, loss=3.445, nll_loss=1.021, ppl=2.03, wps=17036.1, ups=1.61, wpb=10574.3, bsz=374, num_updates=7500, lr=2.52982e-05, gnorm=0.978, train_wall=62, wall=0
2021-01-07 13:18:37 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=0.469, self_kl=0, self_cv=0, loss=3.464, nll_loss=1.036, ppl=2.05, wps=16913.7, ups=1.63, wpb=10386.9, bsz=357.6, num_updates=7600, lr=2.51312e-05, gnorm=0.984, train_wall=61, wall=0
2021-01-07 13:19:39 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=0.473, self_kl=0, self_cv=0, loss=3.478, nll_loss=1.046, ppl=2.06, wps=16691.7, ups=1.61, wpb=10338.9, bsz=367.2, num_updates=7700, lr=2.49675e-05, gnorm=0.991, train_wall=62, wall=0
2021-01-07 13:20:42 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=0.464, self_kl=0, self_cv=0, loss=3.451, nll_loss=1.029, ppl=2.04, wps=16928.8, ups=1.6, wpb=10594.9, bsz=368.6, num_updates=7800, lr=2.48069e-05, gnorm=0.962, train_wall=62, wall=0
2021-01-07 13:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:21:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:21:35 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.256 | nll_loss 3.744 | ppl 13.4 | bleu 22.91 | wps 4837.5 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 22.91
2021-01-07 13:21:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:21:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:21:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:21:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 14 @ 7854 updates, score 22.91) (writing took 4.924454357475042 seconds)
2021-01-07 13:21:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-07 13:21:40 | INFO | train | epoch 014 | symm_kl 0.468 | self_kl 0 | self_cv 0 | loss 3.46 | nll_loss 1.033 | ppl 2.05 | wps 15667.9 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 2.47215e-05 | gnorm 0.979 | train_wall 346 | wall 0
2021-01-07 13:21:40 | INFO | fairseq.trainer | begin training epoch 15
2021-01-07 13:21:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:21:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:22:11 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=0.465, self_kl=0, self_cv=0, loss=3.461, nll_loss=1.04, ppl=2.06, wps=11572, ups=1.11, wpb=10380.4, bsz=382.1, num_updates=7900, lr=2.46494e-05, gnorm=0.982, train_wall=61, wall=0
2021-01-07 13:23:13 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=0.466, self_kl=0, self_cv=0, loss=3.46, nll_loss=1.035, ppl=2.05, wps=16918.5, ups=1.62, wpb=10471, bsz=365.8, num_updates=8000, lr=2.44949e-05, gnorm=0.974, train_wall=62, wall=0
2021-01-07 13:24:15 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.431, nll_loss=1.015, ppl=2.02, wps=16900.6, ups=1.61, wpb=10485.3, bsz=382.9, num_updates=8100, lr=2.43432e-05, gnorm=0.982, train_wall=62, wall=0
2021-01-07 13:25:17 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=0.463, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.029, ppl=2.04, wps=16913, ups=1.61, wpb=10526.2, bsz=362.5, num_updates=8200, lr=2.41943e-05, gnorm=0.974, train_wall=62, wall=0
2021-01-07 13:26:19 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.032, ppl=2.04, wps=16964.5, ups=1.61, wpb=10527.6, bsz=374.3, num_updates=8300, lr=2.40481e-05, gnorm=0.954, train_wall=62, wall=0
2021-01-07 13:27:21 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=0.467, self_kl=0, self_cv=0, loss=3.47, nll_loss=1.047, ppl=2.07, wps=16908.3, ups=1.62, wpb=10468.7, bsz=364.2, num_updates=8400, lr=2.39046e-05, gnorm=0.983, train_wall=62, wall=0
2021-01-07 13:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:27:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:27:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:27:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:27:52 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.259 | nll_loss 3.745 | ppl 13.41 | bleu 22.86 | wps 4608.6 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 22.91
2021-01-07 13:27:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:27:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 15 @ 8415 updates, score 22.86) (writing took 2.9411438796669245 seconds)
2021-01-07 13:27:55 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-07 13:27:55 | INFO | train | epoch 015 | symm_kl 0.463 | self_kl 0 | self_cv 0 | loss 3.45 | nll_loss 1.031 | ppl 2.04 | wps 15702.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 2.38833e-05 | gnorm 0.973 | train_wall 346 | wall 0
2021-01-07 13:27:55 | INFO | fairseq.trainer | begin training epoch 16
2021-01-07 13:27:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:27:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:28:50 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=0.455, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.02, ppl=2.03, wps=11783.2, ups=1.13, wpb=10408.1, bsz=377.6, num_updates=8500, lr=2.37635e-05, gnorm=0.96, train_wall=61, wall=0
2021-01-07 13:29:52 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=0.461, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.03, ppl=2.04, wps=16858.4, ups=1.62, wpb=10433.6, bsz=372.2, num_updates=8600, lr=2.3625e-05, gnorm=0.976, train_wall=62, wall=0
2021-01-07 13:30:53 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=0.461, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.03, ppl=2.04, wps=17182.7, ups=1.63, wpb=10533.1, bsz=379.8, num_updates=8700, lr=2.34888e-05, gnorm=0.971, train_wall=61, wall=0
2021-01-07 13:31:55 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.417, nll_loss=1.014, ppl=2.02, wps=17000.1, ups=1.61, wpb=10551.2, bsz=375.2, num_updates=8800, lr=2.3355e-05, gnorm=0.938, train_wall=62, wall=0
2021-01-07 13:32:57 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.446, nll_loss=1.03, ppl=2.04, wps=17020.8, ups=1.62, wpb=10494.4, bsz=360.2, num_updates=8900, lr=2.32234e-05, gnorm=0.962, train_wall=61, wall=0
2021-01-07 13:33:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:33:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:33:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:33:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:33:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:33:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:33:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:33:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:33:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:33:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:34:04 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.252 | nll_loss 3.739 | ppl 13.36 | bleu 22.78 | wps 4604.7 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 22.91
2021-01-07 13:34:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:34:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:34:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:34:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:34:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:34:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 16 @ 8976 updates, score 22.78) (writing took 2.7884261682629585 seconds)
2021-01-07 13:34:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-07 13:34:07 | INFO | train | epoch 016 | symm_kl 0.458 | self_kl 0 | self_cv 0 | loss 3.443 | nll_loss 1.029 | ppl 2.04 | wps 15791.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 2.31249e-05 | gnorm 0.965 | train_wall 344 | wall 0
2021-01-07 13:34:07 | INFO | fairseq.trainer | begin training epoch 17
2021-01-07 13:34:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:34:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:34:25 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.448, nll_loss=1.039, ppl=2.05, wps=11859.8, ups=1.13, wpb=10459.4, bsz=358.3, num_updates=9000, lr=2.3094e-05, gnorm=0.972, train_wall=61, wall=0
2021-01-07 13:35:26 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.026, ppl=2.04, wps=17015.7, ups=1.62, wpb=10489.1, bsz=374.8, num_updates=9100, lr=2.29668e-05, gnorm=0.954, train_wall=61, wall=0
2021-01-07 13:36:29 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.405, nll_loss=1.004, ppl=2.01, wps=16959, ups=1.6, wpb=10592.8, bsz=369.3, num_updates=9200, lr=2.28416e-05, gnorm=0.946, train_wall=62, wall=0
2021-01-07 13:37:30 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=0.455, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.02, ppl=2.03, wps=17116.5, ups=1.63, wpb=10507.6, bsz=365.5, num_updates=9300, lr=2.27185e-05, gnorm=0.954, train_wall=61, wall=0
2021-01-07 13:38:32 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=0.455, self_kl=0, self_cv=0, loss=3.444, nll_loss=1.036, ppl=2.05, wps=16886.1, ups=1.62, wpb=10397.2, bsz=380.2, num_updates=9400, lr=2.25973e-05, gnorm=0.976, train_wall=61, wall=0
2021-01-07 13:39:33 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.466, nll_loss=1.053, ppl=2.07, wps=17036.2, ups=1.62, wpb=10491.6, bsz=362.4, num_updates=9500, lr=2.24781e-05, gnorm=0.976, train_wall=61, wall=0
2021-01-07 13:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:39:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:39:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:39:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:40:17 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.245 | nll_loss 3.732 | ppl 13.29 | bleu 22.85 | wps 4561.7 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 22.91
2021-01-07 13:40:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:40:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:40:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:40:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 17 @ 9537 updates, score 22.85) (writing took 2.751918537542224 seconds)
2021-01-07 13:40:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-07 13:40:20 | INFO | train | epoch 017 | symm_kl 0.454 | self_kl 0 | self_cv 0 | loss 3.436 | nll_loss 1.028 | ppl 2.04 | wps 15768.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 2.24344e-05 | gnorm 0.961 | train_wall 345 | wall 0
2021-01-07 13:40:20 | INFO | fairseq.trainer | begin training epoch 18
2021-01-07 13:40:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:40:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:41:02 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=0.454, self_kl=0, self_cv=0, loss=3.436, nll_loss=1.028, ppl=2.04, wps=11829.6, ups=1.14, wpb=10417.2, bsz=358.2, num_updates=9600, lr=2.23607e-05, gnorm=0.966, train_wall=61, wall=0
2021-01-07 13:42:03 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=0.451, self_kl=0, self_cv=0, loss=3.43, nll_loss=1.026, ppl=2.04, wps=16990.8, ups=1.62, wpb=10492.8, bsz=370, num_updates=9700, lr=2.22451e-05, gnorm=0.949, train_wall=62, wall=0
2021-01-07 13:43:05 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.019, ppl=2.03, wps=16874.9, ups=1.61, wpb=10456, bsz=363.1, num_updates=9800, lr=2.21313e-05, gnorm=0.954, train_wall=62, wall=0
2021-01-07 13:44:07 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.029, ppl=2.04, wps=16800.1, ups=1.62, wpb=10380.2, bsz=372.9, num_updates=9900, lr=2.20193e-05, gnorm=0.97, train_wall=62, wall=0
2021-01-07 13:45:09 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.027, ppl=2.04, wps=17208, ups=1.62, wpb=10622, bsz=379.6, num_updates=10000, lr=2.19089e-05, gnorm=0.936, train_wall=62, wall=0
2021-01-07 13:46:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:46:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:46:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:46:30 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.244 | nll_loss 3.732 | ppl 13.29 | bleu 22.77 | wps 4642.4 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 22.91
2021-01-07 13:46:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 18 @ 10098 updates, score 22.77) (writing took 2.9205913078039885 seconds)
2021-01-07 13:46:33 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-07 13:46:33 | INFO | train | epoch 018 | symm_kl 0.45 | self_kl 0 | self_cv 0 | loss 3.428 | nll_loss 1.026 | ppl 2.04 | wps 15762.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 2.18023e-05 | gnorm 0.953 | train_wall 345 | wall 0
2021-01-07 13:46:33 | INFO | fairseq.trainer | begin training epoch 19
2021-01-07 13:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:46:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:46:38 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.443, nll_loss=1.04, ppl=2.06, wps=11723.1, ups=1.13, wpb=10417.9, bsz=365.4, num_updates=10100, lr=2.18002e-05, gnorm=0.955, train_wall=62, wall=0
2021-01-07 13:47:38 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.416, nll_loss=1.017, ppl=2.02, wps=17122.7, ups=1.64, wpb=10422.9, bsz=373.4, num_updates=10200, lr=2.1693e-05, gnorm=0.952, train_wall=61, wall=0
2021-01-07 13:48:40 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.403, nll_loss=1.015, ppl=2.02, wps=17253, ups=1.62, wpb=10632.5, bsz=380, num_updates=10300, lr=2.15875e-05, gnorm=0.922, train_wall=61, wall=0
2021-01-07 13:49:42 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=0.448, self_kl=0, self_cv=0, loss=3.426, nll_loss=1.026, ppl=2.04, wps=17033.7, ups=1.62, wpb=10486.3, bsz=361.5, num_updates=10400, lr=2.14834e-05, gnorm=0.958, train_wall=61, wall=0
2021-01-07 13:50:43 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=0.452, self_kl=0, self_cv=0, loss=3.44, nll_loss=1.036, ppl=2.05, wps=17007.5, ups=1.62, wpb=10482.8, bsz=369.7, num_updates=10500, lr=2.13809e-05, gnorm=0.952, train_wall=61, wall=0
2021-01-07 13:51:45 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.413, nll_loss=1.02, ppl=2.03, wps=16911.8, ups=1.62, wpb=10451.7, bsz=365.4, num_updates=10600, lr=2.12798e-05, gnorm=0.942, train_wall=62, wall=0
2021-01-07 13:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:52:42 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.236 | nll_loss 3.725 | ppl 13.23 | bleu 22.87 | wps 4696 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 22.91
2021-01-07 13:52:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:52:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 19 @ 10659 updates, score 22.87) (writing took 2.895346710458398 seconds)
2021-01-07 13:52:45 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-07 13:52:45 | INFO | train | epoch 019 | symm_kl 0.446 | self_kl 0 | self_cv 0 | loss 3.422 | nll_loss 1.025 | ppl 2.03 | wps 15820.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 2.12208e-05 | gnorm 0.951 | train_wall 344 | wall 0
2021-01-07 13:52:45 | INFO | fairseq.trainer | begin training epoch 20
2021-01-07 13:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:52:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:53:13 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=0.444, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.027, ppl=2.04, wps=11962.7, ups=1.14, wpb=10477.3, bsz=372.4, num_updates=10700, lr=2.11801e-05, gnorm=0.962, train_wall=61, wall=0
2021-01-07 13:54:14 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.029, ppl=2.04, wps=17133.6, ups=1.63, wpb=10538.1, bsz=365.8, num_updates=10800, lr=2.10819e-05, gnorm=0.943, train_wall=61, wall=0
2021-01-07 13:55:16 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=0.44, self_kl=0, self_cv=0, loss=3.409, nll_loss=1.02, ppl=2.03, wps=16916.4, ups=1.61, wpb=10496, bsz=377, num_updates=10900, lr=2.09849e-05, gnorm=0.929, train_wall=62, wall=0
2021-01-07 13:56:18 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.421, nll_loss=1.023, ppl=2.03, wps=16988.9, ups=1.61, wpb=10521.2, bsz=361.4, num_updates=11000, lr=2.08893e-05, gnorm=0.943, train_wall=62, wall=0
2021-01-07 13:57:20 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=0.441, self_kl=0, self_cv=0, loss=3.418, nll_loss=1.028, ppl=2.04, wps=16932.7, ups=1.62, wpb=10445.5, bsz=372.4, num_updates=11100, lr=2.0795e-05, gnorm=0.942, train_wall=61, wall=0
2021-01-07 13:58:21 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.425, nll_loss=1.031, ppl=2.04, wps=16905.2, ups=1.62, wpb=10410.2, bsz=367.4, num_updates=11200, lr=2.0702e-05, gnorm=0.96, train_wall=61, wall=0
2021-01-07 13:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 13:58:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:58:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:58:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:58:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:58:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:58:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 13:58:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 13:58:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 13:58:55 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.239 | nll_loss 3.727 | ppl 13.24 | bleu 22.93 | wps 4622.6 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 22.93
2021-01-07 13:58:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 13:58:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:58:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:58:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:58:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:58:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 20 @ 11220 updates, score 22.93) (writing took 4.662094363942742 seconds)
2021-01-07 13:58:59 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-07 13:58:59 | INFO | train | epoch 020 | symm_kl 0.443 | self_kl 0 | self_cv 0 | loss 3.417 | nll_loss 1.024 | ppl 2.03 | wps 15700.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 2.06835e-05 | gnorm 0.941 | train_wall 345 | wall 0
2021-01-07 13:58:59 | INFO | fairseq.trainer | begin training epoch 21
2021-01-07 13:59:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 13:59:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 13:59:51 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=0.441, self_kl=0, self_cv=0, loss=3.419, nll_loss=1.029, ppl=2.04, wps=11468.6, ups=1.11, wpb=10315.2, bsz=376.1, num_updates=11300, lr=2.06102e-05, gnorm=0.95, train_wall=61, wall=0
2021-01-07 14:00:53 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=0.441, self_kl=0, self_cv=0, loss=3.412, nll_loss=1.022, ppl=2.03, wps=17004.5, ups=1.62, wpb=10526.7, bsz=359.6, num_updates=11400, lr=2.05196e-05, gnorm=0.935, train_wall=62, wall=0
2021-01-07 14:01:56 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.012, ppl=2.02, wps=16886.2, ups=1.61, wpb=10514.2, bsz=377.1, num_updates=11500, lr=2.04302e-05, gnorm=0.939, train_wall=62, wall=0
2021-01-07 14:02:57 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.008, ppl=2.01, wps=17165.3, ups=1.62, wpb=10592.6, bsz=375.9, num_updates=11600, lr=2.03419e-05, gnorm=0.919, train_wall=62, wall=0
2021-01-07 14:03:59 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=0.442, self_kl=0, self_cv=0, loss=3.42, nll_loss=1.029, ppl=2.04, wps=16992.6, ups=1.62, wpb=10501.6, bsz=373.3, num_updates=11700, lr=2.02548e-05, gnorm=0.938, train_wall=62, wall=0
2021-01-07 14:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:04:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:04:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:04:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:05:10 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.236 | nll_loss 3.723 | ppl 13.21 | bleu 22.87 | wps 4656.5 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 22.93
2021-01-07 14:05:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:05:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:05:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 21 @ 11781 updates, score 22.87) (writing took 2.8987039290368557 seconds)
2021-01-07 14:05:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-07 14:05:13 | INFO | train | epoch 021 | symm_kl 0.44 | self_kl 0 | self_cv 0 | loss 3.412 | nll_loss 1.023 | ppl 2.03 | wps 15761.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 2.0185e-05 | gnorm 0.939 | train_wall 345 | wall 0
2021-01-07 14:05:13 | INFO | fairseq.trainer | begin training epoch 22
2021-01-07 14:05:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:05:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:05:27 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.429, nll_loss=1.034, ppl=2.05, wps=11880.4, ups=1.13, wpb=10469.7, bsz=353, num_updates=11800, lr=2.01688e-05, gnorm=0.951, train_wall=61, wall=0
2021-01-07 14:06:29 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.011, ppl=2.02, wps=17077.3, ups=1.61, wpb=10589.6, bsz=376.8, num_updates=11900, lr=2.00839e-05, gnorm=0.936, train_wall=62, wall=0
2021-01-07 14:07:31 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.012, ppl=2.02, wps=16993.4, ups=1.62, wpb=10493.2, bsz=374.8, num_updates=12000, lr=2e-05, gnorm=0.924, train_wall=62, wall=0
2021-01-07 14:08:33 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.428, nll_loss=1.036, ppl=2.05, wps=16881.2, ups=1.62, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1.99172e-05, gnorm=0.935, train_wall=61, wall=0
2021-01-07 14:09:34 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=0.436, self_kl=0, self_cv=0, loss=3.392, nll_loss=1.006, ppl=2.01, wps=17061.6, ups=1.62, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1.98354e-05, gnorm=0.939, train_wall=62, wall=0
2021-01-07 14:10:36 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=0.44, self_kl=0, self_cv=0, loss=3.422, nll_loss=1.036, ppl=2.05, wps=17030.9, ups=1.63, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1.97546e-05, gnorm=0.939, train_wall=61, wall=0
2021-01-07 14:11:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:11:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:11:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:11:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:11:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:11:22 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.719 | ppl 13.16 | bleu 23.03 | wps 4585.2 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 23.03
2021-01-07 14:11:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:11:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:11:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 22 @ 12342 updates, score 23.03) (writing took 4.749985562637448 seconds)
2021-01-07 14:11:27 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-07 14:11:27 | INFO | train | epoch 022 | symm_kl 0.438 | self_kl 0 | self_cv 0 | loss 3.407 | nll_loss 1.022 | ppl 2.03 | wps 15699.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.9721e-05 | gnorm 0.934 | train_wall 344 | wall 0
2021-01-07 14:11:27 | INFO | fairseq.trainer | begin training epoch 23
2021-01-07 14:11:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:11:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:12:05 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=0.447, self_kl=0, self_cv=0, loss=3.437, nll_loss=1.041, ppl=2.06, wps=11617.1, ups=1.12, wpb=10402.8, bsz=349, num_updates=12400, lr=1.96748e-05, gnorm=0.948, train_wall=60, wall=0
2021-01-07 14:13:07 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.395, nll_loss=1.019, ppl=2.03, wps=17000.7, ups=1.62, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1.95959e-05, gnorm=0.922, train_wall=62, wall=0
2021-01-07 14:14:09 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.424, nll_loss=1.032, ppl=2.05, wps=16892.6, ups=1.62, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1.9518e-05, gnorm=0.944, train_wall=62, wall=0
2021-01-07 14:15:11 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.391, nll_loss=1.015, ppl=2.02, wps=17041.5, ups=1.61, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1.9441e-05, gnorm=0.921, train_wall=62, wall=0
2021-01-07 14:16:13 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.399, nll_loss=1.02, ppl=2.03, wps=16852.4, ups=1.62, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1.93649e-05, gnorm=0.917, train_wall=62, wall=0
2021-01-07 14:17:14 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.008, ppl=2.01, wps=17071.9, ups=1.62, wpb=10521.5, bsz=381, num_updates=12900, lr=1.92897e-05, gnorm=0.923, train_wall=61, wall=0
2021-01-07 14:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:17:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:17:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:17:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:17:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:17:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:17:38 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.719 | ppl 13.17 | bleu 22.93 | wps 4447.3 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 23.03
2021-01-07 14:17:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:17:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:17:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 23 @ 12903 updates, score 22.93) (writing took 2.9080120027065277 seconds)
2021-01-07 14:17:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-07 14:17:41 | INFO | train | epoch 023 | symm_kl 0.435 | self_kl 0 | self_cv 0 | loss 3.402 | nll_loss 1.02 | ppl 2.03 | wps 15746.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.92875e-05 | gnorm 0.93 | train_wall 345 | wall 0
2021-01-07 14:17:41 | INFO | fairseq.trainer | begin training epoch 24
2021-01-07 14:17:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:17:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:18:44 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.015, ppl=2.02, wps=11782.4, ups=1.12, wpb=10536, bsz=373.3, num_updates=13000, lr=1.92154e-05, gnorm=0.923, train_wall=61, wall=0
2021-01-07 14:19:46 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.015, ppl=2.02, wps=16738.5, ups=1.61, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1.91419e-05, gnorm=0.924, train_wall=62, wall=0
2021-01-07 14:20:48 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.014, ppl=2.02, wps=16977.9, ups=1.61, wpb=10536, bsz=369, num_updates=13200, lr=1.90693e-05, gnorm=0.919, train_wall=62, wall=0
2021-01-07 14:21:50 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=0.433, self_kl=0, self_cv=0, loss=3.406, nll_loss=1.028, ppl=2.04, wps=16651, ups=1.62, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1.89974e-05, gnorm=0.925, train_wall=62, wall=0
2021-01-07 14:22:52 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.414, nll_loss=1.036, ppl=2.05, wps=16991, ups=1.62, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1.89264e-05, gnorm=0.918, train_wall=61, wall=0
2021-01-07 14:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:23:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:23:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:23:52 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.717 | ppl 13.15 | bleu 22.9 | wps 4626 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 23.03
2021-01-07 14:23:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:23:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 24 @ 13464 updates, score 22.9) (writing took 2.916506137698889 seconds)
2021-01-07 14:23:55 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-07 14:23:55 | INFO | train | epoch 024 | symm_kl 0.432 | self_kl 0 | self_cv 0 | loss 3.398 | nll_loss 1.02 | ppl 2.03 | wps 15697.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.88814e-05 | gnorm 0.919 | train_wall 346 | wall 0
2021-01-07 14:23:55 | INFO | fairseq.trainer | begin training epoch 25
2021-01-07 14:23:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:23:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:24:20 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.005, ppl=2.01, wps=11914.2, ups=1.13, wpb=10574, bsz=365.8, num_updates=13500, lr=1.88562e-05, gnorm=0.91, train_wall=61, wall=0
2021-01-07 14:25:23 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.008, ppl=2.01, wps=16937.9, ups=1.6, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1.87867e-05, gnorm=0.916, train_wall=62, wall=0
2021-01-07 14:26:25 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.021, ppl=2.03, wps=16616.3, ups=1.6, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1.8718e-05, gnorm=0.921, train_wall=62, wall=0
2021-01-07 14:27:28 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.02, ppl=2.03, wps=16722.8, ups=1.59, wpb=10508.8, bsz=363, num_updates=13800, lr=1.86501e-05, gnorm=0.914, train_wall=63, wall=0
2021-01-07 14:28:30 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=0.428, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.025, ppl=2.03, wps=17136.6, ups=1.61, wpb=10632, bsz=381.2, num_updates=13900, lr=1.85829e-05, gnorm=0.914, train_wall=62, wall=0
2021-01-07 14:29:32 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.389, nll_loss=1.019, ppl=2.03, wps=16722.5, ups=1.61, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1.85164e-05, gnorm=0.922, train_wall=62, wall=0
2021-01-07 14:29:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:29:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:29:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:29:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:30:09 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.716 | ppl 13.15 | bleu 22.89 | wps 4611.5 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 23.03
2021-01-07 14:30:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:30:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:30:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:30:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:30:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:30:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 25 @ 14025 updates, score 22.89) (writing took 2.9214539919048548 seconds)
2021-01-07 14:30:12 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-07 14:30:12 | INFO | train | epoch 025 | symm_kl 0.429 | self_kl 0 | self_cv 0 | loss 3.393 | nll_loss 1.018 | ppl 2.03 | wps 15629.6 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.84999e-05 | gnorm 0.919 | train_wall 348 | wall 0
2021-01-07 14:30:12 | INFO | fairseq.trainer | begin training epoch 26
2021-01-07 14:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:31:01 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.001, ppl=2, wps=11782.6, ups=1.13, wpb=10395, bsz=387.4, num_updates=14100, lr=1.84506e-05, gnorm=0.911, train_wall=61, wall=0
2021-01-07 14:32:03 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.014, ppl=2.02, wps=16725.8, ups=1.6, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1.83855e-05, gnorm=0.932, train_wall=62, wall=0
2021-01-07 14:33:05 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.009, ppl=2.01, wps=17201, ups=1.61, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1.83211e-05, gnorm=0.892, train_wall=62, wall=0
2021-01-07 14:34:07 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.402, nll_loss=1.025, ppl=2.04, wps=17029.3, ups=1.62, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1.82574e-05, gnorm=0.919, train_wall=62, wall=0
2021-01-07 14:35:09 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.398, nll_loss=1.023, ppl=2.03, wps=17006.7, ups=1.62, wpb=10526, bsz=359.1, num_updates=14500, lr=1.81944e-05, gnorm=0.918, train_wall=62, wall=0
2021-01-07 14:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:36:22 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.715 | ppl 13.13 | bleu 22.89 | wps 4622.4 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 23.03
2021-01-07 14:36:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:36:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 26 @ 14586 updates, score 22.89) (writing took 2.9817493334412575 seconds)
2021-01-07 14:36:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-07 14:36:25 | INFO | train | epoch 026 | symm_kl 0.427 | self_kl 0 | self_cv 0 | loss 3.389 | nll_loss 1.018 | ppl 2.02 | wps 15744.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.81406e-05 | gnorm 0.918 | train_wall 345 | wall 0
2021-01-07 14:36:25 | INFO | fairseq.trainer | begin training epoch 27
2021-01-07 14:36:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:36:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:36:37 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.407, nll_loss=1.034, ppl=2.05, wps=11641, ups=1.13, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1.81319e-05, gnorm=0.947, train_wall=61, wall=0
2021-01-07 14:37:38 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.023, ppl=2.03, wps=17222.7, ups=1.64, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1.80702e-05, gnorm=0.921, train_wall=61, wall=0
2021-01-07 14:38:40 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=0.428, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.017, ppl=2.02, wps=16954.9, ups=1.61, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1.8009e-05, gnorm=0.916, train_wall=62, wall=0
2021-01-07 14:39:42 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.014, ppl=2.02, wps=17044.1, ups=1.62, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1.79485e-05, gnorm=0.9, train_wall=61, wall=0
2021-01-07 14:40:44 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.008, ppl=2.01, wps=17071.5, ups=1.62, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1.78885e-05, gnorm=0.91, train_wall=62, wall=0
2021-01-07 14:41:45 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.017, ppl=2.02, wps=16987.6, ups=1.63, wpb=10447, bsz=358.8, num_updates=15100, lr=1.78292e-05, gnorm=0.922, train_wall=61, wall=0
2021-01-07 14:42:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:42:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:42:35 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.225 | nll_loss 3.713 | ppl 13.12 | bleu 22.83 | wps 4629.3 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 23.03
2021-01-07 14:42:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:42:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 27 @ 15147 updates, score 22.83) (writing took 2.800451163202524 seconds)
2021-01-07 14:42:38 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-07 14:42:38 | INFO | train | epoch 027 | symm_kl 0.425 | self_kl 0 | self_cv 0 | loss 3.386 | nll_loss 1.017 | ppl 2.02 | wps 15773.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.78015e-05 | gnorm 0.917 | train_wall 345 | wall 0
2021-01-07 14:42:38 | INFO | fairseq.trainer | begin training epoch 28
2021-01-07 14:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:42:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:42:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:43:13 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.003, ppl=2, wps=11720.3, ups=1.14, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1.77705e-05, gnorm=0.923, train_wall=61, wall=0
2021-01-07 14:44:15 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.017, ppl=2.02, wps=16887.9, ups=1.61, wpb=10482.5, bsz=354, num_updates=15300, lr=1.77123e-05, gnorm=0.921, train_wall=62, wall=0
2021-01-07 14:45:17 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.008, ppl=2.01, wps=17018.5, ups=1.61, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1.76547e-05, gnorm=0.901, train_wall=62, wall=0
2021-01-07 14:46:19 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.384, nll_loss=1.013, ppl=2.02, wps=17053, ups=1.62, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1.75977e-05, gnorm=0.917, train_wall=62, wall=0
2021-01-07 14:47:20 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.397, nll_loss=1.033, ppl=2.05, wps=17022.8, ups=1.63, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1.75412e-05, gnorm=0.915, train_wall=61, wall=0
2021-01-07 14:48:22 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.396, nll_loss=1.03, ppl=2.04, wps=16972.3, ups=1.61, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1.74852e-05, gnorm=0.918, train_wall=62, wall=0
2021-01-07 14:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:48:48 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.221 | nll_loss 3.709 | ppl 13.08 | bleu 22.96 | wps 4629.5 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 23.03
2021-01-07 14:48:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:48:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:48:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 28 @ 15708 updates, score 22.96) (writing took 2.7612251471728086 seconds)
2021-01-07 14:48:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-07 14:48:51 | INFO | train | epoch 028 | symm_kl 0.423 | self_kl 0 | self_cv 0 | loss 3.382 | nll_loss 1.016 | ppl 2.02 | wps 15779.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.74808e-05 | gnorm 0.916 | train_wall 345 | wall 0
2021-01-07 14:48:51 | INFO | fairseq.trainer | begin training epoch 29
2021-01-07 14:48:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:48:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:49:50 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.008, ppl=2.01, wps=12012.1, ups=1.14, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1.74298e-05, gnorm=0.904, train_wall=60, wall=0
2021-01-07 14:50:51 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.013, ppl=2.02, wps=17149, ups=1.63, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1.73749e-05, gnorm=0.923, train_wall=61, wall=0
2021-01-07 14:51:53 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.388, nll_loss=1.019, ppl=2.03, wps=17096.5, ups=1.62, wpb=10552.2, bsz=366, num_updates=16000, lr=1.73205e-05, gnorm=0.903, train_wall=62, wall=0
2021-01-07 14:52:55 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.011, ppl=2.02, wps=16848.4, ups=1.62, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1.72666e-05, gnorm=0.913, train_wall=62, wall=0
2021-01-07 14:53:56 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.016, ppl=2.02, wps=16861, ups=1.63, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1.72133e-05, gnorm=0.918, train_wall=61, wall=0
2021-01-07 14:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 14:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 14:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 14:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 14:54:59 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.22 | nll_loss 3.709 | ppl 13.08 | bleu 22.85 | wps 4660.7 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 23.03
2021-01-07 14:54:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 14:55:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:55:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:55:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:55:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:55:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 29 @ 16269 updates, score 22.85) (writing took 2.960927590727806 seconds)
2021-01-07 14:55:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-07 14:55:02 | INFO | train | epoch 029 | symm_kl 0.421 | self_kl 0 | self_cv 0 | loss 3.379 | nll_loss 1.015 | ppl 2.02 | wps 15817.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.71767e-05 | gnorm 0.911 | train_wall 344 | wall 0
2021-01-07 14:55:02 | INFO | fairseq.trainer | begin training epoch 30
2021-01-07 14:55:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 14:55:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 14:55:24 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.015, ppl=2.02, wps=11835.4, ups=1.13, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1.71604e-05, gnorm=0.905, train_wall=61, wall=0
2021-01-07 14:56:25 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.37, nll_loss=1.008, ppl=2.01, wps=17344.8, ups=1.64, wpb=10585.6, bsz=366, num_updates=16400, lr=1.7108e-05, gnorm=0.901, train_wall=61, wall=0
2021-01-07 14:57:27 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.374, nll_loss=1.015, ppl=2.02, wps=16961.6, ups=1.63, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1.70561e-05, gnorm=0.913, train_wall=61, wall=0
2021-01-07 14:58:28 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.39, nll_loss=1.021, ppl=2.03, wps=16923.1, ups=1.63, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1.70046e-05, gnorm=0.919, train_wall=61, wall=0
2021-01-07 14:59:30 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.01, ppl=2.01, wps=17148.5, ups=1.62, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1.69536e-05, gnorm=0.896, train_wall=62, wall=0
2021-01-07 15:00:32 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.378, nll_loss=1.018, ppl=2.03, wps=17011, ups=1.62, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1.69031e-05, gnorm=0.916, train_wall=62, wall=0
2021-01-07 15:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:01:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:01:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:01:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:01:11 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.219 | nll_loss 3.706 | ppl 13.05 | bleu 22.96 | wps 4654.1 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 23.03
2021-01-07 15:01:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:01:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 30 @ 16830 updates, score 22.96) (writing took 2.7710144072771072 seconds)
2021-01-07 15:01:14 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-07 15:01:14 | INFO | train | epoch 030 | symm_kl 0.42 | self_kl 0 | self_cv 0 | loss 3.375 | nll_loss 1.014 | ppl 2.02 | wps 15833.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.6888e-05 | gnorm 0.91 | train_wall 343 | wall 0
2021-01-07 15:01:14 | INFO | fairseq.trainer | begin training epoch 31
2021-01-07 15:01:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:01:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:01:59 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.385, nll_loss=1.018, ppl=2.03, wps=11908.4, ups=1.14, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1.6853e-05, gnorm=0.928, train_wall=60, wall=0
2021-01-07 15:03:01 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.363, nll_loss=1.008, ppl=2.01, wps=17043.5, ups=1.61, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1.68034e-05, gnorm=0.879, train_wall=62, wall=0
2021-01-07 15:04:03 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.009, ppl=2.01, wps=17007, ups=1.62, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1.67542e-05, gnorm=0.909, train_wall=61, wall=0
2021-01-07 15:05:05 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.393, nll_loss=1.025, ppl=2.04, wps=16724.4, ups=1.61, wpb=10374.8, bsz=340, num_updates=17200, lr=1.67054e-05, gnorm=0.921, train_wall=62, wall=0
2021-01-07 15:06:07 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.36, nll_loss=1.007, ppl=2.01, wps=16996.7, ups=1.61, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1.6657e-05, gnorm=0.894, train_wall=62, wall=0
2021-01-07 15:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:07:25 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.221 | nll_loss 3.709 | ppl 13.08 | bleu 22.93 | wps 4564.9 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 23.03
2021-01-07 15:07:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:07:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 31 @ 17391 updates, score 22.93) (writing took 2.8200230561196804 seconds)
2021-01-07 15:07:28 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-07 15:07:28 | INFO | train | epoch 031 | symm_kl 0.418 | self_kl 0 | self_cv 0 | loss 3.372 | nll_loss 1.014 | ppl 2.02 | wps 15735.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.66134e-05 | gnorm 0.903 | train_wall 345 | wall 0
2021-01-07 15:07:28 | INFO | fairseq.trainer | begin training epoch 32
2021-01-07 15:07:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:07:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:07:36 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.02, ppl=2.03, wps=11625, ups=1.12, wpb=10367.8, bsz=370, num_updates=17400, lr=1.66091e-05, gnorm=0.911, train_wall=62, wall=0
2021-01-07 15:08:37 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.377, nll_loss=1.015, ppl=2.02, wps=17176.5, ups=1.64, wpb=10453, bsz=358.9, num_updates=17500, lr=1.65616e-05, gnorm=0.901, train_wall=61, wall=0
2021-01-07 15:09:39 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.344, nll_loss=0.996, ppl=1.99, wps=17333.8, ups=1.62, wpb=10691.6, bsz=379.4, num_updates=17600, lr=1.65145e-05, gnorm=0.882, train_wall=61, wall=0
2021-01-07 15:10:40 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.008, ppl=2.01, wps=17109.7, ups=1.62, wpb=10541.6, bsz=365.9, num_updates=17700, lr=1.64677e-05, gnorm=0.896, train_wall=61, wall=0
2021-01-07 15:11:42 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.386, nll_loss=1.022, ppl=2.03, wps=16848.7, ups=1.62, wpb=10411, bsz=347.3, num_updates=17800, lr=1.64214e-05, gnorm=0.924, train_wall=62, wall=0
2021-01-07 15:12:44 | INFO | train_inner | epoch 032:    509 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.369, nll_loss=1.015, ppl=2.02, wps=16830.8, ups=1.62, wpb=10412.4, bsz=390.5, num_updates=17900, lr=1.63755e-05, gnorm=0.898, train_wall=62, wall=0
2021-01-07 15:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:13:37 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.22 | nll_loss 3.708 | ppl 13.07 | bleu 22.93 | wps 4637.3 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 23.03
2021-01-07 15:13:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:13:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:13:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:13:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 32 @ 17952 updates, score 22.93) (writing took 2.8430173341184855 seconds)
2021-01-07 15:13:40 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-07 15:13:40 | INFO | train | epoch 032 | symm_kl 0.416 | self_kl 0 | self_cv 0 | loss 3.369 | nll_loss 1.012 | ppl 2.02 | wps 15800.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.63517e-05 | gnorm 0.902 | train_wall 344 | wall 0
2021-01-07 15:13:40 | INFO | fairseq.trainer | begin training epoch 33
2021-01-07 15:13:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:13:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:14:12 | INFO | train_inner | epoch 033:     48 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.372, nll_loss=1.02, ppl=2.03, wps=11915.8, ups=1.14, wpb=10480.2, bsz=382.2, num_updates=18000, lr=1.63299e-05, gnorm=0.894, train_wall=61, wall=0
2021-01-07 15:15:14 | INFO | train_inner | epoch 033:    148 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.347, nll_loss=1, ppl=2, wps=17004.6, ups=1.62, wpb=10476.8, bsz=378.3, num_updates=18100, lr=1.62848e-05, gnorm=0.884, train_wall=61, wall=0
2021-01-07 15:16:15 | INFO | train_inner | epoch 033:    248 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.362, nll_loss=1.007, ppl=2.01, wps=17153.4, ups=1.63, wpb=10542.1, bsz=374.1, num_updates=18200, lr=1.624e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 15:17:17 | INFO | train_inner | epoch 033:    348 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.373, nll_loss=1.015, ppl=2.02, wps=16938.1, ups=1.62, wpb=10437.8, bsz=375, num_updates=18300, lr=1.61955e-05, gnorm=0.907, train_wall=61, wall=0
2021-01-07 15:18:18 | INFO | train_inner | epoch 033:    448 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.379, nll_loss=1.016, ppl=2.02, wps=17064.7, ups=1.63, wpb=10483.6, bsz=349.7, num_updates=18400, lr=1.61515e-05, gnorm=0.91, train_wall=61, wall=0
2021-01-07 15:19:20 | INFO | train_inner | epoch 033:    548 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.016, ppl=2.02, wps=16901.9, ups=1.61, wpb=10480.7, bsz=370.4, num_updates=18500, lr=1.61077e-05, gnorm=0.91, train_wall=62, wall=0
2021-01-07 15:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:19:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:19:49 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.216 | nll_loss 3.704 | ppl 13.03 | bleu 22.93 | wps 4666.8 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 23.03
2021-01-07 15:19:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:19:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:19:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 33 @ 18513 updates, score 22.93) (writing took 2.8733636997640133 seconds)
2021-01-07 15:19:52 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-07 15:19:52 | INFO | train | epoch 033 | symm_kl 0.415 | self_kl 0 | self_cv 0 | loss 3.366 | nll_loss 1.012 | ppl 2.02 | wps 15819.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.61021e-05 | gnorm 0.9 | train_wall 344 | wall 0
2021-01-07 15:19:52 | INFO | fairseq.trainer | begin training epoch 34
2021-01-07 15:19:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:19:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:20:48 | INFO | train_inner | epoch 034:     87 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.01, ppl=2.01, wps=11991.7, ups=1.14, wpb=10500.6, bsz=376.6, num_updates=18600, lr=1.60644e-05, gnorm=0.9, train_wall=60, wall=0
2021-01-07 15:21:49 | INFO | train_inner | epoch 034:    187 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.387, nll_loss=1.025, ppl=2.04, wps=16954.7, ups=1.62, wpb=10453.2, bsz=372.1, num_updates=18700, lr=1.60214e-05, gnorm=0.897, train_wall=61, wall=0
2021-01-07 15:22:51 | INFO | train_inner | epoch 034:    287 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.362, nll_loss=1.01, ppl=2.01, wps=16884.3, ups=1.61, wpb=10473.3, bsz=368.2, num_updates=18800, lr=1.59787e-05, gnorm=0.891, train_wall=62, wall=0
2021-01-07 15:23:53 | INFO | train_inner | epoch 034:    387 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.003, ppl=2, wps=16829.5, ups=1.62, wpb=10398.1, bsz=377.4, num_updates=18900, lr=1.59364e-05, gnorm=0.89, train_wall=62, wall=0
2021-01-07 15:24:55 | INFO | train_inner | epoch 034:    487 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.331, nll_loss=0.989, ppl=1.98, wps=17181.1, ups=1.61, wpb=10664.6, bsz=372.5, num_updates=19000, lr=1.58944e-05, gnorm=0.879, train_wall=62, wall=0
2021-01-07 15:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:25:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:25:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:26:02 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.215 | nll_loss 3.704 | ppl 13.03 | bleu 22.88 | wps 4667.5 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 23.03
2021-01-07 15:26:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:26:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 34 @ 19074 updates, score 22.88) (writing took 2.8526192158460617 seconds)
2021-01-07 15:26:05 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-07 15:26:05 | INFO | train | epoch 034 | symm_kl 0.413 | self_kl 0 | self_cv 0 | loss 3.363 | nll_loss 1.011 | ppl 2.02 | wps 15751.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.58635e-05 | gnorm 0.895 | train_wall 345 | wall 0
2021-01-07 15:26:05 | INFO | fairseq.trainer | begin training epoch 35
2021-01-07 15:26:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:26:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:26:24 | INFO | train_inner | epoch 035:     26 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.376, nll_loss=1.021, ppl=2.03, wps=11746.8, ups=1.13, wpb=10418.3, bsz=347, num_updates=19100, lr=1.58527e-05, gnorm=0.909, train_wall=62, wall=0
2021-01-07 15:27:26 | INFO | train_inner | epoch 035:    126 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.005, ppl=2.01, wps=16886.5, ups=1.62, wpb=10451.9, bsz=365.3, num_updates=19200, lr=1.58114e-05, gnorm=0.885, train_wall=62, wall=0
2021-01-07 15:28:28 | INFO | train_inner | epoch 035:    226 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.349, nll_loss=1, ppl=2, wps=16881.4, ups=1.61, wpb=10479.3, bsz=382.2, num_updates=19300, lr=1.57704e-05, gnorm=0.9, train_wall=62, wall=0
2021-01-07 15:29:31 | INFO | train_inner | epoch 035:    326 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.365, nll_loss=1.014, ppl=2.02, wps=16863.6, ups=1.6, wpb=10548.4, bsz=368.7, num_updates=19400, lr=1.57297e-05, gnorm=0.891, train_wall=62, wall=0
2021-01-07 15:30:33 | INFO | train_inner | epoch 035:    426 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.016, ppl=2.02, wps=16869.5, ups=1.6, wpb=10546.4, bsz=379.4, num_updates=19500, lr=1.56893e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 15:31:35 | INFO | train_inner | epoch 035:    526 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.382, nll_loss=1.024, ppl=2.03, wps=16684.8, ups=1.6, wpb=10412.2, bsz=355, num_updates=19600, lr=1.56492e-05, gnorm=0.909, train_wall=62, wall=0
2021-01-07 15:31:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:32:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:32:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:32:18 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.213 | nll_loss 3.703 | ppl 13.02 | bleu 22.96 | wps 4655.2 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 23.03
2021-01-07 15:32:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:32:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:32:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:32:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:32:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 35 @ 19635 updates, score 22.96) (writing took 2.716260489076376 seconds)
2021-01-07 15:32:20 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-07 15:32:20 | INFO | train | epoch 035 | symm_kl 0.411 | self_kl 0 | self_cv 0 | loss 3.361 | nll_loss 1.011 | ppl 2.01 | wps 15666.8 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.56353e-05 | gnorm 0.892 | train_wall 348 | wall 0
2021-01-07 15:32:20 | INFO | fairseq.trainer | begin training epoch 36
2021-01-07 15:32:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:32:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:33:03 | INFO | train_inner | epoch 036:     65 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.358, nll_loss=1.008, ppl=2.01, wps=11763.2, ups=1.14, wpb=10344.6, bsz=362.5, num_updates=19700, lr=1.56094e-05, gnorm=0.897, train_wall=61, wall=0
2021-01-07 15:34:05 | INFO | train_inner | epoch 036:    165 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.012, ppl=2.02, wps=16812.2, ups=1.62, wpb=10399, bsz=361.7, num_updates=19800, lr=1.557e-05, gnorm=0.9, train_wall=62, wall=0
2021-01-07 15:35:07 | INFO | train_inner | epoch 036:    265 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.011, ppl=2.02, wps=17096, ups=1.62, wpb=10532.1, bsz=367.1, num_updates=19900, lr=1.55308e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 15:36:09 | INFO | train_inner | epoch 036:    365 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.354, nll_loss=1.008, ppl=2.01, wps=17037.4, ups=1.61, wpb=10583.1, bsz=386.2, num_updates=20000, lr=1.54919e-05, gnorm=0.875, train_wall=62, wall=0
2021-01-07 15:37:11 | INFO | train_inner | epoch 036:    465 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.007, ppl=2.01, wps=17343.9, ups=1.62, wpb=10695.3, bsz=376, num_updates=20100, lr=1.54533e-05, gnorm=0.887, train_wall=61, wall=0
2021-01-07 15:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:38:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:38:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:38:31 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.212 | nll_loss 3.702 | ppl 13.02 | bleu 22.95 | wps 4637.4 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 23.03
2021-01-07 15:38:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:38:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 36 @ 20196 updates, score 22.95) (writing took 2.9425922632217407 seconds)
2021-01-07 15:38:34 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-07 15:38:34 | INFO | train | epoch 036 | symm_kl 0.41 | self_kl 0 | self_cv 0 | loss 3.358 | nll_loss 1.01 | ppl 2.01 | wps 15762 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.54166e-05 | gnorm 0.89 | train_wall 345 | wall 0
2021-01-07 15:38:34 | INFO | fairseq.trainer | begin training epoch 37
2021-01-07 15:38:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:38:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:38:39 | INFO | train_inner | epoch 037:      4 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.017, ppl=2.02, wps=11598.5, ups=1.13, wpb=10275, bsz=363.2, num_updates=20200, lr=1.5415e-05, gnorm=0.9, train_wall=61, wall=0
2021-01-07 15:39:40 | INFO | train_inner | epoch 037:    104 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.362, nll_loss=1.01, ppl=2.01, wps=17092.8, ups=1.64, wpb=10439.9, bsz=356.5, num_updates=20300, lr=1.5377e-05, gnorm=0.904, train_wall=61, wall=0
2021-01-07 15:40:42 | INFO | train_inner | epoch 037:    204 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.015, ppl=2.02, wps=17018.7, ups=1.63, wpb=10457.5, bsz=360.4, num_updates=20400, lr=1.53393e-05, gnorm=0.898, train_wall=61, wall=0
2021-01-07 15:41:44 | INFO | train_inner | epoch 037:    304 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.002, ppl=2, wps=16963.9, ups=1.62, wpb=10499.2, bsz=371, num_updates=20500, lr=1.53018e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 15:42:45 | INFO | train_inner | epoch 037:    404 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.337, nll_loss=0.996, ppl=1.99, wps=17095.3, ups=1.62, wpb=10539.4, bsz=381.9, num_updates=20600, lr=1.52647e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 15:43:47 | INFO | train_inner | epoch 037:    504 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.022, ppl=2.03, wps=17026.7, ups=1.61, wpb=10546.3, bsz=368.3, num_updates=20700, lr=1.52277e-05, gnorm=0.896, train_wall=62, wall=0
2021-01-07 15:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:44:43 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.214 | nll_loss 3.702 | ppl 13.01 | bleu 22.99 | wps 4627.8 | wpb 7508.5 | bsz 272.7 | num_updates 20757 | best_bleu 23.03
2021-01-07 15:44:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:44:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 37 @ 20757 updates, score 22.99) (writing took 2.768131712451577 seconds)
2021-01-07 15:44:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-07 15:44:46 | INFO | train | epoch 037 | symm_kl 0.409 | self_kl 0 | self_cv 0 | loss 3.355 | nll_loss 1.008 | ppl 2.01 | wps 15801 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 20757 | lr 1.52068e-05 | gnorm 0.893 | train_wall 344 | wall 0
2021-01-07 15:44:46 | INFO | fairseq.trainer | begin training epoch 38
2021-01-07 15:44:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:44:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:45:15 | INFO | train_inner | epoch 038:     43 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.349, nll_loss=1.008, ppl=2.01, wps=11772.6, ups=1.14, wpb=10305.5, bsz=371.7, num_updates=20800, lr=1.51911e-05, gnorm=0.894, train_wall=61, wall=0
2021-01-07 15:46:16 | INFO | train_inner | epoch 038:    143 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.001, ppl=2, wps=16845.5, ups=1.62, wpb=10382.8, bsz=381.5, num_updates=20900, lr=1.51547e-05, gnorm=0.891, train_wall=61, wall=0
2021-01-07 15:47:18 | INFO | train_inner | epoch 038:    243 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.367, nll_loss=1.017, ppl=2.02, wps=17258.7, ups=1.63, wpb=10607.4, bsz=373.3, num_updates=21000, lr=1.51186e-05, gnorm=0.883, train_wall=61, wall=0
2021-01-07 15:48:20 | INFO | train_inner | epoch 038:    343 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.346, nll_loss=1.005, ppl=2.01, wps=17039.4, ups=1.62, wpb=10531.3, bsz=375.9, num_updates=21100, lr=1.50827e-05, gnorm=0.884, train_wall=62, wall=0
2021-01-07 15:49:22 | INFO | train_inner | epoch 038:    443 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.017, ppl=2.02, wps=17038.1, ups=1.61, wpb=10578.1, bsz=360.2, num_updates=21200, lr=1.50471e-05, gnorm=0.876, train_wall=62, wall=0
2021-01-07 15:50:24 | INFO | train_inner | epoch 038:    543 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.343, nll_loss=0.999, ppl=2, wps=16902.3, ups=1.61, wpb=10474.3, bsz=365.5, num_updates=21300, lr=1.50117e-05, gnorm=0.892, train_wall=62, wall=0
2021-01-07 15:50:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:50:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:50:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:50:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:50:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:50:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:50:56 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.211 | nll_loss 3.702 | ppl 13.01 | bleu 22.95 | wps 4593.2 | wpb 7508.5 | bsz 272.7 | num_updates 21318 | best_bleu 23.03
2021-01-07 15:50:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:50:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:50:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:50:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:50:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:50:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 38 @ 21318 updates, score 22.95) (writing took 2.800684779882431 seconds)
2021-01-07 15:50:58 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-07 15:50:58 | INFO | train | epoch 038 | symm_kl 0.408 | self_kl 0 | self_cv 0 | loss 3.353 | nll_loss 1.008 | ppl 2.01 | wps 15779.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21318 | lr 1.50054e-05 | gnorm 0.887 | train_wall 345 | wall 0
2021-01-07 15:50:58 | INFO | fairseq.trainer | begin training epoch 39
2021-01-07 15:50:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:51:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:51:51 | INFO | train_inner | epoch 039:     82 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.348, nll_loss=1.004, ppl=2.01, wps=11828.2, ups=1.14, wpb=10369.8, bsz=363, num_updates=21400, lr=1.49766e-05, gnorm=0.895, train_wall=60, wall=0
2021-01-07 15:52:53 | INFO | train_inner | epoch 039:    182 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.338, nll_loss=1, ppl=2, wps=17233.2, ups=1.62, wpb=10627.7, bsz=378.5, num_updates=21500, lr=1.49417e-05, gnorm=0.869, train_wall=61, wall=0
2021-01-07 15:53:55 | INFO | train_inner | epoch 039:    282 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.343, nll_loss=1, ppl=2, wps=16962.5, ups=1.62, wpb=10491.2, bsz=374.5, num_updates=21600, lr=1.49071e-05, gnorm=0.897, train_wall=62, wall=0
2021-01-07 15:54:57 | INFO | train_inner | epoch 039:    382 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.36, nll_loss=1.016, ppl=2.02, wps=17169.6, ups=1.62, wpb=10585.4, bsz=374.2, num_updates=21700, lr=1.48727e-05, gnorm=0.88, train_wall=61, wall=0
2021-01-07 15:55:58 | INFO | train_inner | epoch 039:    482 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.017, ppl=2.02, wps=16884.1, ups=1.63, wpb=10386.4, bsz=370.4, num_updates=21800, lr=1.48386e-05, gnorm=0.892, train_wall=61, wall=0
2021-01-07 15:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:56:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:56:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:56:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 15:56:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 15:56:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 15:57:07 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.211 | nll_loss 3.7 | ppl 13 | bleu 22.95 | wps 4656.2 | wpb 7508.5 | bsz 272.7 | num_updates 21879 | best_bleu 23.03
2021-01-07 15:57:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 15:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:57:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:57:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 39 @ 21879 updates, score 22.95) (writing took 2.782043419778347 seconds)
2021-01-07 15:57:10 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-07 15:57:10 | INFO | train | epoch 039 | symm_kl 0.407 | self_kl 0 | self_cv 0 | loss 3.351 | nll_loss 1.008 | ppl 2.01 | wps 15829.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21879 | lr 1.48118e-05 | gnorm 0.888 | train_wall 344 | wall 0
2021-01-07 15:57:10 | INFO | fairseq.trainer | begin training epoch 40
2021-01-07 15:57:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:57:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:57:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 15:57:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 15:57:26 | INFO | train_inner | epoch 040:     21 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.01, ppl=2.01, wps=11834.8, ups=1.14, wpb=10399.8, bsz=356.1, num_updates=21900, lr=1.48047e-05, gnorm=0.896, train_wall=61, wall=0
2021-01-07 15:58:27 | INFO | train_inner | epoch 040:    121 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.366, nll_loss=1.013, ppl=2.02, wps=16964.2, ups=1.63, wpb=10379.6, bsz=338.5, num_updates=22000, lr=1.4771e-05, gnorm=0.898, train_wall=61, wall=0
2021-01-07 15:59:29 | INFO | train_inner | epoch 040:    221 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.006, ppl=2.01, wps=16973.6, ups=1.62, wpb=10463.9, bsz=383.7, num_updates=22100, lr=1.47375e-05, gnorm=0.887, train_wall=61, wall=0
2021-01-07 16:00:30 | INFO | train_inner | epoch 040:    321 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.004, ppl=2.01, wps=17201.7, ups=1.62, wpb=10604.3, bsz=372.6, num_updates=22200, lr=1.47043e-05, gnorm=0.874, train_wall=61, wall=0
2021-01-07 16:01:32 | INFO | train_inner | epoch 040:    421 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.343, nll_loss=1.004, ppl=2.01, wps=17057.3, ups=1.62, wpb=10543, bsz=368, num_updates=22300, lr=1.46713e-05, gnorm=0.895, train_wall=62, wall=0
2021-01-07 16:02:34 | INFO | train_inner | epoch 040:    521 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.355, nll_loss=1.015, ppl=2.02, wps=17034.3, ups=1.62, wpb=10508.8, bsz=389.1, num_updates=22400, lr=1.46385e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 16:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:03:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:03:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:03:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:03:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:03:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:03:19 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.209 | nll_loss 3.698 | ppl 12.98 | bleu 23.1 | wps 4620.5 | wpb 7508.5 | bsz 272.7 | num_updates 22440 | best_bleu 23.1
2021-01-07 16:03:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:03:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:03:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:03:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 40 @ 22440 updates, score 23.1) (writing took 4.443219367414713 seconds)
2021-01-07 16:03:24 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-07 16:03:24 | INFO | train | epoch 040 | symm_kl 0.405 | self_kl 0 | self_cv 0 | loss 3.348 | nll_loss 1.007 | ppl 2.01 | wps 15725.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 22440 | lr 1.46254e-05 | gnorm 0.888 | train_wall 344 | wall 0
2021-01-07 16:03:24 | INFO | fairseq.trainer | begin training epoch 41
2021-01-07 16:03:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:03:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:04:03 | INFO | train_inner | epoch 041:     60 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.34, nll_loss=1.004, ppl=2.01, wps=11613.8, ups=1.12, wpb=10385.6, bsz=373, num_updates=22500, lr=1.46059e-05, gnorm=0.88, train_wall=61, wall=0
2021-01-07 16:05:05 | INFO | train_inner | epoch 041:    160 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.35, nll_loss=1.007, ppl=2.01, wps=17075.7, ups=1.63, wpb=10494.1, bsz=345.8, num_updates=22600, lr=1.45736e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 16:06:07 | INFO | train_inner | epoch 041:    260 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.988, ppl=1.98, wps=17108.3, ups=1.62, wpb=10557.1, bsz=380.2, num_updates=22700, lr=1.45414e-05, gnorm=0.873, train_wall=62, wall=0
2021-01-07 16:07:08 | INFO | train_inner | epoch 041:    360 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.351, nll_loss=1.014, ppl=2.02, wps=16990.7, ups=1.63, wpb=10438.5, bsz=394.9, num_updates=22800, lr=1.45095e-05, gnorm=0.873, train_wall=61, wall=0
2021-01-07 16:08:09 | INFO | train_inner | epoch 041:    460 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.364, nll_loss=1.017, ppl=2.02, wps=17004.6, ups=1.63, wpb=10456.4, bsz=370.5, num_updates=22900, lr=1.44778e-05, gnorm=0.897, train_wall=61, wall=0
2021-01-07 16:09:11 | INFO | train_inner | epoch 041:    560 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.004, ppl=2.01, wps=17174.7, ups=1.63, wpb=10540.6, bsz=353.5, num_updates=23000, lr=1.44463e-05, gnorm=0.891, train_wall=61, wall=0
2021-01-07 16:09:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:09:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:09:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:09:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:09:32 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.21 | nll_loss 3.698 | ppl 12.98 | bleu 23 | wps 4641.4 | wpb 7508.5 | bsz 272.7 | num_updates 23001 | best_bleu 23.1
2021-01-07 16:09:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:09:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:09:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 41 @ 23001 updates, score 23.0) (writing took 2.713770331814885 seconds)
2021-01-07 16:09:35 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-07 16:09:35 | INFO | train | epoch 041 | symm_kl 0.404 | self_kl 0 | self_cv 0 | loss 3.346 | nll_loss 1.006 | ppl 2.01 | wps 15854.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23001 | lr 1.4446e-05 | gnorm 0.884 | train_wall 343 | wall 0
2021-01-07 16:09:35 | INFO | fairseq.trainer | begin training epoch 42
2021-01-07 16:09:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:09:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:10:38 | INFO | train_inner | epoch 042:     99 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.334, nll_loss=1, ppl=2, wps=11973.4, ups=1.14, wpb=10470.5, bsz=353.7, num_updates=23100, lr=1.4415e-05, gnorm=0.871, train_wall=60, wall=0
2021-01-07 16:11:40 | INFO | train_inner | epoch 042:    199 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.354, nll_loss=1.009, ppl=2.01, wps=16924.4, ups=1.63, wpb=10395.3, bsz=364.5, num_updates=23200, lr=1.43839e-05, gnorm=0.889, train_wall=61, wall=0
2021-01-07 16:12:41 | INFO | train_inner | epoch 042:    299 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.334, nll_loss=0.998, ppl=2, wps=17120.1, ups=1.63, wpb=10517.8, bsz=379.2, num_updates=23300, lr=1.4353e-05, gnorm=0.877, train_wall=61, wall=0
2021-01-07 16:13:43 | INFO | train_inner | epoch 042:    399 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.007, ppl=2.01, wps=17215.1, ups=1.62, wpb=10619.2, bsz=381.1, num_updates=23400, lr=1.43223e-05, gnorm=0.866, train_wall=61, wall=0
2021-01-07 16:14:45 | INFO | train_inner | epoch 042:    499 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.004, ppl=2.01, wps=16817, ups=1.61, wpb=10432.1, bsz=368, num_updates=23500, lr=1.42918e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 16:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:15:44 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.207 | nll_loss 3.697 | ppl 12.97 | bleu 22.96 | wps 4655.2 | wpb 7508.5 | bsz 272.7 | num_updates 23562 | best_bleu 23.1
2021-01-07 16:15:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:15:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:15:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 42 @ 23562 updates, score 22.96) (writing took 2.8316875118762255 seconds)
2021-01-07 16:15:47 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-07 16:15:47 | INFO | train | epoch 042 | symm_kl 0.403 | self_kl 0 | self_cv 0 | loss 3.344 | nll_loss 1.006 | ppl 2.01 | wps 15817.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23562 | lr 1.4273e-05 | gnorm 0.877 | train_wall 344 | wall 0
2021-01-07 16:15:47 | INFO | fairseq.trainer | begin training epoch 43
2021-01-07 16:15:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:15:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:16:13 | INFO | train_inner | epoch 043:     38 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.014, ppl=2.02, wps=11742.5, ups=1.14, wpb=10325.9, bsz=362.3, num_updates=23600, lr=1.42615e-05, gnorm=0.883, train_wall=61, wall=0
2021-01-07 16:17:14 | INFO | train_inner | epoch 043:    138 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.998, ppl=2, wps=17095.7, ups=1.62, wpb=10524.1, bsz=376.6, num_updates=23700, lr=1.42314e-05, gnorm=0.868, train_wall=61, wall=0
2021-01-07 16:18:16 | INFO | train_inner | epoch 043:    238 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.007, ppl=2.01, wps=16985.5, ups=1.63, wpb=10450.7, bsz=352.5, num_updates=23800, lr=1.42014e-05, gnorm=0.891, train_wall=61, wall=0
2021-01-07 16:19:18 | INFO | train_inner | epoch 043:    338 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.013, ppl=2.02, wps=17082.6, ups=1.61, wpb=10589.2, bsz=368.9, num_updates=23900, lr=1.41717e-05, gnorm=0.882, train_wall=62, wall=0
2021-01-07 16:20:20 | INFO | train_inner | epoch 043:    438 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.345, nll_loss=1.01, ppl=2.01, wps=16824.6, ups=1.62, wpb=10396.9, bsz=384.3, num_updates=24000, lr=1.41421e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 16:21:22 | INFO | train_inner | epoch 043:    538 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.003, ppl=2, wps=17020.4, ups=1.61, wpb=10570.1, bsz=376.1, num_updates=24100, lr=1.41128e-05, gnorm=0.866, train_wall=62, wall=0
2021-01-07 16:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:21:57 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.204 | nll_loss 3.693 | ppl 12.94 | bleu 22.94 | wps 4614.5 | wpb 7508.5 | bsz 272.7 | num_updates 24123 | best_bleu 23.1
2021-01-07 16:21:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:21:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:21:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:21:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:22:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 43 @ 24123 updates, score 22.94) (writing took 2.826833140105009 seconds)
2021-01-07 16:22:00 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-07 16:22:00 | INFO | train | epoch 043 | symm_kl 0.402 | self_kl 0 | self_cv 0 | loss 3.342 | nll_loss 1.005 | ppl 2.01 | wps 15773.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 24123 | lr 1.4106e-05 | gnorm 0.88 | train_wall 345 | wall 0
2021-01-07 16:22:00 | INFO | fairseq.trainer | begin training epoch 44
2021-01-07 16:22:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:22:50 | INFO | train_inner | epoch 044:     77 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.004, ppl=2.01, wps=11863.1, ups=1.14, wpb=10441.4, bsz=372.2, num_updates=24200, lr=1.40836e-05, gnorm=0.902, train_wall=61, wall=0
2021-01-07 16:23:52 | INFO | train_inner | epoch 044:    177 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.983, ppl=1.98, wps=16941.3, ups=1.62, wpb=10486.7, bsz=363.9, num_updates=24300, lr=1.40546e-05, gnorm=0.877, train_wall=62, wall=0
2021-01-07 16:24:54 | INFO | train_inner | epoch 044:    277 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.98, ppl=1.97, wps=16809, ups=1.6, wpb=10509.5, bsz=373.7, num_updates=24400, lr=1.40257e-05, gnorm=0.861, train_wall=62, wall=0
2021-01-07 16:25:56 | INFO | train_inner | epoch 044:    377 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.339, nll_loss=1.007, ppl=2.01, wps=16956.1, ups=1.61, wpb=10504.8, bsz=386.4, num_updates=24500, lr=1.39971e-05, gnorm=0.867, train_wall=62, wall=0
2021-01-07 16:26:58 | INFO | train_inner | epoch 044:    477 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.375, nll_loss=1.033, ppl=2.05, wps=16854, ups=1.62, wpb=10427.2, bsz=360.4, num_updates=24600, lr=1.39686e-05, gnorm=0.883, train_wall=62, wall=0
2021-01-07 16:27:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:27:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:27:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:27:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:27:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:27:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:27:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:27:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:28:11 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.206 | nll_loss 3.694 | ppl 12.94 | bleu 23.05 | wps 4693.4 | wpb 7508.5 | bsz 272.7 | num_updates 24684 | best_bleu 23.1
2021-01-07 16:28:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:28:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:28:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:28:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:28:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:28:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 44 @ 24684 updates, score 23.05) (writing took 2.8218506947159767 seconds)
2021-01-07 16:28:14 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-07 16:28:14 | INFO | train | epoch 044 | symm_kl 0.4 | self_kl 0 | self_cv 0 | loss 3.34 | nll_loss 1.004 | ppl 2.01 | wps 15710.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 24684 | lr 1.39448e-05 | gnorm 0.876 | train_wall 346 | wall 0
2021-01-07 16:28:14 | INFO | fairseq.trainer | begin training epoch 45
2021-01-07 16:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:28:27 | INFO | train_inner | epoch 045:     16 / 561 symm_kl=0.406, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.019, ppl=2.03, wps=11800.1, ups=1.12, wpb=10489.5, bsz=351.2, num_updates=24700, lr=1.39403e-05, gnorm=0.888, train_wall=62, wall=0
2021-01-07 16:29:28 | INFO | train_inner | epoch 045:    116 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.328, nll_loss=1.002, ppl=2, wps=16949.3, ups=1.63, wpb=10419.3, bsz=373.3, num_updates=24800, lr=1.39122e-05, gnorm=0.859, train_wall=61, wall=0
2021-01-07 16:30:30 | INFO | train_inner | epoch 045:    216 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.001, ppl=2, wps=17013, ups=1.62, wpb=10490.5, bsz=385.9, num_updates=24900, lr=1.38842e-05, gnorm=0.872, train_wall=61, wall=0
2021-01-07 16:31:32 | INFO | train_inner | epoch 045:    316 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.361, nll_loss=1.021, ppl=2.03, wps=16868.3, ups=1.62, wpb=10437, bsz=359.9, num_updates=25000, lr=1.38564e-05, gnorm=0.894, train_wall=62, wall=0
2021-01-07 16:32:34 | INFO | train_inner | epoch 045:    416 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.992, ppl=1.99, wps=17120.2, ups=1.62, wpb=10557.3, bsz=367.8, num_updates=25100, lr=1.38288e-05, gnorm=0.878, train_wall=61, wall=0
2021-01-07 16:33:35 | INFO | train_inner | epoch 045:    516 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.002, ppl=2, wps=17101.4, ups=1.62, wpb=10577.4, bsz=369.7, num_updates=25200, lr=1.38013e-05, gnorm=0.865, train_wall=62, wall=0
2021-01-07 16:34:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:34:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:34:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:34:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:34:24 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.208 | nll_loss 3.696 | ppl 12.96 | bleu 23.03 | wps 4657.9 | wpb 7508.5 | bsz 272.7 | num_updates 25245 | best_bleu 23.1
2021-01-07 16:34:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:34:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 45 @ 25245 updates, score 23.03) (writing took 2.825773863121867 seconds)
2021-01-07 16:34:27 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-07 16:34:27 | INFO | train | epoch 045 | symm_kl 0.399 | self_kl 0 | self_cv 0 | loss 3.337 | nll_loss 1.003 | ppl 2 | wps 15778.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 25245 | lr 1.3789e-05 | gnorm 0.875 | train_wall 345 | wall 0
2021-01-07 16:34:27 | INFO | fairseq.trainer | begin training epoch 46
2021-01-07 16:34:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:34:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:34:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:34:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:35:03 | INFO | train_inner | epoch 046:     55 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.996, ppl=1.99, wps=11970.9, ups=1.14, wpb=10483.7, bsz=360.6, num_updates=25300, lr=1.3774e-05, gnorm=0.881, train_wall=61, wall=0
2021-01-07 16:36:05 | INFO | train_inner | epoch 046:    155 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.341, nll_loss=1.007, ppl=2.01, wps=16675.3, ups=1.62, wpb=10304.7, bsz=379.3, num_updates=25400, lr=1.37469e-05, gnorm=0.889, train_wall=62, wall=0
2021-01-07 16:37:07 | INFO | train_inner | epoch 046:    255 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.325, nll_loss=0.996, ppl=1.99, wps=17095.9, ups=1.62, wpb=10582.9, bsz=378.7, num_updates=25500, lr=1.37199e-05, gnorm=0.873, train_wall=62, wall=0
2021-01-07 16:38:09 | INFO | train_inner | epoch 046:    355 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.341, nll_loss=1.005, ppl=2.01, wps=17068.8, ups=1.61, wpb=10612.9, bsz=365.6, num_updates=25600, lr=1.36931e-05, gnorm=0.876, train_wall=62, wall=0
2021-01-07 16:39:11 | INFO | train_inner | epoch 046:    455 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.003, ppl=2, wps=16967.4, ups=1.62, wpb=10503, bsz=362.3, num_updates=25700, lr=1.36664e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 16:40:13 | INFO | train_inner | epoch 046:    555 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.011, ppl=2.02, wps=16792.3, ups=1.61, wpb=10397.9, bsz=371.5, num_updates=25800, lr=1.36399e-05, gnorm=0.883, train_wall=62, wall=0
2021-01-07 16:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:40:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:40:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:40:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:40:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:40:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:40:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:40:37 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.205 | nll_loss 3.693 | ppl 12.93 | bleu 23.02 | wps 4664 | wpb 7508.5 | bsz 272.7 | num_updates 25806 | best_bleu 23.1
2021-01-07 16:40:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:40:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:40:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:40:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 46 @ 25806 updates, score 23.02) (writing took 2.8116994109004736 seconds)
2021-01-07 16:40:40 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-07 16:40:40 | INFO | train | epoch 046 | symm_kl 0.399 | self_kl 0 | self_cv 0 | loss 3.336 | nll_loss 1.003 | ppl 2 | wps 15749.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 25806 | lr 1.36383e-05 | gnorm 0.88 | train_wall 345 | wall 0
2021-01-07 16:40:40 | INFO | fairseq.trainer | begin training epoch 47
2021-01-07 16:40:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:40:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:41:41 | INFO | train_inner | epoch 047:     94 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.331, nll_loss=0.999, ppl=2, wps=12025.7, ups=1.14, wpb=10585.2, bsz=362.2, num_updates=25900, lr=1.36135e-05, gnorm=0.862, train_wall=61, wall=0
2021-01-07 16:42:43 | INFO | train_inner | epoch 047:    194 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.005, ppl=2.01, wps=16829.7, ups=1.61, wpb=10442.5, bsz=350.3, num_updates=26000, lr=1.35873e-05, gnorm=0.894, train_wall=62, wall=0
2021-01-07 16:43:45 | INFO | train_inner | epoch 047:    294 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.327, nll_loss=0.999, ppl=2, wps=16983.5, ups=1.62, wpb=10503.2, bsz=374.6, num_updates=26100, lr=1.35613e-05, gnorm=0.869, train_wall=62, wall=0
2021-01-07 16:44:47 | INFO | train_inner | epoch 047:    394 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.003, ppl=2, wps=16866.7, ups=1.61, wpb=10454.3, bsz=371.8, num_updates=26200, lr=1.35354e-05, gnorm=0.876, train_wall=62, wall=0
2021-01-07 16:45:49 | INFO | train_inner | epoch 047:    494 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.323, nll_loss=0.999, ppl=2, wps=16928.8, ups=1.61, wpb=10484.2, bsz=388.4, num_updates=26300, lr=1.35096e-05, gnorm=0.855, train_wall=62, wall=0
2021-01-07 16:46:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:46:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:46:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:46:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:46:51 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.207 | nll_loss 3.697 | ppl 12.97 | bleu 22.97 | wps 4674 | wpb 7508.5 | bsz 272.7 | num_updates 26367 | best_bleu 23.1
2021-01-07 16:46:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:46:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:46:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:46:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 47 @ 26367 updates, score 22.97) (writing took 2.9042330943048 seconds)
2021-01-07 16:46:53 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-07 16:46:53 | INFO | train | epoch 047 | symm_kl 0.397 | self_kl 0 | self_cv 0 | loss 3.333 | nll_loss 1.002 | ppl 2 | wps 15751.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 26367 | lr 1.34924e-05 | gnorm 0.875 | train_wall 345 | wall 0
2021-01-07 16:46:53 | INFO | fairseq.trainer | begin training epoch 48
2021-01-07 16:46:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:46:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:47:17 | INFO | train_inner | epoch 048:     33 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.001, ppl=2, wps=11870.7, ups=1.14, wpb=10450.7, bsz=374.3, num_updates=26400, lr=1.3484e-05, gnorm=0.888, train_wall=61, wall=0
2021-01-07 16:48:17 | INFO | train_inner | epoch 048:    133 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.347, nll_loss=1.01, ppl=2.01, wps=17115, ups=1.65, wpb=10402.3, bsz=369.9, num_updates=26500, lr=1.34585e-05, gnorm=0.89, train_wall=61, wall=0
2021-01-07 16:49:19 | INFO | train_inner | epoch 048:    233 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.005, ppl=2.01, wps=17038.1, ups=1.62, wpb=10517.5, bsz=360.6, num_updates=26600, lr=1.34332e-05, gnorm=0.865, train_wall=62, wall=0
2021-01-07 16:50:20 | INFO | train_inner | epoch 048:    333 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.01, ppl=2.01, wps=17050.3, ups=1.63, wpb=10463.7, bsz=354.2, num_updates=26700, lr=1.3408e-05, gnorm=0.877, train_wall=61, wall=0
2021-01-07 16:51:23 | INFO | train_inner | epoch 048:    433 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.995, ppl=1.99, wps=16912.2, ups=1.6, wpb=10537.4, bsz=385, num_updates=26800, lr=1.3383e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 16:52:25 | INFO | train_inner | epoch 048:    533 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.328, nll_loss=1, ppl=2, wps=16913.5, ups=1.62, wpb=10468.4, bsz=375.1, num_updates=26900, lr=1.33581e-05, gnorm=0.881, train_wall=62, wall=0
2021-01-07 16:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:52:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:52:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:52:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:52:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:53:03 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.204 | nll_loss 3.694 | ppl 12.94 | bleu 22.97 | wps 4683.8 | wpb 7508.5 | bsz 272.7 | num_updates 26928 | best_bleu 23.1
2021-01-07 16:53:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:53:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 48 @ 26928 updates, score 22.97) (writing took 2.8622977063059807 seconds)
2021-01-07 16:53:06 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-07 16:53:06 | INFO | train | epoch 048 | symm_kl 0.396 | self_kl 0 | self_cv 0 | loss 3.332 | nll_loss 1.002 | ppl 2 | wps 15803.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 26928 | lr 1.33511e-05 | gnorm 0.877 | train_wall 344 | wall 0
2021-01-07 16:53:06 | INFO | fairseq.trainer | begin training epoch 49
2021-01-07 16:53:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:53:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:53:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:53:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:53:53 | INFO | train_inner | epoch 049:     72 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.32, nll_loss=0.993, ppl=1.99, wps=11894.8, ups=1.14, wpb=10466.8, bsz=374.3, num_updates=27000, lr=1.33333e-05, gnorm=0.873, train_wall=61, wall=0
2021-01-07 16:54:54 | INFO | train_inner | epoch 049:    172 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.002, ppl=2, wps=16926.7, ups=1.63, wpb=10408.2, bsz=375.4, num_updates=27100, lr=1.33087e-05, gnorm=0.872, train_wall=61, wall=0
2021-01-07 16:55:56 | INFO | train_inner | epoch 049:    272 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.352, nll_loss=1.014, ppl=2.02, wps=17045, ups=1.62, wpb=10517.2, bsz=356.2, num_updates=27200, lr=1.32842e-05, gnorm=0.884, train_wall=61, wall=0
2021-01-07 16:56:57 | INFO | train_inner | epoch 049:    372 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.005, ppl=2.01, wps=16956.5, ups=1.63, wpb=10381.6, bsz=372.5, num_updates=27300, lr=1.32599e-05, gnorm=0.864, train_wall=61, wall=0
2021-01-07 16:57:59 | INFO | train_inner | epoch 049:    472 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.994, ppl=1.99, wps=17160.6, ups=1.63, wpb=10545.9, bsz=360.4, num_updates=27400, lr=1.32357e-05, gnorm=0.861, train_wall=61, wall=0
2021-01-07 16:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 16:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:58:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:59:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:59:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 16:59:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 16:59:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 16:59:14 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.204 | nll_loss 3.693 | ppl 12.94 | bleu 22.98 | wps 4593.2 | wpb 7508.5 | bsz 272.7 | num_updates 27489 | best_bleu 23.1
2021-01-07 16:59:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 16:59:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:59:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:59:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:59:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:59:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 49 @ 27489 updates, score 22.98) (writing took 2.822945324704051 seconds)
2021-01-07 16:59:17 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-07 16:59:17 | INFO | train | epoch 049 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.33 | nll_loss 1.002 | ppl 2 | wps 15828.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 27489 | lr 1.32142e-05 | gnorm 0.869 | train_wall 343 | wall 0
2021-01-07 16:59:17 | INFO | fairseq.trainer | begin training epoch 50
2021-01-07 16:59:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 16:59:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 16:59:27 | INFO | train_inner | epoch 050:     11 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.323, nll_loss=1.001, ppl=2, wps=11861.8, ups=1.13, wpb=10504.5, bsz=380.2, num_updates=27500, lr=1.32116e-05, gnorm=0.859, train_wall=61, wall=0
2021-01-07 17:00:28 | INFO | train_inner | epoch 050:    111 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.336, nll_loss=1.003, ppl=2, wps=17216.5, ups=1.63, wpb=10546.8, bsz=356.6, num_updates=27600, lr=1.31876e-05, gnorm=0.879, train_wall=61, wall=0
2021-01-07 17:01:30 | INFO | train_inner | epoch 050:    211 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.32, nll_loss=0.994, ppl=1.99, wps=16987.2, ups=1.63, wpb=10440.7, bsz=360.7, num_updates=27700, lr=1.31638e-05, gnorm=0.87, train_wall=61, wall=0
2021-01-07 17:02:32 | INFO | train_inner | epoch 050:    311 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.326, nll_loss=1, ppl=2, wps=17008.3, ups=1.61, wpb=10539.4, bsz=374.1, num_updates=27800, lr=1.31401e-05, gnorm=0.856, train_wall=62, wall=0
2021-01-07 17:03:33 | INFO | train_inner | epoch 050:    411 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.996, ppl=1.99, wps=17001.8, ups=1.62, wpb=10476, bsz=379.4, num_updates=27900, lr=1.31165e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 17:04:35 | INFO | train_inner | epoch 050:    511 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.002, ppl=2, wps=17084.6, ups=1.62, wpb=10527.3, bsz=380.4, num_updates=28000, lr=1.30931e-05, gnorm=0.87, train_wall=61, wall=0
2021-01-07 17:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:05:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:05:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:05:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:05:26 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.204 | nll_loss 3.692 | ppl 12.92 | bleu 23.05 | wps 4658.1 | wpb 7508.5 | bsz 272.7 | num_updates 28050 | best_bleu 23.1
2021-01-07 17:05:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:05:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:05:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:05:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 50 @ 28050 updates, score 23.05) (writing took 2.810238193720579 seconds)
2021-01-07 17:05:29 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-07 17:05:29 | INFO | train | epoch 050 | symm_kl 0.395 | self_kl 0 | self_cv 0 | loss 3.328 | nll_loss 1.001 | ppl 2 | wps 15803.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 28050 | lr 1.30814e-05 | gnorm 0.869 | train_wall 344 | wall 0
2021-01-07 17:05:29 | INFO | fairseq.trainer | begin training epoch 51
2021-01-07 17:05:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:05:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:06:03 | INFO | train_inner | epoch 051:     50 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.353, nll_loss=1.018, ppl=2.02, wps=11875.8, ups=1.14, wpb=10389.2, bsz=352.9, num_updates=28100, lr=1.30698e-05, gnorm=0.89, train_wall=60, wall=0
2021-01-07 17:07:04 | INFO | train_inner | epoch 051:    150 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.993, ppl=1.99, wps=17105, ups=1.63, wpb=10509.5, bsz=378.9, num_updates=28200, lr=1.30466e-05, gnorm=0.884, train_wall=61, wall=0
2021-01-07 17:08:06 | INFO | train_inner | epoch 051:    250 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.013, ppl=2.02, wps=16844, ups=1.61, wpb=10455.1, bsz=374, num_updates=28300, lr=1.30235e-05, gnorm=0.868, train_wall=62, wall=0
2021-01-07 17:09:08 | INFO | train_inner | epoch 051:    350 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.004, ppl=2, wps=16855.8, ups=1.61, wpb=10446.7, bsz=371.6, num_updates=28400, lr=1.30005e-05, gnorm=0.874, train_wall=62, wall=0
2021-01-07 17:10:10 | INFO | train_inner | epoch 051:    450 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.99, ppl=1.99, wps=16996.3, ups=1.62, wpb=10520.1, bsz=360.2, num_updates=28500, lr=1.29777e-05, gnorm=0.86, train_wall=62, wall=0
2021-01-07 17:11:12 | INFO | train_inner | epoch 051:    550 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.995, ppl=1.99, wps=16843.6, ups=1.6, wpb=10528.9, bsz=374.8, num_updates=28600, lr=1.2955e-05, gnorm=0.874, train_wall=62, wall=0
2021-01-07 17:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:11:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:11:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:11:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:11:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:11:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:11:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:11:40 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.2 | nll_loss 3.69 | ppl 12.9 | bleu 22.94 | wps 4615.7 | wpb 7508.5 | bsz 272.7 | num_updates 28611 | best_bleu 23.1
2021-01-07 17:11:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:11:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:11:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:11:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 51 @ 28611 updates, score 22.94) (writing took 2.8092248253524303 seconds)
2021-01-07 17:11:43 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-07 17:11:43 | INFO | train | epoch 051 | symm_kl 0.394 | self_kl 0 | self_cv 0 | loss 3.326 | nll_loss 1 | ppl 2 | wps 15748.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 28611 | lr 1.29525e-05 | gnorm 0.872 | train_wall 345 | wall 0
2021-01-07 17:11:43 | INFO | fairseq.trainer | begin training epoch 52
2021-01-07 17:11:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:11:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:12:41 | INFO | train_inner | epoch 052:     89 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.995, ppl=1.99, wps=11942.7, ups=1.13, wpb=10531.1, bsz=358.2, num_updates=28700, lr=1.29324e-05, gnorm=0.871, train_wall=61, wall=0
2021-01-07 17:13:43 | INFO | train_inner | epoch 052:    189 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.002, ppl=2, wps=16997, ups=1.61, wpb=10525.3, bsz=379.4, num_updates=28800, lr=1.29099e-05, gnorm=0.861, train_wall=62, wall=0
2021-01-07 17:14:44 | INFO | train_inner | epoch 052:    289 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.356, nll_loss=1.021, ppl=2.03, wps=16910.8, ups=1.62, wpb=10459.8, bsz=346.9, num_updates=28900, lr=1.28876e-05, gnorm=0.885, train_wall=62, wall=0
2021-01-07 17:15:47 | INFO | train_inner | epoch 052:    389 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.977, ppl=1.97, wps=16872.4, ups=1.61, wpb=10505.1, bsz=372.6, num_updates=29000, lr=1.28654e-05, gnorm=0.854, train_wall=62, wall=0
2021-01-07 17:16:49 | INFO | train_inner | epoch 052:    489 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.344, nll_loss=1.012, ppl=2.02, wps=16607.5, ups=1.61, wpb=10334.4, bsz=367.1, num_updates=29100, lr=1.28432e-05, gnorm=0.883, train_wall=62, wall=0
2021-01-07 17:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:17:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:17:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:17:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:17:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:17:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:17:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:17:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:17:54 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.201 | nll_loss 3.69 | ppl 12.91 | bleu 23.13 | wps 4665.9 | wpb 7508.5 | bsz 272.7 | num_updates 29172 | best_bleu 23.13
2021-01-07 17:17:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:17:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:17:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:17:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:17:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 52 @ 29172 updates, score 23.13) (writing took 4.880076503381133 seconds)
2021-01-07 17:17:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-07 17:17:59 | INFO | train | epoch 052 | symm_kl 0.393 | self_kl 0 | self_cv 0 | loss 3.325 | nll_loss 1 | ppl 2 | wps 15628.1 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 29172 | lr 1.28274e-05 | gnorm 0.87 | train_wall 346 | wall 0
2021-01-07 17:17:59 | INFO | fairseq.trainer | begin training epoch 53
2021-01-07 17:18:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:18:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:18:19 | INFO | train_inner | epoch 053:     28 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.984, ppl=1.98, wps=11704, ups=1.11, wpb=10565.3, bsz=394.1, num_updates=29200, lr=1.28212e-05, gnorm=0.86, train_wall=61, wall=0
2021-01-07 17:19:21 | INFO | train_inner | epoch 053:    128 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.004, ppl=2.01, wps=16999.8, ups=1.61, wpb=10549.4, bsz=362.6, num_updates=29300, lr=1.27993e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 17:20:23 | INFO | train_inner | epoch 053:    228 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.333, nll_loss=1.002, ppl=2, wps=16721.6, ups=1.61, wpb=10404.1, bsz=355.6, num_updates=29400, lr=1.27775e-05, gnorm=0.876, train_wall=62, wall=0
2021-01-07 17:21:26 | INFO | train_inner | epoch 053:    328 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.988, ppl=1.98, wps=16783.3, ups=1.59, wpb=10550, bsz=379.8, num_updates=29500, lr=1.27559e-05, gnorm=0.863, train_wall=63, wall=0
2021-01-07 17:22:29 | INFO | train_inner | epoch 053:    428 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.99, ppl=1.99, wps=16663.7, ups=1.59, wpb=10490.5, bsz=387.4, num_updates=29600, lr=1.27343e-05, gnorm=0.859, train_wall=63, wall=0
2021-01-07 17:23:31 | INFO | train_inner | epoch 053:    528 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.342, nll_loss=1.015, ppl=2.02, wps=16776.2, ups=1.61, wpb=10444.8, bsz=366.7, num_updates=29700, lr=1.27128e-05, gnorm=0.873, train_wall=62, wall=0
2021-01-07 17:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:23:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:23:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:24:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:24:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:24:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:24:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:24:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:24:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:24:13 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.201 | nll_loss 3.69 | ppl 12.91 | bleu 23 | wps 4593.9 | wpb 7508.5 | bsz 272.7 | num_updates 29733 | best_bleu 23.13
2021-01-07 17:24:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:24:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:24:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:24:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:24:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:24:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 53 @ 29733 updates, score 23.0) (writing took 2.8172362614423037 seconds)
2021-01-07 17:24:16 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-07 17:24:16 | INFO | train | epoch 053 | symm_kl 0.392 | self_kl 0 | self_cv 0 | loss 3.324 | nll_loss 1 | ppl 2 | wps 15616.4 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 29733 | lr 1.27058e-05 | gnorm 0.867 | train_wall 348 | wall 0
2021-01-07 17:24:16 | INFO | fairseq.trainer | begin training epoch 54
2021-01-07 17:24:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:24:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:25:00 | INFO | train_inner | epoch 054:     67 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.003, ppl=2, wps=11831.2, ups=1.13, wpb=10441.9, bsz=359.6, num_updates=29800, lr=1.26915e-05, gnorm=0.871, train_wall=61, wall=0
2021-01-07 17:26:02 | INFO | train_inner | epoch 054:    167 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.996, ppl=1.99, wps=16871, ups=1.6, wpb=10526.1, bsz=368.2, num_updates=29900, lr=1.26702e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 17:27:04 | INFO | train_inner | epoch 054:    267 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.007, ppl=2.01, wps=16880.8, ups=1.61, wpb=10491.1, bsz=372.2, num_updates=30000, lr=1.26491e-05, gnorm=0.868, train_wall=62, wall=0
2021-01-07 17:28:07 | INFO | train_inner | epoch 054:    367 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.991, ppl=1.99, wps=16890.7, ups=1.6, wpb=10584.5, bsz=376.3, num_updates=30100, lr=1.26281e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 17:29:09 | INFO | train_inner | epoch 054:    467 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.003, ppl=2, wps=16979.4, ups=1.62, wpb=10493.2, bsz=368.6, num_updates=30200, lr=1.26072e-05, gnorm=0.869, train_wall=62, wall=0
2021-01-07 17:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:30:28 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.2 | nll_loss 3.69 | ppl 12.91 | bleu 23.08 | wps 4662.5 | wpb 7508.5 | bsz 272.7 | num_updates 30294 | best_bleu 23.13
2021-01-07 17:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 54 @ 30294 updates, score 23.08) (writing took 2.820504182949662 seconds)
2021-01-07 17:30:31 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-07 17:30:31 | INFO | train | epoch 054 | symm_kl 0.391 | self_kl 0 | self_cv 0 | loss 3.321 | nll_loss 0.999 | ppl 2 | wps 15687.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 30294 | lr 1.25876e-05 | gnorm 0.87 | train_wall 347 | wall 0
2021-01-07 17:30:31 | INFO | fairseq.trainer | begin training epoch 55
2021-01-07 17:30:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:30:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:30:37 | INFO | train_inner | epoch 055:      6 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.322, nll_loss=1, ppl=2, wps=11616.8, ups=1.13, wpb=10303.8, bsz=364.4, num_updates=30300, lr=1.25863e-05, gnorm=0.891, train_wall=62, wall=0
2021-01-07 17:31:39 | INFO | train_inner | epoch 055:    106 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.007, ppl=2.01, wps=17144.4, ups=1.63, wpb=10511.4, bsz=376.5, num_updates=30400, lr=1.25656e-05, gnorm=0.862, train_wall=61, wall=0
2021-01-07 17:32:41 | INFO | train_inner | epoch 055:    206 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.995, ppl=1.99, wps=16984.3, ups=1.62, wpb=10500.2, bsz=383.4, num_updates=30500, lr=1.2545e-05, gnorm=0.853, train_wall=62, wall=0
2021-01-07 17:33:43 | INFO | train_inner | epoch 055:    306 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.996, ppl=1.99, wps=16952.7, ups=1.61, wpb=10505.5, bsz=358.1, num_updates=30600, lr=1.25245e-05, gnorm=0.861, train_wall=62, wall=0
2021-01-07 17:34:45 | INFO | train_inner | epoch 055:    406 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.994, ppl=1.99, wps=16962.7, ups=1.6, wpb=10591.9, bsz=368.2, num_updates=30700, lr=1.25041e-05, gnorm=0.858, train_wall=62, wall=0
2021-01-07 17:35:47 | INFO | train_inner | epoch 055:    506 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.321, nll_loss=1, ppl=2, wps=16718.1, ups=1.61, wpb=10394.2, bsz=368.7, num_updates=30800, lr=1.24838e-05, gnorm=0.865, train_wall=62, wall=0
2021-01-07 17:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:36:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:36:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:36:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:36:42 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.198 | nll_loss 3.689 | ppl 12.9 | bleu 23 | wps 4647.3 | wpb 7508.5 | bsz 272.7 | num_updates 30855 | best_bleu 23.13
2021-01-07 17:36:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:36:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:36:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:36:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 55 @ 30855 updates, score 23.0) (writing took 2.8385225888341665 seconds)
2021-01-07 17:36:45 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-07 17:36:45 | INFO | train | epoch 055 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.32 | nll_loss 0.998 | ppl 2 | wps 15713 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 30855 | lr 1.24726e-05 | gnorm 0.862 | train_wall 346 | wall 0
2021-01-07 17:36:45 | INFO | fairseq.trainer | begin training epoch 56
2021-01-07 17:36:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:36:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:37:15 | INFO | train_inner | epoch 056:     45 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.331, nll_loss=1.004, ppl=2.01, wps=11724.8, ups=1.13, wpb=10331.1, bsz=356.5, num_updates=30900, lr=1.24635e-05, gnorm=0.897, train_wall=61, wall=0
2021-01-07 17:38:18 | INFO | train_inner | epoch 056:    145 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.315, nll_loss=0.998, ppl=2, wps=16909.6, ups=1.6, wpb=10597, bsz=377, num_updates=31000, lr=1.24434e-05, gnorm=0.854, train_wall=62, wall=0
2021-01-07 17:39:20 | INFO | train_inner | epoch 056:    245 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.986, ppl=1.98, wps=16950.4, ups=1.6, wpb=10582.9, bsz=408.2, num_updates=31100, lr=1.24234e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 17:40:22 | INFO | train_inner | epoch 056:    345 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.324, nll_loss=0.999, ppl=2, wps=16911.6, ups=1.63, wpb=10400.7, bsz=344.2, num_updates=31200, lr=1.24035e-05, gnorm=0.883, train_wall=61, wall=0
2021-01-07 17:41:24 | INFO | train_inner | epoch 056:    445 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.321, nll_loss=0.997, ppl=2, wps=16691.9, ups=1.61, wpb=10398.5, bsz=371.8, num_updates=31300, lr=1.23836e-05, gnorm=0.882, train_wall=62, wall=0
2021-01-07 17:42:26 | INFO | train_inner | epoch 056:    545 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.327, nll_loss=1.004, ppl=2.01, wps=17074.2, ups=1.62, wpb=10556.1, bsz=363.9, num_updates=31400, lr=1.23639e-05, gnorm=0.864, train_wall=62, wall=0
2021-01-07 17:42:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:42:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:42:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:42:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:42:56 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.2 | nll_loss 3.69 | ppl 12.9 | bleu 23.01 | wps 4668.2 | wpb 7508.5 | bsz 272.7 | num_updates 31416 | best_bleu 23.13
2021-01-07 17:42:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:42:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:42:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:42:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 56 @ 31416 updates, score 23.01) (writing took 2.8304561842232943 seconds)
2021-01-07 17:42:59 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-07 17:42:59 | INFO | train | epoch 056 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.319 | nll_loss 0.998 | ppl 2 | wps 15705.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 31416 | lr 1.23608e-05 | gnorm 0.869 | train_wall 347 | wall 0
2021-01-07 17:42:59 | INFO | fairseq.trainer | begin training epoch 57
2021-01-07 17:43:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:43:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:43:54 | INFO | train_inner | epoch 057:     84 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.318, nll_loss=1, ppl=2, wps=11858.4, ups=1.14, wpb=10411.4, bsz=373.3, num_updates=31500, lr=1.23443e-05, gnorm=0.869, train_wall=61, wall=0
2021-01-07 17:44:56 | INFO | train_inner | epoch 057:    184 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.005, ppl=2.01, wps=16897.6, ups=1.61, wpb=10476.3, bsz=375.3, num_updates=31600, lr=1.23247e-05, gnorm=0.871, train_wall=62, wall=0
2021-01-07 17:45:58 | INFO | train_inner | epoch 057:    284 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.323, nll_loss=0.996, ppl=1.99, wps=16862.9, ups=1.62, wpb=10418.2, bsz=348.9, num_updates=31700, lr=1.23053e-05, gnorm=0.873, train_wall=62, wall=0
2021-01-07 17:46:59 | INFO | train_inner | epoch 057:    384 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.996, ppl=1.99, wps=17037.8, ups=1.62, wpb=10518.8, bsz=377.6, num_updates=31800, lr=1.22859e-05, gnorm=0.855, train_wall=62, wall=0
2021-01-07 17:48:01 | INFO | train_inner | epoch 057:    484 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.999, ppl=2, wps=17065.5, ups=1.61, wpb=10597.3, bsz=371.9, num_updates=31900, lr=1.22666e-05, gnorm=0.86, train_wall=62, wall=0
2021-01-07 17:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:48:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:48:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:48:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:49:10 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.197 | nll_loss 3.686 | ppl 12.87 | bleu 23.03 | wps 4714.2 | wpb 7508.5 | bsz 272.7 | num_updates 31977 | best_bleu 23.13
2021-01-07 17:49:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:49:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:49:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:49:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:49:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:49:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 57 @ 31977 updates, score 23.03) (writing took 2.8744586687535048 seconds)
2021-01-07 17:49:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-07 17:49:12 | INFO | train | epoch 057 | symm_kl 0.389 | self_kl 0 | self_cv 0 | loss 3.318 | nll_loss 0.998 | ppl 2 | wps 15760.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 31977 | lr 1.22519e-05 | gnorm 0.865 | train_wall 345 | wall 0
2021-01-07 17:49:12 | INFO | fairseq.trainer | begin training epoch 58
2021-01-07 17:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:49:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:49:30 | INFO | train_inner | epoch 058:     23 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.31, nll_loss=0.992, ppl=1.99, wps=11864.1, ups=1.14, wpb=10448.7, bsz=359.8, num_updates=32000, lr=1.22474e-05, gnorm=0.862, train_wall=61, wall=0
2021-01-07 17:50:31 | INFO | train_inner | epoch 058:    123 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.984, ppl=1.98, wps=17207, ups=1.63, wpb=10578.5, bsz=386.2, num_updates=32100, lr=1.22284e-05, gnorm=0.857, train_wall=61, wall=0
2021-01-07 17:51:33 | INFO | train_inner | epoch 058:    223 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.338, nll_loss=1.013, ppl=2.02, wps=16718.4, ups=1.61, wpb=10363.2, bsz=372.5, num_updates=32200, lr=1.22094e-05, gnorm=0.879, train_wall=62, wall=0
2021-01-07 17:52:35 | INFO | train_inner | epoch 058:    323 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.31, nll_loss=0.991, ppl=1.99, wps=16985, ups=1.61, wpb=10573, bsz=359.3, num_updates=32300, lr=1.21904e-05, gnorm=0.865, train_wall=62, wall=0
2021-01-07 17:53:37 | INFO | train_inner | epoch 058:    423 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.332, nll_loss=1.006, ppl=2.01, wps=16862, ups=1.62, wpb=10422.2, bsz=360.6, num_updates=32400, lr=1.21716e-05, gnorm=0.875, train_wall=62, wall=0
2021-01-07 17:54:39 | INFO | train_inner | epoch 058:    523 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.32, nll_loss=1.002, ppl=2, wps=16912.8, ups=1.62, wpb=10446.5, bsz=361.1, num_updates=32500, lr=1.21529e-05, gnorm=0.86, train_wall=62, wall=0
2021-01-07 17:55:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 17:55:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:55:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:55:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 17:55:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 17:55:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 17:55:23 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.197 | nll_loss 3.688 | ppl 12.88 | bleu 22.96 | wps 4669.5 | wpb 7508.5 | bsz 272.7 | num_updates 32538 | best_bleu 23.13
2021-01-07 17:55:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 17:55:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:55:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:55:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 58 @ 32538 updates, score 22.96) (writing took 2.8177024237811565 seconds)
2021-01-07 17:55:26 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-07 17:55:26 | INFO | train | epoch 058 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.316 | nll_loss 0.998 | ppl 2 | wps 15747 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 32538 | lr 1.21458e-05 | gnorm 0.865 | train_wall 346 | wall 0
2021-01-07 17:55:26 | INFO | fairseq.trainer | begin training epoch 59
2021-01-07 17:55:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 17:55:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 17:56:07 | INFO | train_inner | epoch 059:     62 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.314, nll_loss=1, ppl=2, wps=11913.3, ups=1.14, wpb=10453.1, bsz=379.3, num_updates=32600, lr=1.21342e-05, gnorm=0.86, train_wall=61, wall=0
2021-01-07 17:57:08 | INFO | train_inner | epoch 059:    162 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.989, ppl=1.98, wps=17074.1, ups=1.62, wpb=10539.2, bsz=364.2, num_updates=32700, lr=1.21157e-05, gnorm=0.865, train_wall=62, wall=0
2021-01-07 17:58:11 | INFO | train_inner | epoch 059:    262 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.304, nll_loss=0.992, ppl=1.99, wps=16771.6, ups=1.61, wpb=10439.9, bsz=364.1, num_updates=32800, lr=1.20972e-05, gnorm=0.863, train_wall=62, wall=0
2021-01-07 17:59:12 | INFO | train_inner | epoch 059:    362 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.335, nll_loss=1.007, ppl=2.01, wps=16793.3, ups=1.62, wpb=10395.9, bsz=351, num_updates=32900, lr=1.20788e-05, gnorm=0.88, train_wall=62, wall=0
2021-01-07 18:00:14 | INFO | train_inner | epoch 059:    462 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.981, ppl=1.97, wps=17218.7, ups=1.62, wpb=10653.2, bsz=400.8, num_updates=33000, lr=1.20605e-05, gnorm=0.839, train_wall=62, wall=0
2021-01-07 18:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:01:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:01:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:01:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:01:37 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.198 | nll_loss 3.687 | ppl 12.88 | bleu 23.01 | wps 4702.1 | wpb 7508.5 | bsz 272.7 | num_updates 33099 | best_bleu 23.13
2021-01-07 18:01:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:01:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 59 @ 33099 updates, score 23.01) (writing took 2.848996540531516 seconds)
2021-01-07 18:01:39 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-07 18:01:39 | INFO | train | epoch 059 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.314 | nll_loss 0.997 | ppl 2 | wps 15745.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 33099 | lr 1.20424e-05 | gnorm 0.863 | train_wall 346 | wall 0
2021-01-07 18:01:39 | INFO | fairseq.trainer | begin training epoch 60
2021-01-07 18:01:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:01:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:01:43 | INFO | train_inner | epoch 060:      1 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.329, nll_loss=1.009, ppl=2.01, wps=11697.1, ups=1.12, wpb=10421, bsz=369.7, num_updates=33100, lr=1.20422e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 18:02:45 | INFO | train_inner | epoch 060:    101 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.991, ppl=1.99, wps=17209.4, ups=1.63, wpb=10558, bsz=375.6, num_updates=33200, lr=1.20241e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 18:03:46 | INFO | train_inner | epoch 060:    201 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.006, ppl=2.01, wps=16887.4, ups=1.62, wpb=10401.3, bsz=359.1, num_updates=33300, lr=1.2006e-05, gnorm=0.865, train_wall=61, wall=0
2021-01-07 18:04:49 | INFO | train_inner | epoch 060:    301 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.994, ppl=1.99, wps=16813.8, ups=1.6, wpb=10476.6, bsz=373.2, num_updates=33400, lr=1.1988e-05, gnorm=0.861, train_wall=62, wall=0
2021-01-07 18:05:51 | INFO | train_inner | epoch 060:    401 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.002, ppl=2, wps=17000.1, ups=1.61, wpb=10552.6, bsz=366.6, num_updates=33500, lr=1.19701e-05, gnorm=0.859, train_wall=62, wall=0
2021-01-07 18:06:53 | INFO | train_inner | epoch 060:    501 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.993, ppl=1.99, wps=16832.3, ups=1.61, wpb=10479.3, bsz=376.6, num_updates=33600, lr=1.19523e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 18:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:07:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:07:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:07:51 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.198 | nll_loss 3.689 | ppl 12.9 | bleu 23.02 | wps 4632.9 | wpb 7508.5 | bsz 272.7 | num_updates 33660 | best_bleu 23.13
2021-01-07 18:07:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:07:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:07:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 60 @ 33660 updates, score 23.02) (writing took 2.807733340188861 seconds)
2021-01-07 18:07:54 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-07 18:07:54 | INFO | train | epoch 060 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.313 | nll_loss 0.996 | ppl 1.99 | wps 15707.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 33660 | lr 1.19416e-05 | gnorm 0.864 | train_wall 346 | wall 0
2021-01-07 18:07:54 | INFO | fairseq.trainer | begin training epoch 61
2021-01-07 18:07:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:07:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:08:21 | INFO | train_inner | epoch 061:     40 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.987, ppl=1.98, wps=11833.9, ups=1.13, wpb=10470.8, bsz=371.7, num_updates=33700, lr=1.19345e-05, gnorm=0.877, train_wall=61, wall=0
2021-01-07 18:09:23 | INFO | train_inner | epoch 061:    140 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.316, nll_loss=1, ppl=2, wps=16860.3, ups=1.62, wpb=10423.5, bsz=371.8, num_updates=33800, lr=1.19169e-05, gnorm=0.859, train_wall=62, wall=0
2021-01-07 18:10:26 | INFO | train_inner | epoch 061:    240 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.997, ppl=2, wps=16808.8, ups=1.61, wpb=10466.2, bsz=380.2, num_updates=33900, lr=1.18993e-05, gnorm=0.855, train_wall=62, wall=0
2021-01-07 18:11:28 | INFO | train_inner | epoch 061:    340 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.324, nll_loss=1.004, ppl=2, wps=16867.3, ups=1.61, wpb=10463.9, bsz=356.2, num_updates=34000, lr=1.18818e-05, gnorm=0.867, train_wall=62, wall=0
2021-01-07 18:12:30 | INFO | train_inner | epoch 061:    440 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.984, ppl=1.98, wps=17056.4, ups=1.6, wpb=10642.9, bsz=368.1, num_updates=34100, lr=1.18643e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 18:13:32 | INFO | train_inner | epoch 061:    540 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.994, ppl=1.99, wps=16732.2, ups=1.6, wpb=10444.1, bsz=371.7, num_updates=34200, lr=1.1847e-05, gnorm=0.857, train_wall=62, wall=0
2021-01-07 18:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:13:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:13:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:13:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:13:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:13:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:13:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:13:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:14:06 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.197 | nll_loss 3.688 | ppl 12.89 | bleu 22.94 | wps 4708.2 | wpb 7508.5 | bsz 272.7 | num_updates 34221 | best_bleu 23.13
2021-01-07 18:14:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:14:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:14:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:14:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 61 @ 34221 updates, score 22.94) (writing took 2.84225070476532 seconds)
2021-01-07 18:14:09 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-07 18:14:09 | INFO | train | epoch 061 | symm_kl 0.386 | self_kl 0 | self_cv 0 | loss 3.311 | nll_loss 0.996 | ppl 1.99 | wps 15692.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 34221 | lr 1.18433e-05 | gnorm 0.86 | train_wall 347 | wall 0
2021-01-07 18:14:09 | INFO | fairseq.trainer | begin training epoch 62
2021-01-07 18:14:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:14:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:15:00 | INFO | train_inner | epoch 062:     79 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.002, ppl=2, wps=11860.2, ups=1.14, wpb=10437.3, bsz=367.2, num_updates=34300, lr=1.18297e-05, gnorm=0.87, train_wall=61, wall=0
2021-01-07 18:16:03 | INFO | train_inner | epoch 062:    179 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.983, ppl=1.98, wps=16777, ups=1.6, wpb=10489, bsz=366.9, num_updates=34400, lr=1.18125e-05, gnorm=0.863, train_wall=62, wall=0
2021-01-07 18:17:06 | INFO | train_inner | epoch 062:    279 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.997, ppl=2, wps=16751.4, ups=1.59, wpb=10532.2, bsz=370.1, num_updates=34500, lr=1.17954e-05, gnorm=0.851, train_wall=63, wall=0
2021-01-07 18:18:08 | INFO | train_inner | epoch 062:    379 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.995, ppl=1.99, wps=16837.4, ups=1.61, wpb=10486.1, bsz=382.8, num_updates=34600, lr=1.17783e-05, gnorm=0.869, train_wall=62, wall=0
2021-01-07 18:19:11 | INFO | train_inner | epoch 062:    479 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.319, nll_loss=1.001, ppl=2, wps=16631.8, ups=1.6, wpb=10399.9, bsz=357.1, num_updates=34700, lr=1.17613e-05, gnorm=0.863, train_wall=62, wall=0
2021-01-07 18:20:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:20:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:20:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:20:22 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.199 | nll_loss 3.689 | ppl 12.9 | bleu 23.05 | wps 4604.6 | wpb 7508.5 | bsz 272.7 | num_updates 34782 | best_bleu 23.13
2021-01-07 18:20:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:20:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 62 @ 34782 updates, score 23.05) (writing took 2.6334733683615923 seconds)
2021-01-07 18:20:25 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-07 18:20:25 | INFO | train | epoch 062 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.31 | nll_loss 0.995 | ppl 1.99 | wps 15626.8 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 34782 | lr 1.17474e-05 | gnorm 0.861 | train_wall 348 | wall 0
2021-01-07 18:20:25 | INFO | fairseq.trainer | begin training epoch 63
2021-01-07 18:20:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:20:39 | INFO | train_inner | epoch 063:     18 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.991, ppl=1.99, wps=11850.7, ups=1.13, wpb=10483.3, bsz=366, num_updates=34800, lr=1.17444e-05, gnorm=0.857, train_wall=62, wall=0
2021-01-07 18:21:41 | INFO | train_inner | epoch 063:    118 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.992, ppl=1.99, wps=17303.2, ups=1.62, wpb=10670.8, bsz=364.8, num_updates=34900, lr=1.17276e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 18:22:42 | INFO | train_inner | epoch 063:    218 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.995, ppl=1.99, wps=17065.6, ups=1.63, wpb=10481.1, bsz=372.8, num_updates=35000, lr=1.17108e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 18:23:44 | INFO | train_inner | epoch 063:    318 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.99, ppl=1.99, wps=16997.6, ups=1.62, wpb=10497.3, bsz=383.2, num_updates=35100, lr=1.16941e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 18:24:46 | INFO | train_inner | epoch 063:    418 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.006, ppl=2.01, wps=16713.5, ups=1.61, wpb=10385, bsz=363.8, num_updates=35200, lr=1.16775e-05, gnorm=0.856, train_wall=62, wall=0
2021-01-07 18:25:48 | INFO | train_inner | epoch 063:    518 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.006, ppl=2.01, wps=16795.7, ups=1.61, wpb=10433, bsz=380.1, num_updates=35300, lr=1.16609e-05, gnorm=0.866, train_wall=62, wall=0
2021-01-07 18:26:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:26:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:26:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:26:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:26:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:26:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:26:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:26:35 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.194 | nll_loss 3.685 | ppl 12.86 | bleu 23.09 | wps 4735.1 | wpb 7508.5 | bsz 272.7 | num_updates 35343 | best_bleu 23.13
2021-01-07 18:26:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:26:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:26:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:26:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 63 @ 35343 updates, score 23.09) (writing took 2.829110024496913 seconds)
2021-01-07 18:26:38 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-07 18:26:38 | INFO | train | epoch 063 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.309 | nll_loss 0.995 | ppl 1.99 | wps 15775.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 35343 | lr 1.16538e-05 | gnorm 0.858 | train_wall 345 | wall 0
2021-01-07 18:26:38 | INFO | fairseq.trainer | begin training epoch 64
2021-01-07 18:26:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:27:16 | INFO | train_inner | epoch 064:     57 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.975, ppl=1.97, wps=12010.2, ups=1.14, wpb=10499.9, bsz=366.3, num_updates=35400, lr=1.16445e-05, gnorm=0.85, train_wall=61, wall=0
2021-01-07 18:28:18 | INFO | train_inner | epoch 064:    157 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.996, ppl=1.99, wps=16694.7, ups=1.59, wpb=10487, bsz=366.4, num_updates=35500, lr=1.1628e-05, gnorm=0.85, train_wall=63, wall=0
2021-01-07 18:29:21 | INFO | train_inner | epoch 064:    257 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.997, ppl=2, wps=16841.8, ups=1.61, wpb=10474.1, bsz=362.2, num_updates=35600, lr=1.16117e-05, gnorm=0.853, train_wall=62, wall=0
2021-01-07 18:30:23 | INFO | train_inner | epoch 064:    357 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.315, nll_loss=0.999, ppl=2, wps=16961.2, ups=1.6, wpb=10589.6, bsz=368.4, num_updates=35700, lr=1.15954e-05, gnorm=0.847, train_wall=62, wall=0
2021-01-07 18:31:25 | INFO | train_inner | epoch 064:    457 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.314, nll_loss=0.995, ppl=1.99, wps=16774, ups=1.61, wpb=10413.9, bsz=365.5, num_updates=35800, lr=1.15792e-05, gnorm=0.868, train_wall=62, wall=0
2021-01-07 18:32:28 | INFO | train_inner | epoch 064:    557 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.991, ppl=1.99, wps=16799.7, ups=1.6, wpb=10487.9, bsz=378.7, num_updates=35900, lr=1.15631e-05, gnorm=0.853, train_wall=62, wall=0
2021-01-07 18:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:32:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:32:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:32:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:32:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:32:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:32:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:32:50 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.198 | nll_loss 3.687 | ppl 12.88 | bleu 23.04 | wps 4719.1 | wpb 7508.5 | bsz 272.7 | num_updates 35904 | best_bleu 23.13
2021-01-07 18:32:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:32:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:32:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:32:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 64 @ 35904 updates, score 23.04) (writing took 2.788489418104291 seconds)
2021-01-07 18:32:53 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-07 18:32:53 | INFO | train | epoch 064 | symm_kl 0.384 | self_kl 0 | self_cv 0 | loss 3.307 | nll_loss 0.994 | ppl 1.99 | wps 15670.9 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 35904 | lr 1.15624e-05 | gnorm 0.853 | train_wall 348 | wall 0
2021-01-07 18:32:53 | INFO | fairseq.trainer | begin training epoch 65
2021-01-07 18:32:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:32:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:33:55 | INFO | train_inner | epoch 065:     96 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.314, nll_loss=1, ppl=2, wps=11885.2, ups=1.15, wpb=10335.8, bsz=360.5, num_updates=36000, lr=1.1547e-05, gnorm=0.872, train_wall=60, wall=0
2021-01-07 18:34:57 | INFO | train_inner | epoch 065:    196 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.991, ppl=1.99, wps=16924.5, ups=1.61, wpb=10512.1, bsz=360.4, num_updates=36100, lr=1.1531e-05, gnorm=0.851, train_wall=62, wall=0
2021-01-07 18:35:58 | INFO | train_inner | epoch 065:    296 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.318, nll_loss=1.002, ppl=2, wps=16865.8, ups=1.62, wpb=10413.8, bsz=379.9, num_updates=36200, lr=1.15151e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 18:37:00 | INFO | train_inner | epoch 065:    396 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.292, nll_loss=0.985, ppl=1.98, wps=17167, ups=1.62, wpb=10570, bsz=388.1, num_updates=36300, lr=1.14992e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 18:38:02 | INFO | train_inner | epoch 065:    496 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.994, ppl=1.99, wps=16916.3, ups=1.61, wpb=10506.4, bsz=363.1, num_updates=36400, lr=1.14834e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 18:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:38:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:38:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:38:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:38:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:38:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:38:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:39:03 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.685 | ppl 12.86 | bleu 23.08 | wps 4730.6 | wpb 7508.5 | bsz 272.7 | num_updates 36465 | best_bleu 23.13
2021-01-07 18:39:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:39:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:39:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:39:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 65 @ 36465 updates, score 23.08) (writing took 2.8022834546864033 seconds)
2021-01-07 18:39:06 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-01-07 18:39:06 | INFO | train | epoch 065 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.306 | nll_loss 0.994 | ppl 1.99 | wps 15780.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 36465 | lr 1.14731e-05 | gnorm 0.858 | train_wall 345 | wall 0
2021-01-07 18:39:06 | INFO | fairseq.trainer | begin training epoch 66
2021-01-07 18:39:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:39:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:39:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:39:30 | INFO | train_inner | epoch 066:     35 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.001, ppl=2, wps=11877.6, ups=1.14, wpb=10445.1, bsz=367.7, num_updates=36500, lr=1.14676e-05, gnorm=0.858, train_wall=61, wall=0
2021-01-07 18:40:32 | INFO | train_inner | epoch 066:    135 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.985, ppl=1.98, wps=17003.6, ups=1.62, wpb=10495.9, bsz=377.7, num_updates=36600, lr=1.1452e-05, gnorm=0.849, train_wall=62, wall=0
2021-01-07 18:41:34 | INFO | train_inner | epoch 066:    235 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.99, ppl=1.99, wps=16908.2, ups=1.62, wpb=10465, bsz=376.7, num_updates=36700, lr=1.14364e-05, gnorm=0.85, train_wall=62, wall=0
2021-01-07 18:42:36 | INFO | train_inner | epoch 066:    335 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.972, ppl=1.96, wps=17113.6, ups=1.61, wpb=10611.2, bsz=367, num_updates=36800, lr=1.14208e-05, gnorm=0.841, train_wall=62, wall=0
2021-01-07 18:43:37 | INFO | train_inner | epoch 066:    435 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.315, nll_loss=0.999, ppl=2, wps=17111.3, ups=1.62, wpb=10555.1, bsz=360.2, num_updates=36900, lr=1.14053e-05, gnorm=0.856, train_wall=61, wall=0
2021-01-07 18:44:39 | INFO | train_inner | epoch 066:    535 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.31, nll_loss=1.001, ppl=2, wps=16929.9, ups=1.62, wpb=10443.3, bsz=371, num_updates=37000, lr=1.13899e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 18:44:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:44:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:44:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:44:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:44:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:44:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:44:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:44:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:44:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:44:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:44:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:44:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:44:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:44:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:44:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:44:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:45:15 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.684 | ppl 12.86 | bleu 23.04 | wps 4695.7 | wpb 7508.5 | bsz 272.7 | num_updates 37026 | best_bleu 23.13
2021-01-07 18:45:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:45:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:45:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:45:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:45:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:45:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 66 @ 37026 updates, score 23.04) (writing took 2.8041986282914877 seconds)
2021-01-07 18:45:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-01-07 18:45:18 | INFO | train | epoch 066 | symm_kl 0.383 | self_kl 0 | self_cv 0 | loss 3.305 | nll_loss 0.993 | ppl 1.99 | wps 15798.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 37026 | lr 1.13859e-05 | gnorm 0.852 | train_wall 345 | wall 0
2021-01-07 18:45:18 | INFO | fairseq.trainer | begin training epoch 67
2021-01-07 18:45:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:45:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:46:06 | INFO | train_inner | epoch 067:     74 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.33, nll_loss=1.011, ppl=2.02, wps=11931.7, ups=1.15, wpb=10394.5, bsz=361.8, num_updates=37100, lr=1.13745e-05, gnorm=0.865, train_wall=60, wall=0
2021-01-07 18:47:08 | INFO | train_inner | epoch 067:    174 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.997, ppl=2, wps=17047, ups=1.62, wpb=10514.9, bsz=370.6, num_updates=37200, lr=1.13592e-05, gnorm=0.857, train_wall=61, wall=0
2021-01-07 18:48:10 | INFO | train_inner | epoch 067:    274 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.988, ppl=1.98, wps=17155.1, ups=1.62, wpb=10617.9, bsz=370.6, num_updates=37300, lr=1.1344e-05, gnorm=0.848, train_wall=62, wall=0
2021-01-07 18:49:11 | INFO | train_inner | epoch 067:    374 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.978, ppl=1.97, wps=16993.9, ups=1.63, wpb=10439, bsz=372.6, num_updates=37400, lr=1.13288e-05, gnorm=0.853, train_wall=61, wall=0
2021-01-07 18:50:13 | INFO | train_inner | epoch 067:    474 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.996, ppl=1.99, wps=16929.1, ups=1.62, wpb=10477.9, bsz=364.2, num_updates=37500, lr=1.13137e-05, gnorm=0.859, train_wall=62, wall=0
2021-01-07 18:51:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:51:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:51:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:51:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:51:27 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.687 | ppl 12.88 | bleu 23 | wps 4696.5 | wpb 7508.5 | bsz 272.7 | num_updates 37587 | best_bleu 23.13
2021-01-07 18:51:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:51:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 67 @ 37587 updates, score 23.0) (writing took 2.7692095451056957 seconds)
2021-01-07 18:51:30 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-01-07 18:51:30 | INFO | train | epoch 067 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.304 | nll_loss 0.993 | ppl 1.99 | wps 15826.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 37587 | lr 1.13006e-05 | gnorm 0.857 | train_wall 344 | wall 0
2021-01-07 18:51:30 | INFO | fairseq.trainer | begin training epoch 68
2021-01-07 18:51:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:51:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:51:41 | INFO | train_inner | epoch 068:     13 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.997, ppl=2, wps=11796.9, ups=1.14, wpb=10359.2, bsz=371.4, num_updates=37600, lr=1.12987e-05, gnorm=0.865, train_wall=61, wall=0
2021-01-07 18:52:42 | INFO | train_inner | epoch 068:    113 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.985, ppl=1.98, wps=17113.9, ups=1.64, wpb=10424.8, bsz=377, num_updates=37700, lr=1.12837e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 18:53:43 | INFO | train_inner | epoch 068:    213 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.98, ppl=1.97, wps=17020.7, ups=1.62, wpb=10507.7, bsz=381.4, num_updates=37800, lr=1.12687e-05, gnorm=0.84, train_wall=62, wall=0
2021-01-07 18:54:45 | INFO | train_inner | epoch 068:    313 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.325, nll_loss=1.008, ppl=2.01, wps=16898.7, ups=1.62, wpb=10461.3, bsz=368, num_updates=37900, lr=1.12538e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 18:55:47 | INFO | train_inner | epoch 068:    413 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.995, ppl=1.99, wps=17092.7, ups=1.62, wpb=10522.8, bsz=366.1, num_updates=38000, lr=1.1239e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 18:56:48 | INFO | train_inner | epoch 068:    513 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.999, ppl=2, wps=17139.2, ups=1.63, wpb=10524.8, bsz=360.5, num_updates=38100, lr=1.12243e-05, gnorm=0.849, train_wall=61, wall=0
2021-01-07 18:57:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 18:57:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:57:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:57:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 18:57:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 18:57:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 18:57:38 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.196 | nll_loss 3.687 | ppl 12.88 | bleu 23.04 | wps 4758.1 | wpb 7508.5 | bsz 272.7 | num_updates 38148 | best_bleu 23.13
2021-01-07 18:57:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 18:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:57:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:57:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 68 @ 38148 updates, score 23.04) (writing took 2.793224284425378 seconds)
2021-01-07 18:57:41 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-01-07 18:57:41 | INFO | train | epoch 068 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.303 | nll_loss 0.993 | ppl 1.99 | wps 15829 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 38148 | lr 1.12172e-05 | gnorm 0.853 | train_wall 344 | wall 0
2021-01-07 18:57:41 | INFO | fairseq.trainer | begin training epoch 69
2021-01-07 18:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 18:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 18:58:16 | INFO | train_inner | epoch 069:     52 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.999, ppl=2, wps=11877.8, ups=1.14, wpb=10378.4, bsz=358.9, num_updates=38200, lr=1.12096e-05, gnorm=0.868, train_wall=60, wall=0
2021-01-07 18:59:18 | INFO | train_inner | epoch 069:    152 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.292, nll_loss=0.989, ppl=1.98, wps=16915.2, ups=1.61, wpb=10478.3, bsz=379.9, num_updates=38300, lr=1.11949e-05, gnorm=0.839, train_wall=62, wall=0
2021-01-07 19:00:20 | INFO | train_inner | epoch 069:    252 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.982, ppl=1.98, wps=17134.9, ups=1.61, wpb=10626.7, bsz=373.1, num_updates=38400, lr=1.11803e-05, gnorm=0.85, train_wall=62, wall=0
2021-01-07 19:01:22 | INFO | train_inner | epoch 069:    352 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.99, ppl=1.99, wps=16950.1, ups=1.6, wpb=10563.5, bsz=371.5, num_updates=38500, lr=1.11658e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 19:02:24 | INFO | train_inner | epoch 069:    452 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.995, ppl=1.99, wps=16877.2, ups=1.62, wpb=10445.1, bsz=360, num_updates=38600, lr=1.11513e-05, gnorm=0.858, train_wall=62, wall=0
2021-01-07 19:03:26 | INFO | train_inner | epoch 069:    552 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.998, ppl=2, wps=16828.1, ups=1.62, wpb=10413.8, bsz=372.5, num_updates=38700, lr=1.11369e-05, gnorm=0.866, train_wall=62, wall=0
2021-01-07 19:03:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:03:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:03:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:03:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:03:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:03:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:03:52 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.191 | nll_loss 3.681 | ppl 12.83 | bleu 23.05 | wps 4693.4 | wpb 7508.5 | bsz 272.7 | num_updates 38709 | best_bleu 23.13
2021-01-07 19:03:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:03:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:03:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:03:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 69 @ 38709 updates, score 23.05) (writing took 2.7840898018330336 seconds)
2021-01-07 19:03:54 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-01-07 19:03:54 | INFO | train | epoch 069 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.302 | nll_loss 0.992 | ppl 1.99 | wps 15754.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 38709 | lr 1.11356e-05 | gnorm 0.854 | train_wall 346 | wall 0
2021-01-07 19:03:54 | INFO | fairseq.trainer | begin training epoch 70
2021-01-07 19:03:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:03:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:04:54 | INFO | train_inner | epoch 070:     91 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.982, ppl=1.98, wps=11950.3, ups=1.14, wpb=10486.8, bsz=372.2, num_updates=38800, lr=1.11226e-05, gnorm=0.841, train_wall=61, wall=0
2021-01-07 19:05:56 | INFO | train_inner | epoch 070:    191 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.988, ppl=1.98, wps=16867.7, ups=1.61, wpb=10507.8, bsz=369.6, num_updates=38900, lr=1.11083e-05, gnorm=0.85, train_wall=62, wall=0
2021-01-07 19:06:59 | INFO | train_inner | epoch 070:    291 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.995, ppl=1.99, wps=16630.1, ups=1.59, wpb=10473, bsz=366.5, num_updates=39000, lr=1.1094e-05, gnorm=0.857, train_wall=63, wall=0
2021-01-07 19:08:01 | INFO | train_inner | epoch 070:    391 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.985, ppl=1.98, wps=17022.8, ups=1.61, wpb=10587.3, bsz=378.6, num_updates=39100, lr=1.10798e-05, gnorm=0.849, train_wall=62, wall=0
2021-01-07 19:09:03 | INFO | train_inner | epoch 070:    491 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.315, nll_loss=1.005, ppl=2.01, wps=16781.3, ups=1.61, wpb=10437.4, bsz=363.8, num_updates=39200, lr=1.10657e-05, gnorm=0.856, train_wall=62, wall=0
2021-01-07 19:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:09:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:09:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:09:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:09:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:09:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:09:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:09:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:10:07 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.683 | ppl 12.85 | bleu 22.94 | wps 4668.6 | wpb 7508.5 | bsz 272.7 | num_updates 39270 | best_bleu 23.13
2021-01-07 19:10:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:10:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:10:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:10:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:10:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:10:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 70 @ 39270 updates, score 22.94) (writing took 2.8223279155790806 seconds)
2021-01-07 19:10:10 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-01-07 19:10:10 | INFO | train | epoch 070 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.3 | nll_loss 0.992 | ppl 1.99 | wps 15653.2 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 39270 | lr 1.10558e-05 | gnorm 0.853 | train_wall 348 | wall 0
2021-01-07 19:10:10 | INFO | fairseq.trainer | begin training epoch 71
2021-01-07 19:10:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:10:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:10:31 | INFO | train_inner | epoch 071:     30 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.321, nll_loss=1.007, ppl=2.01, wps=11708.4, ups=1.13, wpb=10331.6, bsz=354.9, num_updates=39300, lr=1.10516e-05, gnorm=0.877, train_wall=61, wall=0
2021-01-07 19:11:34 | INFO | train_inner | epoch 071:    130 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.999, ppl=2, wps=16961.3, ups=1.6, wpb=10597.5, bsz=369.4, num_updates=39400, lr=1.10375e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 19:12:36 | INFO | train_inner | epoch 071:    230 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.983, ppl=1.98, wps=16900.9, ups=1.61, wpb=10467.1, bsz=374.8, num_updates=39500, lr=1.10236e-05, gnorm=0.836, train_wall=62, wall=0
2021-01-07 19:13:39 | INFO | train_inner | epoch 071:    330 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.314, nll_loss=1.001, ppl=2, wps=16673.9, ups=1.59, wpb=10490, bsz=367.7, num_updates=39600, lr=1.10096e-05, gnorm=0.857, train_wall=63, wall=0
2021-01-07 19:14:41 | INFO | train_inner | epoch 071:    430 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.997, ppl=2, wps=17007.3, ups=1.61, wpb=10531.8, bsz=373.4, num_updates=39700, lr=1.09958e-05, gnorm=0.847, train_wall=62, wall=0
2021-01-07 19:15:43 | INFO | train_inner | epoch 071:    530 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.975, ppl=1.97, wps=16777.4, ups=1.61, wpb=10440.2, bsz=385.8, num_updates=39800, lr=1.09819e-05, gnorm=0.868, train_wall=62, wall=0
2021-01-07 19:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:16:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:16:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:16:23 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.685 | ppl 12.86 | bleu 22.96 | wps 4772.9 | wpb 7508.5 | bsz 272.7 | num_updates 39831 | best_bleu 23.13
2021-01-07 19:16:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:16:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:16:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:16:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 71 @ 39831 updates, score 22.96) (writing took 2.7743452545255423 seconds)
2021-01-07 19:16:25 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-01-07 19:16:25 | INFO | train | epoch 071 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.299 | nll_loss 0.992 | ppl 1.99 | wps 15675.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 39831 | lr 1.09777e-05 | gnorm 0.853 | train_wall 348 | wall 0
2021-01-07 19:16:25 | INFO | fairseq.trainer | begin training epoch 72
2021-01-07 19:16:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:16:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:17:11 | INFO | train_inner | epoch 072:     69 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.991, ppl=1.99, wps=11725.6, ups=1.14, wpb=10295.3, bsz=368.1, num_updates=39900, lr=1.09682e-05, gnorm=0.861, train_wall=61, wall=0
2021-01-07 19:18:13 | INFO | train_inner | epoch 072:    169 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.994, ppl=1.99, wps=16904.4, ups=1.6, wpb=10544.8, bsz=363.3, num_updates=40000, lr=1.09545e-05, gnorm=0.847, train_wall=62, wall=0
2021-01-07 19:19:15 | INFO | train_inner | epoch 072:    269 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.002, ppl=2, wps=17035.4, ups=1.61, wpb=10549.6, bsz=365.5, num_updates=40100, lr=1.09408e-05, gnorm=0.85, train_wall=62, wall=0
2021-01-07 19:20:17 | INFO | train_inner | epoch 072:    369 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.985, ppl=1.98, wps=17164.7, ups=1.62, wpb=10581, bsz=367.8, num_updates=40200, lr=1.09272e-05, gnorm=0.85, train_wall=61, wall=0
2021-01-07 19:21:19 | INFO | train_inner | epoch 072:    469 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.984, ppl=1.98, wps=16615.7, ups=1.6, wpb=10356.6, bsz=379.3, num_updates=40300, lr=1.09136e-05, gnorm=0.855, train_wall=62, wall=0
2021-01-07 19:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:22:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:22:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:22:37 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.683 | ppl 12.84 | bleu 23.05 | wps 4766.2 | wpb 7508.5 | bsz 272.7 | num_updates 40392 | best_bleu 23.13
2021-01-07 19:22:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:22:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 72 @ 40392 updates, score 23.05) (writing took 2.7987255323678255 seconds)
2021-01-07 19:22:39 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-01-07 19:22:39 | INFO | train | epoch 072 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.298 | nll_loss 0.991 | ppl 1.99 | wps 15726.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 40392 | lr 1.09012e-05 | gnorm 0.851 | train_wall 346 | wall 0
2021-01-07 19:22:39 | INFO | fairseq.trainer | begin training epoch 73
2021-01-07 19:22:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:22:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:22:47 | INFO | train_inner | epoch 073:      8 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.986, ppl=1.98, wps=11871.2, ups=1.13, wpb=10502.2, bsz=359.5, num_updates=40400, lr=1.09001e-05, gnorm=0.848, train_wall=62, wall=0
2021-01-07 19:23:49 | INFO | train_inner | epoch 073:    108 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.997, ppl=2, wps=17132.4, ups=1.64, wpb=10470.1, bsz=370.7, num_updates=40500, lr=1.08866e-05, gnorm=0.848, train_wall=61, wall=0
2021-01-07 19:24:50 | INFO | train_inner | epoch 073:    208 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.982, ppl=1.97, wps=16759.9, ups=1.62, wpb=10361.4, bsz=355.9, num_updates=40600, lr=1.08732e-05, gnorm=0.862, train_wall=62, wall=0
2021-01-07 19:25:52 | INFO | train_inner | epoch 073:    308 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.993, ppl=1.99, wps=17016.6, ups=1.63, wpb=10459.3, bsz=374.9, num_updates=40700, lr=1.08598e-05, gnorm=0.863, train_wall=61, wall=0
2021-01-07 19:26:54 | INFO | train_inner | epoch 073:    408 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.976, ppl=1.97, wps=16961.8, ups=1.6, wpb=10585.1, bsz=389, num_updates=40800, lr=1.08465e-05, gnorm=0.829, train_wall=62, wall=0
2021-01-07 19:27:56 | INFO | train_inner | epoch 073:    508 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.311, nll_loss=1.002, ppl=2, wps=17086.3, ups=1.62, wpb=10535.8, bsz=366.4, num_updates=40900, lr=1.08333e-05, gnorm=0.851, train_wall=61, wall=0
2021-01-07 19:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:28:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:28:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:28:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:28:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:28:49 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.193 | nll_loss 3.683 | ppl 12.85 | bleu 22.95 | wps 4673.1 | wpb 7508.5 | bsz 272.7 | num_updates 40953 | best_bleu 23.13
2021-01-07 19:28:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:28:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:28:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 73 @ 40953 updates, score 22.95) (writing took 2.788921007886529 seconds)
2021-01-07 19:28:52 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-01-07 19:28:52 | INFO | train | epoch 073 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.297 | nll_loss 0.991 | ppl 1.99 | wps 15770 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 40953 | lr 1.08262e-05 | gnorm 0.85 | train_wall 345 | wall 0
2021-01-07 19:28:52 | INFO | fairseq.trainer | begin training epoch 74
2021-01-07 19:28:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:28:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:29:24 | INFO | train_inner | epoch 074:     47 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.31, nll_loss=1.001, ppl=2, wps=11922.4, ups=1.14, wpb=10470.2, bsz=365.2, num_updates=41000, lr=1.082e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 19:30:25 | INFO | train_inner | epoch 074:    147 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.984, ppl=1.98, wps=17220.6, ups=1.62, wpb=10603.1, bsz=366.6, num_updates=41100, lr=1.08069e-05, gnorm=0.845, train_wall=61, wall=0
2021-01-07 19:31:27 | INFO | train_inner | epoch 074:    247 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.987, ppl=1.98, wps=17095.5, ups=1.63, wpb=10516.6, bsz=373.8, num_updates=41200, lr=1.07937e-05, gnorm=0.838, train_wall=61, wall=0
2021-01-07 19:32:28 | INFO | train_inner | epoch 074:    347 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.991, ppl=1.99, wps=17208, ups=1.63, wpb=10574, bsz=366, num_updates=41300, lr=1.07807e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 19:33:29 | INFO | train_inner | epoch 074:    447 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.994, ppl=1.99, wps=16911.8, ups=1.64, wpb=10310.5, bsz=370.5, num_updates=41400, lr=1.07676e-05, gnorm=0.857, train_wall=61, wall=0
2021-01-07 19:34:31 | INFO | train_inner | epoch 074:    547 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.993, ppl=1.99, wps=16898.3, ups=1.62, wpb=10446.4, bsz=371, num_updates=41500, lr=1.07547e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 19:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:34:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:34:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:34:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:34:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:34:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:34:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:34:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:34:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:34:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:34:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:35:00 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.681 | ppl 12.83 | bleu 23.02 | wps 4739.1 | wpb 7508.5 | bsz 272.7 | num_updates 41514 | best_bleu 23.13
2021-01-07 19:35:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:35:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:35:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:35:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:35:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:35:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 74 @ 41514 updates, score 23.02) (writing took 2.785853886976838 seconds)
2021-01-07 19:35:03 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-01-07 19:35:03 | INFO | train | epoch 074 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.296 | nll_loss 0.99 | ppl 1.99 | wps 15868.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 41514 | lr 1.07528e-05 | gnorm 0.851 | train_wall 343 | wall 0
2021-01-07 19:35:03 | INFO | fairseq.trainer | begin training epoch 75
2021-01-07 19:35:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:35:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:35:58 | INFO | train_inner | epoch 075:     86 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.973, ppl=1.96, wps=11998.5, ups=1.15, wpb=10435.6, bsz=376.5, num_updates=41600, lr=1.07417e-05, gnorm=0.844, train_wall=60, wall=0
2021-01-07 19:37:01 | INFO | train_inner | epoch 075:    186 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.983, ppl=1.98, wps=17006, ups=1.6, wpb=10634, bsz=369.7, num_updates=41700, lr=1.07288e-05, gnorm=0.838, train_wall=62, wall=0
2021-01-07 19:38:03 | INFO | train_inner | epoch 075:    286 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.312, nll_loss=1.001, ppl=2, wps=17083.6, ups=1.61, wpb=10590.4, bsz=359.5, num_updates=41800, lr=1.0716e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 19:39:04 | INFO | train_inner | epoch 075:    386 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.992, ppl=1.99, wps=16663.2, ups=1.62, wpb=10310.9, bsz=368.2, num_updates=41900, lr=1.07032e-05, gnorm=0.853, train_wall=62, wall=0
2021-01-07 19:40:06 | INFO | train_inner | epoch 075:    486 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.309, nll_loss=1.001, ppl=2, wps=16850.2, ups=1.62, wpb=10402, bsz=369.2, num_updates=42000, lr=1.06904e-05, gnorm=0.847, train_wall=62, wall=0
2021-01-07 19:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:40:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:40:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:40:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:40:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:40:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:41:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:41:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:41:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:41:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:41:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:41:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:41:14 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.687 | ppl 12.88 | bleu 22.92 | wps 4400.1 | wpb 7508.5 | bsz 272.7 | num_updates 42075 | best_bleu 23.13
2021-01-07 19:41:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:41:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:41:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:41:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:41:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:41:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 75 @ 42075 updates, score 22.92) (writing took 2.7941560856997967 seconds)
2021-01-07 19:41:17 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-01-07 19:41:17 | INFO | train | epoch 075 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.295 | nll_loss 0.99 | ppl 1.99 | wps 15716.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 42075 | lr 1.06809e-05 | gnorm 0.849 | train_wall 345 | wall 0
2021-01-07 19:41:17 | INFO | fairseq.trainer | begin training epoch 76
2021-01-07 19:41:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:41:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:41:35 | INFO | train_inner | epoch 076:     25 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.981, ppl=1.97, wps=11750.8, ups=1.12, wpb=10483, bsz=380.7, num_updates=42100, lr=1.06777e-05, gnorm=0.861, train_wall=61, wall=0
2021-01-07 19:42:37 | INFO | train_inner | epoch 076:    125 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.988, ppl=1.98, wps=17140.3, ups=1.63, wpb=10537.4, bsz=384.6, num_updates=42200, lr=1.06651e-05, gnorm=0.846, train_wall=61, wall=0
2021-01-07 19:43:39 | INFO | train_inner | epoch 076:    225 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.292, nll_loss=0.989, ppl=1.99, wps=17014.7, ups=1.61, wpb=10582.1, bsz=387.6, num_updates=42300, lr=1.06525e-05, gnorm=0.844, train_wall=62, wall=0
2021-01-07 19:44:41 | INFO | train_inner | epoch 076:    325 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.982, ppl=1.98, wps=16910.7, ups=1.62, wpb=10458.3, bsz=352.1, num_updates=42400, lr=1.06399e-05, gnorm=0.86, train_wall=62, wall=0
2021-01-07 19:45:43 | INFO | train_inner | epoch 076:    425 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.995, ppl=1.99, wps=16825.6, ups=1.61, wpb=10444.2, bsz=351.7, num_updates=42500, lr=1.06274e-05, gnorm=0.864, train_wall=62, wall=0
2021-01-07 19:46:45 | INFO | train_inner | epoch 076:    525 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.996, ppl=1.99, wps=16912.3, ups=1.61, wpb=10514.8, bsz=371.4, num_updates=42600, lr=1.06149e-05, gnorm=0.84, train_wall=62, wall=0
2021-01-07 19:47:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:47:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:47:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:47:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:47:28 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.19 | nll_loss 3.681 | ppl 12.82 | bleu 23.14 | wps 4733.8 | wpb 7508.5 | bsz 272.7 | num_updates 42636 | best_bleu 23.14
2021-01-07 19:47:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:47:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:47:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 76 @ 42636 updates, score 23.14) (writing took 4.768101701512933 seconds)
2021-01-07 19:47:32 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-01-07 19:47:32 | INFO | train | epoch 076 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.293 | nll_loss 0.989 | ppl 1.99 | wps 15667.6 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 42636 | lr 1.06104e-05 | gnorm 0.852 | train_wall 346 | wall 0
2021-01-07 19:47:32 | INFO | fairseq.trainer | begin training epoch 77
2021-01-07 19:47:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:47:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:48:14 | INFO | train_inner | epoch 077:     64 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.984, ppl=1.98, wps=11579.6, ups=1.12, wpb=10301, bsz=362.2, num_updates=42700, lr=1.06025e-05, gnorm=0.867, train_wall=60, wall=0
2021-01-07 19:49:16 | INFO | train_inner | epoch 077:    164 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.978, ppl=1.97, wps=17153.8, ups=1.63, wpb=10546.4, bsz=376.5, num_updates=42800, lr=1.05901e-05, gnorm=0.836, train_wall=61, wall=0
2021-01-07 19:50:17 | INFO | train_inner | epoch 077:    264 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.993, ppl=1.99, wps=17137.5, ups=1.63, wpb=10517.6, bsz=359.5, num_updates=42900, lr=1.05777e-05, gnorm=0.86, train_wall=61, wall=0
2021-01-07 19:51:18 | INFO | train_inner | epoch 077:    364 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.984, ppl=1.98, wps=17029, ups=1.63, wpb=10469.6, bsz=368.8, num_updates=43000, lr=1.05654e-05, gnorm=0.857, train_wall=61, wall=0
2021-01-07 19:52:20 | INFO | train_inner | epoch 077:    464 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.314, nll_loss=1.004, ppl=2.01, wps=17034.3, ups=1.62, wpb=10518.3, bsz=364.5, num_updates=43100, lr=1.05531e-05, gnorm=0.851, train_wall=62, wall=0
2021-01-07 19:53:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:53:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:53:40 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.681 | ppl 12.82 | bleu 23.08 | wps 4784.9 | wpb 7508.5 | bsz 272.7 | num_updates 43197 | best_bleu 23.14
2021-01-07 19:53:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:53:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 77 @ 43197 updates, score 23.08) (writing took 2.941377392038703 seconds)
2021-01-07 19:53:43 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-01-07 19:53:43 | INFO | train | epoch 077 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.292 | nll_loss 0.989 | ppl 1.98 | wps 15870.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 43197 | lr 1.05413e-05 | gnorm 0.85 | train_wall 343 | wall 0
2021-01-07 19:53:43 | INFO | fairseq.trainer | begin training epoch 78
2021-01-07 19:53:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:53:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:53:48 | INFO | train_inner | epoch 078:      3 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.997, ppl=2, wps=11858.4, ups=1.14, wpb=10421.2, bsz=375.9, num_updates=43200, lr=1.05409e-05, gnorm=0.847, train_wall=61, wall=0
2021-01-07 19:54:49 | INFO | train_inner | epoch 078:    103 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.996, ppl=1.99, wps=17151.4, ups=1.65, wpb=10381.6, bsz=367.8, num_updates=43300, lr=1.05287e-05, gnorm=0.857, train_wall=60, wall=0
2021-01-07 19:55:51 | INFO | train_inner | epoch 078:    203 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.989, ppl=1.99, wps=16915.3, ups=1.6, wpb=10541.1, bsz=371.2, num_updates=43400, lr=1.05166e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 19:56:53 | INFO | train_inner | epoch 078:    303 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.981, ppl=1.97, wps=16876.5, ups=1.61, wpb=10506.2, bsz=367.8, num_updates=43500, lr=1.05045e-05, gnorm=0.854, train_wall=62, wall=0
2021-01-07 19:57:55 | INFO | train_inner | epoch 078:    403 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.987, ppl=1.98, wps=17004.9, ups=1.61, wpb=10573, bsz=375.5, num_updates=43600, lr=1.04925e-05, gnorm=0.835, train_wall=62, wall=0
2021-01-07 19:58:57 | INFO | train_inner | epoch 078:    503 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.994, ppl=1.99, wps=16971.5, ups=1.62, wpb=10494.6, bsz=375, num_updates=43700, lr=1.04804e-05, gnorm=0.842, train_wall=62, wall=0
2021-01-07 19:59:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 19:59:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:59:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:59:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 19:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 19:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 19:59:54 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.195 | nll_loss 3.685 | ppl 12.86 | bleu 23.08 | wps 4759.1 | wpb 7508.5 | bsz 272.7 | num_updates 43758 | best_bleu 23.14
2021-01-07 19:59:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 19:59:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:59:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 19:59:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 78 @ 43758 updates, score 23.08) (writing took 2.809611726552248 seconds)
2021-01-07 19:59:56 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-01-07 19:59:56 | INFO | train | epoch 078 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.291 | nll_loss 0.988 | ppl 1.98 | wps 15752.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 43758 | lr 1.04735e-05 | gnorm 0.847 | train_wall 346 | wall 0
2021-01-07 19:59:56 | INFO | fairseq.trainer | begin training epoch 79
2021-01-07 19:59:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 19:59:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:00:25 | INFO | train_inner | epoch 079:     42 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.983, ppl=1.98, wps=11850, ups=1.14, wpb=10363.7, bsz=353.7, num_updates=43800, lr=1.04685e-05, gnorm=0.853, train_wall=61, wall=0
2021-01-07 20:01:27 | INFO | train_inner | epoch 079:    142 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.99, ppl=1.99, wps=16801, ups=1.6, wpb=10473.3, bsz=369.6, num_updates=43900, lr=1.04565e-05, gnorm=0.85, train_wall=62, wall=0
2021-01-07 20:02:29 | INFO | train_inner | epoch 079:    242 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.982, ppl=1.98, wps=16912.5, ups=1.61, wpb=10520.9, bsz=368.5, num_updates=44000, lr=1.04447e-05, gnorm=0.836, train_wall=62, wall=0
2021-01-07 20:03:32 | INFO | train_inner | epoch 079:    342 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.983, ppl=1.98, wps=16911, ups=1.6, wpb=10548.2, bsz=383.3, num_updates=44100, lr=1.04328e-05, gnorm=0.839, train_wall=62, wall=0
2021-01-07 20:04:34 | INFO | train_inner | epoch 079:    442 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.993, ppl=1.99, wps=16745.6, ups=1.61, wpb=10413, bsz=355.9, num_updates=44200, lr=1.0421e-05, gnorm=0.855, train_wall=62, wall=0
2021-01-07 20:05:36 | INFO | train_inner | epoch 079:    542 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.998, ppl=2, wps=17093.2, ups=1.62, wpb=10557.3, bsz=377.7, num_updates=44300, lr=1.04092e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 20:05:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:05:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:05:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:05:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:05:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:05:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:05:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:05:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:05:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:05:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:06:08 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.681 | ppl 12.83 | bleu 23.07 | wps 4733.1 | wpb 7508.5 | bsz 272.7 | num_updates 44319 | best_bleu 23.14
2021-01-07 20:06:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:06:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:06:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:06:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:06:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:06:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 79 @ 44319 updates, score 23.07) (writing took 2.8614729419350624 seconds)
2021-01-07 20:06:11 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-01-07 20:06:11 | INFO | train | epoch 079 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.29 | nll_loss 0.988 | ppl 1.98 | wps 15709.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 44319 | lr 1.0407e-05 | gnorm 0.845 | train_wall 347 | wall 0
2021-01-07 20:06:11 | INFO | fairseq.trainer | begin training epoch 80
2021-01-07 20:06:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:06:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:07:03 | INFO | train_inner | epoch 080:     81 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.978, ppl=1.97, wps=11916.3, ups=1.14, wpb=10456.9, bsz=376.2, num_updates=44400, lr=1.03975e-05, gnorm=0.837, train_wall=61, wall=0
2021-01-07 20:08:06 | INFO | train_inner | epoch 080:    181 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.989, ppl=1.99, wps=16675.1, ups=1.6, wpb=10400.2, bsz=371.1, num_updates=44500, lr=1.03858e-05, gnorm=0.853, train_wall=62, wall=0
2021-01-07 20:09:08 | INFO | train_inner | epoch 080:    281 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.986, ppl=1.98, wps=17039.2, ups=1.61, wpb=10593.7, bsz=368.4, num_updates=44600, lr=1.03742e-05, gnorm=0.836, train_wall=62, wall=0
2021-01-07 20:10:10 | INFO | train_inner | epoch 080:    381 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.996, ppl=1.99, wps=16964.3, ups=1.6, wpb=10608.4, bsz=367, num_updates=44700, lr=1.03626e-05, gnorm=0.84, train_wall=62, wall=0
2021-01-07 20:11:12 | INFO | train_inner | epoch 080:    481 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.982, ppl=1.97, wps=16812, ups=1.62, wpb=10408.4, bsz=357.6, num_updates=44800, lr=1.0351e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 20:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:12:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:12:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:12:22 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.68 | ppl 12.82 | bleu 23.05 | wps 4717.3 | wpb 7508.5 | bsz 272.7 | num_updates 44880 | best_bleu 23.14
2021-01-07 20:12:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:12:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 80 @ 44880 updates, score 23.05) (writing took 2.886794302612543 seconds)
2021-01-07 20:12:25 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-01-07 20:12:25 | INFO | train | epoch 080 | symm_kl 0.375 | self_kl 0 | self_cv 0 | loss 3.289 | nll_loss 0.988 | ppl 1.98 | wps 15700.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 44880 | lr 1.03418e-05 | gnorm 0.846 | train_wall 347 | wall 0
2021-01-07 20:12:25 | INFO | fairseq.trainer | begin training epoch 81
2021-01-07 20:12:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:12:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:12:41 | INFO | train_inner | epoch 081:     20 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.293, nll_loss=0.992, ppl=1.99, wps=11774.3, ups=1.13, wpb=10396.4, bsz=377.8, num_updates=44900, lr=1.03395e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 20:13:42 | INFO | train_inner | epoch 081:    120 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.986, ppl=1.98, wps=17054.7, ups=1.63, wpb=10468.5, bsz=373.5, num_updates=45000, lr=1.0328e-05, gnorm=0.849, train_wall=61, wall=0
2021-01-07 20:14:43 | INFO | train_inner | epoch 081:    220 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.988, ppl=1.98, wps=17093.7, ups=1.63, wpb=10503.9, bsz=370.8, num_updates=45100, lr=1.03165e-05, gnorm=0.839, train_wall=61, wall=0
2021-01-07 20:15:45 | INFO | train_inner | epoch 081:    320 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.992, ppl=1.99, wps=16876.9, ups=1.62, wpb=10441.1, bsz=382.6, num_updates=45200, lr=1.03051e-05, gnorm=0.843, train_wall=62, wall=0
2021-01-07 20:16:47 | INFO | train_inner | epoch 081:    420 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.983, ppl=1.98, wps=17069.9, ups=1.61, wpb=10596.7, bsz=361.8, num_updates=45300, lr=1.02937e-05, gnorm=0.839, train_wall=62, wall=0
2021-01-07 20:17:49 | INFO | train_inner | epoch 081:    520 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.993, ppl=1.99, wps=16902.8, ups=1.61, wpb=10490.6, bsz=372.4, num_updates=45400, lr=1.02824e-05, gnorm=0.844, train_wall=62, wall=0
2021-01-07 20:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:18:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:18:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:18:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:18:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:18:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:18:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:18:35 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.19 | nll_loss 3.682 | ppl 12.83 | bleu 23.09 | wps 4786.3 | wpb 7508.5 | bsz 272.7 | num_updates 45441 | best_bleu 23.14
2021-01-07 20:18:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:18:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:18:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 81 @ 45441 updates, score 23.09) (writing took 2.8026810456067324 seconds)
2021-01-07 20:18:38 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-01-07 20:18:38 | INFO | train | epoch 081 | symm_kl 0.375 | self_kl 0 | self_cv 0 | loss 3.289 | nll_loss 0.988 | ppl 1.98 | wps 15789.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 45441 | lr 1.02777e-05 | gnorm 0.844 | train_wall 345 | wall 0
2021-01-07 20:18:38 | INFO | fairseq.trainer | begin training epoch 82
2021-01-07 20:18:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:18:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:19:17 | INFO | train_inner | epoch 082:     59 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.991, ppl=1.99, wps=11875.9, ups=1.14, wpb=10409.9, bsz=349.3, num_updates=45500, lr=1.02711e-05, gnorm=0.854, train_wall=61, wall=0
2021-01-07 20:20:19 | INFO | train_inner | epoch 082:    159 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.988, ppl=1.98, wps=16863.5, ups=1.61, wpb=10465.7, bsz=374.6, num_updates=45600, lr=1.02598e-05, gnorm=0.837, train_wall=62, wall=0
2021-01-07 20:21:21 | INFO | train_inner | epoch 082:    259 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.988, ppl=1.98, wps=16967.4, ups=1.61, wpb=10547.3, bsz=363.9, num_updates=45700, lr=1.02486e-05, gnorm=0.846, train_wall=62, wall=0
2021-01-07 20:22:24 | INFO | train_inner | epoch 082:    359 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.981, ppl=1.97, wps=17033.4, ups=1.61, wpb=10602, bsz=367.5, num_updates=45800, lr=1.02374e-05, gnorm=0.852, train_wall=62, wall=0
2021-01-07 20:23:25 | INFO | train_inner | epoch 082:    459 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.303, nll_loss=1.003, ppl=2, wps=16665.8, ups=1.62, wpb=10279.6, bsz=367.6, num_updates=45900, lr=1.02262e-05, gnorm=0.851, train_wall=61, wall=0
2021-01-07 20:24:27 | INFO | train_inner | epoch 082:    559 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.975, ppl=1.97, wps=17180.2, ups=1.63, wpb=10568.6, bsz=386.6, num_updates=46000, lr=1.02151e-05, gnorm=0.828, train_wall=61, wall=0
2021-01-07 20:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:24:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:24:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:24:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:24:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:24:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:24:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:24:48 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.192 | nll_loss 3.682 | ppl 12.83 | bleu 22.97 | wps 4709.2 | wpb 7508.5 | bsz 272.7 | num_updates 46002 | best_bleu 23.14
2021-01-07 20:24:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:24:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:24:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:24:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 82 @ 46002 updates, score 22.97) (writing took 2.854831723496318 seconds)
2021-01-07 20:24:51 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-01-07 20:24:51 | INFO | train | epoch 082 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.287 | nll_loss 0.988 | ppl 1.98 | wps 15747.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 46002 | lr 1.02149e-05 | gnorm 0.844 | train_wall 346 | wall 0
2021-01-07 20:24:51 | INFO | fairseq.trainer | begin training epoch 83
2021-01-07 20:24:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:24:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:25:54 | INFO | train_inner | epoch 083:     98 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.991, ppl=1.99, wps=11887, ups=1.14, wpb=10394, bsz=358.3, num_updates=46100, lr=1.0204e-05, gnorm=0.852, train_wall=61, wall=0
2021-01-07 20:26:56 | INFO | train_inner | epoch 083:    198 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.999, ppl=2, wps=16780.4, ups=1.62, wpb=10378.9, bsz=367.9, num_updates=46200, lr=1.01929e-05, gnorm=0.86, train_wall=62, wall=0
2021-01-07 20:27:58 | INFO | train_inner | epoch 083:    298 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.981, ppl=1.97, wps=17025.2, ups=1.61, wpb=10566.5, bsz=366, num_updates=46300, lr=1.01819e-05, gnorm=0.833, train_wall=62, wall=0
2021-01-07 20:29:00 | INFO | train_inner | epoch 083:    398 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.988, ppl=1.98, wps=16883.2, ups=1.62, wpb=10419, bsz=377.3, num_updates=46400, lr=1.0171e-05, gnorm=0.847, train_wall=62, wall=0
2021-01-07 20:30:02 | INFO | train_inner | epoch 083:    498 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.987, ppl=1.98, wps=17042.3, ups=1.61, wpb=10569.6, bsz=385, num_updates=46500, lr=1.016e-05, gnorm=0.841, train_wall=62, wall=0
2021-01-07 20:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:30:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:30:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:30:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:30:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:30:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:30:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:31:01 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.191 | nll_loss 3.683 | ppl 12.84 | bleu 23.03 | wps 4763 | wpb 7508.5 | bsz 272.7 | num_updates 46563 | best_bleu 23.14
2021-01-07 20:31:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:31:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:31:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:31:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:31:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:31:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 83 @ 46563 updates, score 23.03) (writing took 2.7968543097376823 seconds)
2021-01-07 20:31:04 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-01-07 20:31:04 | INFO | train | epoch 083 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.286 | nll_loss 0.987 | ppl 1.98 | wps 15780.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 46563 | lr 1.01531e-05 | gnorm 0.848 | train_wall 345 | wall 0
2021-01-07 20:31:04 | INFO | fairseq.trainer | begin training epoch 84
2021-01-07 20:31:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:31:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:31:30 | INFO | train_inner | epoch 084:     37 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.973, ppl=1.96, wps=12017.8, ups=1.14, wpb=10551.7, bsz=368.1, num_updates=46600, lr=1.01491e-05, gnorm=0.845, train_wall=61, wall=0
2021-01-07 20:32:31 | INFO | train_inner | epoch 084:    137 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.988, ppl=1.98, wps=16912.8, ups=1.63, wpb=10394.7, bsz=362.2, num_updates=46700, lr=1.01382e-05, gnorm=0.849, train_wall=61, wall=0
2021-01-07 20:33:33 | INFO | train_inner | epoch 084:    237 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.992, ppl=1.99, wps=17043.7, ups=1.62, wpb=10508.4, bsz=361.6, num_updates=46800, lr=1.01274e-05, gnorm=0.843, train_wall=61, wall=0
2021-01-07 20:34:35 | INFO | train_inner | epoch 084:    337 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.982, ppl=1.97, wps=16842.4, ups=1.61, wpb=10450.1, bsz=373.6, num_updates=46900, lr=1.01166e-05, gnorm=0.834, train_wall=62, wall=0
2021-01-07 20:35:37 | INFO | train_inner | epoch 084:    437 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.987, ppl=1.98, wps=16855.6, ups=1.6, wpb=10517.8, bsz=380.4, num_updates=47000, lr=1.01058e-05, gnorm=0.837, train_wall=62, wall=0
2021-01-07 20:36:39 | INFO | train_inner | epoch 084:    537 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.98, ppl=1.97, wps=17111.5, ups=1.62, wpb=10554.9, bsz=367, num_updates=47100, lr=1.00951e-05, gnorm=0.838, train_wall=61, wall=0
2021-01-07 20:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:36:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:36:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:36:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:36:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:36:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:36:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:36:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:36:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:37:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:37:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:37:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:37:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:37:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:37:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:37:14 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.193 | nll_loss 3.686 | ppl 12.87 | bleu 22.98 | wps 4721.9 | wpb 7508.5 | bsz 272.7 | num_updates 47124 | best_bleu 23.14
2021-01-07 20:37:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:37:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:37:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:37:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:37:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:37:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 84 @ 47124 updates, score 22.98) (writing took 2.8282295875251293 seconds)
2021-01-07 20:37:17 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2021-01-07 20:37:17 | INFO | train | epoch 084 | symm_kl 0.373 | self_kl 0 | self_cv 0 | loss 3.285 | nll_loss 0.987 | ppl 1.98 | wps 15772.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 47124 | lr 1.00925e-05 | gnorm 0.84 | train_wall 345 | wall 0
2021-01-07 20:37:17 | INFO | fairseq.trainer | begin training epoch 85
2021-01-07 20:37:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:37:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:38:06 | INFO | train_inner | epoch 085:     76 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.978, ppl=1.97, wps=12055.1, ups=1.15, wpb=10521.4, bsz=388.8, num_updates=47200, lr=1.00844e-05, gnorm=0.825, train_wall=61, wall=0
2021-01-07 20:39:08 | INFO | train_inner | epoch 085:    176 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.981, ppl=1.97, wps=16921.5, ups=1.62, wpb=10424.7, bsz=364.1, num_updates=47300, lr=1.00737e-05, gnorm=0.858, train_wall=61, wall=0
2021-01-07 20:40:10 | INFO | train_inner | epoch 085:    276 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.98, ppl=1.97, wps=17186.4, ups=1.62, wpb=10641.3, bsz=372.8, num_updates=47400, lr=1.00631e-05, gnorm=0.839, train_wall=62, wall=0
2021-01-07 20:41:11 | INFO | train_inner | epoch 085:    376 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.999, ppl=2, wps=16939.7, ups=1.63, wpb=10412, bsz=369.3, num_updates=47500, lr=1.00525e-05, gnorm=0.84, train_wall=61, wall=0
2021-01-07 20:42:13 | INFO | train_inner | epoch 085:    476 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.308, nll_loss=1.002, ppl=2, wps=16957.8, ups=1.62, wpb=10454.5, bsz=352.3, num_updates=47600, lr=1.00419e-05, gnorm=0.855, train_wall=61, wall=0
2021-01-07 20:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:43:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:43:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:43:25 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.193 | nll_loss 3.682 | ppl 12.84 | bleu 23.11 | wps 4709.7 | wpb 7508.5 | bsz 272.7 | num_updates 47685 | best_bleu 23.14
2021-01-07 20:43:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 85 @ 47685 updates, score 23.11) (writing took 2.8122371658682823 seconds)
2021-01-07 20:43:28 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2021-01-07 20:43:28 | INFO | train | epoch 085 | symm_kl 0.373 | self_kl 0 | self_cv 0 | loss 3.285 | nll_loss 0.987 | ppl 1.98 | wps 15846.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 47685 | lr 1.0033e-05 | gnorm 0.844 | train_wall 343 | wall 0
2021-01-07 20:43:28 | INFO | fairseq.trainer | begin training epoch 86
2021-01-07 20:43:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:43:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:43:40 | INFO | train_inner | epoch 086:     15 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.994, ppl=1.99, wps=11885.3, ups=1.14, wpb=10414.5, bsz=365.8, num_updates=47700, lr=1.00314e-05, gnorm=0.854, train_wall=61, wall=0
2021-01-07 20:44:42 | INFO | train_inner | epoch 086:    115 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.976, ppl=1.97, wps=17172.3, ups=1.63, wpb=10504.5, bsz=369.2, num_updates=47800, lr=1.00209e-05, gnorm=0.835, train_wall=61, wall=0
2021-01-07 20:45:43 | INFO | train_inner | epoch 086:    215 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.967, ppl=1.95, wps=17146.1, ups=1.62, wpb=10571.3, bsz=370, num_updates=47900, lr=1.00104e-05, gnorm=0.846, train_wall=61, wall=0
2021-01-07 20:46:45 | INFO | train_inner | epoch 086:    315 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.304, nll_loss=1.003, ppl=2, wps=16790.6, ups=1.63, wpb=10311, bsz=359.4, num_updates=48000, lr=1e-05, gnorm=0.847, train_wall=61, wall=0
2021-01-07 20:47:47 | INFO | train_inner | epoch 086:    415 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.299, nll_loss=1.001, ppl=2, wps=17037.5, ups=1.61, wpb=10556, bsz=363.5, num_updates=48100, lr=9.9896e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-07 20:48:49 | INFO | train_inner | epoch 086:    515 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.98, ppl=1.97, wps=16990.9, ups=1.61, wpb=10539.6, bsz=394.6, num_updates=48200, lr=9.97923e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-07 20:49:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:49:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:49:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:49:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:49:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:49:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:49:38 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.191 | nll_loss 3.681 | ppl 12.83 | bleu 23 | wps 4695.2 | wpb 7508.5 | bsz 272.7 | num_updates 48246 | best_bleu 23.14
2021-01-07 20:49:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:49:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:49:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:49:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 86 @ 48246 updates, score 23.0) (writing took 2.8539282958954573 seconds)
2021-01-07 20:49:40 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2021-01-07 20:49:40 | INFO | train | epoch 086 | symm_kl 0.372 | self_kl 0 | self_cv 0 | loss 3.283 | nll_loss 0.986 | ppl 1.98 | wps 15791.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 48246 | lr 9.97447e-06 | gnorm 0.843 | train_wall 345 | wall 0
2021-01-07 20:49:40 | INFO | fairseq.trainer | begin training epoch 87
2021-01-07 20:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:50:16 | INFO | train_inner | epoch 087:     54 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.972, ppl=1.96, wps=11999.2, ups=1.14, wpb=10484.6, bsz=357, num_updates=48300, lr=9.9689e-06, gnorm=0.846, train_wall=61, wall=0
2021-01-07 20:51:18 | INFO | train_inner | epoch 087:    154 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.302, nll_loss=1.002, ppl=2, wps=16584.5, ups=1.62, wpb=10217.7, bsz=374, num_updates=48400, lr=9.95859e-06, gnorm=0.861, train_wall=61, wall=0
2021-01-07 20:52:20 | INFO | train_inner | epoch 087:    254 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.968, ppl=1.96, wps=16915.7, ups=1.59, wpb=10607.1, bsz=361.4, num_updates=48500, lr=9.94832e-06, gnorm=0.828, train_wall=63, wall=0
2021-01-07 20:53:23 | INFO | train_inner | epoch 087:    354 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.989, ppl=1.98, wps=16916.5, ups=1.6, wpb=10544.2, bsz=364.4, num_updates=48600, lr=9.93808e-06, gnorm=0.855, train_wall=62, wall=0
2021-01-07 20:54:25 | INFO | train_inner | epoch 087:    454 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.988, ppl=1.98, wps=16875.5, ups=1.6, wpb=10541.1, bsz=376.4, num_updates=48700, lr=9.92787e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-07 20:55:27 | INFO | train_inner | epoch 087:    554 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.994, ppl=1.99, wps=16945.1, ups=1.61, wpb=10528.8, bsz=381.8, num_updates=48800, lr=9.91769e-06, gnorm=0.837, train_wall=62, wall=0
2021-01-07 20:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 20:55:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:55:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:55:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 20:55:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 20:55:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 20:55:52 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.191 | nll_loss 3.682 | ppl 12.84 | bleu 23.04 | wps 4705.9 | wpb 7508.5 | bsz 272.7 | num_updates 48807 | best_bleu 23.14
2021-01-07 20:55:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 20:55:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:55:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:55:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 87 @ 48807 updates, score 23.04) (writing took 2.830680003389716 seconds)
2021-01-07 20:55:55 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2021-01-07 20:55:55 | INFO | train | epoch 087 | symm_kl 0.372 | self_kl 0 | self_cv 0 | loss 3.283 | nll_loss 0.986 | ppl 1.98 | wps 15711.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 48807 | lr 9.91698e-06 | gnorm 0.844 | train_wall 347 | wall 0
2021-01-07 20:55:55 | INFO | fairseq.trainer | begin training epoch 88
2021-01-07 20:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 20:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 20:56:55 | INFO | train_inner | epoch 088:     93 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.988, ppl=1.98, wps=11814.8, ups=1.14, wpb=10372.7, bsz=363.2, num_updates=48900, lr=9.90755e-06, gnorm=0.847, train_wall=61, wall=0
2021-01-07 20:57:57 | INFO | train_inner | epoch 088:    193 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.976, ppl=1.97, wps=16724.8, ups=1.61, wpb=10376.6, bsz=358.2, num_updates=49000, lr=9.89743e-06, gnorm=0.843, train_wall=62, wall=0
2021-01-07 20:59:00 | INFO | train_inner | epoch 088:    293 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.978, ppl=1.97, wps=16885.4, ups=1.6, wpb=10577.9, bsz=398.9, num_updates=49100, lr=9.88735e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-07 21:00:02 | INFO | train_inner | epoch 088:    393 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.303, nll_loss=0.995, ppl=1.99, wps=16772.2, ups=1.61, wpb=10445.9, bsz=344.1, num_updates=49200, lr=9.8773e-06, gnorm=0.864, train_wall=62, wall=0
2021-01-07 21:01:04 | INFO | train_inner | epoch 088:    493 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.993, ppl=1.99, wps=16830.7, ups=1.6, wpb=10502.6, bsz=377.1, num_updates=49300, lr=9.86727e-06, gnorm=0.842, train_wall=62, wall=0
2021-01-07 21:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:01:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:01:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:01:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:01:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:01:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:01:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:01:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:01:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:01:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:02:07 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.19 | nll_loss 3.681 | ppl 12.83 | bleu 22.97 | wps 4715.2 | wpb 7508.5 | bsz 272.7 | num_updates 49368 | best_bleu 23.14
2021-01-07 21:02:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:02:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:02:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:02:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:02:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:02:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 88 @ 49368 updates, score 22.97) (writing took 2.815053215250373 seconds)
2021-01-07 21:02:10 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2021-01-07 21:02:10 | INFO | train | epoch 088 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.282 | nll_loss 0.986 | ppl 1.98 | wps 15675.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 49368 | lr 9.86048e-06 | gnorm 0.84 | train_wall 348 | wall 0
2021-01-07 21:02:10 | INFO | fairseq.trainer | begin training epoch 89
2021-01-07 21:02:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:02:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:02:33 | INFO | train_inner | epoch 089:     32 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.987, ppl=1.98, wps=11997.8, ups=1.13, wpb=10574.7, bsz=380.6, num_updates=49400, lr=9.85728e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-07 21:03:34 | INFO | train_inner | epoch 089:    132 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.978, ppl=1.97, wps=16845.5, ups=1.63, wpb=10356.2, bsz=364.6, num_updates=49500, lr=9.84732e-06, gnorm=0.85, train_wall=61, wall=0
2021-01-07 21:04:36 | INFO | train_inner | epoch 089:    232 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.976, ppl=1.97, wps=16914.9, ups=1.61, wpb=10526.3, bsz=365.7, num_updates=49600, lr=9.83739e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-07 21:05:39 | INFO | train_inner | epoch 089:    332 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.992, ppl=1.99, wps=16929, ups=1.61, wpb=10531.2, bsz=379.8, num_updates=49700, lr=9.82749e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-07 21:06:40 | INFO | train_inner | epoch 089:    432 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.987, ppl=1.98, wps=17038.2, ups=1.61, wpb=10552.4, bsz=357.4, num_updates=49800, lr=9.81761e-06, gnorm=0.849, train_wall=62, wall=0
2021-01-07 21:07:42 | INFO | train_inner | epoch 089:    532 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.297, nll_loss=0.999, ppl=2, wps=16833.7, ups=1.61, wpb=10439.3, bsz=363.8, num_updates=49900, lr=9.80777e-06, gnorm=0.85, train_wall=62, wall=0
2021-01-07 21:08:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:08:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:08:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:08:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:08:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:08:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:08:21 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.68 | ppl 12.81 | bleu 23.1 | wps 4738.9 | wpb 7508.5 | bsz 272.7 | num_updates 49929 | best_bleu 23.14
2021-01-07 21:08:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:08:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:08:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 89 @ 49929 updates, score 23.1) (writing took 2.8099292125552893 seconds)
2021-01-07 21:08:24 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2021-01-07 21:08:24 | INFO | train | epoch 089 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.281 | nll_loss 0.986 | ppl 1.98 | wps 15739.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 49929 | lr 9.80492e-06 | gnorm 0.841 | train_wall 346 | wall 0
2021-01-07 21:08:24 | INFO | fairseq.trainer | begin training epoch 90
2021-01-07 21:08:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:08:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:09:10 | INFO | train_inner | epoch 090:     71 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.976, ppl=1.97, wps=11954.4, ups=1.14, wpb=10456.7, bsz=367.8, num_updates=50000, lr=9.79796e-06, gnorm=0.851, train_wall=61, wall=0
2021-01-07 21:10:12 | INFO | train_inner | epoch 090:    171 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.977, ppl=1.97, wps=16933.1, ups=1.61, wpb=10492.2, bsz=353.5, num_updates=50100, lr=9.78818e-06, gnorm=0.848, train_wall=62, wall=0
2021-01-07 21:11:13 | INFO | train_inner | epoch 090:    271 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.985, ppl=1.98, wps=17191.9, ups=1.63, wpb=10537.9, bsz=386.1, num_updates=50200, lr=9.77842e-06, gnorm=0.836, train_wall=61, wall=0
2021-01-07 21:12:15 | INFO | train_inner | epoch 090:    371 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.998, ppl=2, wps=16914.9, ups=1.62, wpb=10419.6, bsz=366.2, num_updates=50300, lr=9.7687e-06, gnorm=0.851, train_wall=61, wall=0
2021-01-07 21:13:17 | INFO | train_inner | epoch 090:    471 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.979, ppl=1.97, wps=16834.5, ups=1.61, wpb=10434.1, bsz=370.4, num_updates=50400, lr=9.759e-06, gnorm=0.843, train_wall=62, wall=0
2021-01-07 21:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:14:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:14:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:14:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:14:33 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.187 | nll_loss 3.679 | ppl 12.81 | bleu 23 | wps 4608 | wpb 7508.5 | bsz 272.7 | num_updates 50490 | best_bleu 23.14
2021-01-07 21:14:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:14:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 90 @ 50490 updates, score 23.0) (writing took 2.8603631108999252 seconds)
2021-01-07 21:14:36 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2021-01-07 21:14:36 | INFO | train | epoch 090 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.281 | nll_loss 0.985 | ppl 1.98 | wps 15781.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 50490 | lr 9.7503e-06 | gnorm 0.844 | train_wall 345 | wall 0
2021-01-07 21:14:36 | INFO | fairseq.trainer | begin training epoch 91
2021-01-07 21:14:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:14:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:14:46 | INFO | train_inner | epoch 091:     10 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.99, ppl=1.99, wps=11833.1, ups=1.13, wpb=10516.1, bsz=383.2, num_updates=50500, lr=9.74933e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-07 21:15:47 | INFO | train_inner | epoch 091:    110 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.988, ppl=1.98, wps=17094.1, ups=1.64, wpb=10453.1, bsz=379.8, num_updates=50600, lr=9.7397e-06, gnorm=0.84, train_wall=61, wall=0
2021-01-07 21:16:49 | INFO | train_inner | epoch 091:    210 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.985, ppl=1.98, wps=16848.7, ups=1.61, wpb=10441.5, bsz=368.3, num_updates=50700, lr=9.73009e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-07 21:17:50 | INFO | train_inner | epoch 091:    310 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.993, ppl=1.99, wps=16978.3, ups=1.62, wpb=10470.4, bsz=371.4, num_updates=50800, lr=9.7205e-06, gnorm=0.849, train_wall=61, wall=0
2021-01-07 21:18:53 | INFO | train_inner | epoch 091:    410 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.966, ppl=1.95, wps=17062.7, ups=1.6, wpb=10633.1, bsz=363.4, num_updates=50900, lr=9.71095e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-07 21:19:55 | INFO | train_inner | epoch 091:    510 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.985, ppl=1.98, wps=17022.6, ups=1.62, wpb=10520, bsz=367, num_updates=51000, lr=9.70143e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-07 21:20:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:20:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:20:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:20:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:20:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:20:46 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.681 | ppl 12.83 | bleu 22.94 | wps 4743 | wpb 7508.5 | bsz 272.7 | num_updates 51051 | best_bleu 23.14
2021-01-07 21:20:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:20:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:20:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:20:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 91 @ 51051 updates, score 22.94) (writing took 2.814770694822073 seconds)
2021-01-07 21:20:49 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2021-01-07 21:20:49 | INFO | train | epoch 091 | symm_kl 0.37 | self_kl 0 | self_cv 0 | loss 3.279 | nll_loss 0.984 | ppl 1.98 | wps 15782 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 51051 | lr 9.69658e-06 | gnorm 0.841 | train_wall 345 | wall 0
2021-01-07 21:20:49 | INFO | fairseq.trainer | begin training epoch 92
2021-01-07 21:20:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:20:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:21:22 | INFO | train_inner | epoch 092:     49 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.989, ppl=1.98, wps=11866.3, ups=1.15, wpb=10331.8, bsz=364.4, num_updates=51100, lr=9.69193e-06, gnorm=0.847, train_wall=60, wall=0
2021-01-07 21:22:24 | INFO | train_inner | epoch 092:    149 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.977, ppl=1.97, wps=17043.5, ups=1.62, wpb=10546.3, bsz=378.9, num_updates=51200, lr=9.68246e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-07 21:23:25 | INFO | train_inner | epoch 092:    249 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.995, ppl=1.99, wps=16916.3, ups=1.62, wpb=10432.2, bsz=347.2, num_updates=51300, lr=9.67302e-06, gnorm=0.854, train_wall=61, wall=0
2021-01-07 21:24:27 | INFO | train_inner | epoch 092:    349 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.983, ppl=1.98, wps=17034, ups=1.63, wpb=10466.6, bsz=372.2, num_updates=51400, lr=9.6636e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-07 21:25:28 | INFO | train_inner | epoch 092:    449 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.978, ppl=1.97, wps=17168.3, ups=1.62, wpb=10588, bsz=382.6, num_updates=51500, lr=9.65422e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-07 21:26:30 | INFO | train_inner | epoch 092:    549 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.99, ppl=1.99, wps=16950.7, ups=1.62, wpb=10467.8, bsz=376.6, num_updates=51600, lr=9.64486e-06, gnorm=0.842, train_wall=62, wall=0
2021-01-07 21:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:26:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:26:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:26:58 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.681 | ppl 12.83 | bleu 22.96 | wps 4689.8 | wpb 7508.5 | bsz 272.7 | num_updates 51612 | best_bleu 23.14
2021-01-07 21:26:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:26:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:27:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:27:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 92 @ 51612 updates, score 22.96) (writing took 2.873376766219735 seconds)
2021-01-07 21:27:01 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2021-01-07 21:27:01 | INFO | train | epoch 092 | symm_kl 0.37 | self_kl 0 | self_cv 0 | loss 3.278 | nll_loss 0.984 | ppl 1.98 | wps 15809.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 51612 | lr 9.64374e-06 | gnorm 0.836 | train_wall 344 | wall 0
2021-01-07 21:27:01 | INFO | fairseq.trainer | begin training epoch 93
2021-01-07 21:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:27:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:27:57 | INFO | train_inner | epoch 093:     88 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.98, ppl=1.97, wps=11913.2, ups=1.15, wpb=10401.8, bsz=371.8, num_updates=51700, lr=9.63552e-06, gnorm=0.841, train_wall=60, wall=0
2021-01-07 21:28:59 | INFO | train_inner | epoch 093:    188 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.972, ppl=1.96, wps=16973.7, ups=1.63, wpb=10424.8, bsz=366.7, num_updates=51800, lr=9.62622e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-07 21:30:00 | INFO | train_inner | epoch 093:    288 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.99, ppl=1.99, wps=16881, ups=1.63, wpb=10337.8, bsz=370.7, num_updates=51900, lr=9.61694e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-07 21:31:02 | INFO | train_inner | epoch 093:    388 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.302, nll_loss=0.998, ppl=2, wps=17165.3, ups=1.63, wpb=10556.2, bsz=357.7, num_updates=52000, lr=9.60769e-06, gnorm=0.866, train_wall=61, wall=0
2021-01-07 21:32:03 | INFO | train_inner | epoch 093:    488 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.975, ppl=1.97, wps=17339.7, ups=1.63, wpb=10641.7, bsz=366, num_updates=52100, lr=9.59846e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-07 21:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:32:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:32:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:32:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:32:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:32:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:32:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:32:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:32:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:32:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:33:08 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.68 | ppl 12.82 | bleu 23.05 | wps 4739.3 | wpb 7508.5 | bsz 272.7 | num_updates 52173 | best_bleu 23.14
2021-01-07 21:33:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:33:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:33:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 93 @ 52173 updates, score 23.05) (writing took 2.8227745201438665 seconds)
2021-01-07 21:33:11 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2021-01-07 21:33:11 | INFO | train | epoch 093 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.277 | nll_loss 0.984 | ppl 1.98 | wps 15888.2 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 52173 | lr 9.59175e-06 | gnorm 0.838 | train_wall 343 | wall 0
2021-01-07 21:33:11 | INFO | fairseq.trainer | begin training epoch 94
2021-01-07 21:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:33:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:33:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:33:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:33:31 | INFO | train_inner | epoch 094:     27 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.978, ppl=1.97, wps=12007.4, ups=1.14, wpb=10523, bsz=383, num_updates=52200, lr=9.58927e-06, gnorm=0.836, train_wall=61, wall=0
2021-01-07 21:34:32 | INFO | train_inner | epoch 094:    127 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.975, ppl=1.97, wps=17064.3, ups=1.63, wpb=10446.3, bsz=361.7, num_updates=52300, lr=9.58009e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-07 21:35:33 | INFO | train_inner | epoch 094:    227 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.987, ppl=1.98, wps=17314.5, ups=1.62, wpb=10673, bsz=368.2, num_updates=52400, lr=9.57095e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-07 21:36:35 | INFO | train_inner | epoch 094:    327 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.983, ppl=1.98, wps=17091.5, ups=1.63, wpb=10472.4, bsz=371.3, num_updates=52500, lr=9.56183e-06, gnorm=0.857, train_wall=61, wall=0
2021-01-07 21:37:36 | INFO | train_inner | epoch 094:    427 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.308, nll_loss=1.008, ppl=2.01, wps=16922.7, ups=1.63, wpb=10370.7, bsz=349.8, num_updates=52600, lr=9.55274e-06, gnorm=0.851, train_wall=61, wall=0
2021-01-07 21:38:38 | INFO | train_inner | epoch 094:    527 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.983, ppl=1.98, wps=16898.8, ups=1.62, wpb=10439.9, bsz=385.8, num_updates=52700, lr=9.54367e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-07 21:38:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:39:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:39:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:39:18 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.68 | ppl 12.82 | bleu 23.09 | wps 4955 | wpb 7508.5 | bsz 272.7 | num_updates 52734 | best_bleu 23.14
2021-01-07 21:39:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:39:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 94 @ 52734 updates, score 23.09) (writing took 2.7951311208307743 seconds)
2021-01-07 21:39:21 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2021-01-07 21:39:21 | INFO | train | epoch 094 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.277 | nll_loss 0.984 | ppl 1.98 | wps 15892 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 52734 | lr 9.54059e-06 | gnorm 0.841 | train_wall 343 | wall 0
2021-01-07 21:39:21 | INFO | fairseq.trainer | begin training epoch 95
2021-01-07 21:39:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:39:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:39:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:39:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:40:05 | INFO | train_inner | epoch 095:     66 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.973, ppl=1.96, wps=12069.3, ups=1.15, wpb=10488.1, bsz=372.7, num_updates=52800, lr=9.53463e-06, gnorm=0.845, train_wall=61, wall=0
2021-01-07 21:41:06 | INFO | train_inner | epoch 095:    166 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.993, ppl=1.99, wps=16796.9, ups=1.62, wpb=10385.3, bsz=367.7, num_updates=52900, lr=9.52561e-06, gnorm=0.844, train_wall=62, wall=0
2021-01-07 21:42:08 | INFO | train_inner | epoch 095:    266 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.974, ppl=1.96, wps=16890, ups=1.62, wpb=10440.6, bsz=369.8, num_updates=53000, lr=9.51662e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-07 21:43:10 | INFO | train_inner | epoch 095:    366 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.983, ppl=1.98, wps=17057.5, ups=1.62, wpb=10545.6, bsz=362, num_updates=53100, lr=9.50765e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-07 21:44:13 | INFO | train_inner | epoch 095:    466 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.978, ppl=1.97, wps=16899.2, ups=1.59, wpb=10602.8, bsz=385.2, num_updates=53200, lr=9.49871e-06, gnorm=0.826, train_wall=63, wall=0
2021-01-07 21:45:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:45:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:45:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:45:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:45:32 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.679 | ppl 12.81 | bleu 23.13 | wps 4692.8 | wpb 7508.5 | bsz 272.7 | num_updates 53295 | best_bleu 23.14
2021-01-07 21:45:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:45:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 95 @ 53295 updates, score 23.13) (writing took 2.827215451747179 seconds)
2021-01-07 21:45:35 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2021-01-07 21:45:35 | INFO | train | epoch 095 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.276 | nll_loss 0.983 | ppl 1.98 | wps 15741.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 53295 | lr 9.49024e-06 | gnorm 0.838 | train_wall 346 | wall 0
2021-01-07 21:45:35 | INFO | fairseq.trainer | begin training epoch 96
2021-01-07 21:45:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:45:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:45:41 | INFO | train_inner | epoch 096:      5 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.992, ppl=1.99, wps=11814.2, ups=1.13, wpb=10427, bsz=364.4, num_updates=53300, lr=9.4898e-06, gnorm=0.85, train_wall=61, wall=0
2021-01-07 21:46:43 | INFO | train_inner | epoch 096:    105 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.976, ppl=1.97, wps=17322.2, ups=1.63, wpb=10638.5, bsz=378.6, num_updates=53400, lr=9.48091e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-07 21:47:45 | INFO | train_inner | epoch 096:    205 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.979, ppl=1.97, wps=16942.4, ups=1.61, wpb=10514.3, bsz=376.9, num_updates=53500, lr=9.47204e-06, gnorm=0.843, train_wall=62, wall=0
2021-01-07 21:48:47 | INFO | train_inner | epoch 096:    305 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.982, ppl=1.97, wps=16717.9, ups=1.61, wpb=10380.1, bsz=370.6, num_updates=53600, lr=9.4632e-06, gnorm=0.848, train_wall=62, wall=0
2021-01-07 21:49:48 | INFO | train_inner | epoch 096:    405 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.301, nll_loss=1.002, ppl=2, wps=16962.1, ups=1.62, wpb=10452.4, bsz=369.2, num_updates=53700, lr=9.45439e-06, gnorm=0.847, train_wall=61, wall=0
2021-01-07 21:50:51 | INFO | train_inner | epoch 096:    505 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.977, ppl=1.97, wps=16780.8, ups=1.6, wpb=10479.3, bsz=350.8, num_updates=53800, lr=9.4456e-06, gnorm=0.845, train_wall=62, wall=0
2021-01-07 21:51:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:51:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:51:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:51:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:51:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:51:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:51:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:51:46 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.681 | ppl 12.82 | bleu 23.01 | wps 4730.7 | wpb 7508.5 | bsz 272.7 | num_updates 53856 | best_bleu 23.14
2021-01-07 21:51:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:51:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 96 @ 53856 updates, score 23.01) (writing took 2.8254268895834684 seconds)
2021-01-07 21:51:49 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2021-01-07 21:51:49 | INFO | train | epoch 096 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.275 | nll_loss 0.983 | ppl 1.98 | wps 15728.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 53856 | lr 9.44069e-06 | gnorm 0.841 | train_wall 346 | wall 0
2021-01-07 21:51:49 | INFO | fairseq.trainer | begin training epoch 97
2021-01-07 21:51:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:51:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:52:18 | INFO | train_inner | epoch 097:     44 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.99, ppl=1.99, wps=11944.7, ups=1.14, wpb=10478.1, bsz=383.6, num_updates=53900, lr=9.43683e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-07 21:53:20 | INFO | train_inner | epoch 097:    144 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.987, ppl=1.98, wps=16998.7, ups=1.62, wpb=10521, bsz=379.8, num_updates=54000, lr=9.42809e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-07 21:54:23 | INFO | train_inner | epoch 097:    244 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.988, ppl=1.98, wps=16703.8, ups=1.61, wpb=10389.1, bsz=350, num_updates=54100, lr=9.41937e-06, gnorm=0.852, train_wall=62, wall=0
2021-01-07 21:55:24 | INFO | train_inner | epoch 097:    344 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.976, ppl=1.97, wps=17172.7, ups=1.62, wpb=10621.6, bsz=381.2, num_updates=54200, lr=9.41068e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-07 21:56:26 | INFO | train_inner | epoch 097:    444 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.98, ppl=1.97, wps=16923.1, ups=1.62, wpb=10444.2, bsz=366.3, num_updates=54300, lr=9.40201e-06, gnorm=0.847, train_wall=62, wall=0
2021-01-07 21:57:29 | INFO | train_inner | epoch 097:    544 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.972, ppl=1.96, wps=16761.8, ups=1.6, wpb=10459.4, bsz=363.5, num_updates=54400, lr=9.39336e-06, gnorm=0.84, train_wall=62, wall=0
2021-01-07 21:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 21:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:57:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:57:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:57:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 21:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 21:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 21:58:00 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.189 | nll_loss 3.682 | ppl 12.83 | bleu 22.99 | wps 4666.2 | wpb 7508.5 | bsz 272.7 | num_updates 54417 | best_bleu 23.14
2021-01-07 21:58:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 21:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:58:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:58:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:58:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 97 @ 54417 updates, score 22.99) (writing took 2.8729314524680376 seconds)
2021-01-07 21:58:03 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2021-01-07 21:58:03 | INFO | train | epoch 097 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.275 | nll_loss 0.983 | ppl 1.98 | wps 15727.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 54417 | lr 9.3919e-06 | gnorm 0.84 | train_wall 346 | wall 0
2021-01-07 21:58:03 | INFO | fairseq.trainer | begin training epoch 98
2021-01-07 21:58:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 21:58:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 21:58:56 | INFO | train_inner | epoch 098:     83 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.974, ppl=1.96, wps=11828.8, ups=1.14, wpb=10400.4, bsz=371, num_updates=54500, lr=9.38474e-06, gnorm=0.842, train_wall=61, wall=0
2021-01-07 21:59:59 | INFO | train_inner | epoch 098:    183 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.984, ppl=1.98, wps=16831.4, ups=1.61, wpb=10448.3, bsz=379.5, num_updates=54600, lr=9.37614e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-07 22:01:00 | INFO | train_inner | epoch 098:    283 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.994, ppl=1.99, wps=16977.8, ups=1.62, wpb=10451.5, bsz=370.4, num_updates=54700, lr=9.36757e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-07 22:02:02 | INFO | train_inner | epoch 098:    383 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.981, ppl=1.97, wps=16909.1, ups=1.61, wpb=10491.7, bsz=365.6, num_updates=54800, lr=9.35902e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-07 22:03:04 | INFO | train_inner | epoch 098:    483 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.978, ppl=1.97, wps=17049.1, ups=1.61, wpb=10560, bsz=363.8, num_updates=54900, lr=9.35049e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-07 22:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:03:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:03:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:03:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:03:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:03:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:03:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:03:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:03:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:03:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:04:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:04:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:04:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:04:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:04:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:04:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:04:13 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.187 | nll_loss 3.679 | ppl 12.81 | bleu 22.93 | wps 4733.8 | wpb 7508.5 | bsz 272.7 | num_updates 54978 | best_bleu 23.14
2021-01-07 22:04:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:04:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:04:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:04:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:04:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:04:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 98 @ 54978 updates, score 22.93) (writing took 2.7949800342321396 seconds)
2021-01-07 22:04:15 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2021-01-07 22:04:15 | INFO | train | epoch 098 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.274 | nll_loss 0.983 | ppl 1.98 | wps 15773.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 54978 | lr 9.34386e-06 | gnorm 0.838 | train_wall 345 | wall 0
2021-01-07 22:04:15 | INFO | fairseq.trainer | begin training epoch 99
2021-01-07 22:04:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:04:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:04:32 | INFO | train_inner | epoch 099:     22 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.984, ppl=1.98, wps=11876.4, ups=1.14, wpb=10437.8, bsz=366.3, num_updates=55000, lr=9.34199e-06, gnorm=0.841, train_wall=61, wall=0
2021-01-07 22:05:33 | INFO | train_inner | epoch 099:    122 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.981, ppl=1.97, wps=17068.1, ups=1.63, wpb=10493.9, bsz=373.4, num_updates=55100, lr=9.33351e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-07 22:06:35 | INFO | train_inner | epoch 099:    222 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.975, ppl=1.97, wps=17090.5, ups=1.62, wpb=10520.9, bsz=373.9, num_updates=55200, lr=9.32505e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-07 22:07:37 | INFO | train_inner | epoch 099:    322 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.987, ppl=1.98, wps=16854.3, ups=1.61, wpb=10472.1, bsz=358, num_updates=55300, lr=9.31661e-06, gnorm=0.853, train_wall=62, wall=0
2021-01-07 22:08:39 | INFO | train_inner | epoch 099:    422 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.99, ppl=1.99, wps=16845.6, ups=1.62, wpb=10380.4, bsz=362.1, num_updates=55400, lr=9.3082e-06, gnorm=0.853, train_wall=61, wall=0
2021-01-07 22:09:41 | INFO | train_inner | epoch 099:    522 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.98, ppl=1.97, wps=17044.8, ups=1.61, wpb=10614.4, bsz=372.7, num_updates=55500, lr=9.29981e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-07 22:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:10:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:10:25 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.68 | ppl 12.82 | bleu 22.86 | wps 4704.7 | wpb 7508.5 | bsz 272.7 | num_updates 55539 | best_bleu 23.14
2021-01-07 22:10:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:10:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 99 @ 55539 updates, score 22.86) (writing took 2.8089827559888363 seconds)
2021-01-07 22:10:28 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2021-01-07 22:10:28 | INFO | train | epoch 099 | symm_kl 0.367 | self_kl 0 | self_cv 0 | loss 3.272 | nll_loss 0.982 | ppl 1.98 | wps 15773.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 55539 | lr 9.29655e-06 | gnorm 0.84 | train_wall 345 | wall 0
2021-01-07 22:10:28 | INFO | fairseq.trainer | begin training epoch 100
2021-01-07 22:10:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:10:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:10:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:10:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:11:08 | INFO | train_inner | epoch 100:     61 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.974, ppl=1.96, wps=11978.2, ups=1.14, wpb=10473, bsz=379, num_updates=55600, lr=9.29144e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-07 22:12:10 | INFO | train_inner | epoch 100:    161 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.964, ppl=1.95, wps=17099.8, ups=1.61, wpb=10592.1, bsz=384.8, num_updates=55700, lr=9.2831e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-07 22:13:12 | INFO | train_inner | epoch 100:    261 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.985, ppl=1.98, wps=17123.7, ups=1.62, wpb=10559.2, bsz=375.3, num_updates=55800, lr=9.27478e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-07 22:14:14 | INFO | train_inner | epoch 100:    361 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.971, ppl=1.96, wps=17037, ups=1.62, wpb=10489.3, bsz=363.7, num_updates=55900, lr=9.26648e-06, gnorm=0.84, train_wall=61, wall=0
2021-01-07 22:15:15 | INFO | train_inner | epoch 100:    461 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.31, nll_loss=1.009, ppl=2.01, wps=16813.4, ups=1.62, wpb=10366.2, bsz=351.1, num_updates=56000, lr=9.2582e-06, gnorm=0.857, train_wall=61, wall=0
2021-01-07 22:16:17 | INFO | train_inner | epoch 100:    561 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.993, ppl=1.99, wps=16755, ups=1.61, wpb=10397.8, bsz=372.5, num_updates=56100, lr=9.24995e-06, gnorm=0.851, train_wall=62, wall=0
2021-01-07 22:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:16:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:16:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:16:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:16:38 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.68 | ppl 12.82 | bleu 22.89 | wps 4787.4 | wpb 7508.5 | bsz 272.7 | num_updates 56100 | best_bleu 23.14
2021-01-07 22:16:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:16:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:16:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:16:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 100 @ 56100 updates, score 22.89) (writing took 2.844935515895486 seconds)
2021-01-07 22:16:40 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2021-01-07 22:16:40 | INFO | train | epoch 100 | symm_kl 0.367 | self_kl 0 | self_cv 0 | loss 3.272 | nll_loss 0.982 | ppl 1.98 | wps 15807.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 56100 | lr 9.24995e-06 | gnorm 0.84 | train_wall 345 | wall 0
2021-01-07 22:16:40 | INFO | fairseq.trainer | begin training epoch 101
2021-01-07 22:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:17:44 | INFO | train_inner | epoch 101:    100 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.967, ppl=1.96, wps=11957.7, ups=1.15, wpb=10416.7, bsz=376.1, num_updates=56200, lr=9.24171e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-07 22:18:46 | INFO | train_inner | epoch 101:    200 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.983, ppl=1.98, wps=16990, ups=1.62, wpb=10505.2, bsz=360.7, num_updates=56300, lr=9.2335e-06, gnorm=0.844, train_wall=62, wall=0
2021-01-07 22:19:48 | INFO | train_inner | epoch 101:    300 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.985, ppl=1.98, wps=17168.9, ups=1.63, wpb=10549.9, bsz=369.2, num_updates=56400, lr=9.22531e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-07 22:20:50 | INFO | train_inner | epoch 101:    400 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.978, ppl=1.97, wps=17075.8, ups=1.62, wpb=10559.7, bsz=369.2, num_updates=56500, lr=9.21714e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-07 22:21:52 | INFO | train_inner | epoch 101:    500 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.992, ppl=1.99, wps=16826.2, ups=1.62, wpb=10418, bsz=373.3, num_updates=56600, lr=9.209e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-07 22:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:22:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:22:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:22:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:22:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:22:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:22:50 | INFO | valid | epoch 101 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.188 | nll_loss 3.681 | ppl 12.83 | bleu 22.94 | wps 4698.8 | wpb 7508.5 | bsz 272.7 | num_updates 56661 | best_bleu 23.14
2021-01-07 22:22:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:22:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:22:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 101 @ 56661 updates, score 22.94) (writing took 2.837989142164588 seconds)
2021-01-07 22:22:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2021-01-07 22:22:53 | INFO | train | epoch 101 | symm_kl 0.366 | self_kl 0 | self_cv 0 | loss 3.271 | nll_loss 0.981 | ppl 1.97 | wps 15788.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 56661 | lr 9.20404e-06 | gnorm 0.837 | train_wall 345 | wall 0
2021-01-07 22:22:53 | INFO | fairseq.trainer | begin training epoch 102
2021-01-07 22:22:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:22:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:23:20 | INFO | train_inner | epoch 102:     39 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.982, ppl=1.97, wps=11859.2, ups=1.13, wpb=10449.9, bsz=368.6, num_updates=56700, lr=9.20087e-06, gnorm=0.851, train_wall=61, wall=0
2021-01-07 22:24:21 | INFO | train_inner | epoch 102:    139 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.986, ppl=1.98, wps=17105.1, ups=1.63, wpb=10510.8, bsz=369.1, num_updates=56800, lr=9.19277e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-07 22:25:23 | INFO | train_inner | epoch 102:    239 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.974, ppl=1.96, wps=16815.1, ups=1.61, wpb=10446.7, bsz=374.1, num_updates=56900, lr=9.18469e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-07 22:26:25 | INFO | train_inner | epoch 102:    339 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.983, ppl=1.98, wps=16846.9, ups=1.62, wpb=10379.8, bsz=371.6, num_updates=57000, lr=9.17663e-06, gnorm=0.838, train_wall=61, wall=0
2021-01-07 22:27:26 | INFO | train_inner | epoch 102:    439 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.968, ppl=1.96, wps=17031.4, ups=1.63, wpb=10476.7, bsz=369.4, num_updates=57100, lr=9.16859e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-07 22:28:28 | INFO | train_inner | epoch 102:    539 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.995, ppl=1.99, wps=17245.3, ups=1.62, wpb=10624, bsz=361.5, num_updates=57200, lr=9.16057e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-07 22:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:28:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:28:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:28:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:28:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:28:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:28:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:28:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:29:02 | INFO | valid | epoch 102 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.185 | nll_loss 3.676 | ppl 12.78 | bleu 23 | wps 4701.6 | wpb 7508.5 | bsz 272.7 | num_updates 57222 | best_bleu 23.14
2021-01-07 22:29:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:29:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:29:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:29:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 102 @ 57222 updates, score 23.0) (writing took 2.8052867110818624 seconds)
2021-01-07 22:29:05 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2021-01-07 22:29:05 | INFO | train | epoch 102 | symm_kl 0.366 | self_kl 0 | self_cv 0 | loss 3.27 | nll_loss 0.981 | ppl 1.97 | wps 15801 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 57222 | lr 9.15881e-06 | gnorm 0.836 | train_wall 344 | wall 0
2021-01-07 22:29:05 | INFO | fairseq.trainer | begin training epoch 103
2021-01-07 22:29:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:29:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:29:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:29:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:29:56 | INFO | train_inner | epoch 103:     78 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.978, ppl=1.97, wps=11897.5, ups=1.14, wpb=10480.8, bsz=382.8, num_updates=57300, lr=9.15258e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-07 22:30:58 | INFO | train_inner | epoch 103:    178 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.976, ppl=1.97, wps=16858.1, ups=1.62, wpb=10384.9, bsz=361.6, num_updates=57400, lr=9.1446e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-07 22:31:59 | INFO | train_inner | epoch 103:    278 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.976, ppl=1.97, wps=17028.8, ups=1.62, wpb=10494.5, bsz=361.1, num_updates=57500, lr=9.13664e-06, gnorm=0.838, train_wall=61, wall=0
2021-01-07 22:33:01 | INFO | train_inner | epoch 103:    378 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.976, ppl=1.97, wps=17104, ups=1.61, wpb=10621.6, bsz=372.3, num_updates=57600, lr=9.12871e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-07 22:34:03 | INFO | train_inner | epoch 103:    478 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.991, ppl=1.99, wps=16994.6, ups=1.61, wpb=10524.3, bsz=366.1, num_updates=57700, lr=9.1208e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-07 22:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:34:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:34:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:35:15 | INFO | valid | epoch 103 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.187 | nll_loss 3.68 | ppl 12.82 | bleu 23.04 | wps 4742.5 | wpb 7508.5 | bsz 272.7 | num_updates 57783 | best_bleu 23.14
2021-01-07 22:35:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:35:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:35:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:35:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:35:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:35:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 103 @ 57783 updates, score 23.04) (writing took 2.845736913383007 seconds)
2021-01-07 22:35:18 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2021-01-07 22:35:18 | INFO | train | epoch 103 | symm_kl 0.366 | self_kl 0 | self_cv 0 | loss 3.27 | nll_loss 0.981 | ppl 1.97 | wps 15778.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 57783 | lr 9.11424e-06 | gnorm 0.836 | train_wall 345 | wall 0
2021-01-07 22:35:18 | INFO | fairseq.trainer | begin training epoch 104
2021-01-07 22:35:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:35:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:35:31 | INFO | train_inner | epoch 104:     17 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.988, ppl=1.98, wps=11797, ups=1.14, wpb=10385.8, bsz=379.3, num_updates=57800, lr=9.1129e-06, gnorm=0.846, train_wall=61, wall=0
2021-01-07 22:36:33 | INFO | train_inner | epoch 104:    117 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.987, ppl=1.98, wps=16963.6, ups=1.62, wpb=10484.8, bsz=358.2, num_updates=57900, lr=9.10503e-06, gnorm=0.837, train_wall=62, wall=0
2021-01-07 22:37:35 | INFO | train_inner | epoch 104:    217 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.963, ppl=1.95, wps=16975.6, ups=1.61, wpb=10540.3, bsz=374.9, num_updates=58000, lr=9.09718e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-07 22:38:37 | INFO | train_inner | epoch 104:    317 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.988, ppl=1.98, wps=16857.1, ups=1.61, wpb=10474.1, bsz=373.2, num_updates=58100, lr=9.08934e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-07 22:39:40 | INFO | train_inner | epoch 104:    417 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.987, ppl=1.98, wps=16756, ups=1.6, wpb=10459.7, bsz=370.1, num_updates=58200, lr=9.08153e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-07 22:40:42 | INFO | train_inner | epoch 104:    517 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.983, ppl=1.98, wps=16760.5, ups=1.6, wpb=10491.1, bsz=373, num_updates=58300, lr=9.07374e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-07 22:41:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:41:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:41:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:41:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:41:30 | INFO | valid | epoch 104 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.187 | nll_loss 3.68 | ppl 12.82 | bleu 22.94 | wps 4750.1 | wpb 7508.5 | bsz 272.7 | num_updates 58344 | best_bleu 23.14
2021-01-07 22:41:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:41:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:41:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:41:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 104 @ 58344 updates, score 22.94) (writing took 2.8070075772702694 seconds)
2021-01-07 22:41:33 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2021-01-07 22:41:33 | INFO | train | epoch 104 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.269 | nll_loss 0.981 | ppl 1.97 | wps 15694.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 58344 | lr 9.07032e-06 | gnorm 0.833 | train_wall 347 | wall 0
2021-01-07 22:41:33 | INFO | fairseq.trainer | begin training epoch 105
2021-01-07 22:41:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:41:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:42:10 | INFO | train_inner | epoch 105:     56 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.979, ppl=1.97, wps=11850.5, ups=1.14, wpb=10358.9, bsz=355.6, num_updates=58400, lr=9.06597e-06, gnorm=0.836, train_wall=61, wall=0
2021-01-07 22:43:13 | INFO | train_inner | epoch 105:    156 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.976, ppl=1.97, wps=16805.7, ups=1.59, wpb=10539.8, bsz=392.9, num_updates=58500, lr=9.05822e-06, gnorm=0.823, train_wall=63, wall=0
2021-01-07 22:44:14 | INFO | train_inner | epoch 105:    256 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.979, ppl=1.97, wps=17029.9, ups=1.63, wpb=10467.5, bsz=356.3, num_updates=58600, lr=9.05048e-06, gnorm=0.849, train_wall=61, wall=0
2021-01-07 22:45:16 | INFO | train_inner | epoch 105:    356 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.978, ppl=1.97, wps=17019.8, ups=1.62, wpb=10493.4, bsz=360.9, num_updates=58700, lr=9.04277e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-07 22:46:18 | INFO | train_inner | epoch 105:    456 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.989, ppl=1.98, wps=16985.9, ups=1.6, wpb=10605.8, bsz=375.3, num_updates=58800, lr=9.03508e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-07 22:47:20 | INFO | train_inner | epoch 105:    556 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.979, ppl=1.97, wps=16997.7, ups=1.62, wpb=10472.5, bsz=374.4, num_updates=58900, lr=9.02741e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-07 22:47:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:47:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:47:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:47:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:47:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:47:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:47:43 | INFO | valid | epoch 105 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.675 | ppl 12.78 | bleu 23.04 | wps 4682.6 | wpb 7508.5 | bsz 272.7 | num_updates 58905 | best_bleu 23.14
2021-01-07 22:47:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:47:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:47:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:47:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 105 @ 58905 updates, score 23.04) (writing took 2.809071419760585 seconds)
2021-01-07 22:47:46 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2021-01-07 22:47:46 | INFO | train | epoch 105 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.268 | nll_loss 0.981 | ppl 1.97 | wps 15742.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 58905 | lr 9.02702e-06 | gnorm 0.834 | train_wall 346 | wall 0
2021-01-07 22:47:46 | INFO | fairseq.trainer | begin training epoch 106
2021-01-07 22:47:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:47:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:48:48 | INFO | train_inner | epoch 106:     95 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.976, ppl=1.97, wps=11754.9, ups=1.14, wpb=10356, bsz=363.8, num_updates=59000, lr=9.01975e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-07 22:49:49 | INFO | train_inner | epoch 106:    195 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.984, ppl=1.98, wps=17051.9, ups=1.63, wpb=10488.8, bsz=375.1, num_updates=59100, lr=9.01212e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-07 22:50:51 | INFO | train_inner | epoch 106:    295 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.965, ppl=1.95, wps=16915.5, ups=1.61, wpb=10496.6, bsz=377.3, num_updates=59200, lr=9.0045e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-07 22:51:53 | INFO | train_inner | epoch 106:    395 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.294, nll_loss=1, ppl=2, wps=16911.6, ups=1.61, wpb=10484, bsz=360, num_updates=59300, lr=8.99691e-06, gnorm=0.84, train_wall=62, wall=0
2021-01-07 22:52:56 | INFO | train_inner | epoch 106:    495 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.964, ppl=1.95, wps=16934.2, ups=1.61, wpb=10550, bsz=373.9, num_updates=59400, lr=8.98933e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-07 22:53:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:53:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:53:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:53:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:53:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:53:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:53:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:53:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:53:57 | INFO | valid | epoch 106 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.186 | nll_loss 3.678 | ppl 12.8 | bleu 23.03 | wps 4774.5 | wpb 7508.5 | bsz 272.7 | num_updates 59466 | best_bleu 23.14
2021-01-07 22:53:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 22:53:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:53:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:53:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:53:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:53:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 106 @ 59466 updates, score 23.03) (writing took 2.805656896904111 seconds)
2021-01-07 22:53:59 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2021-01-07 22:53:59 | INFO | train | epoch 106 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.267 | nll_loss 0.98 | ppl 1.97 | wps 15757 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 59466 | lr 8.98434e-06 | gnorm 0.832 | train_wall 346 | wall 0
2021-01-07 22:53:59 | INFO | fairseq.trainer | begin training epoch 107
2021-01-07 22:54:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:54:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:54:23 | INFO | train_inner | epoch 107:     34 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.977, ppl=1.97, wps=12007.2, ups=1.14, wpb=10520.1, bsz=368, num_updates=59500, lr=8.98177e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-07 22:55:25 | INFO | train_inner | epoch 107:    134 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.99, ppl=1.99, wps=16922.4, ups=1.61, wpb=10495.2, bsz=351.2, num_updates=59600, lr=8.97424e-06, gnorm=0.848, train_wall=62, wall=0
2021-01-07 22:56:27 | INFO | train_inner | epoch 107:    234 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.972, ppl=1.96, wps=16954.9, ups=1.61, wpb=10537, bsz=386.2, num_updates=59700, lr=8.96672e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-07 22:57:29 | INFO | train_inner | epoch 107:    334 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.978, ppl=1.97, wps=16881, ups=1.62, wpb=10429.7, bsz=378.9, num_updates=59800, lr=8.95922e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-07 22:58:31 | INFO | train_inner | epoch 107:    434 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.997, ppl=2, wps=16862.5, ups=1.62, wpb=10418.8, bsz=362.9, num_updates=59900, lr=8.95173e-06, gnorm=0.84, train_wall=62, wall=0
2021-01-07 22:59:33 | INFO | train_inner | epoch 107:    534 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.983, ppl=1.98, wps=16967.9, ups=1.62, wpb=10467.5, bsz=366, num_updates=60000, lr=8.94427e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-07 22:59:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 22:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 22:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 22:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 22:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 22:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 22:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:00:10 | INFO | valid | epoch 107 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.675 | ppl 12.77 | bleu 23.03 | wps 4762.5 | wpb 7508.5 | bsz 272.7 | num_updates 60027 | best_bleu 23.14
2021-01-07 23:00:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:00:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:00:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:00:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 107 @ 60027 updates, score 23.03) (writing took 2.8619018122553825 seconds)
2021-01-07 23:00:13 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2021-01-07 23:00:13 | INFO | train | epoch 107 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.267 | nll_loss 0.98 | ppl 1.97 | wps 15754.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 60027 | lr 8.94226e-06 | gnorm 0.833 | train_wall 346 | wall 0
2021-01-07 23:00:13 | INFO | fairseq.trainer | begin training epoch 108
2021-01-07 23:00:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:00:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:01:00 | INFO | train_inner | epoch 108:     73 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.96, ppl=1.95, wps=12113.9, ups=1.14, wpb=10594, bsz=385.7, num_updates=60100, lr=8.93683e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-07 23:02:02 | INFO | train_inner | epoch 108:    173 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.977, ppl=1.97, wps=16925.3, ups=1.63, wpb=10396.6, bsz=357.2, num_updates=60200, lr=8.9294e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-07 23:03:04 | INFO | train_inner | epoch 108:    273 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.985, ppl=1.98, wps=16774.1, ups=1.61, wpb=10410.2, bsz=350.9, num_updates=60300, lr=8.92199e-06, gnorm=0.851, train_wall=62, wall=0
2021-01-07 23:04:05 | INFO | train_inner | epoch 108:    373 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.976, ppl=1.97, wps=17014.6, ups=1.62, wpb=10493.9, bsz=380.9, num_updates=60400, lr=8.91461e-06, gnorm=0.844, train_wall=61, wall=0
2021-01-07 23:05:07 | INFO | train_inner | epoch 108:    473 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.98, ppl=1.97, wps=16938.2, ups=1.61, wpb=10509.5, bsz=358.2, num_updates=60500, lr=8.90724e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-07 23:06:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:06:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:06:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:06:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:06:22 | INFO | valid | epoch 108 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.677 | ppl 12.79 | bleu 22.96 | wps 4735.7 | wpb 7508.5 | bsz 272.7 | num_updates 60588 | best_bleu 23.14
2021-01-07 23:06:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:06:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 108 @ 60588 updates, score 22.96) (writing took 2.8194558080285788 seconds)
2021-01-07 23:06:25 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2021-01-07 23:06:25 | INFO | train | epoch 108 | symm_kl 0.364 | self_kl 0 | self_cv 0 | loss 3.266 | nll_loss 0.979 | ppl 1.97 | wps 15782.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 60588 | lr 8.90076e-06 | gnorm 0.836 | train_wall 345 | wall 0
2021-01-07 23:06:25 | INFO | fairseq.trainer | begin training epoch 109
2021-01-07 23:06:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:06:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:06:36 | INFO | train_inner | epoch 109:     12 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.993, ppl=1.99, wps=11786.9, ups=1.13, wpb=10406.5, bsz=387.7, num_updates=60600, lr=8.89988e-06, gnorm=0.837, train_wall=62, wall=0
2021-01-07 23:07:37 | INFO | train_inner | epoch 109:    112 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.982, ppl=1.97, wps=17207, ups=1.63, wpb=10535.5, bsz=379.2, num_updates=60700, lr=8.89255e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-07 23:08:39 | INFO | train_inner | epoch 109:    212 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.988, ppl=1.98, wps=16882.2, ups=1.62, wpb=10409.3, bsz=363.3, num_updates=60800, lr=8.88523e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-07 23:09:40 | INFO | train_inner | epoch 109:    312 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.973, ppl=1.96, wps=17072.6, ups=1.62, wpb=10513.5, bsz=366.7, num_updates=60900, lr=8.87794e-06, gnorm=0.844, train_wall=61, wall=0
2021-01-07 23:10:42 | INFO | train_inner | epoch 109:    412 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.973, ppl=1.96, wps=17035.4, ups=1.62, wpb=10508.6, bsz=369.4, num_updates=61000, lr=8.87066e-06, gnorm=0.841, train_wall=61, wall=0
2021-01-07 23:11:44 | INFO | train_inner | epoch 109:    512 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.977, ppl=1.97, wps=17019.8, ups=1.62, wpb=10524.9, bsz=375.1, num_updates=61100, lr=8.86339e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-07 23:12:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:12:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:12:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:12:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:12:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:12:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:12:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:12:34 | INFO | valid | epoch 109 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.187 | nll_loss 3.679 | ppl 12.81 | bleu 22.88 | wps 4682.4 | wpb 7508.5 | bsz 272.7 | num_updates 61149 | best_bleu 23.14
2021-01-07 23:12:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:12:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:12:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:12:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 109 @ 61149 updates, score 22.88) (writing took 2.7921690605580807 seconds)
2021-01-07 23:12:37 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2021-01-07 23:12:37 | INFO | train | epoch 109 | symm_kl 0.364 | self_kl 0 | self_cv 0 | loss 3.265 | nll_loss 0.979 | ppl 1.97 | wps 15810.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 61149 | lr 8.85984e-06 | gnorm 0.835 | train_wall 344 | wall 0
2021-01-07 23:12:37 | INFO | fairseq.trainer | begin training epoch 110
2021-01-07 23:12:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:12:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:13:11 | INFO | train_inner | epoch 110:     51 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.974, ppl=1.96, wps=11907, ups=1.14, wpb=10408.9, bsz=363, num_updates=61200, lr=8.85615e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-07 23:14:13 | INFO | train_inner | epoch 110:    151 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.981, ppl=1.97, wps=16914.7, ups=1.61, wpb=10498.6, bsz=358.2, num_updates=61300, lr=8.84892e-06, gnorm=0.842, train_wall=62, wall=0
2021-01-07 23:15:15 | INFO | train_inner | epoch 110:    251 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.965, ppl=1.95, wps=17130.2, ups=1.62, wpb=10600.3, bsz=362.2, num_updates=61400, lr=8.84171e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-07 23:16:17 | INFO | train_inner | epoch 110:    351 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.988, ppl=1.98, wps=16679.8, ups=1.62, wpb=10325.5, bsz=372.1, num_updates=61500, lr=8.83452e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-07 23:17:19 | INFO | train_inner | epoch 110:    451 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.991, ppl=1.99, wps=17012.4, ups=1.62, wpb=10510.1, bsz=383, num_updates=61600, lr=8.82735e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-07 23:18:20 | INFO | train_inner | epoch 110:    551 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.976, ppl=1.97, wps=17220.8, ups=1.63, wpb=10591.7, bsz=375, num_updates=61700, lr=8.82019e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-07 23:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:18:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:18:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:18:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:18:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:18:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:18:47 | INFO | valid | epoch 110 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.185 | nll_loss 3.677 | ppl 12.79 | bleu 22.94 | wps 4691.4 | wpb 7508.5 | bsz 272.7 | num_updates 61710 | best_bleu 23.14
2021-01-07 23:18:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:18:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:18:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 110 @ 61710 updates, score 22.94) (writing took 2.8254611492156982 seconds)
2021-01-07 23:18:50 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2021-01-07 23:18:50 | INFO | train | epoch 110 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.264 | nll_loss 0.979 | ppl 1.97 | wps 15789.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 61710 | lr 8.81948e-06 | gnorm 0.836 | train_wall 345 | wall 0
2021-01-07 23:18:50 | INFO | fairseq.trainer | begin training epoch 111
2021-01-07 23:18:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:18:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:19:48 | INFO | train_inner | epoch 111:     90 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.971, ppl=1.96, wps=12069.2, ups=1.14, wpb=10577.6, bsz=371.8, num_updates=61800, lr=8.81305e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-07 23:20:50 | INFO | train_inner | epoch 111:    190 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.985, ppl=1.98, wps=17072.2, ups=1.61, wpb=10573.8, bsz=373.4, num_updates=61900, lr=8.80593e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-07 23:21:51 | INFO | train_inner | epoch 111:    290 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.987, ppl=1.98, wps=16925.4, ups=1.62, wpb=10437.5, bsz=357.4, num_updates=62000, lr=8.79883e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-07 23:22:53 | INFO | train_inner | epoch 111:    390 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.977, ppl=1.97, wps=16963, ups=1.62, wpb=10438.8, bsz=369, num_updates=62100, lr=8.79174e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-07 23:23:55 | INFO | train_inner | epoch 111:    490 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.971, ppl=1.96, wps=16865.6, ups=1.61, wpb=10444.3, bsz=363.4, num_updates=62200, lr=8.78467e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-07 23:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:24:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:24:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:24:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:24:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:24:59 | INFO | valid | epoch 111 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.185 | nll_loss 3.677 | ppl 12.79 | bleu 23.03 | wps 4748.7 | wpb 7508.5 | bsz 272.7 | num_updates 62271 | best_bleu 23.14
2021-01-07 23:24:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:25:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:25:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:25:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:25:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:25:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 111 @ 62271 updates, score 23.03) (writing took 2.798899345099926 seconds)
2021-01-07 23:25:02 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2021-01-07 23:25:02 | INFO | train | epoch 111 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.264 | nll_loss 0.978 | ppl 1.97 | wps 15790.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 62271 | lr 8.77966e-06 | gnorm 0.835 | train_wall 345 | wall 0
2021-01-07 23:25:02 | INFO | fairseq.trainer | begin training epoch 112
2021-01-07 23:25:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:25:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:25:23 | INFO | train_inner | epoch 112:     29 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.979, ppl=1.97, wps=11873.3, ups=1.14, wpb=10444, bsz=378.3, num_updates=62300, lr=8.77762e-06, gnorm=0.847, train_wall=61, wall=0
2021-01-07 23:26:24 | INFO | train_inner | epoch 112:    129 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.973, ppl=1.96, wps=17190.8, ups=1.63, wpb=10569.4, bsz=368.6, num_updates=62400, lr=8.77058e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-07 23:27:26 | INFO | train_inner | epoch 112:    229 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.972, ppl=1.96, wps=16956.2, ups=1.63, wpb=10410.1, bsz=373, num_updates=62500, lr=8.76356e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-07 23:28:28 | INFO | train_inner | epoch 112:    329 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.276, nll_loss=0.987, ppl=1.98, wps=16705.4, ups=1.62, wpb=10342.3, bsz=352.3, num_updates=62600, lr=8.75656e-06, gnorm=0.851, train_wall=62, wall=0
2021-01-07 23:29:30 | INFO | train_inner | epoch 112:    429 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.973, ppl=1.96, wps=17030.1, ups=1.62, wpb=10542.9, bsz=370.4, num_updates=62700, lr=8.74957e-06, gnorm=0.835, train_wall=62, wall=0
2021-01-07 23:30:32 | INFO | train_inner | epoch 112:    529 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.979, ppl=1.97, wps=16979.4, ups=1.6, wpb=10588.7, bsz=386.8, num_updates=62800, lr=8.7426e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-07 23:30:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:30:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:30:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:30:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:30:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:30:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:30:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:30:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:30:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:31:12 | INFO | valid | epoch 112 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.186 | nll_loss 3.677 | ppl 12.79 | bleu 23 | wps 4701.5 | wpb 7508.5 | bsz 272.7 | num_updates 62832 | best_bleu 23.14
2021-01-07 23:31:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:31:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:31:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:31:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 112 @ 62832 updates, score 23.0) (writing took 2.7940753251314163 seconds)
2021-01-07 23:31:15 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2021-01-07 23:31:15 | INFO | train | epoch 112 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.263 | nll_loss 0.978 | ppl 1.97 | wps 15771.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 62832 | lr 8.74038e-06 | gnorm 0.832 | train_wall 345 | wall 0
2021-01-07 23:31:15 | INFO | fairseq.trainer | begin training epoch 113
2021-01-07 23:31:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:31:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:31:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:31:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:32:00 | INFO | train_inner | epoch 113:     68 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.992, ppl=1.99, wps=11819.6, ups=1.14, wpb=10391.5, bsz=362.3, num_updates=62900, lr=8.73565e-06, gnorm=0.838, train_wall=61, wall=0
2021-01-07 23:33:02 | INFO | train_inner | epoch 113:    168 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.973, ppl=1.96, wps=16894, ups=1.6, wpb=10551, bsz=376.7, num_updates=63000, lr=8.72872e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-07 23:34:05 | INFO | train_inner | epoch 113:    268 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.965, ppl=1.95, wps=16846.5, ups=1.6, wpb=10547.2, bsz=392.5, num_updates=63100, lr=8.7218e-06, gnorm=0.814, train_wall=62, wall=0
2021-01-07 23:35:08 | INFO | train_inner | epoch 113:    368 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.967, ppl=1.96, wps=16797.8, ups=1.59, wpb=10553.5, bsz=368.2, num_updates=63200, lr=8.71489e-06, gnorm=0.828, train_wall=63, wall=0
2021-01-07 23:36:10 | INFO | train_inner | epoch 113:    468 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.982, ppl=1.98, wps=16729.2, ups=1.61, wpb=10419.6, bsz=368.2, num_updates=63300, lr=8.70801e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-07 23:37:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:37:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:37:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:37:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:37:28 | INFO | valid | epoch 113 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.674 | ppl 12.77 | bleu 23.08 | wps 4706.3 | wpb 7508.5 | bsz 272.7 | num_updates 63393 | best_bleu 23.14
2021-01-07 23:37:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:37:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 113 @ 63393 updates, score 23.08) (writing took 2.832469206303358 seconds)
2021-01-07 23:37:31 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2021-01-07 23:37:31 | INFO | train | epoch 113 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.262 | nll_loss 0.978 | ppl 1.97 | wps 15640.5 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 63393 | lr 8.70162e-06 | gnorm 0.831 | train_wall 348 | wall 0
2021-01-07 23:37:31 | INFO | fairseq.trainer | begin training epoch 114
2021-01-07 23:37:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:37:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:37:39 | INFO | train_inner | epoch 114:      7 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.996, ppl=1.99, wps=11682.9, ups=1.13, wpb=10353.9, bsz=349.5, num_updates=63400, lr=8.70114e-06, gnorm=0.858, train_wall=62, wall=0
2021-01-07 23:38:40 | INFO | train_inner | epoch 114:    107 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.99, ppl=1.99, wps=17205.9, ups=1.63, wpb=10544.9, bsz=354.9, num_updates=63500, lr=8.69428e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-07 23:39:42 | INFO | train_inner | epoch 114:    207 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.992, ppl=1.99, wps=16688.9, ups=1.61, wpb=10390.9, bsz=355.6, num_updates=63600, lr=8.68744e-06, gnorm=0.849, train_wall=62, wall=0
2021-01-07 23:40:45 | INFO | train_inner | epoch 114:    307 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.973, ppl=1.96, wps=16811.7, ups=1.59, wpb=10550.5, bsz=380.5, num_updates=63700, lr=8.68062e-06, gnorm=0.813, train_wall=63, wall=0
2021-01-07 23:41:47 | INFO | train_inner | epoch 114:    407 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.982, ppl=1.97, wps=16749.6, ups=1.61, wpb=10383, bsz=352.6, num_updates=63800, lr=8.67382e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-07 23:42:50 | INFO | train_inner | epoch 114:    507 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.958, ppl=1.94, wps=16695.1, ups=1.58, wpb=10584.4, bsz=397, num_updates=63900, lr=8.66703e-06, gnorm=0.809, train_wall=63, wall=0
2021-01-07 23:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:43:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:43:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:43:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:43:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:43:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:43:44 | INFO | valid | epoch 114 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.677 | ppl 12.79 | bleu 23.06 | wps 4724.2 | wpb 7508.5 | bsz 272.7 | num_updates 63954 | best_bleu 23.14
2021-01-07 23:43:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:43:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 114 @ 63954 updates, score 23.06) (writing took 2.8618897777050734 seconds)
2021-01-07 23:43:47 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2021-01-07 23:43:47 | INFO | train | epoch 114 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.261 | nll_loss 0.977 | ppl 1.97 | wps 15634.5 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 63954 | lr 8.66337e-06 | gnorm 0.828 | train_wall 349 | wall 0
2021-01-07 23:43:47 | INFO | fairseq.trainer | begin training epoch 115
2021-01-07 23:43:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:43:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:44:18 | INFO | train_inner | epoch 115:     46 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.973, ppl=1.96, wps=11824.5, ups=1.13, wpb=10419.5, bsz=383.5, num_updates=64000, lr=8.66025e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-07 23:45:21 | INFO | train_inner | epoch 115:    146 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.976, ppl=1.97, wps=16751.9, ups=1.59, wpb=10512.7, bsz=373, num_updates=64100, lr=8.6535e-06, gnorm=0.822, train_wall=63, wall=0
2021-01-07 23:46:24 | INFO | train_inner | epoch 115:    246 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.982, ppl=1.98, wps=16816.3, ups=1.6, wpb=10505.5, bsz=369, num_updates=64200, lr=8.64675e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-07 23:47:26 | INFO | train_inner | epoch 115:    346 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.967, ppl=1.95, wps=16854.8, ups=1.61, wpb=10467.7, bsz=351.8, num_updates=64300, lr=8.64003e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-07 23:48:29 | INFO | train_inner | epoch 115:    446 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.97, ppl=1.96, wps=16686.1, ups=1.59, wpb=10463, bsz=375.5, num_updates=64400, lr=8.63332e-06, gnorm=0.831, train_wall=63, wall=0
2021-01-07 23:49:31 | INFO | train_inner | epoch 115:    546 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.983, ppl=1.98, wps=16852.4, ups=1.6, wpb=10533.9, bsz=376.4, num_updates=64500, lr=8.62662e-06, gnorm=0.826, train_wall=62, wall=0
2021-01-07 23:49:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:49:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:49:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:50:01 | INFO | valid | epoch 115 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.185 | nll_loss 3.678 | ppl 12.8 | bleu 23.05 | wps 4718.1 | wpb 7508.5 | bsz 272.7 | num_updates 64515 | best_bleu 23.14
2021-01-07 23:50:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:50:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:50:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:50:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:50:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:50:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 115 @ 64515 updates, score 23.05) (writing took 2.877723941579461 seconds)
2021-01-07 23:50:04 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2021-01-07 23:50:04 | INFO | train | epoch 115 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.261 | nll_loss 0.977 | ppl 1.97 | wps 15627.5 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 64515 | lr 8.62562e-06 | gnorm 0.829 | train_wall 349 | wall 0
2021-01-07 23:50:04 | INFO | fairseq.trainer | begin training epoch 116
2021-01-07 23:50:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:50:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:50:59 | INFO | train_inner | epoch 116:     85 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.967, ppl=1.95, wps=11913, ups=1.14, wpb=10490.9, bsz=371.4, num_updates=64600, lr=8.61994e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-07 23:52:02 | INFO | train_inner | epoch 116:    185 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.971, ppl=1.96, wps=16930.6, ups=1.59, wpb=10627.5, bsz=379.8, num_updates=64700, lr=8.61328e-06, gnorm=0.824, train_wall=63, wall=0
2021-01-07 23:53:04 | INFO | train_inner | epoch 116:    285 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.979, ppl=1.97, wps=16867.7, ups=1.61, wpb=10458.3, bsz=364.9, num_updates=64800, lr=8.60663e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-07 23:54:06 | INFO | train_inner | epoch 116:    385 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.982, ppl=1.97, wps=16700.2, ups=1.62, wpb=10313.1, bsz=356.6, num_updates=64900, lr=8.6e-06, gnorm=0.845, train_wall=62, wall=0
2021-01-07 23:55:08 | INFO | train_inner | epoch 116:    485 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.981, ppl=1.97, wps=17015.2, ups=1.61, wpb=10558.7, bsz=373.2, num_updates=65000, lr=8.59338e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-07 23:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-07 23:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:55:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:55:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:55:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:55:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:55:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:55:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:55:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:55:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:55:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:55:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-07 23:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-07 23:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-07 23:56:18 | INFO | valid | epoch 116 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.186 | nll_loss 3.679 | ppl 12.81 | bleu 23.01 | wps 4077.9 | wpb 7508.5 | bsz 272.7 | num_updates 65076 | best_bleu 23.14
2021-01-07 23:56:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-07 23:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:56:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:56:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 116 @ 65076 updates, score 23.01) (writing took 2.804455542936921 seconds)
2021-01-07 23:56:21 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2021-01-07 23:56:21 | INFO | train | epoch 116 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.26 | nll_loss 0.977 | ppl 1.97 | wps 15604.4 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 65076 | lr 8.58836e-06 | gnorm 0.832 | train_wall 347 | wall 0
2021-01-07 23:56:21 | INFO | fairseq.trainer | begin training epoch 117
2021-01-07 23:56:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-07 23:56:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-07 23:56:38 | INFO | train_inner | epoch 117:     24 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.99, ppl=1.99, wps=11527.1, ups=1.1, wpb=10433.2, bsz=364.9, num_updates=65100, lr=8.58678e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-07 23:57:40 | INFO | train_inner | epoch 117:    124 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.982, ppl=1.98, wps=16866.2, ups=1.62, wpb=10409.8, bsz=361.1, num_updates=65200, lr=8.58019e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-07 23:58:42 | INFO | train_inner | epoch 117:    224 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.966, ppl=1.95, wps=17058.8, ups=1.6, wpb=10633.3, bsz=378.2, num_updates=65300, lr=8.57362e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-07 23:59:45 | INFO | train_inner | epoch 117:    324 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.981, ppl=1.97, wps=16860.4, ups=1.6, wpb=10513.5, bsz=373.8, num_updates=65400, lr=8.56706e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-08 00:00:47 | INFO | train_inner | epoch 117:    424 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.971, ppl=1.96, wps=16572.6, ups=1.61, wpb=10312.1, bsz=366.6, num_updates=65500, lr=8.56052e-06, gnorm=0.841, train_wall=62, wall=0
2021-01-08 00:01:49 | INFO | train_inner | epoch 117:    524 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.978, ppl=1.97, wps=16974.1, ups=1.6, wpb=10586.1, bsz=376.6, num_updates=65600, lr=8.55399e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 00:02:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:02:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:02:32 | INFO | valid | epoch 117 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.185 | nll_loss 3.677 | ppl 12.79 | bleu 22.99 | wps 4721.6 | wpb 7508.5 | bsz 272.7 | num_updates 65637 | best_bleu 23.14
2021-01-08 00:02:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:02:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 117 @ 65637 updates, score 22.99) (writing took 2.874241938814521 seconds)
2021-01-08 00:02:35 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2021-01-08 00:02:35 | INFO | train | epoch 117 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.26 | nll_loss 0.977 | ppl 1.97 | wps 15688.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 65637 | lr 8.55158e-06 | gnorm 0.83 | train_wall 347 | wall 0
2021-01-08 00:02:35 | INFO | fairseq.trainer | begin training epoch 118
2021-01-08 00:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:02:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:02:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:03:17 | INFO | train_inner | epoch 118:     63 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.981, ppl=1.97, wps=11942.3, ups=1.14, wpb=10507.2, bsz=371.1, num_updates=65700, lr=8.54748e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 00:04:19 | INFO | train_inner | epoch 118:    163 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.972, ppl=1.96, wps=16962.3, ups=1.62, wpb=10468.5, bsz=364.8, num_updates=65800, lr=8.54098e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 00:05:21 | INFO | train_inner | epoch 118:    263 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.982, ppl=1.97, wps=17014.6, ups=1.61, wpb=10547.3, bsz=352.7, num_updates=65900, lr=8.5345e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 00:06:22 | INFO | train_inner | epoch 118:    363 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.979, ppl=1.97, wps=16855.5, ups=1.63, wpb=10317.5, bsz=377.6, num_updates=66000, lr=8.52803e-06, gnorm=0.849, train_wall=61, wall=0
2021-01-08 00:07:25 | INFO | train_inner | epoch 118:    463 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.977, ppl=1.97, wps=16752.6, ups=1.6, wpb=10461.9, bsz=371.4, num_updates=66100, lr=8.52158e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-08 00:08:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:08:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:08:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:08:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:08:46 | INFO | valid | epoch 118 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.186 | nll_loss 3.678 | ppl 12.8 | bleu 22.95 | wps 4733.7 | wpb 7508.5 | bsz 272.7 | num_updates 66198 | best_bleu 23.14
2021-01-08 00:08:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 118 @ 66198 updates, score 22.95) (writing took 2.826365053653717 seconds)
2021-01-08 00:08:49 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2021-01-08 00:08:49 | INFO | train | epoch 118 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.259 | nll_loss 0.976 | ppl 1.97 | wps 15760.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 66198 | lr 8.51527e-06 | gnorm 0.831 | train_wall 346 | wall 0
2021-01-08 00:08:49 | INFO | fairseq.trainer | begin training epoch 119
2021-01-08 00:08:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:08:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:08:53 | INFO | train_inner | epoch 119:      2 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.976, ppl=1.97, wps=11917.5, ups=1.13, wpb=10538.9, bsz=381, num_updates=66200, lr=8.51514e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 00:09:54 | INFO | train_inner | epoch 119:    102 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.976, ppl=1.97, wps=17131.9, ups=1.64, wpb=10468.9, bsz=361.2, num_updates=66300, lr=8.50871e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 00:10:56 | INFO | train_inner | epoch 119:    202 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.977, ppl=1.97, wps=16918.9, ups=1.62, wpb=10470, bsz=361.1, num_updates=66400, lr=8.5023e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 00:11:57 | INFO | train_inner | epoch 119:    302 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.973, ppl=1.96, wps=17090.1, ups=1.63, wpb=10504.7, bsz=354.4, num_updates=66500, lr=8.49591e-06, gnorm=0.85, train_wall=61, wall=0
2021-01-08 00:12:59 | INFO | train_inner | epoch 119:    402 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.988, ppl=1.98, wps=16923.5, ups=1.61, wpb=10500.5, bsz=387, num_updates=66600, lr=8.48953e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 00:14:01 | INFO | train_inner | epoch 119:    502 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.97, ppl=1.96, wps=16992.9, ups=1.63, wpb=10435.2, bsz=377, num_updates=66700, lr=8.48316e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 00:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:14:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:14:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:14:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:14:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:14:58 | INFO | valid | epoch 119 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.677 | ppl 12.79 | bleu 22.97 | wps 4748.9 | wpb 7508.5 | bsz 272.7 | num_updates 66759 | best_bleu 23.14
2021-01-08 00:14:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:15:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:15:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:15:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 119 @ 66759 updates, score 22.97) (writing took 2.9721094872802496 seconds)
2021-01-08 00:15:01 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2021-01-08 00:15:01 | INFO | train | epoch 119 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.976 | ppl 1.97 | wps 15807.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 66759 | lr 8.47941e-06 | gnorm 0.831 | train_wall 344 | wall 0
2021-01-08 00:15:01 | INFO | fairseq.trainer | begin training epoch 120
2021-01-08 00:15:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:15:28 | INFO | train_inner | epoch 120:     41 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.97, ppl=1.96, wps=11964.8, ups=1.14, wpb=10483.7, bsz=370.9, num_updates=66800, lr=8.47681e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 00:16:30 | INFO | train_inner | epoch 120:    141 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.972, ppl=1.96, wps=17154.1, ups=1.62, wpb=10607.1, bsz=374.2, num_updates=66900, lr=8.47047e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 00:17:32 | INFO | train_inner | epoch 120:    241 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.971, ppl=1.96, wps=16909.6, ups=1.62, wpb=10456.1, bsz=366.8, num_updates=67000, lr=8.46415e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 00:18:34 | INFO | train_inner | epoch 120:    341 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.968, ppl=1.96, wps=16811, ups=1.61, wpb=10444.4, bsz=357.8, num_updates=67100, lr=8.45784e-06, gnorm=0.842, train_wall=62, wall=0
2021-01-08 00:19:36 | INFO | train_inner | epoch 120:    441 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.979, ppl=1.97, wps=16886.9, ups=1.62, wpb=10424.8, bsz=368.3, num_updates=67200, lr=8.45154e-06, gnorm=0.843, train_wall=62, wall=0
2021-01-08 00:20:38 | INFO | train_inner | epoch 120:    541 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.991, ppl=1.99, wps=17092.4, ups=1.61, wpb=10605.6, bsz=375.4, num_updates=67300, lr=8.44526e-06, gnorm=0.836, train_wall=62, wall=0
2021-01-08 00:20:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:21:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:21:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:21:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:21:11 | INFO | valid | epoch 120 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.675 | ppl 12.77 | bleu 23.04 | wps 4720.8 | wpb 7508.5 | bsz 272.7 | num_updates 67320 | best_bleu 23.14
2021-01-08 00:21:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:21:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:21:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:21:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 120 @ 67320 updates, score 23.04) (writing took 2.7919138558208942 seconds)
2021-01-08 00:21:14 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2021-01-08 00:21:14 | INFO | train | epoch 120 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.976 | ppl 1.97 | wps 15764.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 67320 | lr 8.44401e-06 | gnorm 0.837 | train_wall 345 | wall 0
2021-01-08 00:21:14 | INFO | fairseq.trainer | begin training epoch 121
2021-01-08 00:21:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:21:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:22:05 | INFO | train_inner | epoch 121:     80 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.973, ppl=1.96, wps=11888.9, ups=1.15, wpb=10360, bsz=377.4, num_updates=67400, lr=8.43899e-06, gnorm=0.84, train_wall=60, wall=0
2021-01-08 00:23:07 | INFO | train_inner | epoch 121:    180 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.98, ppl=1.97, wps=16895.4, ups=1.62, wpb=10438.5, bsz=365, num_updates=67500, lr=8.43274e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-08 00:24:09 | INFO | train_inner | epoch 121:    280 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.97, ppl=1.96, wps=16840.4, ups=1.6, wpb=10495.8, bsz=381.8, num_updates=67600, lr=8.4265e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 00:25:12 | INFO | train_inner | epoch 121:    380 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.97, ppl=1.96, wps=17049.9, ups=1.61, wpb=10614.3, bsz=373, num_updates=67700, lr=8.42028e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 00:26:14 | INFO | train_inner | epoch 121:    480 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.987, ppl=1.98, wps=16717.5, ups=1.61, wpb=10379.1, bsz=374.7, num_updates=67800, lr=8.41406e-06, gnorm=0.839, train_wall=62, wall=0
2021-01-08 00:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:27:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:27:27 | INFO | valid | epoch 121 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.676 | ppl 12.78 | bleu 22.93 | wps 4245 | wpb 7508.5 | bsz 272.7 | num_updates 67881 | best_bleu 23.14
2021-01-08 00:27:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:27:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 121 @ 67881 updates, score 22.93) (writing took 2.843061923980713 seconds)
2021-01-08 00:27:29 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2021-01-08 00:27:29 | INFO | train | epoch 121 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.977 | ppl 1.97 | wps 15647.5 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 67881 | lr 8.40904e-06 | gnorm 0.829 | train_wall 346 | wall 0
2021-01-08 00:27:29 | INFO | fairseq.trainer | begin training epoch 122
2021-01-08 00:27:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:27:44 | INFO | train_inner | epoch 122:     19 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.974, ppl=1.96, wps=11635.7, ups=1.1, wpb=10543.3, bsz=353.2, num_updates=67900, lr=8.40787e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 00:28:46 | INFO | train_inner | epoch 122:    119 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.986, ppl=1.98, wps=16916.5, ups=1.62, wpb=10469.1, bsz=367.1, num_updates=68000, lr=8.40168e-06, gnorm=0.851, train_wall=62, wall=0
2021-01-08 00:29:49 | INFO | train_inner | epoch 122:    219 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.977, ppl=1.97, wps=17026.8, ups=1.6, wpb=10627.5, bsz=372, num_updates=68100, lr=8.39551e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 00:30:51 | INFO | train_inner | epoch 122:    319 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.97, ppl=1.96, wps=16783.9, ups=1.6, wpb=10475.4, bsz=381, num_updates=68200, lr=8.38935e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 00:31:53 | INFO | train_inner | epoch 122:    419 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.969, ppl=1.96, wps=16917.2, ups=1.6, wpb=10560.7, bsz=379.2, num_updates=68300, lr=8.38321e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 00:32:56 | INFO | train_inner | epoch 122:    519 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.273, nll_loss=0.987, ppl=1.98, wps=16666.8, ups=1.6, wpb=10416.8, bsz=347.6, num_updates=68400, lr=8.37708e-06, gnorm=0.847, train_wall=62, wall=0
2021-01-08 00:33:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:33:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:33:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:33:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:33:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:33:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:33:42 | INFO | valid | epoch 122 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.674 | ppl 12.77 | bleu 23.01 | wps 4719.6 | wpb 7508.5 | bsz 272.7 | num_updates 68442 | best_bleu 23.14
2021-01-08 00:33:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:33:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:33:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:33:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 122 @ 68442 updates, score 23.01) (writing took 2.8140894770622253 seconds)
2021-01-08 00:33:45 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2021-01-08 00:33:45 | INFO | train | epoch 122 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.257 | nll_loss 0.976 | ppl 1.97 | wps 15652.8 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 68442 | lr 8.37451e-06 | gnorm 0.834 | train_wall 348 | wall 0
2021-01-08 00:33:45 | INFO | fairseq.trainer | begin training epoch 123
2021-01-08 00:33:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:33:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:34:24 | INFO | train_inner | epoch 123:     58 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.976, ppl=1.97, wps=11759.7, ups=1.14, wpb=10306, bsz=367.7, num_updates=68500, lr=8.37096e-06, gnorm=0.839, train_wall=61, wall=0
2021-01-08 00:35:26 | INFO | train_inner | epoch 123:    158 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.975, ppl=1.97, wps=16805.1, ups=1.61, wpb=10458.7, bsz=364.2, num_updates=68600, lr=8.36486e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 00:36:29 | INFO | train_inner | epoch 123:    258 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.97, ppl=1.96, wps=16859.3, ups=1.59, wpb=10583.4, bsz=376.8, num_updates=68700, lr=8.35877e-06, gnorm=0.818, train_wall=63, wall=0
2021-01-08 00:37:31 | INFO | train_inner | epoch 123:    358 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.98, ppl=1.97, wps=16897.8, ups=1.61, wpb=10491.3, bsz=374.3, num_updates=68800, lr=8.35269e-06, gnorm=0.847, train_wall=62, wall=0
2021-01-08 00:38:33 | INFO | train_inner | epoch 123:    458 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.975, ppl=1.97, wps=16839, ups=1.6, wpb=10495.3, bsz=372.2, num_updates=68900, lr=8.34663e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 00:39:35 | INFO | train_inner | epoch 123:    558 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.973, ppl=1.96, wps=16895, ups=1.61, wpb=10468.9, bsz=362.9, num_updates=69000, lr=8.34058e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 00:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:39:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:39:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:39:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:39:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:39:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:39:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:39:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:39:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:39:57 | INFO | valid | epoch 123 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.676 | ppl 12.78 | bleu 23.04 | wps 4701.5 | wpb 7508.5 | bsz 272.7 | num_updates 69003 | best_bleu 23.14
2021-01-08 00:39:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:40:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 123 @ 69003 updates, score 23.04) (writing took 2.7864678911864758 seconds)
2021-01-08 00:40:00 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2021-01-08 00:40:00 | INFO | train | epoch 123 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.255 | nll_loss 0.975 | ppl 1.97 | wps 15685.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 69003 | lr 8.3404e-06 | gnorm 0.83 | train_wall 347 | wall 0
2021-01-08 00:40:00 | INFO | fairseq.trainer | begin training epoch 124
2021-01-08 00:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:40:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:40:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:40:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:41:03 | INFO | train_inner | epoch 124:     97 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.964, ppl=1.95, wps=12044.3, ups=1.14, wpb=10563.2, bsz=370.6, num_updates=69100, lr=8.33454e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 00:42:05 | INFO | train_inner | epoch 124:    197 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.974, ppl=1.96, wps=16906.6, ups=1.61, wpb=10503.8, bsz=367.7, num_updates=69200, lr=8.32851e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-08 00:43:07 | INFO | train_inner | epoch 124:    297 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.971, ppl=1.96, wps=16858.4, ups=1.61, wpb=10488.1, bsz=373, num_updates=69300, lr=8.3225e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 00:44:09 | INFO | train_inner | epoch 124:    397 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.978, ppl=1.97, wps=16818.1, ups=1.61, wpb=10455.3, bsz=373.1, num_updates=69400, lr=8.31651e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 00:45:12 | INFO | train_inner | epoch 124:    497 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.979, ppl=1.97, wps=16799.5, ups=1.6, wpb=10484.8, bsz=373.7, num_updates=69500, lr=8.31052e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 00:45:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:45:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:45:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:45:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:45:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:45:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:45:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:46:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:46:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:46:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:46:12 | INFO | valid | epoch 124 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.675 | ppl 12.77 | bleu 22.92 | wps 4618.7 | wpb 7508.5 | bsz 272.7 | num_updates 69564 | best_bleu 23.14
2021-01-08 00:46:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:46:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:46:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:46:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 124 @ 69564 updates, score 22.92) (writing took 2.819627383723855 seconds)
2021-01-08 00:46:15 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2021-01-08 00:46:15 | INFO | train | epoch 124 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.255 | nll_loss 0.976 | ppl 1.97 | wps 15690.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 69564 | lr 8.3067e-06 | gnorm 0.829 | train_wall 347 | wall 0
2021-01-08 00:46:15 | INFO | fairseq.trainer | begin training epoch 125
2021-01-08 00:46:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:46:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:46:40 | INFO | train_inner | epoch 125:     36 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.983, ppl=1.98, wps=11749.8, ups=1.13, wpb=10378.5, bsz=358.7, num_updates=69600, lr=8.30455e-06, gnorm=0.845, train_wall=61, wall=0
2021-01-08 00:47:42 | INFO | train_inner | epoch 125:    136 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.974, ppl=1.96, wps=16816.4, ups=1.62, wpb=10390.2, bsz=368.1, num_updates=69700, lr=8.29859e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 00:48:44 | INFO | train_inner | epoch 125:    236 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.976, ppl=1.97, wps=16971.3, ups=1.62, wpb=10505.2, bsz=377, num_updates=69800, lr=8.29264e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 00:49:46 | INFO | train_inner | epoch 125:    336 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.972, ppl=1.96, wps=17023.3, ups=1.61, wpb=10584.2, bsz=357.4, num_updates=69900, lr=8.28671e-06, gnorm=0.837, train_wall=62, wall=0
2021-01-08 00:50:48 | INFO | train_inner | epoch 125:    436 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.978, ppl=1.97, wps=16931, ups=1.61, wpb=10513.1, bsz=380.5, num_updates=70000, lr=8.28079e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 00:51:50 | INFO | train_inner | epoch 125:    536 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.976, ppl=1.97, wps=16990.1, ups=1.62, wpb=10504.4, bsz=363.2, num_updates=70100, lr=8.27488e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 00:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:52:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:52:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:52:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:52:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:52:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:52:26 | INFO | valid | epoch 125 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.675 | ppl 12.77 | bleu 23 | wps 4734.7 | wpb 7508.5 | bsz 272.7 | num_updates 70125 | best_bleu 23.14
2021-01-08 00:52:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:52:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:52:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 125 @ 70125 updates, score 23.0) (writing took 2.8205335456877947 seconds)
2021-01-08 00:52:28 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2021-01-08 00:52:28 | INFO | train | epoch 125 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.254 | nll_loss 0.975 | ppl 1.97 | wps 15750.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 70125 | lr 8.2734e-06 | gnorm 0.83 | train_wall 346 | wall 0
2021-01-08 00:52:28 | INFO | fairseq.trainer | begin training epoch 126
2021-01-08 00:52:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:52:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:53:17 | INFO | train_inner | epoch 126:     75 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.968, ppl=1.96, wps=11975.5, ups=1.14, wpb=10462.3, bsz=384.6, num_updates=70200, lr=8.26898e-06, gnorm=0.823, train_wall=60, wall=0
2021-01-08 00:54:19 | INFO | train_inner | epoch 126:    175 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.967, ppl=1.95, wps=17175.3, ups=1.62, wpb=10603.6, bsz=381.5, num_updates=70300, lr=8.2631e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 00:55:21 | INFO | train_inner | epoch 126:    275 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.974, ppl=1.96, wps=16921.3, ups=1.62, wpb=10473.9, bsz=369.4, num_updates=70400, lr=8.25723e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 00:56:22 | INFO | train_inner | epoch 126:    375 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.996, ppl=1.99, wps=16803.6, ups=1.62, wpb=10348.3, bsz=361.8, num_updates=70500, lr=8.25137e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-08 00:57:24 | INFO | train_inner | epoch 126:    475 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.965, ppl=1.95, wps=16913.4, ups=1.62, wpb=10418.3, bsz=367.7, num_updates=70600, lr=8.24552e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 00:58:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 00:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 00:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 00:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 00:58:37 | INFO | valid | epoch 126 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.677 | ppl 12.79 | bleu 23.02 | wps 4799.7 | wpb 7508.5 | bsz 272.7 | num_updates 70686 | best_bleu 23.14
2021-01-08 00:58:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 00:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 126 @ 70686 updates, score 23.02) (writing took 2.809133481234312 seconds)
2021-01-08 00:58:40 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2021-01-08 00:58:40 | INFO | train | epoch 126 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.254 | nll_loss 0.974 | ppl 1.96 | wps 15825.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 70686 | lr 8.24051e-06 | gnorm 0.828 | train_wall 344 | wall 0
2021-01-08 00:58:40 | INFO | fairseq.trainer | begin training epoch 127
2021-01-08 00:58:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 00:58:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 00:58:52 | INFO | train_inner | epoch 127:     14 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.976, ppl=1.97, wps=11967.3, ups=1.14, wpb=10526.9, bsz=356.7, num_updates=70700, lr=8.23969e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 00:59:53 | INFO | train_inner | epoch 127:    114 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.979, ppl=1.97, wps=17071.1, ups=1.64, wpb=10381.8, bsz=354.9, num_updates=70800, lr=8.23387e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 01:00:55 | INFO | train_inner | epoch 127:    214 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.97, ppl=1.96, wps=16925, ups=1.61, wpb=10507.1, bsz=379.6, num_updates=70900, lr=8.22806e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 01:01:57 | INFO | train_inner | epoch 127:    314 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.965, ppl=1.95, wps=16987.6, ups=1.61, wpb=10528.8, bsz=372.4, num_updates=71000, lr=8.22226e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 01:02:58 | INFO | train_inner | epoch 127:    414 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.979, ppl=1.97, wps=17063.6, ups=1.62, wpb=10513.1, bsz=379.4, num_updates=71100, lr=8.21648e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 01:04:00 | INFO | train_inner | epoch 127:    514 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.974, ppl=1.96, wps=17124, ups=1.63, wpb=10520.8, bsz=356.9, num_updates=71200, lr=8.21071e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-08 01:04:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:04:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:04:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:04:49 | INFO | valid | epoch 127 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.676 | ppl 12.78 | bleu 22.99 | wps 4765.8 | wpb 7508.5 | bsz 272.7 | num_updates 71247 | best_bleu 23.14
2021-01-08 01:04:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:04:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:04:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 127 @ 71247 updates, score 22.99) (writing took 2.808730823919177 seconds)
2021-01-08 01:04:52 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2021-01-08 01:04:52 | INFO | train | epoch 127 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.253 | nll_loss 0.974 | ppl 1.96 | wps 15816.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 71247 | lr 8.208e-06 | gnorm 0.832 | train_wall 344 | wall 0
2021-01-08 01:04:52 | INFO | fairseq.trainer | begin training epoch 128
2021-01-08 01:04:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:04:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:05:27 | INFO | train_inner | epoch 128:     53 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.967, ppl=1.95, wps=12078.1, ups=1.15, wpb=10524, bsz=389.6, num_updates=71300, lr=8.20495e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 01:06:29 | INFO | train_inner | epoch 128:    153 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.986, ppl=1.98, wps=16818.4, ups=1.61, wpb=10427.3, bsz=347.7, num_updates=71400, lr=8.1992e-06, gnorm=0.846, train_wall=62, wall=0
2021-01-08 01:07:31 | INFO | train_inner | epoch 128:    253 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.973, ppl=1.96, wps=16859.4, ups=1.61, wpb=10450, bsz=360.7, num_updates=71500, lr=8.19346e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 01:08:33 | INFO | train_inner | epoch 128:    353 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.97, ppl=1.96, wps=17054.8, ups=1.61, wpb=10606, bsz=391.2, num_updates=71600, lr=8.18774e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 01:09:35 | INFO | train_inner | epoch 128:    453 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.976, ppl=1.97, wps=16946.2, ups=1.62, wpb=10448.5, bsz=358.1, num_updates=71700, lr=8.18203e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-08 01:10:37 | INFO | train_inner | epoch 128:    553 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.978, ppl=1.97, wps=16900.7, ups=1.62, wpb=10441.7, bsz=376.9, num_updates=71800, lr=8.17633e-06, gnorm=0.826, train_wall=62, wall=0
2021-01-08 01:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:10:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:10:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:10:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:10:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:10:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:10:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:10:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:10:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:10:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:10:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:11:02 | INFO | valid | epoch 128 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.677 | ppl 12.79 | bleu 23.02 | wps 4702.5 | wpb 7508.5 | bsz 272.7 | num_updates 71808 | best_bleu 23.14
2021-01-08 01:11:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:11:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:11:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:11:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:11:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 128 @ 71808 updates, score 23.02) (writing took 2.7998658884316683 seconds)
2021-01-08 01:11:05 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2021-01-08 01:11:05 | INFO | train | epoch 128 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.253 | nll_loss 0.974 | ppl 1.96 | wps 15771.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 71808 | lr 8.17587e-06 | gnorm 0.832 | train_wall 345 | wall 0
2021-01-08 01:11:05 | INFO | fairseq.trainer | begin training epoch 129
2021-01-08 01:11:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:11:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:12:04 | INFO | train_inner | epoch 129:     92 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.964, ppl=1.95, wps=11799.6, ups=1.14, wpb=10349.7, bsz=379.8, num_updates=71900, lr=8.17064e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 01:13:06 | INFO | train_inner | epoch 129:    192 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.978, ppl=1.97, wps=17040.7, ups=1.63, wpb=10468.5, bsz=374.8, num_updates=72000, lr=8.16497e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 01:14:07 | INFO | train_inner | epoch 129:    292 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.97, ppl=1.96, wps=17144.6, ups=1.63, wpb=10500.2, bsz=396.3, num_updates=72100, lr=8.1593e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 01:15:09 | INFO | train_inner | epoch 129:    392 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.969, ppl=1.96, wps=17160.2, ups=1.61, wpb=10625.6, bsz=353.6, num_updates=72200, lr=8.15365e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 01:16:11 | INFO | train_inner | epoch 129:    492 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.986, ppl=1.98, wps=16932.2, ups=1.61, wpb=10485.4, bsz=364.5, num_updates=72300, lr=8.14801e-06, gnorm=0.846, train_wall=62, wall=0
2021-01-08 01:16:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:16:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:16:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:16:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:16:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:16:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:16:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:17:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:17:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:17:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:17:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:17:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:17:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:17:14 | INFO | valid | epoch 129 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.674 | ppl 12.77 | bleu 23 | wps 4531.8 | wpb 7508.5 | bsz 272.7 | num_updates 72369 | best_bleu 23.14
2021-01-08 01:17:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:17:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:17:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:17:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:17:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 129 @ 72369 updates, score 23.0) (writing took 2.8147104680538177 seconds)
2021-01-08 01:17:17 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2021-01-08 01:17:17 | INFO | train | epoch 129 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.252 | nll_loss 0.974 | ppl 1.96 | wps 15786.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 72369 | lr 8.14412e-06 | gnorm 0.833 | train_wall 344 | wall 0
2021-01-08 01:17:17 | INFO | fairseq.trainer | begin training epoch 130
2021-01-08 01:17:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:17:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:17:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:17:39 | INFO | train_inner | epoch 130:     31 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.972, ppl=1.96, wps=11833, ups=1.13, wpb=10465.5, bsz=347, num_updates=72400, lr=8.14238e-06, gnorm=0.836, train_wall=61, wall=0
2021-01-08 01:18:41 | INFO | train_inner | epoch 130:    131 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.969, ppl=1.96, wps=17246.6, ups=1.63, wpb=10573.7, bsz=376.2, num_updates=72500, lr=8.13676e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 01:19:42 | INFO | train_inner | epoch 130:    231 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.968, ppl=1.96, wps=16989.2, ups=1.63, wpb=10435.8, bsz=363.8, num_updates=72600, lr=8.13116e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 01:20:44 | INFO | train_inner | epoch 130:    331 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.277, nll_loss=0.994, ppl=1.99, wps=16757, ups=1.62, wpb=10375.1, bsz=368.6, num_updates=72700, lr=8.12556e-06, gnorm=0.845, train_wall=62, wall=0
2021-01-08 01:21:46 | INFO | train_inner | epoch 130:    431 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.974, ppl=1.96, wps=16948.4, ups=1.62, wpb=10461.4, bsz=362.6, num_updates=72800, lr=8.11998e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 01:22:48 | INFO | train_inner | epoch 130:    531 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.964, ppl=1.95, wps=16939.7, ups=1.6, wpb=10573.9, bsz=382.3, num_updates=72900, lr=8.11441e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 01:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:23:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:23:27 | INFO | valid | epoch 130 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23 | wps 4730.6 | wpb 7508.5 | bsz 272.7 | num_updates 72930 | best_bleu 23.14
2021-01-08 01:23:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 130 @ 72930 updates, score 23.0) (writing took 2.8251812532544136 seconds)
2021-01-08 01:23:30 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2021-01-08 01:23:30 | INFO | train | epoch 130 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.251 | nll_loss 0.974 | ppl 1.96 | wps 15782.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 72930 | lr 8.11274e-06 | gnorm 0.832 | train_wall 345 | wall 0
2021-01-08 01:23:30 | INFO | fairseq.trainer | begin training epoch 131
2021-01-08 01:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:23:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:24:16 | INFO | train_inner | epoch 131:     70 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.973, ppl=1.96, wps=11961.6, ups=1.14, wpb=10529, bsz=379.6, num_updates=73000, lr=8.10885e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 01:25:18 | INFO | train_inner | epoch 131:    170 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.965, ppl=1.95, wps=16654.1, ups=1.6, wpb=10393.9, bsz=360.8, num_updates=73100, lr=8.1033e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 01:26:20 | INFO | train_inner | epoch 131:    270 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.994, ppl=1.99, wps=17058, ups=1.62, wpb=10550.2, bsz=373.8, num_updates=73200, lr=8.09776e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 01:27:22 | INFO | train_inner | epoch 131:    370 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.976, ppl=1.97, wps=16701.3, ups=1.61, wpb=10368.5, bsz=365.8, num_updates=73300, lr=8.09224e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 01:28:24 | INFO | train_inner | epoch 131:    470 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.963, ppl=1.95, wps=17122.6, ups=1.61, wpb=10624, bsz=372.2, num_updates=73400, lr=8.08672e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 01:29:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:29:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:29:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:29:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:29:41 | INFO | valid | epoch 131 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.678 | ppl 12.8 | bleu 23.09 | wps 4711 | wpb 7508.5 | bsz 272.7 | num_updates 73491 | best_bleu 23.14
2021-01-08 01:29:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 131 @ 73491 updates, score 23.09) (writing took 2.788070533424616 seconds)
2021-01-08 01:29:44 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2021-01-08 01:29:44 | INFO | train | epoch 131 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.25 | nll_loss 0.973 | ppl 1.96 | wps 15717.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 73491 | lr 8.08172e-06 | gnorm 0.824 | train_wall 346 | wall 0
2021-01-08 01:29:44 | INFO | fairseq.trainer | begin training epoch 132
2021-01-08 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:29:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:29:53 | INFO | train_inner | epoch 132:      9 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.975, ppl=1.97, wps=11761.4, ups=1.13, wpb=10393.4, bsz=362.4, num_updates=73500, lr=8.08122e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-08 01:30:55 | INFO | train_inner | epoch 132:    109 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.985, ppl=1.98, wps=17099.9, ups=1.62, wpb=10585.9, bsz=363.9, num_updates=73600, lr=8.07573e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-08 01:31:57 | INFO | train_inner | epoch 132:    209 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.966, ppl=1.95, wps=16875.5, ups=1.6, wpb=10547.7, bsz=377.2, num_updates=73700, lr=8.07025e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 01:33:00 | INFO | train_inner | epoch 132:    309 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.957, ppl=1.94, wps=17013, ups=1.59, wpb=10693.8, bsz=377.1, num_updates=73800, lr=8.06478e-06, gnorm=0.81, train_wall=63, wall=0
2021-01-08 01:34:03 | INFO | train_inner | epoch 132:    409 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.974, ppl=1.96, wps=16649.5, ups=1.6, wpb=10414.1, bsz=359.6, num_updates=73900, lr=8.05932e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 01:35:04 | INFO | train_inner | epoch 132:    509 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.977, ppl=1.97, wps=16846.1, ups=1.63, wpb=10338.3, bsz=377.4, num_updates=74000, lr=8.05387e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 01:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:35:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:35:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:35:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:35:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:35:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:35:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:35:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:35:56 | INFO | valid | epoch 132 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.77 | bleu 23.1 | wps 4770.9 | wpb 7508.5 | bsz 272.7 | num_updates 74052 | best_bleu 23.14
2021-01-08 01:35:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:35:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:35:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:35:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:35:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:35:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 132 @ 74052 updates, score 23.1) (writing took 2.8031879998743534 seconds)
2021-01-08 01:35:59 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2021-01-08 01:35:59 | INFO | train | epoch 132 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.25 | nll_loss 0.973 | ppl 1.96 | wps 15690.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 74052 | lr 8.05104e-06 | gnorm 0.827 | train_wall 347 | wall 0
2021-01-08 01:35:59 | INFO | fairseq.trainer | begin training epoch 133
2021-01-08 01:36:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:36:31 | INFO | train_inner | epoch 133:     48 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.969, ppl=1.96, wps=11843.4, ups=1.14, wpb=10357.6, bsz=368.8, num_updates=74100, lr=8.04844e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-08 01:37:33 | INFO | train_inner | epoch 133:    148 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.976, ppl=1.97, wps=16812.9, ups=1.61, wpb=10430.8, bsz=374.3, num_updates=74200, lr=8.04301e-06, gnorm=0.842, train_wall=62, wall=0
2021-01-08 01:38:35 | INFO | train_inner | epoch 133:    248 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.982, ppl=1.98, wps=16908, ups=1.62, wpb=10460.1, bsz=367, num_updates=74300, lr=8.0376e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 01:39:37 | INFO | train_inner | epoch 133:    348 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.973, ppl=1.96, wps=17079.6, ups=1.63, wpb=10485.7, bsz=363.1, num_updates=74400, lr=8.03219e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 01:40:39 | INFO | train_inner | epoch 133:    448 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.979, ppl=1.97, wps=16953.2, ups=1.6, wpb=10565.8, bsz=380.2, num_updates=74500, lr=8.0268e-06, gnorm=0.809, train_wall=62, wall=0
2021-01-08 01:41:41 | INFO | train_inner | epoch 133:    548 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.964, ppl=1.95, wps=16844.3, ups=1.6, wpb=10501.4, bsz=360.1, num_updates=74600, lr=8.02142e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 01:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:41:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:41:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:42:10 | INFO | valid | epoch 133 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.77 | bleu 23.05 | wps 4690.1 | wpb 7508.5 | bsz 272.7 | num_updates 74613 | best_bleu 23.14
2021-01-08 01:42:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:42:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:42:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:42:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:42:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:42:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 133 @ 74613 updates, score 23.05) (writing took 2.795319676399231 seconds)
2021-01-08 01:42:13 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2021-01-08 01:42:13 | INFO | train | epoch 133 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.249 | nll_loss 0.973 | ppl 1.96 | wps 15738.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 74613 | lr 8.02072e-06 | gnorm 0.828 | train_wall 346 | wall 0
2021-01-08 01:42:13 | INFO | fairseq.trainer | begin training epoch 134
2021-01-08 01:42:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:43:09 | INFO | train_inner | epoch 134:     87 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.963, ppl=1.95, wps=11828.5, ups=1.14, wpb=10354.4, bsz=369.2, num_updates=74700, lr=8.01605e-06, gnorm=0.845, train_wall=61, wall=0
2021-01-08 01:44:10 | INFO | train_inner | epoch 134:    187 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.975, ppl=1.97, wps=16972.3, ups=1.63, wpb=10432.7, bsz=375.8, num_updates=74800, lr=8.01069e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 01:45:13 | INFO | train_inner | epoch 134:    287 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.978, ppl=1.97, wps=16850.1, ups=1.61, wpb=10488.2, bsz=370.4, num_updates=74900, lr=8.00534e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 01:46:15 | INFO | train_inner | epoch 134:    387 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.268, nll_loss=0.984, ppl=1.98, wps=16820.1, ups=1.62, wpb=10411, bsz=356.1, num_updates=75000, lr=8e-06, gnorm=0.84, train_wall=62, wall=0
2021-01-08 01:47:16 | INFO | train_inner | epoch 134:    487 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.966, ppl=1.95, wps=17229.2, ups=1.62, wpb=10643.5, bsz=360.5, num_updates=75100, lr=7.99467e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 01:48:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:48:22 | INFO | valid | epoch 134 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.674 | ppl 12.76 | bleu 23.09 | wps 4731.7 | wpb 7508.5 | bsz 272.7 | num_updates 75174 | best_bleu 23.14
2021-01-08 01:48:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:48:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:25 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 134 @ 75174 updates, score 23.09) (writing took 2.7806640788912773 seconds)
2021-01-08 01:48:25 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2021-01-08 01:48:25 | INFO | train | epoch 134 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.25 | nll_loss 0.973 | ppl 1.96 | wps 15781.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 75174 | lr 7.99074e-06 | gnorm 0.829 | train_wall 345 | wall 0
2021-01-08 01:48:25 | INFO | fairseq.trainer | begin training epoch 135
2021-01-08 01:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:48:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:48:44 | INFO | train_inner | epoch 135:     26 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.984, ppl=1.98, wps=11846.8, ups=1.14, wpb=10413.3, bsz=377.8, num_updates=75200, lr=7.98935e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 01:49:46 | INFO | train_inner | epoch 135:    126 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.981, ppl=1.97, wps=17128.4, ups=1.63, wpb=10525.2, bsz=370.8, num_updates=75300, lr=7.98405e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 01:50:48 | INFO | train_inner | epoch 135:    226 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.971, ppl=1.96, wps=17177.4, ups=1.62, wpb=10631.2, bsz=383, num_updates=75400, lr=7.97875e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 01:51:49 | INFO | train_inner | epoch 135:    326 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.97, ppl=1.96, wps=17099.1, ups=1.62, wpb=10530.1, bsz=372.2, num_updates=75500, lr=7.97347e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 01:52:50 | INFO | train_inner | epoch 135:    426 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.964, ppl=1.95, wps=17048.2, ups=1.63, wpb=10446.6, bsz=358.9, num_updates=75600, lr=7.96819e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 01:53:52 | INFO | train_inner | epoch 135:    526 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.971, ppl=1.96, wps=16810.9, ups=1.62, wpb=10388.3, bsz=370.1, num_updates=75700, lr=7.96293e-06, gnorm=0.835, train_wall=62, wall=0
2021-01-08 01:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 01:54:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:54:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:54:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:54:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 01:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 01:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 01:54:34 | INFO | valid | epoch 135 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.184 | nll_loss 3.678 | ppl 12.8 | bleu 23.11 | wps 4724.4 | wpb 7508.5 | bsz 272.7 | num_updates 75735 | best_bleu 23.14
2021-01-08 01:54:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 01:54:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:54:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:54:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 135 @ 75735 updates, score 23.11) (writing took 2.8544972334057093 seconds)
2021-01-08 01:54:37 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2021-01-08 01:54:37 | INFO | train | epoch 135 | symm_kl 0.357 | self_kl 0 | self_cv 0 | loss 3.249 | nll_loss 0.972 | ppl 1.96 | wps 15812.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 75735 | lr 7.96109e-06 | gnorm 0.829 | train_wall 344 | wall 0
2021-01-08 01:54:37 | INFO | fairseq.trainer | begin training epoch 136
2021-01-08 01:54:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 01:54:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 01:55:20 | INFO | train_inner | epoch 136:     65 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.976, ppl=1.97, wps=11885, ups=1.14, wpb=10398.3, bsz=358.2, num_updates=75800, lr=7.95767e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-08 01:56:21 | INFO | train_inner | epoch 136:    165 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.98, ppl=1.97, wps=17024.9, ups=1.63, wpb=10431.4, bsz=360, num_updates=75900, lr=7.95243e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 01:57:22 | INFO | train_inner | epoch 136:    265 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.968, ppl=1.96, wps=17087.3, ups=1.63, wpb=10489.8, bsz=379.8, num_updates=76000, lr=7.94719e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 01:58:24 | INFO | train_inner | epoch 136:    365 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.966, ppl=1.95, wps=17105.9, ups=1.62, wpb=10536.5, bsz=379.3, num_updates=76100, lr=7.94197e-06, gnorm=0.814, train_wall=61, wall=0
2021-01-08 01:59:25 | INFO | train_inner | epoch 136:    465 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.966, ppl=1.95, wps=17104.1, ups=1.63, wpb=10496.9, bsz=372.5, num_updates=76200, lr=7.93676e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 02:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:00:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:00:45 | INFO | valid | epoch 136 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.75 | bleu 23.18 | wps 4739.2 | wpb 7508.5 | bsz 272.7 | num_updates 76296 | best_bleu 23.18
2021-01-08 02:00:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:00:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 136 @ 76296 updates, score 23.18) (writing took 4.823430322110653 seconds)
2021-01-08 02:00:50 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2021-01-08 02:00:50 | INFO | train | epoch 136 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.248 | nll_loss 0.972 | ppl 1.96 | wps 15777.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 76296 | lr 7.93176e-06 | gnorm 0.825 | train_wall 343 | wall 0
2021-01-08 02:00:50 | INFO | fairseq.trainer | begin training epoch 137
2021-01-08 02:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:00:56 | INFO | train_inner | epoch 137:      4 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.97, ppl=1.96, wps=11660.7, ups=1.11, wpb=10521.6, bsz=367.4, num_updates=76300, lr=7.93156e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 02:01:56 | INFO | train_inner | epoch 137:    104 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.982, ppl=1.98, wps=17171.7, ups=1.64, wpb=10442.6, bsz=353.8, num_updates=76400, lr=7.92636e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 02:02:58 | INFO | train_inner | epoch 137:    204 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.955, ppl=1.94, wps=17168.1, ups=1.63, wpb=10556, bsz=387.8, num_updates=76500, lr=7.92118e-06, gnorm=0.81, train_wall=61, wall=0
2021-01-08 02:04:00 | INFO | train_inner | epoch 137:    304 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.961, ppl=1.95, wps=17130.4, ups=1.61, wpb=10617.4, bsz=375, num_updates=76600, lr=7.91601e-06, gnorm=0.809, train_wall=62, wall=0
2021-01-08 02:05:02 | INFO | train_inner | epoch 137:    404 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.981, ppl=1.97, wps=17002.5, ups=1.62, wpb=10506.6, bsz=379.9, num_updates=76700, lr=7.91085e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 02:06:04 | INFO | train_inner | epoch 137:    504 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.976, ppl=1.97, wps=16885.3, ups=1.61, wpb=10475.1, bsz=361.7, num_updates=76800, lr=7.90569e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 02:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:06:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:06:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:06:59 | INFO | valid | epoch 137 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.675 | ppl 12.77 | bleu 23.02 | wps 4701.7 | wpb 7508.5 | bsz 272.7 | num_updates 76857 | best_bleu 23.18
2021-01-08 02:06:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:07:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:07:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:07:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:07:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:07:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 137 @ 76857 updates, score 23.02) (writing took 2.881773328408599 seconds)
2021-01-08 02:07:02 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2021-01-08 02:07:02 | INFO | train | epoch 137 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.248 | nll_loss 0.972 | ppl 1.96 | wps 15810.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 76857 | lr 7.90276e-06 | gnorm 0.822 | train_wall 344 | wall 0
2021-01-08 02:07:02 | INFO | fairseq.trainer | begin training epoch 138
2021-01-08 02:07:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:07:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:07:31 | INFO | train_inner | epoch 138:     43 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.971, ppl=1.96, wps=11822.3, ups=1.14, wpb=10349.7, bsz=349.7, num_updates=76900, lr=7.90055e-06, gnorm=0.843, train_wall=61, wall=0
2021-01-08 02:08:33 | INFO | train_inner | epoch 138:    143 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.97, ppl=1.96, wps=16926.1, ups=1.62, wpb=10441.3, bsz=368.6, num_updates=77000, lr=7.89542e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 02:09:35 | INFO | train_inner | epoch 138:    243 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.971, ppl=1.96, wps=17030.7, ups=1.61, wpb=10568.2, bsz=374.2, num_updates=77100, lr=7.8903e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 02:10:37 | INFO | train_inner | epoch 138:    343 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.97, ppl=1.96, wps=16702.8, ups=1.6, wpb=10418, bsz=371.9, num_updates=77200, lr=7.88519e-06, gnorm=0.838, train_wall=62, wall=0
2021-01-08 02:11:39 | INFO | train_inner | epoch 138:    443 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.98, ppl=1.97, wps=17058.1, ups=1.63, wpb=10469.6, bsz=372.6, num_updates=77300, lr=7.88008e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 02:12:41 | INFO | train_inner | epoch 138:    543 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.966, ppl=1.95, wps=16976.6, ups=1.61, wpb=10547.6, bsz=372.6, num_updates=77400, lr=7.87499e-06, gnorm=0.811, train_wall=62, wall=0
2021-01-08 02:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:12:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:12:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:12:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:12:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:12:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:12:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:12:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:12:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:13:15 | INFO | valid | epoch 138 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.676 | ppl 12.78 | bleu 22.96 | wps 4105.5 | wpb 7508.5 | bsz 272.7 | num_updates 77418 | best_bleu 23.18
2021-01-08 02:13:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:13:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:13:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:13:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:13:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:13:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 138 @ 77418 updates, score 22.96) (writing took 2.8680062163621187 seconds)
2021-01-08 02:13:18 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2021-01-08 02:13:18 | INFO | train | epoch 138 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.247 | nll_loss 0.972 | ppl 1.96 | wps 15643.8 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 77418 | lr 7.87408e-06 | gnorm 0.825 | train_wall 346 | wall 0
2021-01-08 02:13:18 | INFO | fairseq.trainer | begin training epoch 139
2021-01-08 02:13:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:13:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:14:11 | INFO | train_inner | epoch 139:     82 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.968, ppl=1.96, wps=11640.7, ups=1.11, wpb=10500, bsz=377.7, num_updates=77500, lr=7.86991e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 02:15:13 | INFO | train_inner | epoch 139:    182 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.96, ppl=1.95, wps=16962.2, ups=1.62, wpb=10501.1, bsz=372.3, num_updates=77600, lr=7.86484e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 02:16:15 | INFO | train_inner | epoch 139:    282 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.26, nll_loss=0.979, ppl=1.97, wps=16997, ups=1.61, wpb=10536, bsz=354.8, num_updates=77700, lr=7.85977e-06, gnorm=0.84, train_wall=62, wall=0
2021-01-08 02:17:17 | INFO | train_inner | epoch 139:    382 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.982, ppl=1.98, wps=16940, ups=1.62, wpb=10472.6, bsz=385.4, num_updates=77800, lr=7.85472e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 02:18:18 | INFO | train_inner | epoch 139:    482 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.981, ppl=1.97, wps=16999.8, ups=1.62, wpb=10486.8, bsz=363.2, num_updates=77900, lr=7.84968e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 02:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:19:28 | INFO | valid | epoch 139 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23.18 | wps 4741.4 | wpb 7508.5 | bsz 272.7 | num_updates 77979 | best_bleu 23.18
2021-01-08 02:19:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 139 @ 77979 updates, score 23.18) (writing took 4.903995826840401 seconds)
2021-01-08 02:19:33 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2021-01-08 02:19:33 | INFO | train | epoch 139 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.247 | nll_loss 0.972 | ppl 1.96 | wps 15694.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 77979 | lr 7.8457e-06 | gnorm 0.827 | train_wall 345 | wall 0
2021-01-08 02:19:33 | INFO | fairseq.trainer | begin training epoch 140
2021-01-08 02:19:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:19:48 | INFO | train_inner | epoch 140:     21 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.967, ppl=1.95, wps=11476, ups=1.11, wpb=10327, bsz=357.1, num_updates=78000, lr=7.84465e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 02:20:50 | INFO | train_inner | epoch 140:    121 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.973, ppl=1.96, wps=17183.2, ups=1.63, wpb=10521.8, bsz=379.1, num_updates=78100, lr=7.83962e-06, gnorm=0.844, train_wall=61, wall=0
2021-01-08 02:21:52 | INFO | train_inner | epoch 140:    221 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.965, ppl=1.95, wps=16779.7, ups=1.61, wpb=10436.9, bsz=365, num_updates=78200, lr=7.83461e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 02:22:53 | INFO | train_inner | epoch 140:    321 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.98, ppl=1.97, wps=16994, ups=1.62, wpb=10467.4, bsz=389.4, num_updates=78300, lr=7.8296e-06, gnorm=0.813, train_wall=61, wall=0
2021-01-08 02:23:55 | INFO | train_inner | epoch 140:    421 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.97, ppl=1.96, wps=17205.5, ups=1.63, wpb=10582.9, bsz=355.1, num_updates=78400, lr=7.82461e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 02:24:57 | INFO | train_inner | epoch 140:    521 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.983, ppl=1.98, wps=16862, ups=1.62, wpb=10430.2, bsz=354.1, num_updates=78500, lr=7.81962e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-08 02:25:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:25:42 | INFO | valid | epoch 140 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.672 | ppl 12.75 | bleu 23.17 | wps 4692.3 | wpb 7508.5 | bsz 272.7 | num_updates 78540 | best_bleu 23.18
2021-01-08 02:25:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:25:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:25:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:25:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 140 @ 78540 updates, score 23.17) (writing took 2.7892349492758512 seconds)
2021-01-08 02:25:45 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2021-01-08 02:25:45 | INFO | train | epoch 140 | symm_kl 0.356 | self_kl 0 | self_cv 0 | loss 3.247 | nll_loss 0.972 | ppl 1.96 | wps 15796.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 78540 | lr 7.81763e-06 | gnorm 0.829 | train_wall 345 | wall 0
2021-01-08 02:25:45 | INFO | fairseq.trainer | begin training epoch 141
2021-01-08 02:25:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:25:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:26:25 | INFO | train_inner | epoch 141:     60 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.975, ppl=1.97, wps=11826.5, ups=1.14, wpb=10391.1, bsz=381, num_updates=78600, lr=7.81465e-06, gnorm=0.84, train_wall=61, wall=0
2021-01-08 02:27:27 | INFO | train_inner | epoch 141:    160 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.977, ppl=1.97, wps=16925.7, ups=1.62, wpb=10463, bsz=370.2, num_updates=78700, lr=7.80968e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-08 02:28:29 | INFO | train_inner | epoch 141:    260 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.975, ppl=1.97, wps=16727, ups=1.6, wpb=10444.9, bsz=370.3, num_updates=78800, lr=7.80472e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 02:29:31 | INFO | train_inner | epoch 141:    360 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.951, ppl=1.93, wps=17085.4, ups=1.61, wpb=10613.5, bsz=369, num_updates=78900, lr=7.79978e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 02:30:33 | INFO | train_inner | epoch 141:    460 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.975, ppl=1.97, wps=16998.9, ups=1.61, wpb=10538.9, bsz=381.1, num_updates=79000, lr=7.79484e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 02:31:35 | INFO | train_inner | epoch 141:    560 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.968, ppl=1.96, wps=16869.1, ups=1.61, wpb=10481.5, bsz=358.1, num_updates=79100, lr=7.78991e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-08 02:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:31:56 | INFO | valid | epoch 141 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.78 | bleu 23.03 | wps 4720.1 | wpb 7508.5 | bsz 272.7 | num_updates 79101 | best_bleu 23.18
2021-01-08 02:31:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:31:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 141 @ 79101 updates, score 23.03) (writing took 2.8230849634855986 seconds)
2021-01-08 02:31:59 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2021-01-08 02:31:59 | INFO | train | epoch 141 | symm_kl 0.355 | self_kl 0 | self_cv 0 | loss 3.245 | nll_loss 0.971 | ppl 1.96 | wps 15717.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 79101 | lr 7.78986e-06 | gnorm 0.824 | train_wall 346 | wall 0
2021-01-08 02:31:59 | INFO | fairseq.trainer | begin training epoch 142
2021-01-08 02:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:33:03 | INFO | train_inner | epoch 142:     99 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.969, ppl=1.96, wps=11998.8, ups=1.14, wpb=10536.8, bsz=374.2, num_updates=79200, lr=7.78499e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 02:34:05 | INFO | train_inner | epoch 142:    199 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.98, ppl=1.97, wps=17130.2, ups=1.62, wpb=10545.1, bsz=372.9, num_updates=79300, lr=7.78008e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 02:35:06 | INFO | train_inner | epoch 142:    299 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.959, ppl=1.94, wps=16923.9, ups=1.62, wpb=10439.3, bsz=374.3, num_updates=79400, lr=7.77518e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 02:36:08 | INFO | train_inner | epoch 142:    399 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.97, ppl=1.96, wps=17035.3, ups=1.62, wpb=10533.4, bsz=364.2, num_updates=79500, lr=7.77029e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 02:37:10 | INFO | train_inner | epoch 142:    499 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.975, ppl=1.97, wps=16985.4, ups=1.62, wpb=10490.5, bsz=363, num_updates=79600, lr=7.7654e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 02:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:37:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:37:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:37:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:38:08 | INFO | valid | epoch 142 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.674 | ppl 12.77 | bleu 23.13 | wps 4736.2 | wpb 7508.5 | bsz 272.7 | num_updates 79662 | best_bleu 23.18
2021-01-08 02:38:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:38:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:38:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:38:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:38:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 142 @ 79662 updates, score 23.13) (writing took 2.857103256508708 seconds)
2021-01-08 02:38:11 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2021-01-08 02:38:11 | INFO | train | epoch 142 | symm_kl 0.355 | self_kl 0 | self_cv 0 | loss 3.245 | nll_loss 0.971 | ppl 1.96 | wps 15797.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 79662 | lr 7.76238e-06 | gnorm 0.827 | train_wall 345 | wall 0
2021-01-08 02:38:11 | INFO | fairseq.trainer | begin training epoch 143
2021-01-08 02:38:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:38:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:38:37 | INFO | train_inner | epoch 143:     38 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.964, ppl=1.95, wps=11733.9, ups=1.14, wpb=10276.2, bsz=365.1, num_updates=79700, lr=7.76053e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 02:39:39 | INFO | train_inner | epoch 143:    138 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.967, ppl=1.95, wps=17038, ups=1.63, wpb=10468.8, bsz=365.1, num_updates=79800, lr=7.75567e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 02:40:40 | INFO | train_inner | epoch 143:    238 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.972, ppl=1.96, wps=17225.9, ups=1.63, wpb=10588.6, bsz=363, num_updates=79900, lr=7.75081e-06, gnorm=0.812, train_wall=61, wall=0
2021-01-08 02:41:42 | INFO | train_inner | epoch 143:    338 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.966, ppl=1.95, wps=17224.6, ups=1.63, wpb=10593.4, bsz=373.7, num_updates=80000, lr=7.74597e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 02:42:44 | INFO | train_inner | epoch 143:    438 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.98, ppl=1.97, wps=17040.6, ups=1.62, wpb=10503.1, bsz=381.1, num_updates=80100, lr=7.74113e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 02:43:45 | INFO | train_inner | epoch 143:    538 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.972, ppl=1.96, wps=16829.4, ups=1.62, wpb=10394.4, bsz=369, num_updates=80200, lr=7.7363e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 02:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:44:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:44:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:44:20 | INFO | valid | epoch 143 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.674 | ppl 12.77 | bleu 23.13 | wps 4732.6 | wpb 7508.5 | bsz 272.7 | num_updates 80223 | best_bleu 23.18
2021-01-08 02:44:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:44:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:44:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 143 @ 80223 updates, score 23.13) (writing took 2.855159092694521 seconds)
2021-01-08 02:44:23 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2021-01-08 02:44:23 | INFO | train | epoch 143 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.244 | nll_loss 0.971 | ppl 1.96 | wps 15837.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 80223 | lr 7.73519e-06 | gnorm 0.822 | train_wall 344 | wall 0
2021-01-08 02:44:23 | INFO | fairseq.trainer | begin training epoch 144
2021-01-08 02:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:45:12 | INFO | train_inner | epoch 144:     77 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.98, ppl=1.97, wps=11994, ups=1.15, wpb=10458.4, bsz=376.9, num_updates=80300, lr=7.73148e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 02:46:14 | INFO | train_inner | epoch 144:    177 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.953, ppl=1.94, wps=17180.2, ups=1.62, wpb=10575, bsz=380.6, num_updates=80400, lr=7.72667e-06, gnorm=0.806, train_wall=61, wall=0
2021-01-08 02:47:15 | INFO | train_inner | epoch 144:    277 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.976, ppl=1.97, wps=17145.7, ups=1.63, wpb=10527.2, bsz=365, num_updates=80500, lr=7.72187e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 02:48:17 | INFO | train_inner | epoch 144:    377 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.975, ppl=1.97, wps=16835.1, ups=1.63, wpb=10314.7, bsz=380.6, num_updates=80600, lr=7.71708e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 02:49:18 | INFO | train_inner | epoch 144:    477 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.974, ppl=1.96, wps=17003, ups=1.63, wpb=10441.3, bsz=355, num_updates=80700, lr=7.7123e-06, gnorm=0.836, train_wall=61, wall=0
2021-01-08 02:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:50:30 | INFO | valid | epoch 144 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.676 | ppl 12.78 | bleu 23 | wps 4758.6 | wpb 7508.5 | bsz 272.7 | num_updates 80784 | best_bleu 23.18
2021-01-08 02:50:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:50:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 144 @ 80784 updates, score 23.0) (writing took 2.845910955220461 seconds)
2021-01-08 02:50:33 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2021-01-08 02:50:33 | INFO | train | epoch 144 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.244 | nll_loss 0.97 | ppl 1.96 | wps 15885.1 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 80784 | lr 7.70829e-06 | gnorm 0.822 | train_wall 343 | wall 0
2021-01-08 02:50:33 | INFO | fairseq.trainer | begin training epoch 145
2021-01-08 02:50:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:50:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:50:46 | INFO | train_inner | epoch 145:     16 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.965, ppl=1.95, wps=11845.2, ups=1.14, wpb=10392.5, bsz=357.2, num_updates=80800, lr=7.70752e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 02:51:47 | INFO | train_inner | epoch 145:    116 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.965, ppl=1.95, wps=17187.5, ups=1.64, wpb=10500.2, bsz=368.5, num_updates=80900, lr=7.70276e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 02:52:49 | INFO | train_inner | epoch 145:    216 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.969, ppl=1.96, wps=17234.8, ups=1.62, wpb=10611.6, bsz=388.5, num_updates=81000, lr=7.698e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 02:53:50 | INFO | train_inner | epoch 145:    316 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.971, ppl=1.96, wps=17111.1, ups=1.62, wpb=10542.5, bsz=379.4, num_updates=81100, lr=7.69326e-06, gnorm=0.814, train_wall=61, wall=0
2021-01-08 02:54:52 | INFO | train_inner | epoch 145:    416 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.974, ppl=1.96, wps=16736.7, ups=1.61, wpb=10388.6, bsz=353.4, num_updates=81200, lr=7.68852e-06, gnorm=0.856, train_wall=62, wall=0
2021-01-08 02:55:54 | INFO | train_inner | epoch 145:    516 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.975, ppl=1.97, wps=17130.6, ups=1.63, wpb=10513.6, bsz=362.7, num_updates=81300, lr=7.68379e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 02:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 02:56:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:56:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:56:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 02:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 02:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 02:56:42 | INFO | valid | epoch 145 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.77 | bleu 23.03 | wps 4693.2 | wpb 7508.5 | bsz 272.7 | num_updates 81345 | best_bleu 23.18
2021-01-08 02:56:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 02:56:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 145 @ 81345 updates, score 23.03) (writing took 2.8515372443944216 seconds)
2021-01-08 02:56:45 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2021-01-08 02:56:45 | INFO | train | epoch 145 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.243 | nll_loss 0.97 | ppl 1.96 | wps 15814.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 81345 | lr 7.68166e-06 | gnorm 0.83 | train_wall 344 | wall 0
2021-01-08 02:56:45 | INFO | fairseq.trainer | begin training epoch 146
2021-01-08 02:56:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:56:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:56:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 02:56:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 02:57:21 | INFO | train_inner | epoch 146:     55 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.975, ppl=1.97, wps=11816.5, ups=1.14, wpb=10352.9, bsz=358.1, num_updates=81400, lr=7.67907e-06, gnorm=0.839, train_wall=61, wall=0
2021-01-08 02:58:23 | INFO | train_inner | epoch 146:    155 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.963, ppl=1.95, wps=16821.8, ups=1.61, wpb=10449.6, bsz=371, num_updates=81500, lr=7.67435e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 02:59:25 | INFO | train_inner | epoch 146:    255 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.988, ppl=1.98, wps=17006.9, ups=1.62, wpb=10511.3, bsz=387.8, num_updates=81600, lr=7.66965e-06, gnorm=0.811, train_wall=62, wall=0
2021-01-08 03:00:27 | INFO | train_inner | epoch 146:    355 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.978, ppl=1.97, wps=17255.2, ups=1.63, wpb=10615.6, bsz=354.9, num_updates=81700, lr=7.66495e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 03:01:28 | INFO | train_inner | epoch 146:    455 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.96, ppl=1.95, wps=17073.3, ups=1.62, wpb=10548.8, bsz=364.4, num_updates=81800, lr=7.66027e-06, gnorm=0.815, train_wall=62, wall=0
2021-01-08 03:02:30 | INFO | train_inner | epoch 146:    555 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.958, ppl=1.94, wps=16852.5, ups=1.62, wpb=10433.6, bsz=382.9, num_updates=81900, lr=7.65559e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 03:02:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:02:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:02:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:02:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:02:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:02:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:02:54 | INFO | valid | epoch 146 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.676 | ppl 12.78 | bleu 23.12 | wps 4793 | wpb 7508.5 | bsz 272.7 | num_updates 81906 | best_bleu 23.18
2021-01-08 03:02:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:02:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:02:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:02:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 146 @ 81906 updates, score 23.12) (writing took 2.8166746497154236 seconds)
2021-01-08 03:02:57 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2021-01-08 03:02:57 | INFO | train | epoch 146 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.243 | nll_loss 0.971 | ppl 1.96 | wps 15799.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 81906 | lr 7.65531e-06 | gnorm 0.821 | train_wall 345 | wall 0
2021-01-08 03:02:57 | INFO | fairseq.trainer | begin training epoch 147
2021-01-08 03:02:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:02:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:02:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:03:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:03:57 | INFO | train_inner | epoch 147:     94 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.958, ppl=1.94, wps=11969, ups=1.15, wpb=10433, bsz=376.1, num_updates=82000, lr=7.65092e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 03:04:59 | INFO | train_inner | epoch 147:    194 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.977, ppl=1.97, wps=17108.1, ups=1.62, wpb=10579.4, bsz=358.9, num_updates=82100, lr=7.64626e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 03:06:01 | INFO | train_inner | epoch 147:    294 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.971, ppl=1.96, wps=17065.2, ups=1.62, wpb=10503.5, bsz=363.3, num_updates=82200, lr=7.64161e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 03:07:02 | INFO | train_inner | epoch 147:    394 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.978, ppl=1.97, wps=17064.5, ups=1.62, wpb=10509.5, bsz=361.1, num_updates=82300, lr=7.63696e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 03:08:04 | INFO | train_inner | epoch 147:    494 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.969, ppl=1.96, wps=17080.1, ups=1.63, wpb=10463.1, bsz=367.4, num_updates=82400, lr=7.63233e-06, gnorm=0.814, train_wall=61, wall=0
2021-01-08 03:08:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:08:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:08:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:08:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:08:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:08:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:08:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:09:06 | INFO | valid | epoch 147 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.75 | bleu 23.17 | wps 4681.4 | wpb 7508.5 | bsz 272.7 | num_updates 82467 | best_bleu 23.18
2021-01-08 03:09:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:09:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:09:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:09:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:09:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:09:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 147 @ 82467 updates, score 23.17) (writing took 2.841721970587969 seconds)
2021-01-08 03:09:08 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2021-01-08 03:09:08 | INFO | train | epoch 147 | symm_kl 0.354 | self_kl 0 | self_cv 0 | loss 3.242 | nll_loss 0.97 | ppl 1.96 | wps 15834.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 82467 | lr 7.62923e-06 | gnorm 0.822 | train_wall 344 | wall 0
2021-01-08 03:09:08 | INFO | fairseq.trainer | begin training epoch 148
2021-01-08 03:09:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:09:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:09:31 | INFO | train_inner | epoch 148:     33 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.972, ppl=1.96, wps=11741.7, ups=1.14, wpb=10291.4, bsz=379.8, num_updates=82500, lr=7.6277e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 03:10:33 | INFO | train_inner | epoch 148:    133 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.97, ppl=1.96, wps=17086.7, ups=1.63, wpb=10501.3, bsz=377.8, num_updates=82600, lr=7.62308e-06, gnorm=0.816, train_wall=61, wall=0
2021-01-08 03:11:34 | INFO | train_inner | epoch 148:    233 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.961, ppl=1.95, wps=17059.5, ups=1.63, wpb=10471.4, bsz=382.7, num_updates=82700, lr=7.61847e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 03:12:36 | INFO | train_inner | epoch 148:    333 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.975, ppl=1.97, wps=16989.1, ups=1.62, wpb=10501.4, bsz=368.2, num_updates=82800, lr=7.61387e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 03:13:38 | INFO | train_inner | epoch 148:    433 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.965, ppl=1.95, wps=17046.6, ups=1.62, wpb=10523.6, bsz=366.1, num_updates=82900, lr=7.60928e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 03:14:39 | INFO | train_inner | epoch 148:    533 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.975, ppl=1.97, wps=17053.9, ups=1.62, wpb=10514.7, bsz=364, num_updates=83000, lr=7.60469e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 03:14:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:14:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:14:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:14:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:15:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:15:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:15:17 | INFO | valid | epoch 148 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 23.1 | wps 4716.4 | wpb 7508.5 | bsz 272.7 | num_updates 83028 | best_bleu 23.18
2021-01-08 03:15:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:15:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:15:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:15:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:15:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:15:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 148 @ 83028 updates, score 23.1) (writing took 2.8499615732580423 seconds)
2021-01-08 03:15:20 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2021-01-08 03:15:20 | INFO | train | epoch 148 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.242 | nll_loss 0.97 | ppl 1.96 | wps 15831.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 83028 | lr 7.60341e-06 | gnorm 0.824 | train_wall 344 | wall 0
2021-01-08 03:15:20 | INFO | fairseq.trainer | begin training epoch 149
2021-01-08 03:15:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:15:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:16:07 | INFO | train_inner | epoch 149:     72 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.968, ppl=1.96, wps=11998.4, ups=1.14, wpb=10493, bsz=366.6, num_updates=83100, lr=7.60011e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 03:17:08 | INFO | train_inner | epoch 149:    172 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.982, ppl=1.97, wps=16700.5, ups=1.62, wpb=10277.4, bsz=350.8, num_updates=83200, lr=7.59555e-06, gnorm=0.838, train_wall=61, wall=0
2021-01-08 03:18:10 | INFO | train_inner | epoch 149:    272 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.97, ppl=1.96, wps=17009.7, ups=1.62, wpb=10495.6, bsz=378.1, num_updates=83300, lr=7.59098e-06, gnorm=0.814, train_wall=62, wall=0
2021-01-08 03:19:12 | INFO | train_inner | epoch 149:    372 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.963, ppl=1.95, wps=17133.1, ups=1.62, wpb=10589.2, bsz=364, num_updates=83400, lr=7.58643e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 03:20:14 | INFO | train_inner | epoch 149:    472 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.969, ppl=1.96, wps=17032, ups=1.62, wpb=10522.2, bsz=370.2, num_updates=83500, lr=7.58189e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 03:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:21:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:21:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:21:29 | INFO | valid | epoch 149 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.183 | nll_loss 3.675 | ppl 12.77 | bleu 23.02 | wps 4774.7 | wpb 7508.5 | bsz 272.7 | num_updates 83589 | best_bleu 23.18
2021-01-08 03:21:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:21:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 149 @ 83589 updates, score 23.02) (writing took 2.7981510758399963 seconds)
2021-01-08 03:21:32 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2021-01-08 03:21:32 | INFO | train | epoch 149 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.969 | ppl 1.96 | wps 15816.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 83589 | lr 7.57785e-06 | gnorm 0.821 | train_wall 344 | wall 0
2021-01-08 03:21:32 | INFO | fairseq.trainer | begin training epoch 150
2021-01-08 03:21:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:21:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:21:42 | INFO | train_inner | epoch 150:     11 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.964, ppl=1.95, wps=11882.9, ups=1.14, wpb=10435.9, bsz=383.4, num_updates=83600, lr=7.57735e-06, gnorm=0.805, train_wall=61, wall=0
2021-01-08 03:22:43 | INFO | train_inner | epoch 150:    111 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.969, ppl=1.96, wps=17271.2, ups=1.64, wpb=10533.6, bsz=374.9, num_updates=83700, lr=7.57282e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 03:23:44 | INFO | train_inner | epoch 150:    211 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.968, ppl=1.96, wps=17038.1, ups=1.63, wpb=10446.7, bsz=364.9, num_updates=83800, lr=7.5683e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 03:24:45 | INFO | train_inner | epoch 150:    311 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.979, ppl=1.97, wps=16875.4, ups=1.63, wpb=10325.6, bsz=362.9, num_updates=83900, lr=7.56379e-06, gnorm=0.845, train_wall=61, wall=0
2021-01-08 03:25:47 | INFO | train_inner | epoch 150:    411 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.964, ppl=1.95, wps=17229.1, ups=1.62, wpb=10644.5, bsz=380.7, num_updates=84000, lr=7.55929e-06, gnorm=0.808, train_wall=62, wall=0
2021-01-08 03:26:49 | INFO | train_inner | epoch 150:    511 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.968, ppl=1.96, wps=17133.4, ups=1.61, wpb=10631.8, bsz=381.5, num_updates=84100, lr=7.55479e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 03:27:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:27:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:27:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:27:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:27:40 | INFO | valid | epoch 150 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.76 | bleu 23.02 | wps 4742.7 | wpb 7508.5 | bsz 272.7 | num_updates 84150 | best_bleu 23.18
2021-01-08 03:27:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:27:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:27:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:27:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 150 @ 84150 updates, score 23.02) (writing took 2.8185973782092333 seconds)
2021-01-08 03:27:43 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2021-01-08 03:27:43 | INFO | train | epoch 150 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.969 | ppl 1.96 | wps 15844.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 84150 | lr 7.55255e-06 | gnorm 0.824 | train_wall 344 | wall 0
2021-01-08 03:27:43 | INFO | fairseq.trainer | begin training epoch 151
2021-01-08 03:27:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:27:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:28:17 | INFO | train_inner | epoch 151:     50 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.959, ppl=1.94, wps=11817.4, ups=1.14, wpb=10392.8, bsz=353.7, num_updates=84200, lr=7.55031e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 03:29:19 | INFO | train_inner | epoch 151:    150 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.977, ppl=1.97, wps=16786.4, ups=1.61, wpb=10411.7, bsz=365.5, num_updates=84300, lr=7.54583e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 03:30:21 | INFO | train_inner | epoch 151:    250 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.963, ppl=1.95, wps=16868.1, ups=1.61, wpb=10498.4, bsz=360.1, num_updates=84400, lr=7.54136e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 03:31:23 | INFO | train_inner | epoch 151:    350 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.965, ppl=1.95, wps=16755, ups=1.62, wpb=10335.2, bsz=378, num_updates=84500, lr=7.53689e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 03:32:25 | INFO | train_inner | epoch 151:    450 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.974, ppl=1.96, wps=17190.5, ups=1.61, wpb=10663.2, bsz=359.9, num_updates=84600, lr=7.53244e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 03:33:27 | INFO | train_inner | epoch 151:    550 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.973, ppl=1.96, wps=17032.5, ups=1.61, wpb=10579.2, bsz=390.6, num_updates=84700, lr=7.52799e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 03:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:33:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:33:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:33:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:33:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:33:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:33:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:33:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:33:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:33:56 | INFO | valid | epoch 151 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.672 | ppl 12.75 | bleu 23.06 | wps 4357.2 | wpb 7508.5 | bsz 272.7 | num_updates 84711 | best_bleu 23.18
2021-01-08 03:33:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:33:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:33:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:33:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:33:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:33:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 151 @ 84711 updates, score 23.06) (writing took 2.804785056039691 seconds)
2021-01-08 03:33:58 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2021-01-08 03:33:58 | INFO | train | epoch 151 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.97 | ppl 1.96 | wps 15665.2 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 84711 | lr 7.5275e-06 | gnorm 0.826 | train_wall 346 | wall 0
2021-01-08 03:33:58 | INFO | fairseq.trainer | begin training epoch 152
2021-01-08 03:33:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:34:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:34:56 | INFO | train_inner | epoch 152:     89 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.96, ppl=1.95, wps=11744.4, ups=1.12, wpb=10478.1, bsz=362.9, num_updates=84800, lr=7.52355e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 03:35:58 | INFO | train_inner | epoch 152:    189 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.97, ppl=1.96, wps=16840.5, ups=1.62, wpb=10414.6, bsz=367.6, num_updates=84900, lr=7.51912e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 03:37:00 | INFO | train_inner | epoch 152:    289 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.967, ppl=1.95, wps=17004.9, ups=1.61, wpb=10530.3, bsz=373.8, num_updates=85000, lr=7.51469e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 03:38:02 | INFO | train_inner | epoch 152:    389 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.974, ppl=1.96, wps=16875.6, ups=1.61, wpb=10501.4, bsz=358.2, num_updates=85100, lr=7.51027e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 03:39:04 | INFO | train_inner | epoch 152:    489 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.986, ppl=1.98, wps=16812, ups=1.61, wpb=10450.4, bsz=375.9, num_updates=85200, lr=7.50587e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 03:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:39:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:39:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:39:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:39:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:39:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:39:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:39:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:40:11 | INFO | valid | epoch 152 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.673 | ppl 12.76 | bleu 23.18 | wps 4305.9 | wpb 7508.5 | bsz 272.7 | num_updates 85272 | best_bleu 23.18
2021-01-08 03:40:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:40:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:40:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:40:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:40:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 152 @ 85272 updates, score 23.18) (writing took 4.830355763435364 seconds)
2021-01-08 03:40:16 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2021-01-08 03:40:16 | INFO | train | epoch 152 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.24 | nll_loss 0.969 | ppl 1.96 | wps 15589.6 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 85272 | lr 7.5027e-06 | gnorm 0.826 | train_wall 346 | wall 0
2021-01-08 03:40:16 | INFO | fairseq.trainer | begin training epoch 153
2021-01-08 03:40:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:40:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:40:36 | INFO | train_inner | epoch 153:     28 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.961, ppl=1.95, wps=11289.1, ups=1.09, wpb=10315.4, bsz=369, num_updates=85300, lr=7.50147e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-08 03:41:37 | INFO | train_inner | epoch 153:    128 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.956, ppl=1.94, wps=17017.1, ups=1.62, wpb=10522.9, bsz=378.9, num_updates=85400, lr=7.49707e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 03:42:39 | INFO | train_inner | epoch 153:    228 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.975, ppl=1.97, wps=16936.1, ups=1.62, wpb=10456.6, bsz=378.2, num_updates=85500, lr=7.49269e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 03:43:41 | INFO | train_inner | epoch 153:    328 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.976, ppl=1.97, wps=16973.4, ups=1.61, wpb=10549.7, bsz=370.6, num_updates=85600, lr=7.48831e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 03:44:43 | INFO | train_inner | epoch 153:    428 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.981, ppl=1.97, wps=16846.1, ups=1.62, wpb=10403.7, bsz=355.1, num_updates=85700, lr=7.48394e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 03:45:45 | INFO | train_inner | epoch 153:    528 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.961, ppl=1.95, wps=17064.2, ups=1.62, wpb=10562.2, bsz=369.8, num_updates=85800, lr=7.47958e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 03:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:46:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:46:26 | INFO | valid | epoch 153 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.78 | bleu 23.09 | wps 4756.1 | wpb 7508.5 | bsz 272.7 | num_updates 85833 | best_bleu 23.18
2021-01-08 03:46:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:46:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:46:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 153 @ 85833 updates, score 23.09) (writing took 2.840412400662899 seconds)
2021-01-08 03:46:29 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2021-01-08 03:46:29 | INFO | train | epoch 153 | symm_kl 0.353 | self_kl 0 | self_cv 0 | loss 3.24 | nll_loss 0.969 | ppl 1.96 | wps 15757.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 85833 | lr 7.47814e-06 | gnorm 0.821 | train_wall 346 | wall 0
2021-01-08 03:46:29 | INFO | fairseq.trainer | begin training epoch 154
2021-01-08 03:46:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:46:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:47:12 | INFO | train_inner | epoch 154:     67 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.955, ppl=1.94, wps=11976, ups=1.14, wpb=10467.9, bsz=375.7, num_updates=85900, lr=7.47522e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 03:48:14 | INFO | train_inner | epoch 154:    167 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.974, ppl=1.96, wps=17143.7, ups=1.62, wpb=10603.5, bsz=357.9, num_updates=86000, lr=7.47087e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 03:49:16 | INFO | train_inner | epoch 154:    267 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.971, ppl=1.96, wps=16862.8, ups=1.62, wpb=10385.9, bsz=366, num_updates=86100, lr=7.46653e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 03:50:18 | INFO | train_inner | epoch 154:    367 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.964, ppl=1.95, wps=17415.4, ups=1.62, wpb=10756.2, bsz=375.4, num_updates=86200, lr=7.4622e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 03:51:19 | INFO | train_inner | epoch 154:    467 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.972, ppl=1.96, wps=16700.2, ups=1.62, wpb=10313.2, bsz=372.1, num_updates=86300, lr=7.45788e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 03:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:52:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:52:37 | INFO | valid | epoch 154 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.77 | bleu 23.09 | wps 4745.5 | wpb 7508.5 | bsz 272.7 | num_updates 86394 | best_bleu 23.18
2021-01-08 03:52:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 154 @ 86394 updates, score 23.09) (writing took 2.8059336002916098 seconds)
2021-01-08 03:52:40 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2021-01-08 03:52:40 | INFO | train | epoch 154 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.239 | nll_loss 0.968 | ppl 1.96 | wps 15836.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 86394 | lr 7.45382e-06 | gnorm 0.82 | train_wall 344 | wall 0
2021-01-08 03:52:40 | INFO | fairseq.trainer | begin training epoch 155
2021-01-08 03:52:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:52:47 | INFO | train_inner | epoch 155:      6 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.964, ppl=1.95, wps=11875, ups=1.14, wpb=10423.9, bsz=370.4, num_updates=86400, lr=7.45356e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 03:53:48 | INFO | train_inner | epoch 155:    106 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.96, ppl=1.95, wps=17261, ups=1.64, wpb=10500.4, bsz=379.3, num_updates=86500, lr=7.44925e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 03:54:50 | INFO | train_inner | epoch 155:    206 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.972, ppl=1.96, wps=17006, ups=1.62, wpb=10468.2, bsz=378.1, num_updates=86600, lr=7.44495e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 03:55:51 | INFO | train_inner | epoch 155:    306 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.968, ppl=1.96, wps=16932.6, ups=1.62, wpb=10474.7, bsz=368.5, num_updates=86700, lr=7.44065e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 03:56:53 | INFO | train_inner | epoch 155:    406 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.969, ppl=1.96, wps=16846.2, ups=1.61, wpb=10449.6, bsz=382.3, num_updates=86800, lr=7.43637e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 03:57:55 | INFO | train_inner | epoch 155:    506 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.951, ppl=1.93, wps=17112.2, ups=1.62, wpb=10547.3, bsz=354.5, num_updates=86900, lr=7.43209e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 03:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 03:58:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 03:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 03:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 03:58:50 | INFO | valid | epoch 155 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.675 | ppl 12.77 | bleu 23 | wps 4745.1 | wpb 7508.5 | bsz 272.7 | num_updates 86955 | best_bleu 23.18
2021-01-08 03:58:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 03:58:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:58:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:58:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 155 @ 86955 updates, score 23.0) (writing took 2.8739550821483135 seconds)
2021-01-08 03:58:52 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2021-01-08 03:58:52 | INFO | train | epoch 155 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.239 | nll_loss 0.968 | ppl 1.96 | wps 15797.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 86955 | lr 7.42974e-06 | gnorm 0.822 | train_wall 345 | wall 0
2021-01-08 03:58:52 | INFO | fairseq.trainer | begin training epoch 156
2021-01-08 03:58:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 03:58:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 03:59:23 | INFO | train_inner | epoch 156:     45 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.991, ppl=1.99, wps=12017.4, ups=1.14, wpb=10554.8, bsz=367.5, num_updates=87000, lr=7.42781e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 04:00:25 | INFO | train_inner | epoch 156:    145 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.962, ppl=1.95, wps=16708.8, ups=1.61, wpb=10374.4, bsz=356.9, num_updates=87100, lr=7.42355e-06, gnorm=0.835, train_wall=62, wall=0
2021-01-08 04:01:27 | INFO | train_inner | epoch 156:    245 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.965, ppl=1.95, wps=16964.3, ups=1.61, wpb=10527.3, bsz=365.1, num_updates=87200, lr=7.41929e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 04:02:29 | INFO | train_inner | epoch 156:    345 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.972, ppl=1.96, wps=17036.3, ups=1.62, wpb=10518.7, bsz=381.3, num_updates=87300, lr=7.41504e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 04:03:30 | INFO | train_inner | epoch 156:    445 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.966, ppl=1.95, wps=17067.3, ups=1.63, wpb=10486.5, bsz=372.8, num_updates=87400, lr=7.4108e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 04:04:32 | INFO | train_inner | epoch 156:    545 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.975, ppl=1.97, wps=16854.4, ups=1.62, wpb=10426.9, bsz=362.7, num_updates=87500, lr=7.40656e-06, gnorm=0.834, train_wall=62, wall=0
2021-01-08 04:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:04:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:05:02 | INFO | valid | epoch 156 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.673 | ppl 12.75 | bleu 23.14 | wps 4715.8 | wpb 7508.5 | bsz 272.7 | num_updates 87516 | best_bleu 23.18
2021-01-08 04:05:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:05:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:05:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:05:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:05:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:05:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 156 @ 87516 updates, score 23.14) (writing took 2.7947442028671503 seconds)
2021-01-08 04:05:05 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2021-01-08 04:05:05 | INFO | train | epoch 156 | symm_kl 0.352 | self_kl 0 | self_cv 0 | loss 3.238 | nll_loss 0.968 | ppl 1.96 | wps 15777.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 87516 | lr 7.40588e-06 | gnorm 0.823 | train_wall 345 | wall 0
2021-01-08 04:05:05 | INFO | fairseq.trainer | begin training epoch 157
2021-01-08 04:05:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:05:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:05:59 | INFO | train_inner | epoch 157:     84 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.963, ppl=1.95, wps=11916.6, ups=1.15, wpb=10398.2, bsz=356.6, num_updates=87600, lr=7.40233e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 04:07:02 | INFO | train_inner | epoch 157:    184 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.97, ppl=1.96, wps=16842, ups=1.61, wpb=10462.5, bsz=370.2, num_updates=87700, lr=7.39811e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 04:08:03 | INFO | train_inner | epoch 157:    284 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.968, ppl=1.96, wps=17018.9, ups=1.62, wpb=10498.9, bsz=377, num_updates=87800, lr=7.3939e-06, gnorm=0.803, train_wall=62, wall=0
2021-01-08 04:09:05 | INFO | train_inner | epoch 157:    384 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.961, ppl=1.95, wps=16904.3, ups=1.62, wpb=10455.1, bsz=378.1, num_updates=87900, lr=7.38969e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 04:10:07 | INFO | train_inner | epoch 157:    484 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.973, ppl=1.96, wps=17081.9, ups=1.62, wpb=10521.1, bsz=361.4, num_updates=88000, lr=7.38549e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 04:10:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:11:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:11:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:11:14 | INFO | valid | epoch 157 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.76 | bleu 23.09 | wps 4798.2 | wpb 7508.5 | bsz 272.7 | num_updates 88077 | best_bleu 23.18
2021-01-08 04:11:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:11:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 157 @ 88077 updates, score 23.09) (writing took 2.8276679906994104 seconds)
2021-01-08 04:11:17 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2021-01-08 04:11:17 | INFO | train | epoch 157 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.237 | nll_loss 0.968 | ppl 1.96 | wps 15808.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 88077 | lr 7.38226e-06 | gnorm 0.822 | train_wall 345 | wall 0
2021-01-08 04:11:17 | INFO | fairseq.trainer | begin training epoch 158
2021-01-08 04:11:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:11:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:11:34 | INFO | train_inner | epoch 158:     23 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.974, ppl=1.96, wps=11959.8, ups=1.14, wpb=10461.8, bsz=363, num_updates=88100, lr=7.3813e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 04:12:36 | INFO | train_inner | epoch 158:    123 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.971, ppl=1.96, wps=17142.3, ups=1.62, wpb=10559.1, bsz=367.3, num_updates=88200, lr=7.37711e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 04:13:37 | INFO | train_inner | epoch 158:    223 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.209, nll_loss=0.948, ppl=1.93, wps=17017.1, ups=1.62, wpb=10508.7, bsz=393.1, num_updates=88300, lr=7.37293e-06, gnorm=0.806, train_wall=62, wall=0
2021-01-08 04:14:39 | INFO | train_inner | epoch 158:    323 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.973, ppl=1.96, wps=17071.8, ups=1.61, wpb=10589.6, bsz=363.3, num_updates=88400, lr=7.36876e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 04:15:41 | INFO | train_inner | epoch 158:    423 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.97, ppl=1.96, wps=16806.6, ups=1.62, wpb=10379, bsz=361.4, num_updates=88500, lr=7.3646e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 04:16:43 | INFO | train_inner | epoch 158:    523 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.964, ppl=1.95, wps=17038, ups=1.62, wpb=10516, bsz=371.4, num_updates=88600, lr=7.36044e-06, gnorm=0.815, train_wall=62, wall=0
2021-01-08 04:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:17:27 | INFO | valid | epoch 158 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.676 | ppl 12.78 | bleu 23.12 | wps 4650.5 | wpb 7508.5 | bsz 272.7 | num_updates 88638 | best_bleu 23.18
2021-01-08 04:17:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:17:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:17:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 158 @ 88638 updates, score 23.12) (writing took 2.8156441878527403 seconds)
2021-01-08 04:17:30 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2021-01-08 04:17:30 | INFO | train | epoch 158 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.237 | nll_loss 0.967 | ppl 1.96 | wps 15784.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 88638 | lr 7.35886e-06 | gnorm 0.824 | train_wall 345 | wall 0
2021-01-08 04:17:30 | INFO | fairseq.trainer | begin training epoch 159
2021-01-08 04:17:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:18:11 | INFO | train_inner | epoch 159:     62 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.955, ppl=1.94, wps=11959.6, ups=1.14, wpb=10478.2, bsz=356.5, num_updates=88700, lr=7.35629e-06, gnorm=0.845, train_wall=61, wall=0
2021-01-08 04:19:12 | INFO | train_inner | epoch 159:    162 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.983, ppl=1.98, wps=17059.9, ups=1.62, wpb=10552, bsz=362.9, num_updates=88800, lr=7.35215e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 04:20:14 | INFO | train_inner | epoch 159:    262 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.97, ppl=1.96, wps=16994.1, ups=1.61, wpb=10525.1, bsz=376.4, num_updates=88900, lr=7.34801e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 04:21:16 | INFO | train_inner | epoch 159:    362 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.959, ppl=1.94, wps=16810.8, ups=1.63, wpb=10301.3, bsz=373.6, num_updates=89000, lr=7.34388e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 04:22:18 | INFO | train_inner | epoch 159:    462 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.968, ppl=1.96, wps=17015.8, ups=1.62, wpb=10535, bsz=374, num_updates=89100, lr=7.33976e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 04:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:23:39 | INFO | valid | epoch 159 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23.13 | wps 4727.7 | wpb 7508.5 | bsz 272.7 | num_updates 89199 | best_bleu 23.18
2021-01-08 04:23:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:23:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 159 @ 89199 updates, score 23.13) (writing took 2.805406328290701 seconds)
2021-01-08 04:23:42 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2021-01-08 04:23:42 | INFO | train | epoch 159 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.236 | nll_loss 0.967 | ppl 1.95 | wps 15821.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 89199 | lr 7.33568e-06 | gnorm 0.825 | train_wall 344 | wall 0
2021-01-08 04:23:42 | INFO | fairseq.trainer | begin training epoch 160
2021-01-08 04:23:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:23:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:23:46 | INFO | train_inner | epoch 160:      1 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.97, ppl=1.96, wps=11857, ups=1.14, wpb=10426.8, bsz=378.2, num_updates=89200, lr=7.33564e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 04:24:46 | INFO | train_inner | epoch 160:    101 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.972, ppl=1.96, wps=17226.9, ups=1.64, wpb=10475.8, bsz=381.2, num_updates=89300, lr=7.33153e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 04:25:48 | INFO | train_inner | epoch 160:    201 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.964, ppl=1.95, wps=16757.1, ups=1.62, wpb=10332.2, bsz=355.5, num_updates=89400, lr=7.32743e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 04:26:50 | INFO | train_inner | epoch 160:    301 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.966, ppl=1.95, wps=16986.9, ups=1.61, wpb=10527, bsz=375.7, num_updates=89500, lr=7.32334e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 04:27:52 | INFO | train_inner | epoch 160:    401 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.975, ppl=1.97, wps=16892.1, ups=1.61, wpb=10474.4, bsz=347.8, num_updates=89600, lr=7.31925e-06, gnorm=0.833, train_wall=62, wall=0
2021-01-08 04:28:54 | INFO | train_inner | epoch 160:    501 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.963, ppl=1.95, wps=17071.6, ups=1.61, wpb=10609.6, bsz=376.2, num_updates=89700, lr=7.31517e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 04:29:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:29:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:29:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:29:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:29:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:29:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:29:53 | INFO | valid | epoch 160 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.77 | bleu 23.11 | wps 4425.9 | wpb 7508.5 | bsz 272.7 | num_updates 89760 | best_bleu 23.18
2021-01-08 04:29:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:29:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:29:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:29:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 160 @ 89760 updates, score 23.11) (writing took 2.8025995548814535 seconds)
2021-01-08 04:29:56 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2021-01-08 04:29:56 | INFO | train | epoch 160 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.236 | nll_loss 0.967 | ppl 1.96 | wps 15706.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 89760 | lr 7.31272e-06 | gnorm 0.822 | train_wall 346 | wall 0
2021-01-08 04:29:56 | INFO | fairseq.trainer | begin training epoch 161
2021-01-08 04:29:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:29:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:30:23 | INFO | train_inner | epoch 161:     40 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.956, ppl=1.94, wps=11789, ups=1.12, wpb=10526.9, bsz=384.2, num_updates=89800, lr=7.3111e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 04:31:25 | INFO | train_inner | epoch 161:    140 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.969, ppl=1.96, wps=16669.7, ups=1.62, wpb=10283.5, bsz=369.5, num_updates=89900, lr=7.30703e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 04:32:28 | INFO | train_inner | epoch 161:    240 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.96, ppl=1.95, wps=16797.4, ups=1.6, wpb=10530.1, bsz=377.6, num_updates=90000, lr=7.30297e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 04:33:30 | INFO | train_inner | epoch 161:    340 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.965, ppl=1.95, wps=16792.3, ups=1.61, wpb=10420.5, bsz=362.1, num_updates=90100, lr=7.29891e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 04:34:32 | INFO | train_inner | epoch 161:    440 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.968, ppl=1.96, wps=16924.1, ups=1.6, wpb=10575.6, bsz=371.4, num_updates=90200, lr=7.29487e-06, gnorm=0.805, train_wall=62, wall=0
2021-01-08 04:35:34 | INFO | train_inner | epoch 161:    540 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.977, ppl=1.97, wps=17145.8, ups=1.61, wpb=10623.3, bsz=368.4, num_updates=90300, lr=7.29083e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 04:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:35:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:35:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:35:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:35:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:35:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:35:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:36:08 | INFO | valid | epoch 161 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.75 | bleu 23.1 | wps 4722.4 | wpb 7508.5 | bsz 272.7 | num_updates 90321 | best_bleu 23.18
2021-01-08 04:36:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:36:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:36:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:36:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 161 @ 90321 updates, score 23.1) (writing took 2.844207815825939 seconds)
2021-01-08 04:36:11 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2021-01-08 04:36:11 | INFO | train | epoch 161 | symm_kl 0.351 | self_kl 0 | self_cv 0 | loss 3.235 | nll_loss 0.967 | ppl 1.95 | wps 15699.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 90321 | lr 7.28998e-06 | gnorm 0.818 | train_wall 347 | wall 0
2021-01-08 04:36:11 | INFO | fairseq.trainer | begin training epoch 162
2021-01-08 04:36:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:36:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:37:02 | INFO | train_inner | epoch 162:     79 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.976, ppl=1.97, wps=11944.2, ups=1.14, wpb=10510.1, bsz=384.2, num_updates=90400, lr=7.28679e-06, gnorm=0.812, train_wall=61, wall=0
2021-01-08 04:38:04 | INFO | train_inner | epoch 162:    179 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.965, ppl=1.95, wps=16765.2, ups=1.61, wpb=10408.5, bsz=360.1, num_updates=90500, lr=7.28277e-06, gnorm=0.837, train_wall=62, wall=0
2021-01-08 04:39:06 | INFO | train_inner | epoch 162:    279 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.964, ppl=1.95, wps=16968.4, ups=1.62, wpb=10470.7, bsz=365.1, num_updates=90600, lr=7.27875e-06, gnorm=0.826, train_wall=62, wall=0
2021-01-08 04:40:08 | INFO | train_inner | epoch 162:    379 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.959, ppl=1.94, wps=16960.5, ups=1.61, wpb=10547.8, bsz=381.9, num_updates=90700, lr=7.27473e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 04:41:10 | INFO | train_inner | epoch 162:    479 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.966, ppl=1.95, wps=16960.7, ups=1.62, wpb=10457.3, bsz=358, num_updates=90800, lr=7.27072e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 04:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:42:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:42:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:42:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:42:21 | INFO | valid | epoch 162 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.76 | bleu 23.1 | wps 4775.7 | wpb 7508.5 | bsz 272.7 | num_updates 90882 | best_bleu 23.18
2021-01-08 04:42:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:42:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 162 @ 90882 updates, score 23.1) (writing took 2.813472080975771 seconds)
2021-01-08 04:42:24 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2021-01-08 04:42:24 | INFO | train | epoch 162 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.235 | nll_loss 0.966 | ppl 1.95 | wps 15753.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 90882 | lr 7.26744e-06 | gnorm 0.821 | train_wall 346 | wall 0
2021-01-08 04:42:24 | INFO | fairseq.trainer | begin training epoch 163
2021-01-08 04:42:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:42:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:42:38 | INFO | train_inner | epoch 163:     18 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.965, ppl=1.95, wps=11895.2, ups=1.14, wpb=10471.3, bsz=366.6, num_updates=90900, lr=7.26672e-06, gnorm=0.814, train_wall=61, wall=0
2021-01-08 04:43:39 | INFO | train_inner | epoch 163:    118 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.968, ppl=1.96, wps=16952.7, ups=1.63, wpb=10431.1, bsz=359.9, num_updates=91000, lr=7.26273e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 04:44:42 | INFO | train_inner | epoch 163:    218 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.958, ppl=1.94, wps=16867.1, ups=1.61, wpb=10475.1, bsz=362, num_updates=91100, lr=7.25874e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 04:45:43 | INFO | train_inner | epoch 163:    318 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.958, ppl=1.94, wps=16929.1, ups=1.62, wpb=10453.1, bsz=371.7, num_updates=91200, lr=7.25476e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 04:46:45 | INFO | train_inner | epoch 163:    418 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.975, ppl=1.97, wps=17017.3, ups=1.62, wpb=10488.4, bsz=373.2, num_updates=91300, lr=7.25079e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 04:47:47 | INFO | train_inner | epoch 163:    518 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.981, ppl=1.97, wps=16950, ups=1.61, wpb=10545.3, bsz=368.3, num_updates=91400, lr=7.24682e-06, gnorm=0.827, train_wall=62, wall=0
2021-01-08 04:48:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:48:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:48:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:48:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:48:34 | INFO | valid | epoch 163 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.671 | ppl 12.73 | bleu 23.2 | wps 4788 | wpb 7508.5 | bsz 272.7 | num_updates 91443 | best_bleu 23.2
2021-01-08 04:48:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:48:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:48:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 163 @ 91443 updates, score 23.2) (writing took 4.857273988425732 seconds)
2021-01-08 04:48:39 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2021-01-08 04:48:39 | INFO | train | epoch 163 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.234 | nll_loss 0.966 | ppl 1.95 | wps 15680.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 91443 | lr 7.24512e-06 | gnorm 0.823 | train_wall 346 | wall 0
2021-01-08 04:48:39 | INFO | fairseq.trainer | begin training epoch 164
2021-01-08 04:48:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:48:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:49:17 | INFO | train_inner | epoch 164:     57 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.957, ppl=1.94, wps=11760.6, ups=1.12, wpb=10520, bsz=377.4, num_updates=91500, lr=7.24286e-06, gnorm=0.81, train_wall=61, wall=0
2021-01-08 04:50:18 | INFO | train_inner | epoch 164:    157 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.959, ppl=1.94, wps=17278.7, ups=1.62, wpb=10667.6, bsz=374.7, num_updates=91600, lr=7.23891e-06, gnorm=0.811, train_wall=62, wall=0
2021-01-08 04:51:20 | INFO | train_inner | epoch 164:    257 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.967, ppl=1.95, wps=16981.7, ups=1.63, wpb=10437.2, bsz=370.4, num_updates=91700, lr=7.23496e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-08 04:52:21 | INFO | train_inner | epoch 164:    357 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.973, ppl=1.96, wps=17009.5, ups=1.63, wpb=10452.2, bsz=363.2, num_updates=91800, lr=7.23102e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 04:53:23 | INFO | train_inner | epoch 164:    457 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.975, ppl=1.97, wps=16971.9, ups=1.63, wpb=10410.6, bsz=365, num_updates=91900, lr=7.22708e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 04:54:24 | INFO | train_inner | epoch 164:    557 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.963, ppl=1.95, wps=16990, ups=1.62, wpb=10489.6, bsz=379.6, num_updates=92000, lr=7.22315e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 04:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 04:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:54:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 04:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 04:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 04:54:47 | INFO | valid | epoch 164 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.672 | ppl 12.75 | bleu 23.1 | wps 4748.1 | wpb 7508.5 | bsz 272.7 | num_updates 92004 | best_bleu 23.2
2021-01-08 04:54:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 04:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:54:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:54:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 164 @ 92004 updates, score 23.1) (writing took 2.8478102181106806 seconds)
2021-01-08 04:54:50 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2021-01-08 04:54:50 | INFO | train | epoch 164 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.234 | nll_loss 0.966 | ppl 1.95 | wps 15851.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 92004 | lr 7.22299e-06 | gnorm 0.824 | train_wall 343 | wall 0
2021-01-08 04:54:50 | INFO | fairseq.trainer | begin training epoch 165
2021-01-08 04:54:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 04:54:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 04:55:51 | INFO | train_inner | epoch 165:     96 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.975, ppl=1.97, wps=12092.4, ups=1.15, wpb=10498, bsz=355.6, num_updates=92100, lr=7.21923e-06, gnorm=0.819, train_wall=60, wall=0
2021-01-08 04:56:53 | INFO | train_inner | epoch 165:    196 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.965, ppl=1.95, wps=16998.2, ups=1.62, wpb=10515.1, bsz=371.7, num_updates=92200, lr=7.21531e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 04:57:55 | INFO | train_inner | epoch 165:    296 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.963, ppl=1.95, wps=16995.3, ups=1.62, wpb=10464, bsz=364.5, num_updates=92300, lr=7.2114e-06, gnorm=0.826, train_wall=61, wall=0
2021-01-08 04:58:57 | INFO | train_inner | epoch 165:    396 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.959, ppl=1.94, wps=16813.4, ups=1.62, wpb=10410.3, bsz=381.8, num_updates=92400, lr=7.2075e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 04:59:59 | INFO | train_inner | epoch 165:    496 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.97, ppl=1.96, wps=17011.8, ups=1.61, wpb=10569.8, bsz=383, num_updates=92500, lr=7.2036e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 05:00:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:00:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:00:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:00:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:00:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:00:59 | INFO | valid | epoch 165 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.674 | ppl 12.77 | bleu 23.14 | wps 4806.6 | wpb 7508.5 | bsz 272.7 | num_updates 92565 | best_bleu 23.2
2021-01-08 05:00:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:01:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:01:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:01:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:01:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:01:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 165 @ 92565 updates, score 23.14) (writing took 2.800676390528679 seconds)
2021-01-08 05:01:02 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2021-01-08 05:01:02 | INFO | train | epoch 165 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.234 | nll_loss 0.966 | ppl 1.95 | wps 15819.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 92565 | lr 7.20107e-06 | gnorm 0.82 | train_wall 345 | wall 0
2021-01-08 05:01:02 | INFO | fairseq.trainer | begin training epoch 166
2021-01-08 05:01:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:01:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:01:26 | INFO | train_inner | epoch 166:     35 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.958, ppl=1.94, wps=11829.1, ups=1.14, wpb=10369.7, bsz=373.7, num_updates=92600, lr=7.19971e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 05:02:28 | INFO | train_inner | epoch 166:    135 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.96, ppl=1.94, wps=16992.1, ups=1.62, wpb=10471.3, bsz=360, num_updates=92700, lr=7.19583e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 05:03:30 | INFO | train_inner | epoch 166:    235 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.972, ppl=1.96, wps=16912.3, ups=1.61, wpb=10488.6, bsz=369.3, num_updates=92800, lr=7.19195e-06, gnorm=0.814, train_wall=62, wall=0
2021-01-08 05:04:31 | INFO | train_inner | epoch 166:    335 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.978, ppl=1.97, wps=17086, ups=1.64, wpb=10432.6, bsz=367.9, num_updates=92900, lr=7.18808e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 05:05:33 | INFO | train_inner | epoch 166:    435 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.965, ppl=1.95, wps=17074.8, ups=1.62, wpb=10537.4, bsz=377.2, num_updates=93000, lr=7.18421e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 05:06:34 | INFO | train_inner | epoch 166:    535 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.968, ppl=1.96, wps=17175.5, ups=1.63, wpb=10545, bsz=358.2, num_updates=93100, lr=7.18035e-06, gnorm=0.809, train_wall=61, wall=0
2021-01-08 05:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:06:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:06:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:07:10 | INFO | valid | epoch 166 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.671 | ppl 12.74 | bleu 23.21 | wps 4756 | wpb 7508.5 | bsz 272.7 | num_updates 93126 | best_bleu 23.21
2021-01-08 05:07:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:07:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:07:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:07:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:07:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:07:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 166 @ 93126 updates, score 23.21) (writing took 4.83819330111146 seconds)
2021-01-08 05:07:15 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2021-01-08 05:07:15 | INFO | train | epoch 166 | symm_kl 0.35 | self_kl 0 | self_cv 0 | loss 3.233 | nll_loss 0.965 | ppl 1.95 | wps 15752.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 93126 | lr 7.17935e-06 | gnorm 0.819 | train_wall 344 | wall 0
2021-01-08 05:07:15 | INFO | fairseq.trainer | begin training epoch 167
2021-01-08 05:07:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:07:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:08:03 | INFO | train_inner | epoch 167:     74 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.964, ppl=1.95, wps=11702.2, ups=1.12, wpb=10410.2, bsz=364.6, num_updates=93200, lr=7.1765e-06, gnorm=0.833, train_wall=60, wall=0
2021-01-08 05:09:05 | INFO | train_inner | epoch 167:    174 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.966, ppl=1.95, wps=17073.5, ups=1.62, wpb=10541.4, bsz=372.4, num_updates=93300, lr=7.17265e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-08 05:10:06 | INFO | train_inner | epoch 167:    274 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.959, ppl=1.94, wps=17049.1, ups=1.63, wpb=10485, bsz=359.6, num_updates=93400, lr=7.16881e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 05:11:08 | INFO | train_inner | epoch 167:    374 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.976, ppl=1.97, wps=17104.4, ups=1.62, wpb=10553.5, bsz=376.1, num_updates=93500, lr=7.16498e-06, gnorm=0.808, train_wall=62, wall=0
2021-01-08 05:12:10 | INFO | train_inner | epoch 167:    474 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.955, ppl=1.94, wps=17134.8, ups=1.62, wpb=10600.4, bsz=387.2, num_updates=93600, lr=7.16115e-06, gnorm=0.796, train_wall=62, wall=0
2021-01-08 05:13:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:13:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:13:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:13:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:13:24 | INFO | valid | epoch 167 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.671 | ppl 12.74 | bleu 23.23 | wps 4647.4 | wpb 7508.5 | bsz 272.7 | num_updates 93687 | best_bleu 23.23
2021-01-08 05:13:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:13:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 167 @ 93687 updates, score 23.23) (writing took 4.898354064673185 seconds)
2021-01-08 05:13:29 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2021-01-08 05:13:29 | INFO | train | epoch 167 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.965 | ppl 1.95 | wps 15740 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 93687 | lr 7.15782e-06 | gnorm 0.822 | train_wall 344 | wall 0
2021-01-08 05:13:29 | INFO | fairseq.trainer | begin training epoch 168
2021-01-08 05:13:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:13:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:13:40 | INFO | train_inner | epoch 168:     13 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.964, ppl=1.95, wps=11451.6, ups=1.11, wpb=10304.7, bsz=358.4, num_updates=93700, lr=7.15733e-06, gnorm=0.847, train_wall=61, wall=0
2021-01-08 05:14:41 | INFO | train_inner | epoch 168:    113 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.982, ppl=1.98, wps=16986, ups=1.63, wpb=10428.7, bsz=376.5, num_updates=93800, lr=7.15351e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 05:15:44 | INFO | train_inner | epoch 168:    213 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.958, ppl=1.94, wps=17006.6, ups=1.61, wpb=10585.6, bsz=366.7, num_updates=93900, lr=7.1497e-06, gnorm=0.814, train_wall=62, wall=0
2021-01-08 05:16:45 | INFO | train_inner | epoch 168:    313 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.964, ppl=1.95, wps=16662.7, ups=1.61, wpb=10323.6, bsz=350.9, num_updates=94000, lr=7.1459e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 05:17:47 | INFO | train_inner | epoch 168:    413 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.962, ppl=1.95, wps=17114.9, ups=1.62, wpb=10570.7, bsz=381, num_updates=94100, lr=7.1421e-06, gnorm=0.808, train_wall=62, wall=0
2021-01-08 05:18:49 | INFO | train_inner | epoch 168:    513 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.958, ppl=1.94, wps=17092, ups=1.62, wpb=10531.9, bsz=374.2, num_updates=94200, lr=7.13831e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 05:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:19:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:19:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:19:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:19:39 | INFO | valid | epoch 168 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23.07 | wps 4762 | wpb 7508.5 | bsz 272.7 | num_updates 94248 | best_bleu 23.23
2021-01-08 05:19:39 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:19:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:19:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:19:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 168 @ 94248 updates, score 23.07) (writing took 2.7858107779175043 seconds)
2021-01-08 05:19:41 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2021-01-08 05:19:41 | INFO | train | epoch 168 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.965 | ppl 1.95 | wps 15781.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 94248 | lr 7.13649e-06 | gnorm 0.82 | train_wall 345 | wall 0
2021-01-08 05:19:41 | INFO | fairseq.trainer | begin training epoch 169
2021-01-08 05:19:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:19:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:20:16 | INFO | train_inner | epoch 169:     52 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.966, ppl=1.95, wps=11943.1, ups=1.15, wpb=10399.5, bsz=379.9, num_updates=94300, lr=7.13452e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 05:21:18 | INFO | train_inner | epoch 169:    152 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.216, nll_loss=0.95, ppl=1.93, wps=17116.6, ups=1.62, wpb=10548.1, bsz=366.4, num_updates=94400, lr=7.13074e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 05:22:19 | INFO | train_inner | epoch 169:    252 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.963, ppl=1.95, wps=17173, ups=1.62, wpb=10592.4, bsz=360.7, num_updates=94500, lr=7.12697e-06, gnorm=0.83, train_wall=61, wall=0
2021-01-08 05:23:21 | INFO | train_inner | epoch 169:    352 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.96, ppl=1.95, wps=17033.3, ups=1.61, wpb=10574.9, bsz=377.5, num_updates=94600, lr=7.1232e-06, gnorm=0.814, train_wall=62, wall=0
2021-01-08 05:24:23 | INFO | train_inner | epoch 169:    452 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.981, ppl=1.97, wps=16899, ups=1.63, wpb=10366.8, bsz=358.8, num_updates=94700, lr=7.11944e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 05:25:25 | INFO | train_inner | epoch 169:    552 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.969, ppl=1.96, wps=16781.6, ups=1.62, wpb=10388.2, bsz=374.2, num_updates=94800, lr=7.11568e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 05:25:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:25:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:25:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:25:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:25:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:25:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:25:51 | INFO | valid | epoch 169 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.675 | ppl 12.78 | bleu 23.08 | wps 4742.9 | wpb 7508.5 | bsz 272.7 | num_updates 94809 | best_bleu 23.23
2021-01-08 05:25:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:25:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 169 @ 94809 updates, score 23.08) (writing took 2.844536418095231 seconds)
2021-01-08 05:25:53 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2021-01-08 05:25:53 | INFO | train | epoch 169 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.965 | ppl 1.95 | wps 15811.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 94809 | lr 7.11534e-06 | gnorm 0.823 | train_wall 344 | wall 0
2021-01-08 05:25:53 | INFO | fairseq.trainer | begin training epoch 170
2021-01-08 05:25:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:25:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:26:52 | INFO | train_inner | epoch 170:     91 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.968, ppl=1.96, wps=11975.3, ups=1.15, wpb=10434.5, bsz=377.5, num_updates=94900, lr=7.11193e-06, gnorm=0.808, train_wall=60, wall=0
2021-01-08 05:27:54 | INFO | train_inner | epoch 170:    191 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.974, ppl=1.96, wps=16863.2, ups=1.61, wpb=10448.8, bsz=361.8, num_updates=95000, lr=7.10819e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 05:28:56 | INFO | train_inner | epoch 170:    291 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.961, ppl=1.95, wps=17002.4, ups=1.62, wpb=10525.1, bsz=368.5, num_updates=95100, lr=7.10445e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 05:29:57 | INFO | train_inner | epoch 170:    391 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.961, ppl=1.95, wps=17050.2, ups=1.62, wpb=10535.7, bsz=380, num_updates=95200, lr=7.10072e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 05:30:59 | INFO | train_inner | epoch 170:    491 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.966, ppl=1.95, wps=16891.5, ups=1.63, wpb=10360.1, bsz=366.6, num_updates=95300, lr=7.09699e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 05:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:31:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:31:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:31:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:31:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:31:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:31:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:32:02 | INFO | valid | epoch 170 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.671 | ppl 12.74 | bleu 23.14 | wps 4761.5 | wpb 7508.5 | bsz 272.7 | num_updates 95370 | best_bleu 23.23
2021-01-08 05:32:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:32:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:32:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:32:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 170 @ 95370 updates, score 23.14) (writing took 2.8136281836777925 seconds)
2021-01-08 05:32:05 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2021-01-08 05:32:05 | INFO | train | epoch 170 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.965 | ppl 1.95 | wps 15819.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 95370 | lr 7.09438e-06 | gnorm 0.823 | train_wall 344 | wall 0
2021-01-08 05:32:05 | INFO | fairseq.trainer | begin training epoch 171
2021-01-08 05:32:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:32:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:32:26 | INFO | train_inner | epoch 171:     30 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.958, ppl=1.94, wps=12068.9, ups=1.14, wpb=10589.8, bsz=355.7, num_updates=95400, lr=7.09327e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-08 05:33:29 | INFO | train_inner | epoch 171:    130 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.966, ppl=1.95, wps=16992.7, ups=1.61, wpb=10567.8, bsz=384.6, num_updates=95500, lr=7.08955e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 05:34:31 | INFO | train_inner | epoch 171:    230 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.963, ppl=1.95, wps=16676.3, ups=1.6, wpb=10392.6, bsz=362.3, num_updates=95600, lr=7.08585e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-08 05:35:33 | INFO | train_inner | epoch 171:    330 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.959, ppl=1.94, wps=16978.4, ups=1.62, wpb=10507.2, bsz=377.4, num_updates=95700, lr=7.08214e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 05:36:35 | INFO | train_inner | epoch 171:    430 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.964, ppl=1.95, wps=16894.6, ups=1.61, wpb=10477.5, bsz=368.2, num_updates=95800, lr=7.07845e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 05:37:37 | INFO | train_inner | epoch 171:    530 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.967, ppl=1.95, wps=16985, ups=1.61, wpb=10541.3, bsz=363.4, num_updates=95900, lr=7.07475e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 05:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:37:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:37:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:38:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:38:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:38:17 | INFO | valid | epoch 171 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.675 | ppl 12.77 | bleu 23.1 | wps 4648.7 | wpb 7508.5 | bsz 272.7 | num_updates 95931 | best_bleu 23.23
2021-01-08 05:38:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:38:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:38:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:38:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:38:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:38:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 171 @ 95931 updates, score 23.1) (writing took 2.854880403727293 seconds)
2021-01-08 05:38:20 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2021-01-08 05:38:20 | INFO | train | epoch 171 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.23 | nll_loss 0.964 | ppl 1.95 | wps 15698.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 95931 | lr 7.07361e-06 | gnorm 0.82 | train_wall 347 | wall 0
2021-01-08 05:38:20 | INFO | fairseq.trainer | begin training epoch 172
2021-01-08 05:38:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:38:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:39:05 | INFO | train_inner | epoch 172:     69 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.964, ppl=1.95, wps=11747.8, ups=1.14, wpb=10320.8, bsz=361.8, num_updates=96000, lr=7.07107e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 05:40:07 | INFO | train_inner | epoch 172:    169 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.973, ppl=1.96, wps=16755.3, ups=1.61, wpb=10395.7, bsz=367.2, num_updates=96100, lr=7.06739e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 05:41:09 | INFO | train_inner | epoch 172:    269 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.961, ppl=1.95, wps=17044.3, ups=1.62, wpb=10517.1, bsz=376.3, num_updates=96200, lr=7.06371e-06, gnorm=0.813, train_wall=62, wall=0
2021-01-08 05:42:10 | INFO | train_inner | epoch 172:    369 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.961, ppl=1.95, wps=17146.8, ups=1.62, wpb=10580.3, bsz=374.6, num_updates=96300, lr=7.06005e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 05:43:12 | INFO | train_inner | epoch 172:    469 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.969, ppl=1.96, wps=16892.1, ups=1.61, wpb=10506.3, bsz=369.3, num_updates=96400, lr=7.05638e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 05:44:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:44:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:44:30 | INFO | valid | epoch 172 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.672 | ppl 12.75 | bleu 23.15 | wps 4698.1 | wpb 7508.5 | bsz 272.7 | num_updates 96492 | best_bleu 23.23
2021-01-08 05:44:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 172 @ 96492 updates, score 23.15) (writing took 2.8733592592179775 seconds)
2021-01-08 05:44:33 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2021-01-08 05:44:33 | INFO | train | epoch 172 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.231 | nll_loss 0.964 | ppl 1.95 | wps 15775.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 96492 | lr 7.05302e-06 | gnorm 0.821 | train_wall 345 | wall 0
2021-01-08 05:44:33 | INFO | fairseq.trainer | begin training epoch 173
2021-01-08 05:44:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:44:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:44:41 | INFO | train_inner | epoch 173:      8 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.961, ppl=1.95, wps=11839.2, ups=1.13, wpb=10452.9, bsz=363.2, num_updates=96500, lr=7.05273e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-08 05:45:42 | INFO | train_inner | epoch 173:    108 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.961, ppl=1.95, wps=17058, ups=1.63, wpb=10482.5, bsz=351.8, num_updates=96600, lr=7.04907e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 05:46:44 | INFO | train_inner | epoch 173:    208 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.972, ppl=1.96, wps=16850.8, ups=1.63, wpb=10339.5, bsz=373, num_updates=96700, lr=7.04543e-06, gnorm=0.839, train_wall=61, wall=0
2021-01-08 05:47:46 | INFO | train_inner | epoch 173:    308 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.968, ppl=1.96, wps=17070.2, ups=1.61, wpb=10587.7, bsz=382.9, num_updates=96800, lr=7.04179e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 05:48:47 | INFO | train_inner | epoch 173:    408 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.965, ppl=1.95, wps=17070.5, ups=1.62, wpb=10512.6, bsz=366, num_updates=96900, lr=7.03815e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 05:49:49 | INFO | train_inner | epoch 173:    508 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.961, ppl=1.95, wps=17112.1, ups=1.61, wpb=10607.4, bsz=381.5, num_updates=97000, lr=7.03452e-06, gnorm=0.8, train_wall=62, wall=0
2021-01-08 05:50:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:50:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:50:43 | INFO | valid | epoch 173 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.75 | bleu 23.1 | wps 4526.9 | wpb 7508.5 | bsz 272.7 | num_updates 97053 | best_bleu 23.23
2021-01-08 05:50:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 173 @ 97053 updates, score 23.1) (writing took 2.8592885080724955 seconds)
2021-01-08 05:50:46 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2021-01-08 05:50:46 | INFO | train | epoch 173 | symm_kl 0.349 | self_kl 0 | self_cv 0 | loss 3.23 | nll_loss 0.964 | ppl 1.95 | wps 15740.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 97053 | lr 7.0326e-06 | gnorm 0.823 | train_wall 345 | wall 0
2021-01-08 05:50:46 | INFO | fairseq.trainer | begin training epoch 174
2021-01-08 05:50:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:50:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:50:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:51:17 | INFO | train_inner | epoch 174:     47 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.962, ppl=1.95, wps=11645, ups=1.13, wpb=10281.5, bsz=373.8, num_updates=97100, lr=7.0309e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 05:52:19 | INFO | train_inner | epoch 174:    147 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.963, ppl=1.95, wps=17077.2, ups=1.63, wpb=10506.8, bsz=361.8, num_updates=97200, lr=7.02728e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 05:53:20 | INFO | train_inner | epoch 174:    247 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.958, ppl=1.94, wps=17131.5, ups=1.63, wpb=10541.6, bsz=403.8, num_updates=97300, lr=7.02367e-06, gnorm=0.793, train_wall=61, wall=0
2021-01-08 05:54:22 | INFO | train_inner | epoch 174:    347 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.959, ppl=1.94, wps=17011, ups=1.61, wpb=10534.9, bsz=357.2, num_updates=97400, lr=7.02007e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 05:55:24 | INFO | train_inner | epoch 174:    447 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.956, ppl=1.94, wps=16913.2, ups=1.62, wpb=10434.5, bsz=370.3, num_updates=97500, lr=7.01646e-06, gnorm=0.819, train_wall=62, wall=0
2021-01-08 05:56:25 | INFO | train_inner | epoch 174:    547 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.979, ppl=1.97, wps=17220.2, ups=1.63, wpb=10554.2, bsz=356.5, num_updates=97600, lr=7.01287e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 05:56:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 05:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:56:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:56:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:56:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 05:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 05:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 05:56:54 | INFO | valid | epoch 174 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.672 | ppl 12.74 | bleu 23.07 | wps 4757.1 | wpb 7508.5 | bsz 272.7 | num_updates 97614 | best_bleu 23.23
2021-01-08 05:56:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 05:56:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:56:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:56:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:56:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:56:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 174 @ 97614 updates, score 23.07) (writing took 2.851326199248433 seconds)
2021-01-08 05:56:57 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2021-01-08 05:56:57 | INFO | train | epoch 174 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.229 | nll_loss 0.963 | ppl 1.95 | wps 15850.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 97614 | lr 7.01237e-06 | gnorm 0.82 | train_wall 344 | wall 0
2021-01-08 05:56:57 | INFO | fairseq.trainer | begin training epoch 175
2021-01-08 05:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 05:57:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 05:57:52 | INFO | train_inner | epoch 175:     86 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.953, ppl=1.94, wps=11963.7, ups=1.15, wpb=10414.3, bsz=358.3, num_updates=97700, lr=7.00928e-06, gnorm=0.831, train_wall=60, wall=0
2021-01-08 05:58:54 | INFO | train_inner | epoch 175:    186 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.954, ppl=1.94, wps=17064.2, ups=1.62, wpb=10506.9, bsz=372, num_updates=97800, lr=7.00569e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 05:59:56 | INFO | train_inner | epoch 175:    286 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.969, ppl=1.96, wps=17142.4, ups=1.62, wpb=10569.3, bsz=388.1, num_updates=97900, lr=7.00212e-06, gnorm=0.803, train_wall=61, wall=0
2021-01-08 06:00:57 | INFO | train_inner | epoch 175:    386 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.979, ppl=1.97, wps=16986.9, ups=1.63, wpb=10429.3, bsz=372.9, num_updates=98000, lr=6.99854e-06, gnorm=0.837, train_wall=61, wall=0
2021-01-08 06:01:59 | INFO | train_inner | epoch 175:    486 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.969, ppl=1.96, wps=16878.8, ups=1.62, wpb=10429.4, bsz=355.7, num_updates=98100, lr=6.99497e-06, gnorm=0.832, train_wall=62, wall=0
2021-01-08 06:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:02:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:02:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:03:06 | INFO | valid | epoch 175 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.76 | bleu 23.02 | wps 4760.6 | wpb 7508.5 | bsz 272.7 | num_updates 98175 | best_bleu 23.23
2021-01-08 06:03:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:03:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 175 @ 98175 updates, score 23.02) (writing took 2.8448115326464176 seconds)
2021-01-08 06:03:09 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2021-01-08 06:03:09 | INFO | train | epoch 175 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.229 | nll_loss 0.964 | ppl 1.95 | wps 15819 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 98175 | lr 6.9923e-06 | gnorm 0.821 | train_wall 344 | wall 0
2021-01-08 06:03:09 | INFO | fairseq.trainer | begin training epoch 176
2021-01-08 06:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:03:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:03:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:03:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:03:27 | INFO | train_inner | epoch 176:     25 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.957, ppl=1.94, wps=11930.4, ups=1.13, wpb=10554.7, bsz=368.8, num_updates=98200, lr=6.99141e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 06:04:29 | INFO | train_inner | epoch 176:    125 / 561 symm_kl=0.352, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.973, ppl=1.96, wps=17095.4, ups=1.63, wpb=10500.9, bsz=359.1, num_updates=98300, lr=6.98785e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 06:05:30 | INFO | train_inner | epoch 176:    225 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.965, ppl=1.95, wps=16972, ups=1.62, wpb=10466.2, bsz=366.6, num_updates=98400, lr=6.9843e-06, gnorm=0.832, train_wall=61, wall=0
2021-01-08 06:06:32 | INFO | train_inner | epoch 176:    325 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.962, ppl=1.95, wps=16916.4, ups=1.62, wpb=10433.9, bsz=390.8, num_updates=98500, lr=6.98076e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 06:07:34 | INFO | train_inner | epoch 176:    425 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.961, ppl=1.95, wps=16960.3, ups=1.62, wpb=10483.1, bsz=373.2, num_updates=98600, lr=6.97722e-06, gnorm=0.806, train_wall=62, wall=0
2021-01-08 06:08:36 | INFO | train_inner | epoch 176:    525 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.959, ppl=1.94, wps=16917.7, ups=1.61, wpb=10515.6, bsz=353.4, num_updates=98700, lr=6.97368e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 06:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:08:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:08:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:08:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:09:19 | INFO | valid | epoch 176 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.675 | ppl 12.78 | bleu 23.08 | wps 4708.5 | wpb 7508.5 | bsz 272.7 | num_updates 98736 | best_bleu 23.23
2021-01-08 06:09:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:09:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:09:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:09:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:09:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:09:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 176 @ 98736 updates, score 23.08) (writing took 2.8317144475877285 seconds)
2021-01-08 06:09:22 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2021-01-08 06:09:22 | INFO | train | epoch 176 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.229 | nll_loss 0.964 | ppl 1.95 | wps 15781.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 98736 | lr 6.97241e-06 | gnorm 0.821 | train_wall 345 | wall 0
2021-01-08 06:09:22 | INFO | fairseq.trainer | begin training epoch 177
2021-01-08 06:09:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:09:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:10:04 | INFO | train_inner | epoch 177:     64 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.96, ppl=1.95, wps=11895.6, ups=1.14, wpb=10453.7, bsz=375.4, num_updates=98800, lr=6.97015e-06, gnorm=0.816, train_wall=61, wall=0
2021-01-08 06:11:06 | INFO | train_inner | epoch 177:    164 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.965, ppl=1.95, wps=17036.8, ups=1.62, wpb=10519.2, bsz=366.5, num_updates=98900, lr=6.96663e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 06:12:07 | INFO | train_inner | epoch 177:    264 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.966, ppl=1.95, wps=16891.2, ups=1.63, wpb=10363.5, bsz=381.8, num_updates=99000, lr=6.96311e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 06:13:09 | INFO | train_inner | epoch 177:    364 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.962, ppl=1.95, wps=17021, ups=1.63, wpb=10468.1, bsz=357.3, num_updates=99100, lr=6.95959e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 06:14:10 | INFO | train_inner | epoch 177:    464 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.959, ppl=1.94, wps=17081.2, ups=1.62, wpb=10543.5, bsz=366.1, num_updates=99200, lr=6.95608e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 06:15:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:15:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:15:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:15:33 | INFO | valid | epoch 177 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23.1 | wps 4106.9 | wpb 7508.5 | bsz 272.7 | num_updates 99297 | best_bleu 23.23
2021-01-08 06:15:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:15:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 177 @ 99297 updates, score 23.1) (writing took 2.8781138472259045 seconds)
2021-01-08 06:15:36 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2021-01-08 06:15:36 | INFO | train | epoch 177 | symm_kl 0.348 | self_kl 0 | self_cv 0 | loss 3.228 | nll_loss 0.963 | ppl 1.95 | wps 15725.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 99297 | lr 6.95269e-06 | gnorm 0.819 | train_wall 344 | wall 0
2021-01-08 06:15:36 | INFO | fairseq.trainer | begin training epoch 178
2021-01-08 06:15:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:15:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:15:41 | INFO | train_inner | epoch 178:      3 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.97, ppl=1.96, wps=11593.1, ups=1.1, wpb=10491.7, bsz=371.7, num_updates=99300, lr=6.95258e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 06:16:41 | INFO | train_inner | epoch 178:    103 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.968, ppl=1.96, wps=17370.1, ups=1.65, wpb=10542.3, bsz=379.6, num_updates=99400, lr=6.94908e-06, gnorm=0.811, train_wall=61, wall=0
2021-01-08 06:17:43 | INFO | train_inner | epoch 178:    203 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.969, ppl=1.96, wps=17086.8, ups=1.62, wpb=10545.8, bsz=371, num_updates=99500, lr=6.94559e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 06:18:45 | INFO | train_inner | epoch 178:    303 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.97, ppl=1.96, wps=17003.3, ups=1.61, wpb=10532.9, bsz=371.3, num_updates=99600, lr=6.9421e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 06:19:47 | INFO | train_inner | epoch 178:    403 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.954, ppl=1.94, wps=17016.9, ups=1.62, wpb=10508, bsz=373.8, num_updates=99700, lr=6.93862e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 06:20:48 | INFO | train_inner | epoch 178:    503 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.951, ppl=1.93, wps=16874.4, ups=1.63, wpb=10371.3, bsz=357.2, num_updates=99800, lr=6.93514e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 06:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:21:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:21:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:21:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:21:45 | INFO | valid | epoch 178 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.675 | ppl 12.77 | bleu 23.07 | wps 4770.1 | wpb 7508.5 | bsz 272.7 | num_updates 99858 | best_bleu 23.23
2021-01-08 06:21:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:21:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 178 @ 99858 updates, score 23.07) (writing took 2.8517754413187504 seconds)
2021-01-08 06:21:48 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2021-01-08 06:21:48 | INFO | train | epoch 178 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.227 | nll_loss 0.963 | ppl 1.95 | wps 15809.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 99858 | lr 6.93313e-06 | gnorm 0.821 | train_wall 345 | wall 0
2021-01-08 06:21:48 | INFO | fairseq.trainer | begin training epoch 179
2021-01-08 06:21:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:21:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:21:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:22:16 | INFO | train_inner | epoch 179:     42 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.968, ppl=1.96, wps=11853.9, ups=1.14, wpb=10382.6, bsz=360.3, num_updates=99900, lr=6.93167e-06, gnorm=0.841, train_wall=61, wall=0
2021-01-08 06:23:18 | INFO | train_inner | epoch 179:    142 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.959, ppl=1.94, wps=17068.2, ups=1.61, wpb=10575, bsz=355, num_updates=100000, lr=6.9282e-06, gnorm=0.826, train_wall=62, wall=0
2021-01-08 06:24:19 | INFO | train_inner | epoch 179:    242 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.967, ppl=1.95, wps=17121.1, ups=1.63, wpb=10520.9, bsz=371.8, num_updates=100100, lr=6.92474e-06, gnorm=0.813, train_wall=61, wall=0
2021-01-08 06:25:21 | INFO | train_inner | epoch 179:    342 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.211, nll_loss=0.954, ppl=1.94, wps=17024.9, ups=1.62, wpb=10532.8, bsz=390.2, num_updates=100200, lr=6.92129e-06, gnorm=0.803, train_wall=62, wall=0
2021-01-08 06:26:23 | INFO | train_inner | epoch 179:    442 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.969, ppl=1.96, wps=16966.5, ups=1.63, wpb=10422.2, bsz=367.4, num_updates=100300, lr=6.91783e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 06:27:24 | INFO | train_inner | epoch 179:    542 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.967, ppl=1.95, wps=17066.9, ups=1.63, wpb=10490.1, bsz=369.4, num_updates=100400, lr=6.91439e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 06:27:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:27:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:27:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:27:56 | INFO | valid | epoch 179 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.181 | nll_loss 3.674 | ppl 12.76 | bleu 23.1 | wps 4746.8 | wpb 7508.5 | bsz 272.7 | num_updates 100419 | best_bleu 23.23
2021-01-08 06:27:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:27:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:27:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:27:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:27:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 179 @ 100419 updates, score 23.1) (writing took 2.799701824784279 seconds)
2021-01-08 06:27:59 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2021-01-08 06:27:59 | INFO | train | epoch 179 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.228 | nll_loss 0.964 | ppl 1.95 | wps 15846.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 100419 | lr 6.91373e-06 | gnorm 0.821 | train_wall 344 | wall 0
2021-01-08 06:27:59 | INFO | fairseq.trainer | begin training epoch 180
2021-01-08 06:28:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:28:51 | INFO | train_inner | epoch 180:     81 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.202, nll_loss=0.942, ppl=1.92, wps=12086.6, ups=1.14, wpb=10556, bsz=365.1, num_updates=100500, lr=6.91095e-06, gnorm=0.809, train_wall=61, wall=0
2021-01-08 06:29:53 | INFO | train_inner | epoch 180:    181 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.963, ppl=1.95, wps=17027.2, ups=1.62, wpb=10541.8, bsz=367.8, num_updates=100600, lr=6.90751e-06, gnorm=0.816, train_wall=62, wall=0
2021-01-08 06:30:55 | INFO | train_inner | epoch 180:    281 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.972, ppl=1.96, wps=16931.4, ups=1.62, wpb=10466.7, bsz=370.7, num_updates=100700, lr=6.90408e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 06:31:57 | INFO | train_inner | epoch 180:    381 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.961, ppl=1.95, wps=16885.4, ups=1.61, wpb=10488.1, bsz=378.7, num_updates=100800, lr=6.90066e-06, gnorm=0.807, train_wall=62, wall=0
2021-01-08 06:32:59 | INFO | train_inner | epoch 180:    481 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.962, ppl=1.95, wps=16748.6, ups=1.62, wpb=10321.4, bsz=366.1, num_updates=100900, lr=6.89724e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 06:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:33:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:33:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:33:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:33:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:33:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:33:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:33:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:34:08 | INFO | valid | epoch 180 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.674 | ppl 12.76 | bleu 23.04 | wps 4747 | wpb 7508.5 | bsz 272.7 | num_updates 100980 | best_bleu 23.23
2021-01-08 06:34:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:34:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:34:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:34:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 180 @ 100980 updates, score 23.04) (writing took 2.797292932868004 seconds)
2021-01-08 06:34:11 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2021-01-08 06:34:11 | INFO | train | epoch 180 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.227 | nll_loss 0.962 | ppl 1.95 | wps 15793.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 100980 | lr 6.8945e-06 | gnorm 0.819 | train_wall 345 | wall 0
2021-01-08 06:34:11 | INFO | fairseq.trainer | begin training epoch 181
2021-01-08 06:34:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:34:26 | INFO | train_inner | epoch 181:     20 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.978, ppl=1.97, wps=11910.9, ups=1.14, wpb=10419.5, bsz=364.4, num_updates=101000, lr=6.89382e-06, gnorm=0.839, train_wall=61, wall=0
2021-01-08 06:35:28 | INFO | train_inner | epoch 181:    120 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.216, nll_loss=0.954, ppl=1.94, wps=17124.8, ups=1.63, wpb=10522.6, bsz=375, num_updates=101100, lr=6.89041e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 06:36:29 | INFO | train_inner | epoch 181:    220 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.97, ppl=1.96, wps=17059.4, ups=1.62, wpb=10508.8, bsz=372.6, num_updates=101200, lr=6.887e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 06:37:31 | INFO | train_inner | epoch 181:    320 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.97, ppl=1.96, wps=16939.8, ups=1.62, wpb=10450.9, bsz=360.7, num_updates=101300, lr=6.8836e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 06:38:33 | INFO | train_inner | epoch 181:    420 / 561 symm_kl=0.34, self_kl=0, self_cv=0, loss=3.203, nll_loss=0.947, ppl=1.93, wps=17538.2, ups=1.63, wpb=10763.3, bsz=377.1, num_updates=101400, lr=6.88021e-06, gnorm=0.796, train_wall=61, wall=0
2021-01-08 06:39:35 | INFO | train_inner | epoch 181:    520 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.971, ppl=1.96, wps=16578.1, ups=1.61, wpb=10277.3, bsz=371.4, num_updates=101500, lr=6.87682e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 06:40:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:40:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:40:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:40:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:40:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:40:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:40:20 | INFO | valid | epoch 181 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.673 | ppl 12.75 | bleu 23.08 | wps 4774.6 | wpb 7508.5 | bsz 272.7 | num_updates 101541 | best_bleu 23.23
2021-01-08 06:40:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:40:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:40:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:40:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 181 @ 101541 updates, score 23.08) (writing took 2.809259520843625 seconds)
2021-01-08 06:40:23 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2021-01-08 06:40:23 | INFO | train | epoch 181 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.227 | nll_loss 0.963 | ppl 1.95 | wps 15826.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 101541 | lr 6.87543e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 06:40:23 | INFO | fairseq.trainer | begin training epoch 182
2021-01-08 06:40:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:40:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:41:02 | INFO | train_inner | epoch 182:     59 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.964, ppl=1.95, wps=11821.9, ups=1.14, wpb=10328.9, bsz=357.4, num_updates=101600, lr=6.87343e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 06:42:04 | INFO | train_inner | epoch 182:    159 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.957, ppl=1.94, wps=16879.7, ups=1.62, wpb=10450.6, bsz=369.8, num_updates=101700, lr=6.87005e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 06:43:05 | INFO | train_inner | epoch 182:    259 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.952, ppl=1.93, wps=17115.8, ups=1.63, wpb=10500.2, bsz=376.2, num_updates=101800, lr=6.86668e-06, gnorm=0.818, train_wall=61, wall=0
2021-01-08 06:44:07 | INFO | train_inner | epoch 182:    359 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.973, ppl=1.96, wps=16976, ups=1.62, wpb=10498.3, bsz=367.8, num_updates=101900, lr=6.86331e-06, gnorm=0.826, train_wall=62, wall=0
2021-01-08 06:45:09 | INFO | train_inner | epoch 182:    459 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.96, ppl=1.95, wps=17118, ups=1.62, wpb=10555.9, bsz=363.4, num_updates=102000, lr=6.85994e-06, gnorm=0.817, train_wall=61, wall=0
2021-01-08 06:46:10 | INFO | train_inner | epoch 182:    559 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.966, ppl=1.95, wps=17167.5, ups=1.63, wpb=10550.7, bsz=379.8, num_updates=102100, lr=6.85658e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 06:46:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:46:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:46:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:46:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:46:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:46:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:46:32 | INFO | valid | epoch 182 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.671 | ppl 12.74 | bleu 23.11 | wps 4741.4 | wpb 7508.5 | bsz 272.7 | num_updates 102102 | best_bleu 23.23
2021-01-08 06:46:32 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:46:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:46:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:46:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 182 @ 102102 updates, score 23.11) (writing took 2.8095588590949774 seconds)
2021-01-08 06:46:35 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2021-01-08 06:46:35 | INFO | train | epoch 182 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.226 | nll_loss 0.962 | ppl 1.95 | wps 15820.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 102102 | lr 6.85652e-06 | gnorm 0.822 | train_wall 344 | wall 0
2021-01-08 06:46:35 | INFO | fairseq.trainer | begin training epoch 183
2021-01-08 06:46:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:47:37 | INFO | train_inner | epoch 183:     98 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.968, ppl=1.96, wps=12116.7, ups=1.15, wpb=10535.2, bsz=375.8, num_updates=102200, lr=6.85323e-06, gnorm=0.818, train_wall=60, wall=0
2021-01-08 06:48:39 | INFO | train_inner | epoch 183:    198 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.964, ppl=1.95, wps=17090.2, ups=1.62, wpb=10518.9, bsz=369.6, num_updates=102300, lr=6.84988e-06, gnorm=0.816, train_wall=61, wall=0
2021-01-08 06:49:40 | INFO | train_inner | epoch 183:    298 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.959, ppl=1.94, wps=16813.8, ups=1.62, wpb=10360.5, bsz=383, num_updates=102400, lr=6.84653e-06, gnorm=0.813, train_wall=61, wall=0
2021-01-08 06:50:42 | INFO | train_inner | epoch 183:    398 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.954, ppl=1.94, wps=17123.2, ups=1.63, wpb=10521.4, bsz=360, num_updates=102500, lr=6.84319e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 06:51:43 | INFO | train_inner | epoch 183:    498 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.958, ppl=1.94, wps=17186.8, ups=1.63, wpb=10547.4, bsz=361.6, num_updates=102600, lr=6.83986e-06, gnorm=0.813, train_wall=61, wall=0
2021-01-08 06:52:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:52:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:52:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:52:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:52:42 | INFO | valid | epoch 183 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.176 | nll_loss 3.671 | ppl 12.73 | bleu 23.05 | wps 4753.7 | wpb 7508.5 | bsz 272.7 | num_updates 102663 | best_bleu 23.23
2021-01-08 06:52:42 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:52:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:52:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 183 @ 102663 updates, score 23.05) (writing took 2.876842288300395 seconds)
2021-01-08 06:52:45 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2021-01-08 06:52:45 | INFO | train | epoch 183 | symm_kl 0.347 | self_kl 0 | self_cv 0 | loss 3.226 | nll_loss 0.963 | ppl 1.95 | wps 15869.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 102663 | lr 6.83776e-06 | gnorm 0.819 | train_wall 343 | wall 0
2021-01-08 06:52:45 | INFO | fairseq.trainer | begin training epoch 184
2021-01-08 06:52:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:53:11 | INFO | train_inner | epoch 184:     37 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.975, ppl=1.97, wps=11799.7, ups=1.14, wpb=10334.1, bsz=367.8, num_updates=102700, lr=6.83652e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 06:54:12 | INFO | train_inner | epoch 184:    137 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.968, ppl=1.96, wps=17067.3, ups=1.64, wpb=10428.4, bsz=378.4, num_updates=102800, lr=6.8332e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 06:55:13 | INFO | train_inner | epoch 184:    237 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.966, ppl=1.95, wps=17108, ups=1.62, wpb=10557.7, bsz=370.3, num_updates=102900, lr=6.82988e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-08 06:56:14 | INFO | train_inner | epoch 184:    337 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.961, ppl=1.95, wps=16995.7, ups=1.64, wpb=10368, bsz=359.8, num_updates=103000, lr=6.82656e-06, gnorm=0.841, train_wall=61, wall=0
2021-01-08 06:57:16 | INFO | train_inner | epoch 184:    437 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.958, ppl=1.94, wps=17064.4, ups=1.62, wpb=10528.5, bsz=393, num_updates=103100, lr=6.82325e-06, gnorm=0.815, train_wall=62, wall=0
2021-01-08 06:58:18 | INFO | train_inner | epoch 184:    537 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.958, ppl=1.94, wps=17252.7, ups=1.63, wpb=10591.6, bsz=347.5, num_updates=103200, lr=6.81994e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 06:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 06:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:58:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 06:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 06:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 06:58:53 | INFO | valid | epoch 184 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 22.97 | wps 4749.3 | wpb 7508.5 | bsz 272.7 | num_updates 103224 | best_bleu 23.23
2021-01-08 06:58:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 06:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:58:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:58:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 184 @ 103224 updates, score 22.97) (writing took 2.7963087428361177 seconds)
2021-01-08 06:58:55 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2021-01-08 06:58:55 | INFO | train | epoch 184 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.226 | nll_loss 0.963 | ppl 1.95 | wps 15882.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 103224 | lr 6.81915e-06 | gnorm 0.824 | train_wall 343 | wall 0
2021-01-08 06:58:55 | INFO | fairseq.trainer | begin training epoch 185
2021-01-08 06:58:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 06:58:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 06:59:44 | INFO | train_inner | epoch 185:     76 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.959, ppl=1.94, wps=11872.5, ups=1.15, wpb=10304.3, bsz=385.7, num_updates=103300, lr=6.81664e-06, gnorm=0.838, train_wall=60, wall=0
2021-01-08 07:00:46 | INFO | train_inner | epoch 185:    176 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.204, nll_loss=0.945, ppl=1.93, wps=17238.7, ups=1.62, wpb=10635.5, bsz=375.4, num_updates=103400, lr=6.81334e-06, gnorm=0.802, train_wall=62, wall=0
2021-01-08 07:01:47 | INFO | train_inner | epoch 185:    276 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.963, ppl=1.95, wps=17020.8, ups=1.63, wpb=10441.3, bsz=369.7, num_updates=103500, lr=6.81005e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 07:02:49 | INFO | train_inner | epoch 185:    376 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.971, ppl=1.96, wps=17066.9, ups=1.63, wpb=10491.1, bsz=358.8, num_updates=103600, lr=6.80676e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 07:03:51 | INFO | train_inner | epoch 185:    476 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.973, ppl=1.96, wps=17040.4, ups=1.62, wpb=10508, bsz=353.2, num_updates=103700, lr=6.80348e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 07:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:04:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:04:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:04:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:04:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:05:03 | INFO | valid | epoch 185 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.76 | bleu 22.99 | wps 4759.6 | wpb 7508.5 | bsz 272.7 | num_updates 103785 | best_bleu 23.23
2021-01-08 07:05:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:05:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:05:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:05:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:05:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:05:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 185 @ 103785 updates, score 22.99) (writing took 2.9099280666559935 seconds)
2021-01-08 07:05:06 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2021-01-08 07:05:06 | INFO | train | epoch 185 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.225 | nll_loss 0.962 | ppl 1.95 | wps 15873.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 103785 | lr 6.8007e-06 | gnorm 0.82 | train_wall 343 | wall 0
2021-01-08 07:05:06 | INFO | fairseq.trainer | begin training epoch 186
2021-01-08 07:05:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:05:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:05:18 | INFO | train_inner | epoch 186:     15 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.965, ppl=1.95, wps=11955.9, ups=1.14, wpb=10465.6, bsz=374.5, num_updates=103800, lr=6.8002e-06, gnorm=0.824, train_wall=61, wall=0
2021-01-08 07:06:19 | INFO | train_inner | epoch 186:    115 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.957, ppl=1.94, wps=17194.8, ups=1.63, wpb=10536.2, bsz=344.6, num_updates=103900, lr=6.79693e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 07:07:21 | INFO | train_inner | epoch 186:    215 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.965, ppl=1.95, wps=16927.9, ups=1.62, wpb=10456.6, bsz=373, num_updates=104000, lr=6.79366e-06, gnorm=0.803, train_wall=62, wall=0
2021-01-08 07:08:23 | INFO | train_inner | epoch 186:    315 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.971, ppl=1.96, wps=16861.8, ups=1.6, wpb=10507.5, bsz=369.5, num_updates=104100, lr=6.7904e-06, gnorm=0.813, train_wall=62, wall=0
2021-01-08 07:09:25 | INFO | train_inner | epoch 186:    415 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.954, ppl=1.94, wps=17089.1, ups=1.63, wpb=10496, bsz=381.9, num_updates=104200, lr=6.78714e-06, gnorm=0.813, train_wall=61, wall=0
2021-01-08 07:10:26 | INFO | train_inner | epoch 186:    515 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.958, ppl=1.94, wps=17215.7, ups=1.62, wpb=10596.1, bsz=388.3, num_updates=104300, lr=6.78388e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 07:10:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:10:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:10:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:10:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:10:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:10:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:11:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:11:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:11:15 | INFO | valid | epoch 186 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.672 | ppl 12.75 | bleu 23.04 | wps 4736.7 | wpb 7508.5 | bsz 272.7 | num_updates 104346 | best_bleu 23.23
2021-01-08 07:11:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:11:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:11:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:11:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 186 @ 104346 updates, score 23.04) (writing took 2.8409945107996464 seconds)
2021-01-08 07:11:18 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2021-01-08 07:11:18 | INFO | train | epoch 186 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.225 | nll_loss 0.962 | ppl 1.95 | wps 15820.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 104346 | lr 6.78239e-06 | gnorm 0.819 | train_wall 344 | wall 0
2021-01-08 07:11:18 | INFO | fairseq.trainer | begin training epoch 187
2021-01-08 07:11:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:11:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:11:54 | INFO | train_inner | epoch 187:     54 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.96, ppl=1.95, wps=11886.1, ups=1.15, wpb=10356.6, bsz=350.4, num_updates=104400, lr=6.78064e-06, gnorm=0.839, train_wall=61, wall=0
2021-01-08 07:12:55 | INFO | train_inner | epoch 187:    154 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.206, nll_loss=0.948, ppl=1.93, wps=17281.6, ups=1.62, wpb=10651.2, bsz=373, num_updates=104500, lr=6.77739e-06, gnorm=0.81, train_wall=61, wall=0
2021-01-08 07:13:57 | INFO | train_inner | epoch 187:    254 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.969, ppl=1.96, wps=16686.2, ups=1.61, wpb=10357.7, bsz=369, num_updates=104600, lr=6.77415e-06, gnorm=0.83, train_wall=62, wall=0
2021-01-08 07:14:59 | INFO | train_inner | epoch 187:    354 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.965, ppl=1.95, wps=16967.4, ups=1.62, wpb=10460.2, bsz=374.3, num_updates=104700, lr=6.77091e-06, gnorm=0.805, train_wall=61, wall=0
2021-01-08 07:16:01 | INFO | train_inner | epoch 187:    454 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.964, ppl=1.95, wps=16954.1, ups=1.62, wpb=10483.9, bsz=369.4, num_updates=104800, lr=6.76768e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 07:17:02 | INFO | train_inner | epoch 187:    554 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.964, ppl=1.95, wps=17110.2, ups=1.63, wpb=10511.6, bsz=374.3, num_updates=104900, lr=6.76446e-06, gnorm=0.812, train_wall=61, wall=0
2021-01-08 07:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:17:27 | INFO | valid | epoch 187 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.673 | ppl 12.76 | bleu 23.06 | wps 4767.1 | wpb 7508.5 | bsz 272.7 | num_updates 104907 | best_bleu 23.23
2021-01-08 07:17:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:17:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:17:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 187 @ 104907 updates, score 23.06) (writing took 2.8544188048690557 seconds)
2021-01-08 07:17:29 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2021-01-08 07:17:29 | INFO | train | epoch 187 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.224 | nll_loss 0.961 | ppl 1.95 | wps 15818.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 104907 | lr 6.76423e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 07:17:29 | INFO | fairseq.trainer | begin training epoch 188
2021-01-08 07:17:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:18:29 | INFO | train_inner | epoch 188:     93 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.956, ppl=1.94, wps=11976.7, ups=1.15, wpb=10381.2, bsz=354.6, num_updates=105000, lr=6.76123e-06, gnorm=0.838, train_wall=60, wall=0
2021-01-08 07:19:30 | INFO | train_inner | epoch 188:    193 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.967, ppl=1.95, wps=17049.7, ups=1.63, wpb=10481.8, bsz=373.8, num_updates=105100, lr=6.75802e-06, gnorm=0.812, train_wall=61, wall=0
2021-01-08 07:20:32 | INFO | train_inner | epoch 188:    293 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.976, ppl=1.97, wps=16857.5, ups=1.62, wpb=10430.6, bsz=368.4, num_updates=105200, lr=6.7548e-06, gnorm=0.835, train_wall=62, wall=0
2021-01-08 07:21:34 | INFO | train_inner | epoch 188:    393 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.962, ppl=1.95, wps=17035.5, ups=1.63, wpb=10465.4, bsz=363.6, num_updates=105300, lr=6.7516e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 07:22:35 | INFO | train_inner | epoch 188:    493 / 561 symm_kl=0.339, self_kl=0, self_cv=0, loss=3.202, nll_loss=0.947, ppl=1.93, wps=17190, ups=1.63, wpb=10577.3, bsz=389.2, num_updates=105400, lr=6.74839e-06, gnorm=0.801, train_wall=61, wall=0
2021-01-08 07:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:23:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:23:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:23:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:23:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:23:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:23:38 | INFO | valid | epoch 188 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.76 | bleu 23.04 | wps 4746.9 | wpb 7508.5 | bsz 272.7 | num_updates 105468 | best_bleu 23.23
2021-01-08 07:23:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:23:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 188 @ 105468 updates, score 23.04) (writing took 2.893468417227268 seconds)
2021-01-08 07:23:41 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2021-01-08 07:23:41 | INFO | train | epoch 188 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.224 | nll_loss 0.962 | ppl 1.95 | wps 15845.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 105468 | lr 6.74622e-06 | gnorm 0.82 | train_wall 343 | wall 0
2021-01-08 07:23:41 | INFO | fairseq.trainer | begin training epoch 189
2021-01-08 07:23:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:23:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:24:03 | INFO | train_inner | epoch 189:     32 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.968, ppl=1.96, wps=11908.8, ups=1.14, wpb=10459.8, bsz=368, num_updates=105500, lr=6.74519e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 07:25:04 | INFO | train_inner | epoch 189:    132 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.966, ppl=1.95, wps=17236.5, ups=1.63, wpb=10553.1, bsz=379.8, num_updates=105600, lr=6.742e-06, gnorm=0.812, train_wall=61, wall=0
2021-01-08 07:26:06 | INFO | train_inner | epoch 189:    232 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.96, ppl=1.94, wps=16905.8, ups=1.63, wpb=10389.9, bsz=367.5, num_updates=105700, lr=6.73881e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 07:27:07 | INFO | train_inner | epoch 189:    332 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.958, ppl=1.94, wps=17022.3, ups=1.62, wpb=10501.9, bsz=361.1, num_updates=105800, lr=6.73562e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 07:28:09 | INFO | train_inner | epoch 189:    432 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.951, ppl=1.93, wps=17255.3, ups=1.62, wpb=10636.8, bsz=379, num_updates=105900, lr=6.73244e-06, gnorm=0.803, train_wall=61, wall=0
2021-01-08 07:29:11 | INFO | train_inner | epoch 189:    532 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.959, ppl=1.94, wps=16987.7, ups=1.62, wpb=10464.4, bsz=361.4, num_updates=106000, lr=6.72927e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 07:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:29:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:29:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:29:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:29:49 | INFO | valid | epoch 189 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.672 | ppl 12.75 | bleu 23.01 | wps 4793.1 | wpb 7508.5 | bsz 272.7 | num_updates 106029 | best_bleu 23.23
2021-01-08 07:29:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 189 @ 106029 updates, score 23.01) (writing took 2.8095906767994165 seconds)
2021-01-08 07:29:52 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2021-01-08 07:29:52 | INFO | train | epoch 189 | symm_kl 0.346 | self_kl 0 | self_cv 0 | loss 3.223 | nll_loss 0.961 | ppl 1.95 | wps 15850.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 106029 | lr 6.72835e-06 | gnorm 0.82 | train_wall 344 | wall 0
2021-01-08 07:29:52 | INFO | fairseq.trainer | begin training epoch 190
2021-01-08 07:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:29:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:29:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:30:38 | INFO | train_inner | epoch 190:     71 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.96, ppl=1.94, wps=11934.4, ups=1.14, wpb=10452.8, bsz=355.2, num_updates=106100, lr=6.72609e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 07:31:40 | INFO | train_inner | epoch 190:    171 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.203, nll_loss=0.946, ppl=1.93, wps=16936, ups=1.63, wpb=10418.3, bsz=393.5, num_updates=106200, lr=6.72293e-06, gnorm=0.804, train_wall=61, wall=0
2021-01-08 07:32:41 | INFO | train_inner | epoch 190:    271 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.959, ppl=1.94, wps=17174.7, ups=1.63, wpb=10560, bsz=388.9, num_updates=106300, lr=6.71976e-06, gnorm=0.814, train_wall=61, wall=0
2021-01-08 07:33:43 | INFO | train_inner | epoch 190:    371 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.97, ppl=1.96, wps=17084.7, ups=1.63, wpb=10487.4, bsz=366, num_updates=106400, lr=6.7166e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 07:34:45 | INFO | train_inner | epoch 190:    471 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.968, ppl=1.96, wps=16920.8, ups=1.61, wpb=10478.5, bsz=364.9, num_updates=106500, lr=6.71345e-06, gnorm=0.809, train_wall=62, wall=0
2021-01-08 07:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:36:00 | INFO | valid | epoch 190 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 23.02 | wps 4809.8 | wpb 7508.5 | bsz 272.7 | num_updates 106590 | best_bleu 23.23
2021-01-08 07:36:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:36:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 190 @ 106590 updates, score 23.02) (writing took 2.846900576725602 seconds)
2021-01-08 07:36:03 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2021-01-08 07:36:03 | INFO | train | epoch 190 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.223 | nll_loss 0.961 | ppl 1.95 | wps 15850.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 106590 | lr 6.71062e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 07:36:03 | INFO | fairseq.trainer | begin training epoch 191
2021-01-08 07:36:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:36:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:36:12 | INFO | train_inner | epoch 191:     10 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.965, ppl=1.95, wps=11882, ups=1.14, wpb=10394.8, bsz=351, num_updates=106600, lr=6.7103e-06, gnorm=0.844, train_wall=61, wall=0
2021-01-08 07:37:13 | INFO | train_inner | epoch 191:    110 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.976, ppl=1.97, wps=17303.7, ups=1.65, wpb=10512.3, bsz=386.2, num_updates=106700, lr=6.70716e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 07:38:15 | INFO | train_inner | epoch 191:    210 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.205, nll_loss=0.949, ppl=1.93, wps=16864.5, ups=1.61, wpb=10468.8, bsz=370.1, num_updates=106800, lr=6.70402e-06, gnorm=0.813, train_wall=62, wall=0
2021-01-08 07:39:16 | INFO | train_inner | epoch 191:    310 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.959, ppl=1.94, wps=17179.1, ups=1.63, wpb=10551.1, bsz=362, num_updates=106900, lr=6.70088e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 07:40:18 | INFO | train_inner | epoch 191:    410 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.95, ppl=1.93, wps=17100.1, ups=1.62, wpb=10542.5, bsz=364.5, num_updates=107000, lr=6.69775e-06, gnorm=0.81, train_wall=61, wall=0
2021-01-08 07:41:20 | INFO | train_inner | epoch 191:    510 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.971, ppl=1.96, wps=16885.2, ups=1.61, wpb=10457.4, bsz=372.7, num_updates=107100, lr=6.69462e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 07:41:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:41:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:41:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:41:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:41:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:41:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:41:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:42:11 | INFO | valid | epoch 191 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.177 | nll_loss 3.672 | ppl 12.74 | bleu 23.12 | wps 4716.4 | wpb 7508.5 | bsz 272.7 | num_updates 107151 | best_bleu 23.23
2021-01-08 07:42:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:42:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:42:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:42:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 191 @ 107151 updates, score 23.12) (writing took 2.809220280498266 seconds)
2021-01-08 07:42:14 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2021-01-08 07:42:14 | INFO | train | epoch 191 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.223 | nll_loss 0.961 | ppl 1.95 | wps 15826.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 107151 | lr 6.69303e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 07:42:14 | INFO | fairseq.trainer | begin training epoch 192
2021-01-08 07:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:42:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:42:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:42:47 | INFO | train_inner | epoch 192:     49 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.963, ppl=1.95, wps=11909.1, ups=1.14, wpb=10414.9, bsz=366.2, num_updates=107200, lr=6.6915e-06, gnorm=0.825, train_wall=61, wall=0
2021-01-08 07:43:49 | INFO | train_inner | epoch 192:    149 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.957, ppl=1.94, wps=17081.7, ups=1.62, wpb=10551.1, bsz=365.7, num_updates=107300, lr=6.68838e-06, gnorm=0.817, train_wall=62, wall=0
2021-01-08 07:44:51 | INFO | train_inner | epoch 192:    249 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.967, ppl=1.95, wps=17019.7, ups=1.62, wpb=10502.9, bsz=365.4, num_updates=107400, lr=6.68526e-06, gnorm=0.82, train_wall=62, wall=0
2021-01-08 07:45:52 | INFO | train_inner | epoch 192:    349 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.976, ppl=1.97, wps=16885.3, ups=1.63, wpb=10388.5, bsz=364.8, num_updates=107500, lr=6.68215e-06, gnorm=0.835, train_wall=61, wall=0
2021-01-08 07:46:54 | INFO | train_inner | epoch 192:    449 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.956, ppl=1.94, wps=17054.1, ups=1.63, wpb=10491.6, bsz=364.1, num_updates=107600, lr=6.67905e-06, gnorm=0.818, train_wall=61, wall=0
2021-01-08 07:47:55 | INFO | train_inner | epoch 192:    549 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.208, nll_loss=0.951, ppl=1.93, wps=17068.4, ups=1.63, wpb=10455.6, bsz=382.2, num_updates=107700, lr=6.67595e-06, gnorm=0.806, train_wall=61, wall=0
2021-01-08 07:48:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:48:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:48:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:48:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:48:25 | INFO | valid | epoch 192 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.18 | nll_loss 3.674 | ppl 12.76 | bleu 22.98 | wps 4117.3 | wpb 7508.5 | bsz 272.7 | num_updates 107712 | best_bleu 23.23
2021-01-08 07:48:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 192 @ 107712 updates, score 22.98) (writing took 2.8801158983260393 seconds)
2021-01-08 07:48:28 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2021-01-08 07:48:28 | INFO | train | epoch 192 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.222 | nll_loss 0.961 | ppl 1.95 | wps 15725.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 107712 | lr 6.67557e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 07:48:28 | INFO | fairseq.trainer | begin training epoch 193
2021-01-08 07:48:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:48:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:48:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:49:25 | INFO | train_inner | epoch 193:     88 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.203, nll_loss=0.946, ppl=1.93, wps=11735.2, ups=1.11, wpb=10545, bsz=372.2, num_updates=107800, lr=6.67285e-06, gnorm=0.815, train_wall=61, wall=0
2021-01-08 07:50:27 | INFO | train_inner | epoch 193:    188 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.977, ppl=1.97, wps=16789.2, ups=1.62, wpb=10350.9, bsz=376.3, num_updates=107900, lr=6.66976e-06, gnorm=0.827, train_wall=61, wall=0
2021-01-08 07:51:28 | INFO | train_inner | epoch 193:    288 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.955, ppl=1.94, wps=16852.5, ups=1.62, wpb=10420.8, bsz=369.6, num_updates=108000, lr=6.66667e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-08 07:52:30 | INFO | train_inner | epoch 193:    388 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.952, ppl=1.93, wps=17170.7, ups=1.62, wpb=10600.7, bsz=365, num_updates=108100, lr=6.66358e-06, gnorm=0.804, train_wall=62, wall=0
2021-01-08 07:53:32 | INFO | train_inner | epoch 193:    488 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.966, ppl=1.95, wps=16881.9, ups=1.62, wpb=10429, bsz=364.3, num_updates=108200, lr=6.6605e-06, gnorm=0.831, train_wall=62, wall=0
2021-01-08 07:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 07:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:54:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 07:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 07:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 07:54:38 | INFO | valid | epoch 193 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.674 | ppl 12.76 | bleu 23.04 | wps 4614.3 | wpb 7508.5 | bsz 272.7 | num_updates 108273 | best_bleu 23.23
2021-01-08 07:54:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 07:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:54:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:54:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 193 @ 108273 updates, score 23.04) (writing took 2.8496366664767265 seconds)
2021-01-08 07:54:41 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2021-01-08 07:54:41 | INFO | train | epoch 193 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.222 | nll_loss 0.961 | ppl 1.95 | wps 15786.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 108273 | lr 6.65826e-06 | gnorm 0.818 | train_wall 344 | wall 0
2021-01-08 07:54:41 | INFO | fairseq.trainer | begin training epoch 194
2021-01-08 07:54:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 07:54:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 07:55:00 | INFO | train_inner | epoch 194:     27 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.958, ppl=1.94, wps=11961.2, ups=1.13, wpb=10584, bsz=379.8, num_updates=108300, lr=6.65743e-06, gnorm=0.805, train_wall=61, wall=0
2021-01-08 07:56:02 | INFO | train_inner | epoch 194:    127 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.962, ppl=1.95, wps=17164.1, ups=1.63, wpb=10511, bsz=380.8, num_updates=108400, lr=6.65436e-06, gnorm=0.811, train_wall=61, wall=0
2021-01-08 07:57:03 | INFO | train_inner | epoch 194:    227 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.966, ppl=1.95, wps=16968.6, ups=1.63, wpb=10419.7, bsz=362.2, num_updates=108500, lr=6.65129e-06, gnorm=0.818, train_wall=61, wall=0
2021-01-08 07:58:05 | INFO | train_inner | epoch 194:    327 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.959, ppl=1.94, wps=16927.2, ups=1.61, wpb=10500.1, bsz=358.6, num_updates=108600, lr=6.64822e-06, gnorm=0.821, train_wall=62, wall=0
2021-01-08 07:59:07 | INFO | train_inner | epoch 194:    427 / 561 symm_kl=0.35, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.973, ppl=1.96, wps=17042.5, ups=1.62, wpb=10517.1, bsz=358.1, num_updates=108700, lr=6.64517e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 08:00:08 | INFO | train_inner | epoch 194:    527 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.957, ppl=1.94, wps=17017.3, ups=1.62, wpb=10486.8, bsz=374.2, num_updates=108800, lr=6.64211e-06, gnorm=0.816, train_wall=61, wall=0
2021-01-08 08:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:00:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:00:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:00:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:00:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:00:50 | INFO | valid | epoch 194 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.672 | ppl 12.75 | bleu 23.03 | wps 4672.7 | wpb 7508.5 | bsz 272.7 | num_updates 108834 | best_bleu 23.23
2021-01-08 08:00:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:00:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 194 @ 108834 updates, score 23.03) (writing took 2.847573544830084 seconds)
2021-01-08 08:00:53 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2021-01-08 08:00:53 | INFO | train | epoch 194 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.222 | nll_loss 0.961 | ppl 1.95 | wps 15813.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 108834 | lr 6.64107e-06 | gnorm 0.815 | train_wall 344 | wall 0
2021-01-08 08:00:53 | INFO | fairseq.trainer | begin training epoch 195
2021-01-08 08:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:00:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:01:36 | INFO | train_inner | epoch 195:     66 / 561 symm_kl=0.337, self_kl=0, self_cv=0, loss=3.196, nll_loss=0.946, ppl=1.93, wps=11926.6, ups=1.14, wpb=10455.6, bsz=376.6, num_updates=108900, lr=6.63906e-06, gnorm=0.808, train_wall=61, wall=0
2021-01-08 08:02:38 | INFO | train_inner | epoch 195:    166 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.947, ppl=1.93, wps=16940.1, ups=1.62, wpb=10480.8, bsz=351.2, num_updates=109000, lr=6.63602e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 08:03:40 | INFO | train_inner | epoch 195:    266 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.969, ppl=1.96, wps=16955.4, ups=1.62, wpb=10481.9, bsz=375.2, num_updates=109100, lr=6.63297e-06, gnorm=0.823, train_wall=62, wall=0
2021-01-08 08:04:42 | INFO | train_inner | epoch 195:    366 / 561 symm_kl=0.34, self_kl=0, self_cv=0, loss=3.207, nll_loss=0.951, ppl=1.93, wps=16938.7, ups=1.62, wpb=10477.6, bsz=386, num_updates=109200, lr=6.62994e-06, gnorm=0.803, train_wall=62, wall=0
2021-01-08 08:05:44 | INFO | train_inner | epoch 195:    466 / 561 symm_kl=0.351, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.973, ppl=1.96, wps=16872.7, ups=1.61, wpb=10511.3, bsz=363.5, num_updates=109300, lr=6.6269e-06, gnorm=0.828, train_wall=62, wall=0
2021-01-08 08:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:06:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:06:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:06:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:06:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:06:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:07:03 | INFO | valid | epoch 195 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 23.06 | wps 4727.2 | wpb 7508.5 | bsz 272.7 | num_updates 109395 | best_bleu 23.23
2021-01-08 08:07:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:07:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:07:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:07:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:07:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:07:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 195 @ 109395 updates, score 23.06) (writing took 2.8176805544644594 seconds)
2021-01-08 08:07:06 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2021-01-08 08:07:06 | INFO | train | epoch 195 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.221 | nll_loss 0.96 | ppl 1.95 | wps 15771.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 109395 | lr 6.62402e-06 | gnorm 0.82 | train_wall 345 | wall 0
2021-01-08 08:07:06 | INFO | fairseq.trainer | begin training epoch 196
2021-01-08 08:07:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:07:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:07:12 | INFO | train_inner | epoch 196:      5 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.965, ppl=1.95, wps=11870.5, ups=1.14, wpb=10438, bsz=367.1, num_updates=109400, lr=6.62387e-06, gnorm=0.833, train_wall=61, wall=0
2021-01-08 08:08:13 | INFO | train_inner | epoch 196:    105 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.966, ppl=1.95, wps=17368.7, ups=1.63, wpb=10661.4, bsz=389.9, num_updates=109500, lr=6.62085e-06, gnorm=0.791, train_wall=61, wall=0
2021-01-08 08:09:15 | INFO | train_inner | epoch 196:    205 / 561 symm_kl=0.349, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.965, ppl=1.95, wps=16790.6, ups=1.62, wpb=10355.7, bsz=366.3, num_updates=109600, lr=6.61783e-06, gnorm=0.831, train_wall=61, wall=0
2021-01-08 08:10:17 | INFO | train_inner | epoch 196:    305 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.956, ppl=1.94, wps=16833.4, ups=1.61, wpb=10434, bsz=362.3, num_updates=109700, lr=6.61481e-06, gnorm=0.829, train_wall=62, wall=0
2021-01-08 08:11:19 | INFO | train_inner | epoch 196:    405 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.226, nll_loss=0.964, ppl=1.95, wps=16690.9, ups=1.61, wpb=10361.7, bsz=363.5, num_updates=109800, lr=6.6118e-06, gnorm=0.825, train_wall=62, wall=0
2021-01-08 08:12:21 | INFO | train_inner | epoch 196:    505 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.21, nll_loss=0.952, ppl=1.94, wps=17068.4, ups=1.61, wpb=10609.2, bsz=377.7, num_updates=109900, lr=6.60879e-06, gnorm=0.802, train_wall=62, wall=0
2021-01-08 08:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:12:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:12:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:13:16 | INFO | valid | epoch 196 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 23.16 | wps 4711.4 | wpb 7508.5 | bsz 272.7 | num_updates 109956 | best_bleu 23.23
2021-01-08 08:13:16 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:13:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:13:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 196 @ 109956 updates, score 23.16) (writing took 2.8548831548541784 seconds)
2021-01-08 08:13:19 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2021-01-08 08:13:19 | INFO | train | epoch 196 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.221 | nll_loss 0.96 | ppl 1.95 | wps 15755.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 109956 | lr 6.6071e-06 | gnorm 0.817 | train_wall 346 | wall 0
2021-01-08 08:13:19 | INFO | fairseq.trainer | begin training epoch 197
2021-01-08 08:13:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:13:49 | INFO | train_inner | epoch 197:     44 / 561 symm_kl=0.342, self_kl=0, self_cv=0, loss=3.21, nll_loss=0.953, ppl=1.94, wps=11933.5, ups=1.14, wpb=10489.5, bsz=365.1, num_updates=110000, lr=6.60578e-06, gnorm=0.823, train_wall=61, wall=0
2021-01-08 08:14:51 | INFO | train_inner | epoch 197:    144 / 561 symm_kl=0.339, self_kl=0, self_cv=0, loss=3.196, nll_loss=0.941, ppl=1.92, wps=17127.6, ups=1.63, wpb=10532.8, bsz=386.9, num_updates=110100, lr=6.60278e-06, gnorm=0.81, train_wall=61, wall=0
2021-01-08 08:15:52 | INFO | train_inner | epoch 197:    244 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.968, ppl=1.96, wps=16902.5, ups=1.62, wpb=10428.7, bsz=364.2, num_updates=110200, lr=6.59979e-06, gnorm=0.824, train_wall=62, wall=0
2021-01-08 08:16:54 | INFO | train_inner | epoch 197:    344 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.962, ppl=1.95, wps=16930.4, ups=1.62, wpb=10427.3, bsz=366.5, num_updates=110300, lr=6.59679e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 08:17:55 | INFO | train_inner | epoch 197:    444 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.962, ppl=1.95, wps=17188.4, ups=1.63, wpb=10558.3, bsz=365.8, num_updates=110400, lr=6.5938e-06, gnorm=0.829, train_wall=61, wall=0
2021-01-08 08:18:57 | INFO | train_inner | epoch 197:    544 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.966, ppl=1.95, wps=17053.9, ups=1.62, wpb=10530.4, bsz=363.7, num_updates=110500, lr=6.59082e-06, gnorm=0.815, train_wall=62, wall=0
2021-01-08 08:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:19:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:19:30 | INFO | valid | epoch 197 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.673 | ppl 12.75 | bleu 22.97 | wps 4184.4 | wpb 7508.5 | bsz 272.7 | num_updates 110517 | best_bleu 23.23
2021-01-08 08:19:30 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:19:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:19:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 197 @ 110517 updates, score 22.97) (writing took 2.8698386158794165 seconds)
2021-01-08 08:19:33 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2021-01-08 08:19:33 | INFO | train | epoch 197 | symm_kl 0.345 | self_kl 0 | self_cv 0 | loss 3.22 | nll_loss 0.96 | ppl 1.94 | wps 15729.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 110517 | lr 6.59031e-06 | gnorm 0.82 | train_wall 344 | wall 0
2021-01-08 08:19:33 | INFO | fairseq.trainer | begin training epoch 198
2021-01-08 08:19:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:20:26 | INFO | train_inner | epoch 198:     83 / 561 symm_kl=0.345, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.957, ppl=1.94, wps=11604.9, ups=1.12, wpb=10378.4, bsz=359.7, num_updates=110600, lr=6.58784e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 08:21:28 | INFO | train_inner | epoch 198:    183 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.964, ppl=1.95, wps=17069.8, ups=1.63, wpb=10503.3, bsz=377.1, num_updates=110700, lr=6.58486e-06, gnorm=0.828, train_wall=61, wall=0
2021-01-08 08:22:30 | INFO | train_inner | epoch 198:    283 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.959, ppl=1.94, wps=17121, ups=1.62, wpb=10571.6, bsz=368.8, num_updates=110800, lr=6.58189e-06, gnorm=0.809, train_wall=62, wall=0
2021-01-08 08:23:31 | INFO | train_inner | epoch 198:    383 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.214, nll_loss=0.955, ppl=1.94, wps=17128.7, ups=1.62, wpb=10555.2, bsz=373, num_updates=110900, lr=6.57892e-06, gnorm=0.819, train_wall=61, wall=0
2021-01-08 08:24:33 | INFO | train_inner | epoch 198:    483 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.961, ppl=1.95, wps=17033.9, ups=1.63, wpb=10480.9, bsz=366.3, num_updates=111000, lr=6.57596e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 08:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:25:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:25:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:25:44 | INFO | valid | epoch 198 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.178 | nll_loss 3.671 | ppl 12.74 | bleu 23.08 | wps 4146 | wpb 7508.5 | bsz 272.7 | num_updates 111078 | best_bleu 23.23
2021-01-08 08:25:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:25:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:25:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 198 @ 111078 updates, score 23.08) (writing took 2.8138966094702482 seconds)
2021-01-08 08:25:47 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2021-01-08 08:25:47 | INFO | train | epoch 198 | symm_kl 0.344 | self_kl 0 | self_cv 0 | loss 3.22 | nll_loss 0.959 | ppl 1.94 | wps 15735.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 111078 | lr 6.57365e-06 | gnorm 0.821 | train_wall 344 | wall 0
2021-01-08 08:25:47 | INFO | fairseq.trainer | begin training epoch 199
2021-01-08 08:25:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:25:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:26:03 | INFO | train_inner | epoch 199:     22 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.957, ppl=1.94, wps=11462.8, ups=1.11, wpb=10339.7, bsz=364.9, num_updates=111100, lr=6.573e-06, gnorm=0.821, train_wall=61, wall=0
2021-01-08 08:27:04 | INFO | train_inner | epoch 199:    122 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.224, nll_loss=0.961, ppl=1.95, wps=17212, ups=1.64, wpb=10472.8, bsz=364.6, num_updates=111200, lr=6.57004e-06, gnorm=0.822, train_wall=61, wall=0
2021-01-08 08:28:06 | INFO | train_inner | epoch 199:    222 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.974, ppl=1.96, wps=16765.5, ups=1.61, wpb=10384.7, bsz=386.1, num_updates=111300, lr=6.56709e-06, gnorm=0.81, train_wall=62, wall=0
2021-01-08 08:29:07 | INFO | train_inner | epoch 199:    322 / 561 symm_kl=0.348, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.963, ppl=1.95, wps=16990.4, ups=1.62, wpb=10467.9, bsz=366.4, num_updates=111400, lr=6.56414e-06, gnorm=0.834, train_wall=61, wall=0
2021-01-08 08:30:09 | INFO | train_inner | epoch 199:    422 / 561 symm_kl=0.339, self_kl=0, self_cv=0, loss=3.198, nll_loss=0.944, ppl=1.92, wps=17189.8, ups=1.62, wpb=10629.2, bsz=367.8, num_updates=111500, lr=6.5612e-06, gnorm=0.798, train_wall=62, wall=0
2021-01-08 08:31:11 | INFO | train_inner | epoch 199:    522 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.958, ppl=1.94, wps=17016.4, ups=1.62, wpb=10518.3, bsz=368.2, num_updates=111600, lr=6.55826e-06, gnorm=0.812, train_wall=62, wall=0
2021-01-08 08:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:31:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:31:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:31:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:31:56 | INFO | valid | epoch 199 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.179 | nll_loss 3.675 | ppl 12.77 | bleu 22.9 | wps 4492.8 | wpb 7508.5 | bsz 272.7 | num_updates 111639 | best_bleu 23.23
2021-01-08 08:31:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:31:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 199 @ 111639 updates, score 22.9) (writing took 2.831572787836194 seconds)
2021-01-08 08:31:59 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2021-01-08 08:31:59 | INFO | train | epoch 199 | symm_kl 0.344 | self_kl 0 | self_cv 0 | loss 3.22 | nll_loss 0.96 | ppl 1.94 | wps 15786.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 111639 | lr 6.55711e-06 | gnorm 0.817 | train_wall 344 | wall 0
2021-01-08 08:31:59 | INFO | fairseq.trainer | begin training epoch 200
2021-01-08 08:32:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:32:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:32:39 | INFO | train_inner | epoch 200:     61 / 561 symm_kl=0.344, self_kl=0, self_cv=0, loss=3.215, nll_loss=0.956, ppl=1.94, wps=11916.4, ups=1.14, wpb=10474.3, bsz=358.9, num_updates=111700, lr=6.55532e-06, gnorm=0.825, train_wall=60, wall=0
2021-01-08 08:33:41 | INFO | train_inner | epoch 200:    161 / 561 symm_kl=0.341, self_kl=0, self_cv=0, loss=3.209, nll_loss=0.953, ppl=1.94, wps=17058.2, ups=1.61, wpb=10581.7, bsz=397.6, num_updates=111800, lr=6.55239e-06, gnorm=0.806, train_wall=62, wall=0
2021-01-08 08:34:43 | INFO | train_inner | epoch 200:    261 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.959, ppl=1.94, wps=16937.5, ups=1.62, wpb=10477.6, bsz=365.1, num_updates=111900, lr=6.54946e-06, gnorm=0.818, train_wall=62, wall=0
2021-01-08 08:35:44 | INFO | train_inner | epoch 200:    361 / 561 symm_kl=0.343, self_kl=0, self_cv=0, loss=3.217, nll_loss=0.96, ppl=1.95, wps=16946.7, ups=1.63, wpb=10410.8, bsz=377.2, num_updates=112000, lr=6.54654e-06, gnorm=0.816, train_wall=61, wall=0
2021-01-08 08:36:46 | INFO | train_inner | epoch 200:    461 / 561 symm_kl=0.347, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.968, ppl=1.96, wps=17025.6, ups=1.63, wpb=10448.4, bsz=350.5, num_updates=112100, lr=6.54362e-06, gnorm=0.82, train_wall=61, wall=0
2021-01-08 08:37:47 | INFO | train_inner | epoch 200:    561 / 561 symm_kl=0.346, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.967, ppl=1.95, wps=16890.9, ups=1.62, wpb=10435.3, bsz=366.3, num_updates=112200, lr=6.5407e-06, gnorm=0.822, train_wall=62, wall=0
2021-01-08 08:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 08:37:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:37:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:37:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 08:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:37:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 08:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 08:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 08:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 08:38:08 | INFO | valid | epoch 200 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.182 | nll_loss 3.675 | ppl 12.78 | bleu 23.07 | wps 4727.5 | wpb 7508.5 | bsz 272.7 | num_updates 112200 | best_bleu 23.23
2021-01-08 08:38:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 08:38:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 200 @ 112200 updates, score 23.07) (writing took 2.8410770539194345 seconds)
2021-01-08 08:38:11 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2021-01-08 08:38:11 | INFO | train | epoch 200 | symm_kl 0.344 | self_kl 0 | self_cv 0 | loss 3.219 | nll_loss 0.959 | ppl 1.94 | wps 15821.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 112200 | lr 6.5407e-06 | gnorm 0.816 | train_wall 344 | wall 0
2021-01-08 08:38:11 | INFO | fairseq_cli.train | done training in 74235.3 seconds
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1194 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
